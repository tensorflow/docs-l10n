{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DADUIVuPIqYi"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Neural Structured Learning Authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Cu1zPez8Ip1S"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlIh_TPLI54s"
      },
      "source": [
        "# Regularização de grafos para classificação de documentos utilizando grafos naturais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL9fF9FWI-Q1"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/neural_structured_learning/tutorials/graph_keras_mlp_cora.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/neural_structured_learning/tutorials/graph_keras_mlp_cora.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/neural_structured_learning/tutorials/graph_keras_mlp_cora.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJlN8oxhNGto"
      },
      "source": [
        "## Visão geral\n",
        "\n",
        "A regularização de grafos é uma técnica específica sob o paradigma mais amplo de Aprendizagem de Grafos Neurais (\"Neural Graph Learning\", [Bui et al., 2018](https://research.google/pubs/pub46568.pdf)). A ideia central é treinar modelos de redes neurais com um objetivo regularizado por grafos, aproveitando dados rotulados e não rotulados.\n",
        "\n",
        "Neste tutorial, exploraremos o uso da regularização de grafos para classificar documentos que formam um grafo natural (orgânico).\n",
        "\n",
        "A receita geral para criar um modelo regularizado por grafos usando o framework Neural Structured Learning (NSL) é a seguinte:\n",
        "\n",
        "1. Gere dados de treinamento a partir do grafo de entrada e de características de amostra. Os nós do grafo correspondem a amostras e as arestas do grafo correspondem à similaridade entre pares de amostras. Os dados de treinamento resultantes conterão características vizinhas além das características originais do nó.\n",
        "2. Crie uma rede neural como modelo de referência usando a API sequencial, funcional ou de subclasse do `Keras`.\n",
        "3. Envolva o modelo de referência com a classe wrapper **`GraphRegularization`**, que é fornecida pelo framework NSL, para criar um novo modelo de grafo `Keras`. Este novo modelo incluirá uma perda de regularização do grafo como termo de regularização em seu objetivo de treinamento.\n",
        "4. Treine e avalie o modelo do grafo `Keras`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOFbB34KY1R"
      },
      "source": [
        "## Configuração\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgSLDi0SyBuO"
      },
      "source": [
        "Instale o pacote Neural Structured Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVnjPmOaQlnH"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet neural-structured-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AiNrPJaX_Lb"
      },
      "source": [
        "## Dependências e importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sEamf-wZJkX"
      },
      "outputs": [],
      "source": [
        "import neural_structured_learning as nsl\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Resets notebook state\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\n",
        "    \"GPU is\",\n",
        "    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtbcGZ_N6Ll9"
      },
      "source": [
        "## Dataset Cora\n",
        "\n",
        "O [dataset Cora](https://linqs.soe.ucsc.edu/data) é um grafo de citações onde os nós representam artigos de aprendizado de máquina e as bordas representam citações entre pares de artigos. A tarefa envolvida é a classificação de documentos onde o objetivo é categorizar cada artigo em uma das 7 categorias. Em outras palavras, este é um problema de classificação multiclasse com 7 classes.\n",
        "\n",
        "### Grafo\n",
        "\n",
        "O grafo original é direcionado. No entanto, para efeitos deste exemplo, consideraremos a versão não direcionada deste grafo. Portanto, se o artigo A cita o artigo B, também consideramos que o artigo B citou A. Embora isto não seja necessariamente verdade, neste exemplo, consideramos as citações como substituto para similaridade, que geralmente é uma propriedade comutativa.\n",
        "\n",
        "### Características\n",
        "\n",
        "Cada artigo na entrada contém efetivamente 2 características:\n",
        "\n",
        "1. **Words**: uma representação densa e multi-hot do texto no artigo. O vocabulário do dataset Cora contém 1.433 palavras únicas. Portanto, o comprimento dessa característica é 1433, e o valor na posição 'i' é 0/1, indicando se a palavra 'i' no vocabulário existe ou não no artigo em questão.\n",
        "\n",
        "2. **Label**: Um único inteiro representando o ID da classe (categoria) do artigo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2-FVpVHEyIS"
      },
      "source": [
        "### Baixe o dataset Cora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nSZjKqwE6Rn"
      },
      "outputs": [],
      "source": [
        "!wget --quiet -P /tmp https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
        "!tar -C /tmp -xvzf /tmp/cora.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylYP32_IoqZI"
      },
      "source": [
        "### Converta os dados do Cora para o formato NSL\n",
        "\n",
        "Para pré-processar o dataset Cora e convertê-lo para o formato exigido pelo Neural Structured Learning, executaremos o script **'preprocess_cora_dataset.py'**, que está incluído no repositório NSL github. Este script faz o seguinte:\n",
        "\n",
        "1. Gere características vizinhas usando as características do nó original e o grafo.\n",
        "2. Gere divisões de dados de treinamento e teste contendo instâncias `tf.train.Example`.\n",
        "3. Persista os dados de treinamento e teste resultantes no formato `TFRecord`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myz01LVZQ8Uh"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/tensorflow/neural-structured-learning/master/neural_structured_learning/examples/preprocess/cora/preprocess_cora_dataset.py\n",
        "\n",
        "!python preprocess_cora_dataset.py \\\n",
        "--input_cora_content=/tmp/cora/cora.content \\\n",
        "--input_cora_graph=/tmp/cora/cora.cites \\\n",
        "--max_nbrs=5 \\\n",
        "--output_train_data=/tmp/cora/train_merged_examples.tfr \\\n",
        "--output_test_data=/tmp/cora/test_examples.tfr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXoyHIdV5hEe"
      },
      "source": [
        "## Variáveis ​​globais\n",
        "\n",
        "Os caminhos de arquivo para os dados de treinamento e teste são baseados nos valores dos flags de linha de comando usados ​​para executar o script **'preprocess_cora_dataset.py'** acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKAmzKIH6I9f"
      },
      "outputs": [],
      "source": [
        "### Experiment dataset\n",
        "TRAIN_DATA_PATH = '/tmp/cora/train_merged_examples.tfr'\n",
        "TEST_DATA_PATH = '/tmp/cora/test_examples.tfr'\n",
        "\n",
        "### Constants used to identify neighbor features in the input.\n",
        "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
        "NBR_WEIGHT_SUFFIX = '_weight'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gYWAqJqZ76I"
      },
      "source": [
        "## Hiperparâmetros\n",
        "\n",
        "Usaremos uma instância de `HParams` para incluir vários hiperparâmetros e constantes usados ​​para treinamento e avaliação. Descrevemos brevemente cada um deles abaixo:\n",
        "\n",
        "- **num_classes**: há um total de 7 classes diferentes\n",
        "\n",
        "- **max_seq_length**: este é o tamanho do vocabulário e todas as instâncias na entrada têm uma representação bag-of-words (saco de palavras) densa e multi-hot. Em outras palavras, um valor 1 para uma palavra indica que a palavra está presente na entrada e um valor 0 indica que não está.\n",
        "\n",
        "- **distance_type**: é a métrica de distância usada para regularizar a amostra com seus vizinhos.\n",
        "\n",
        "- **graph_regularization_multiplier**: controla o peso relativo do termo de regularização do grafo na função de perda geral.\n",
        "\n",
        "- **num_neighbors**: o número de vizinhos usados ​​para regularização do grafo. Este valor deve ser menor ou igual ao argumento da linha de comando `max_nbrs` usado acima ao executar `preprocess_cora_dataset.py`.\n",
        "\n",
        "- **num_fc_units**: o número de camadas totalmente conectadas em nossa rede neural.\n",
        "\n",
        "- **train_epochs**: o número de épocas de treinamento.\n",
        "\n",
        "- **batch_size**: tamanho do lote usado para treinamento e avaliação.\n",
        "\n",
        "- **dropout_rate**: controla a taxa de dropout após cada camada totalmente conectada\n",
        "\n",
        "- **eval_steps**: o número de lotes a serem processados ​​antes que a avaliação seja concluída. Se definido como `None`, todas as instâncias no dataset de testes serão avaliadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N03EFEkcOBaJ"
      },
      "outputs": [],
      "source": [
        "class HParams(object):\n",
        "  \"\"\"Hyperparameters used for training.\"\"\"\n",
        "  def __init__(self):\n",
        "    ### dataset parameters\n",
        "    self.num_classes = 7\n",
        "    self.max_seq_length = 1433\n",
        "    ### neural graph learning parameters\n",
        "    self.distance_type = nsl.configs.DistanceType.L2\n",
        "    self.graph_regularization_multiplier = 0.1\n",
        "    self.num_neighbors = 1\n",
        "    ### model architecture\n",
        "    self.num_fc_units = [50, 50]\n",
        "    ### training parameters\n",
        "    self.train_epochs = 100\n",
        "    self.batch_size = 128\n",
        "    self.dropout_rate = 0.5\n",
        "    ### eval parameters\n",
        "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
        "\n",
        "HPARAMS = HParams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMp34x0MfMMZ"
      },
      "source": [
        "## Carregue os dados de treinamento e teste\n",
        "\n",
        "Conforme descrito anteriormente neste notebook, os dados de treinamento e teste de entrada foram criados pelo **'preprocess_cora_dataset.py'**. Iremos carregá-los em dois objetos `tf.data.Dataset` - um para treinamento e outro para teste.\n",
        "\n",
        "Na camada de entrada do nosso modelo, extrairemos não apenas as características 'words' e 'label' de cada amostra, mas também as características vizinhas correspondentes com base no valor `hparams.num_neighbors`. Instâncias com menos vizinhos que `hparams.num_neighbors` receberão valores fictícios para essas características vizinhas inexistentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCKQVKGee1ST"
      },
      "outputs": [],
      "source": [
        "def make_dataset(file_path, training=False):\n",
        "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
        "\n",
        "  Args:\n",
        "    file_path: Name of the file in the `.tfrecord` format containing\n",
        "      `tf.train.Example` objects.\n",
        "    training: Boolean indicating if we are in training mode.\n",
        "\n",
        "  Returns:\n",
        "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
        "    objects.\n",
        "  \"\"\"\n",
        "\n",
        "  def parse_example(example_proto):\n",
        "    \"\"\"Extracts relevant fields from the `example_proto`.\n",
        "\n",
        "    Args:\n",
        "      example_proto: An instance of `tf.train.Example`.\n",
        "\n",
        "    Returns:\n",
        "      A pair whose first value is a dictionary containing relevant features\n",
        "      and whose second value contains the ground truth label.\n",
        "    \"\"\"\n",
        "    # The 'words' feature is a multi-hot, bag-of-words representation of the\n",
        "    # original raw text. A default value is required for examples that don't\n",
        "    # have the feature.\n",
        "    feature_spec = {\n",
        "        'words':\n",
        "            tf.io.FixedLenFeature([HPARAMS.max_seq_length],\n",
        "                                  tf.int64,\n",
        "                                  default_value=tf.constant(\n",
        "                                      0,\n",
        "                                      dtype=tf.int64,\n",
        "                                      shape=[HPARAMS.max_seq_length])),\n",
        "        'label':\n",
        "            tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
        "    }\n",
        "    # We also extract corresponding neighbor features in a similar manner to\n",
        "    # the features above during training.\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i,\n",
        "                                         NBR_WEIGHT_SUFFIX)\n",
        "        feature_spec[nbr_feature_key] = tf.io.FixedLenFeature(\n",
        "            [HPARAMS.max_seq_length],\n",
        "            tf.int64,\n",
        "            default_value=tf.constant(\n",
        "                0, dtype=tf.int64, shape=[HPARAMS.max_seq_length]))\n",
        "\n",
        "        # We assign a default value of 0.0 for the neighbor weight so that\n",
        "        # graph regularization is done on samples based on their exact number\n",
        "        # of neighbors. In other words, non-existent neighbors are discounted.\n",
        "        feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
        "            [1], tf.float32, default_value=tf.constant([0.0]))\n",
        "\n",
        "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "\n",
        "    label = features.pop('label')\n",
        "    return features, label\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset([file_path])\n",
        "  if training:\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.map(parse_example)\n",
        "  dataset = dataset.batch(HPARAMS.batch_size)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = make_dataset(TRAIN_DATA_PATH, training=True)\n",
        "test_dataset = make_dataset(TEST_DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEWEGhtVzI2p"
      },
      "source": [
        "Vamos dar uma olhada no dataset de treinamento para ver seu conteúdo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx-YFaBoCOcl"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in train_dataset.take(1):\n",
        "  print('Feature list:', list(feature_batch.keys()))\n",
        "  print('Batch of inputs:', feature_batch['words'])\n",
        "  nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, 0, 'words')\n",
        "  nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, 0, NBR_WEIGHT_SUFFIX)\n",
        "  print('Batch of neighbor inputs:', feature_batch[nbr_feature_key])\n",
        "  print('Batch of neighbor weights:',\n",
        "        tf.reshape(feature_batch[nbr_weight_key], [-1]))\n",
        "  print('Batch of labels:', label_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7B30hRPzOBE"
      },
      "source": [
        "Vamos dar uma olhada no dataset de teste para ver seu conteúdo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNJuM9yJiFtj"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in test_dataset.take(1):\n",
        "  print('Feature list:', list(feature_batch.keys()))\n",
        "  print('Batch of inputs:', feature_batch['words'])\n",
        "  print('Batch of labels:', label_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZhsTo8yio8i"
      },
      "source": [
        "## Definição do modelo\n",
        "\n",
        "Para demonstrar o uso da regularização de grafos, construímos primeiro um modelo de referência para este problema. Usaremos uma rede neural feed-forward simples com 2 camadas ocultas e dropout entre elas. Ilustramos a criação do modelo de referência usando todos os tipos de modelo suportados pelo framework `tf.Keras`: sequencial, funcional e subclasse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_kBDDfFiuyI"
      },
      "source": [
        "### Modelo de referência sequencial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pecJmegfWijx"
      },
      "outputs": [],
      "source": [
        "def make_mlp_sequential_model(hparams):\n",
        "  \"\"\"Creates a sequential multi-layer perceptron model.\"\"\"\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(\n",
        "      tf.keras.layers.InputLayer(\n",
        "          input_shape=(hparams.max_seq_length,), name='words'))\n",
        "  # Input is already one-hot encoded in the integer format. We cast it to\n",
        "  # floating point format here.\n",
        "  model.add(\n",
        "      tf.keras.layers.Lambda(lambda x: tf.keras.backend.cast(x, tf.float32)))\n",
        "  for num_units in hparams.num_fc_units:\n",
        "    model.add(tf.keras.layers.Dense(num_units, activation='relu'))\n",
        "    # For sequential models, by default, Keras ensures that the 'dropout' layer\n",
        "    # is invoked only during training.\n",
        "    model.add(tf.keras.layers.Dropout(hparams.dropout_rate))\n",
        "  model.add(tf.keras.layers.Dense(hparams.num_classes))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfZWxqVPiz_f"
      },
      "source": [
        "### Modelo de referência funcional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU-YE4NXi7PK"
      },
      "outputs": [],
      "source": [
        "def make_mlp_functional_model(hparams):\n",
        "  \"\"\"Creates a functional API-based multi-layer perceptron model.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(hparams.max_seq_length,), dtype='int64', name='words')\n",
        "\n",
        "  # Input is already one-hot encoded in the integer format. We cast it to\n",
        "  # floating point format here.\n",
        "  cur_layer = tf.keras.layers.Lambda(\n",
        "      lambda x: tf.keras.backend.cast(x, tf.float32))(\n",
        "          inputs)\n",
        "\n",
        "  for num_units in hparams.num_fc_units:\n",
        "    cur_layer = tf.keras.layers.Dense(num_units, activation='relu')(cur_layer)\n",
        "    # For functional models, by default, Keras ensures that the 'dropout' layer\n",
        "    # is invoked only during training.\n",
        "    cur_layer = tf.keras.layers.Dropout(hparams.dropout_rate)(cur_layer)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(hparams.num_classes)(cur_layer)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs=outputs)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LmAhITRi8M0"
      },
      "source": [
        "### Modelo de referência de subclasse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l1aK9b_jAw6"
      },
      "outputs": [],
      "source": [
        "def make_mlp_subclass_model(hparams):\n",
        "  \"\"\"Creates a multi-layer perceptron subclass model in Keras.\"\"\"\n",
        "\n",
        "  class MLP(tf.keras.Model):\n",
        "    \"\"\"Subclass model defining a multi-layer perceptron.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "      super(MLP, self).__init__()\n",
        "      # Input is already one-hot encoded in the integer format. We create a\n",
        "      # layer to cast it to floating point format here.\n",
        "      self.cast_to_float_layer = tf.keras.layers.Lambda(\n",
        "          lambda x: tf.keras.backend.cast(x, tf.float32))\n",
        "      self.dense_layers = [\n",
        "          tf.keras.layers.Dense(num_units, activation='relu')\n",
        "          for num_units in hparams.num_fc_units\n",
        "      ]\n",
        "      self.dropout_layer = tf.keras.layers.Dropout(hparams.dropout_rate)\n",
        "      self.output_layer = tf.keras.layers.Dense(hparams.num_classes)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "      cur_layer = self.cast_to_float_layer(inputs['words'])\n",
        "      for dense_layer in self.dense_layers:\n",
        "        cur_layer = dense_layer(cur_layer)\n",
        "        cur_layer = self.dropout_layer(cur_layer, training=training)\n",
        "\n",
        "      outputs = self.output_layer(cur_layer)\n",
        "\n",
        "      return outputs\n",
        "\n",
        "  return MLP()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbGpIbscjIAo"
      },
      "source": [
        "## Crie modelos base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzMBxiMGjCO4"
      },
      "outputs": [],
      "source": [
        "# Create a base MLP model using the functional API.\n",
        "# Alternatively, you can also create a sequential or subclass base model using\n",
        "# the make_mlp_sequential_model() or make_mlp_subclass_model() functions\n",
        "# respectively, defined above. Note that if a subclass model is used, its\n",
        "# summary cannot be generated until it is built.\n",
        "base_model_tag, base_model = 'FUNCTIONAL', make_mlp_functional_model(HPARAMS)\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1uEboimjiW7"
      },
      "source": [
        "## Treine o modelo de referência MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JALapM4PoCvi"
      },
      "outputs": [],
      "source": [
        "# Compile and train the base MLP model\n",
        "base_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "base_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPRioqydQD_8"
      },
      "source": [
        "## Avalie o modelo de referência MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NcsJVt6FSmZ"
      },
      "outputs": [],
      "source": [
        "# Helper function to print evaluation metrics.\n",
        "def print_metrics(model_desc, eval_metrics):\n",
        "  \"\"\"Prints evaluation metrics.\n",
        "\n",
        "  Args:\n",
        "    model_desc: A description of the model.\n",
        "    eval_metrics: A dictionary mapping metric names to corresponding values. It\n",
        "      must contain the loss and accuracy metrics.\n",
        "  \"\"\"\n",
        "  print('\\n')\n",
        "  print('Eval accuracy for ', model_desc, ': ', eval_metrics['accuracy'])\n",
        "  print('Eval loss for ', model_desc, ': ', eval_metrics['loss'])\n",
        "  if 'graph_loss' in eval_metrics:\n",
        "    print('Eval graph loss for ', model_desc, ': ', eval_metrics['graph_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-myfttwIQAwc"
      },
      "outputs": [],
      "source": [
        "eval_results = dict(\n",
        "    zip(base_model.metrics_names,\n",
        "        base_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
        "print_metrics('Base MLP model', eval_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGwSzS9Spaiu"
      },
      "source": [
        "## Treine o modelo de referência MLP com regularização de grafo\n",
        "\n",
        "Incorporar a regularização de grafos no termo de perdas de um `tf.Keras.Model` existente requer apenas umas poucas linhas de código. O modelo de referência é empacotado para criar um novo modelo de subclasse `tf.Keras`, cuja perda inclui a regularização do grafo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIGN8KQH0jR"
      },
      "source": [
        "Para avaliar o benefício incremental da regularização de grafos, criaremos uma nova instância do modelo de referência. Isto é necessário porque `base_model` já foi treinado por algumas iterações, e reutilizar esse modelo treinado para criar um modelo regularizado por grafo não produziria uma comparação justa para `base_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fvLei9dLCH0"
      },
      "outputs": [],
      "source": [
        "# Build a new base MLP model.\n",
        "base_reg_model_tag, base_reg_model = 'FUNCTIONAL', make_mlp_functional_model(\n",
        "    HPARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT3mpC8Lo1UZ"
      },
      "outputs": [],
      "source": [
        "# Wrap the base MLP model with graph regularization.\n",
        "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
        "    max_neighbors=HPARAMS.num_neighbors,\n",
        "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
        "    distance_type=HPARAMS.distance_type,\n",
        "    sum_over_axis=-1)\n",
        "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
        "                                                graph_reg_config)\n",
        "graph_reg_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "graph_reg_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6409ylRVQS7q"
      },
      "source": [
        "## Avalie o modelo de referência MLP com regularização de grafo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TsOE1bAQTqD"
      },
      "outputs": [],
      "source": [
        "eval_results = dict(\n",
        "    zip(graph_reg_model.metrics_names,\n",
        "        graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
        "print_metrics('MLP + graph regularization', eval_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adc-r84EOSQi"
      },
      "source": [
        "A exatidão do modelo com regularização de grafos é cerca de 2-3% maior que a do modelo de referência (`base_model`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEXQHFNvJQJe"
      },
      "source": [
        "## Conclusão\n",
        "\n",
        "Demonstramos o uso da regularização de grafos para classificação de documentos em um grafo de citações naturais (Cora) usando o framework Neural Structured Learning (NSL). Nosso [tutorial avançado](graph_keras_lstm_imdb.ipynb) envolve a síntese de grafos com base em exemplos de embeddings antes de treinar uma rede neural com regularização de grafos. Esta abordagem é útil se a entrada não contiver um grafo explícito.\n",
        "\n",
        "Incentivamos os usuários a experimentar mais, variando a quantidade de supervisão, bem como experimentando diferentes arquiteturas neurais para a regularização de grafos."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "graph_keras_mlp_cora.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
