{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24gYiJcWNlpA"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Neural Structured Learning Authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ioaprt5q5US7"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# Regularização de grafos para classificação de sentimentos usando grafos sintetizados\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://tfhub.dev/\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelo do TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3otbdCMmJiJ"
      },
      "source": [
        "## Visão geral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "Este notebook classifica avaliações de filmes como *positivas* ou *negativas*, com base no texto da avaliação. Este é um exemplo de classificação *binária*, um tipo de problema de aprendizado de máquina importante, com diversas aplicações.\n",
        "\n",
        "Demonstraremos o uso da regularização de grafos neste notebook construindo um grafo a partir da entrada fornecida. A receita geral para construir um modelo regularizado por grafos usando o framework Neural Structured Learning (NSL) quando a entrada não contém um grafo explícito é a seguinte:\n",
        "\n",
        "1. Crie embeddings para cada amostra de texto na entrada. Isto pode ser feito usando modelos pré-treinados como [word2vec](https://arxiv.org/pdf/1310.4546.pdf), [Swivel](https://arxiv.org/abs/1602.02215), [BERT](https://arxiv.org/abs/1810.04805), etc.\n",
        "2. Construa um grafo com base nesses embeddings usando uma métrica de similaridade, como distância 'L2', distância 'cosseno', etc. Os nós no grafo correspondem a amostras e as arestas no grafo correspondem à similaridade entre pares de amostras.\n",
        "3. Gere dados de treinamento a partir do grafo sintetizado acima e de características de amostra. Os dados de treinamento resultantes conterão características vizinhas além das características originais do nó.\n",
        "4. Crie uma rede neural como modelo de referência usando a API sequencial, funcional ou de subclasse do <code>Keras</code>.\n",
        "5. Envolva o modelo base com a classe wrapper <strong><code>GraphRegularization</code></strong>, que é fornecida pelo framework NSL, para criar um novo modelo de grafo <code>Keras</code>. Este novo modelo incluirá uma perda de regularização do grafo como termo de regularização em seu objetivo de treinamento.\n",
        "6. Treine e avalie o modelo do grafo <code>Keras</code>.\n",
        "\n",
        "**Observação**: Esperamos que os leitores levem cerca de 1 hora para concluir este tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOFbB34KY1R"
      },
      "source": [
        "## Requisitos\n",
        "\n",
        "1. Instale o pacote Neural Structured Learning.\n",
        "2. Instale o tensorflow-hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVnjPmOaQlnH"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet neural-structured-learning\n",
        "!pip install --quiet tensorflow-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6FJ64qMNLez"
      },
      "source": [
        "## Dependências e importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ew7HTbPpCJH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import neural_structured_learning as nsl\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Resets notebook state\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\n",
        "    \"GPU is\",\n",
        "    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGwwFd99n42P"
      },
      "source": [
        "## Dataset IMDB\n",
        "\n",
        "O [dataset do IMDB](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) contém o texto de 50.000 avaliações de filmes do [Internet Movie Database](https://www.imdb.com/) . Elas são divididas em 25.000 avaliações para treinamento e 25.000 avaliações para testes. Os datasets de treinamento e teste são *balanceados*, o que significa que contêm um número igual de avaliações positivas e negativas.\n",
        "\n",
        "Neste tutorial, usaremos uma versão pré-processada do dataset IMDB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "source": [
        "### Baixe o dataset do IMDB pré-processado\n",
        "\n",
        "O dataset IMDB vem com o TensorFlow. Já foi pré-processado de forma que as avaliações (sequências de palavras) foram convertidas em sequências de inteiros, onde cada inteiro representa uma palavra específica num dicionário.\n",
        "\n",
        "O código a seguir baixa o dataset IMDB (ou usa uma cópia em cache se já tiver sido baixado):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXXx5Oc3pOmN"
      },
      "outputs": [],
      "source": [
        "imdb = tf.keras.datasets.imdb\n",
        "(pp_train_data, pp_train_labels), (pp_test_data, pp_test_labels) = (\n",
        "    imdb.load_data(num_words=10000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odr-KlzO-lkL"
      },
      "source": [
        "O argumento `num_words=10000` mantém as 10.000 palavras que ocorrem com mais frequência nos dados de treinamento. As palavras raras são descartadas para manter o tamanho do vocabulário controlável."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l50X3GfjpU4r"
      },
      "source": [
        "### Explore os dados\n",
        "\n",
        "Vamos dedicar um momento para entender o formato dos dados. O dataset vem pré-processado: cada exemplo é um array de números inteiros que representa as palavras da crítica do filme. Cada rótulo é um valor inteiro de 0 ou 1, onde 0 é uma avaliação negativa e 1 é uma avaliação positiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8qCnve_-lkO"
      },
      "outputs": [],
      "source": [
        "print('Training entries: {}, labels: {}'.format(\n",
        "    len(pp_train_data), len(pp_train_labels)))\n",
        "training_samples_count = len(pp_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnKvHWW4-lkW"
      },
      "source": [
        "O texto das avaliações foi convertido para números inteiros, onde cada número inteiro representa uma palavra específica num dicionário. Eis como se parece a primeira avaliação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtTS4kpEpjbi"
      },
      "outputs": [],
      "source": [
        "print(pp_train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIE4l_72x7DP"
      },
      "source": [
        "As avaliações de filmes podem ter comprimentos diferentes. O código abaixo mostra o número de palavras na primeira e na segunda avaliações. Como as entradas de uma rede neural devem ter o mesmo comprimento, vamos ter que resolver isso mais tarde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-6Ii9Pfx6Nr"
      },
      "outputs": [],
      "source": [
        "len(pp_train_data[0]), len(pp_train_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wJg2FiYpuoX"
      },
      "source": [
        "### Converta os inteiros de volta para palavras\n",
        "\n",
        "Pode ser útil saber como converter números inteiros de volta ao texto correspondente. Aqui, criaremos uma função helper para pesquisar um objeto de dicionário que contém o mapeamento de número inteiro para string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr5s_1alpzop"
      },
      "outputs": [],
      "source": [
        "def build_reverse_word_index():\n",
        "  # A dictionary mapping words to an integer index\n",
        "  word_index = imdb.get_word_index()\n",
        "\n",
        "  # The first indices are reserved\n",
        "  word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "  word_index['<PAD>'] = 0\n",
        "  word_index['<START>'] = 1\n",
        "  word_index['<UNK>'] = 2  # unknown\n",
        "  word_index['<UNUSED>'] = 3\n",
        "  return dict((value, key) for (key, value) in word_index.items())\n",
        "\n",
        "reverse_word_index = build_reverse_word_index()\n",
        "\n",
        "def decode_review(text):\n",
        "  return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3CNRvEZVppl"
      },
      "source": [
        "Agora podemos usar a função `decode_review` para exibir o texto da primeira avaliação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_OqxmH6-lkn"
      },
      "outputs": [],
      "source": [
        "decode_review(pp_train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVmqL-zcWm8v"
      },
      "source": [
        "## Construção do grafo\n",
        "\n",
        "A construção de grafos envolve a criação de embeddings para amostras de texto e, em seguida, o uso de uma função de similaridade para comparar os embeddings.\n",
        "\n",
        "Antes de prosseguir, primeiro criamos um diretório para armazenar os artefatos criados por este tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZicFxFOeL2J"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /tmp/imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUyHEa-3TB2X"
      },
      "source": [
        "### Crie embeddings de amostra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCe9vOy7-Br9"
      },
      "source": [
        "Usaremos embeddings Swivel pré-treinados para criar embeddings no formato `tf.train.Example` para cada amostra na entrada. Armazenaremos os embeddings resultantes no formato `TFRecord` junto com uma característica adicional que representa o ID de cada amostra. Isto é importante e nos permitirá posteriormente combinar os embeddings de amostra com os nós correspondentes no grafo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq2Ohd9CuZv_"
      },
      "outputs": [],
      "source": [
        "pretrained_embedding = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\n",
        "\n",
        "hub_layer = hub.KerasLayer(\n",
        "    pretrained_embedding, input_shape=[], dtype=tf.string, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXJ3RaboTSKQ"
      },
      "outputs": [],
      "source": [
        "def _int64_feature(value):\n",
        "  \"\"\"Returns int64 tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value.tolist()))\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns bytes tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(\n",
        "      bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
        "\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns float tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=value.tolist()))\n",
        "\n",
        "\n",
        "def create_embedding_example(word_vector, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's embedding and its ID.\"\"\"\n",
        "\n",
        "  text = decode_review(word_vector)\n",
        "\n",
        "  # Shape = [batch_size,].\n",
        "  sentence_embedding = hub_layer(tf.reshape(text, shape=[-1,]))\n",
        "\n",
        "  # Flatten the sentence embedding back to 1-D.\n",
        "  sentence_embedding = tf.reshape(sentence_embedding, shape=[-1])\n",
        "\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'embedding': _float_feature(sentence_embedding.numpy())\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "\n",
        "def create_embeddings(word_vectors, output_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(output_path) as writer:\n",
        "    for word_vector in word_vectors:\n",
        "      example = create_embedding_example(word_vector, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "\n",
        "# Persist TF.Example features containing embeddings for training data in\n",
        "# TFRecord format.\n",
        "create_embeddings(pp_train_data, '/tmp/imdb/embeddings.tfr', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8s06RuI_vKs"
      },
      "source": [
        "### Construa um grafo\n",
        "\n",
        "Agora que temos os embeddings das amostras, podemos utilizá-los para construir um grafo de similaridade, ou seja, os nós neste grafo corresponderão a amostras e as arestas do grafo corresponderão à similaridade entre pares de nós.\n",
        "\n",
        "O Neural Structured Learning fornece uma biblioteca de construção de grafos para construir um grafo com base em embeddings de amostras. Ele usa <strong>similaridade de cosseno</strong> como medida de similaridade para comparar embeddings e construir arestas entre eles. Também nos permite especificar um limiar de similaridade, que pode ser usado para descartar arestas diferentes do grafo final. Neste exemplo, usando 0,99 como limiar de similaridade, obtemos um grafo que possui 429.415 arestas bidirecionais. Aqui estamos usando o suporte do construtor de grafos para [hashing sensível à localidade](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) (LSH) para acelerar a construção de grafos. Para detalhes sobre como usar o suporte LSH do construtor de grafos, consulte a documentação da API [`build_graph_from_config`](https://www.tensorflow.org/neural_structured_learning/api_docs/python/nsl/tools/build_graph_from_config)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY6lqhNkBh2Q"
      },
      "outputs": [],
      "source": [
        "graph_builder_config = nsl.configs.GraphBuilderConfig(\n",
        "    similarity_threshold=0.99, lsh_splits=32, lsh_rounds=15, random_seed=12345)\n",
        "nsl.tools.build_graph_from_config(['/tmp/imdb/embeddings.tfr'],\n",
        "                                  '/tmp/imdb/graph_99.tsv',\n",
        "                                  graph_builder_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dk9xfQcK553"
      },
      "source": [
        "Cada aresta bidirecional é representada por duas arestas direcionadas no arquivo TSV de saída, de modo que o arquivo contém 429.415 * 2 = 858.830 linhas no total:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDPwTpZcJ3zF"
      },
      "outputs": [],
      "source": [
        "!wc -l /tmp/imdb/graph_99.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06QrEVCIlTvV"
      },
      "source": [
        "**Observação:** A qualidade do grafo e, por extensão, a qualidade dos embeddings, são muito importantes para a regularização do grafo. Embora usemos embeddings Swivel neste notebook, usar embeddings BERT, por exemplo, provavelmente capturará a semântica da avaliação com mais exatidão. Incentivamos os usuários a usarem os embeddings de sua escolha e conforme seja apropriado às suas necessidades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USkfut69gNW"
      },
      "source": [
        "## Características de amostra\n",
        "\n",
        "Criamos características de amostra para nosso problema usando o formato `tf.train.Example` e os persistimos no formato `TFRecord`. Cada amostra incluirá as três características a seguir:\n",
        "\n",
        "1. **id**: o ID do nó da amostra.\n",
        "2. **words**: uma lista int64 contendo IDs de palavras.\n",
        "3. **label**: um singleton int64 que identifica a classe alvo da avaliação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PcUF4_B9grB"
      },
      "outputs": [],
      "source": [
        "def create_example(word_vector, label, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's word vector, label, and ID.\"\"\"\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'words': _int64_feature(np.asarray(word_vector)),\n",
        "      'label': _int64_feature(np.asarray([label])),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "def create_records(word_vectors, labels, record_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(record_path) as writer:\n",
        "    for word_vector, label in zip(word_vectors, labels):\n",
        "      example = create_example(word_vector, label, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "# Persist TF.Example features (word vectors and labels) for training and test\n",
        "# data in TFRecord format.\n",
        "next_record_id = create_records(pp_train_data, pp_train_labels,\n",
        "                                '/tmp/imdb/train_data.tfr', 0)\n",
        "create_records(pp_test_data, pp_test_labels, '/tmp/imdb/test_data.tfr',\n",
        "               next_record_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhFO9sZ8Aa_g"
      },
      "source": [
        "## Aumente os dados de treinamento com vizinhos de grafo\n",
        "\n",
        "Como temos as características de amostra e o grafo sintetizado, podemos gerar os dados de treinamento aumentados para o Neural Structured Learning. O framework NSL fornece uma biblioteca para combinar o grafo e as características de amostra para produzir os dados de treinamento finais para regularização do grafo. Os dados de treinamento resultantes incluirão características de amostra originais, bem como características de seus vizinhos correspondentes.\n",
        "\n",
        "Neste tutorial, consideramos arestas não direcionadas e usamos no máximo 3 vizinhos por amostra para aumentar os dados de treinamento com vizinhos do grafo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSCHj4rIBj_A"
      },
      "outputs": [],
      "source": [
        "nsl.tools.pack_nbrs(\n",
        "    '/tmp/imdb/train_data.tfr',\n",
        "    '',\n",
        "    '/tmp/imdb/graph_99.tsv',\n",
        "    '/tmp/imdb/nsl_train_data.tfr',\n",
        "    add_undirected_edges=True,\n",
        "    max_nbrs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzBWdWkBqlMy"
      },
      "source": [
        "## Modelo de referência\n",
        "\n",
        "Agora estamos prontos para construir um modelo base sem regularização de grafos. Para construir este modelo, podemos usar embeddings que foram usados ​​na construção do grafo, ou podemos aprender novos embeddings juntamente com a tarefa de classificação. Para os fins deste notebook, faremos o último."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSbRFguBUNl"
      },
      "source": [
        "### Variáveis ​​globais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsA8HuvvwGri"
      },
      "outputs": [],
      "source": [
        "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
        "NBR_WEIGHT_SUFFIX = '_weight'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8gMVBw6t6CI"
      },
      "source": [
        "### Hiperparâmetros\n",
        "\n",
        "Usaremos uma instância de `HParams` para incluir vários hiperparâmetros e constantes usados ​​para treinamento e avaliação. Descrevemos brevemente cada um deles abaixo:\n",
        "\n",
        "- **num_classes**: há 2 classes – *positive* e *negative*.\n",
        "\n",
        "- **max_seq_length**: é o número máximo de palavras consideradas em cada avaliação de filme neste exemplo.\n",
        "\n",
        "- **vocab_size**: é o tamanho do vocabulário considerado neste exemplo.\n",
        "\n",
        "- **distance_type**: é a métrica de distância usada para regularizar a amostra com seus vizinhos.\n",
        "\n",
        "- **graph_regularization_multiplier**: controla o peso relativo do termo de regularização do grafo na função de perda geral.\n",
        "\n",
        "- **num_neighbors**: o número de vizinhos usados ​​para regularização do grafo. Este valor deve ser menor ou igual ao argumento `max_nbrs` usado acima ao invocar `nsl.tools.pack_nbrs`.\n",
        "\n",
        "- **num_fc_units**: o número de unidades na camada totalmente conectada da rede neural.\n",
        "\n",
        "- **train_epochs**: o número de épocas de treinamento.\n",
        "\n",
        "- **batch_size**: tamanho do lote usado para treinamento e avaliação.\n",
        "\n",
        "- **eval_steps**: o número de lotes a serem processados ​​antes que a avaliação seja concluída. Se definido como `None`, todas as instâncias no dataset de testes serão avaliadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlTmug7auQ2r"
      },
      "outputs": [],
      "source": [
        "class HParams(object):\n",
        "  \"\"\"Hyperparameters used for training.\"\"\"\n",
        "  def __init__(self):\n",
        "    ### dataset parameters\n",
        "    self.num_classes = 2\n",
        "    self.max_seq_length = 256\n",
        "    self.vocab_size = 10000\n",
        "    ### neural graph learning parameters\n",
        "    self.distance_type = nsl.configs.DistanceType.L2\n",
        "    self.graph_regularization_multiplier = 0.1\n",
        "    self.num_neighbors = 2\n",
        "    ### model architecture\n",
        "    self.num_embedding_dims = 16\n",
        "    self.num_lstm_dims = 64\n",
        "    self.num_fc_units = 64\n",
        "    ### training parameters\n",
        "    self.train_epochs = 10\n",
        "    self.batch_size = 128\n",
        "    ### eval parameters\n",
        "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
        "\n",
        "HPARAMS = HParams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFP_XKVRp4_S"
      },
      "source": [
        "### Preparação dos dados\n",
        "\n",
        "As avaliações – os arrays de inteiros – devem ser convertidas em tensores antes de serem alimentadas na rede neural. Essa conversão pode ser feita de duas maneiras:\n",
        "\n",
        "- Converta os arrays em vetores de valores `0` e `1` indicando a ocorrência de palavras, semelhante a uma one-hot encoding. Por exemplo, a sequência `[3, 5]` se tornaria um vetor de `10000` dimensões que é todo zero, exceto os índices `3` e `5`, que serão 1. Em seguida, torne esta a primeira camada em nossa rede – uma camada `Dense` – que poderá lidar com dados vetoriais em ponto flutuante. Essa abordagem consome muita memória, exigindo uma matriz de tamanho `num_words * num_reviews`.\n",
        "\n",
        "- Como alternativa, podemos preencher os arrays para que todos tenham o mesmo comprimento e, em seguida, criar um tensor inteiro de forma `max_length * num_reviews`. Podemos usar uma camada de embedding capaz de lidar com esse formato como a primeira camada da nossa rede.\n",
        "\n",
        "Neste tutorial, usaremos a segunda abordagem.\n",
        "\n",
        "Como as avaliações de filmes devem ter a mesma duração, usaremos a função `pad_sequence` definida abaixo para padronizar as durações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5lkZVynuHWs"
      },
      "outputs": [],
      "source": [
        "def make_dataset(file_path, training=False):\n",
        "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
        "\n",
        "  Args:\n",
        "    file_path: Name of the file in the `.tfrecord` format containing\n",
        "      `tf.train.Example` objects.\n",
        "    training: Boolean indicating if we are in training mode.\n",
        "\n",
        "  Returns:\n",
        "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
        "    objects.\n",
        "  \"\"\"\n",
        "\n",
        "  def pad_sequence(sequence, max_seq_length):\n",
        "    \"\"\"Pads the input sequence (a `tf.SparseTensor`) to `max_seq_length`.\"\"\"\n",
        "    pad_size = tf.maximum([0], max_seq_length - tf.shape(sequence)[0])\n",
        "    padded = tf.concat(\n",
        "        [sequence.values,\n",
        "         tf.fill((pad_size), tf.cast(0, sequence.dtype))],\n",
        "        axis=0)\n",
        "    # The input sequence may be larger than max_seq_length. Truncate down if\n",
        "    # necessary.\n",
        "    return tf.slice(padded, [0], [max_seq_length])\n",
        "\n",
        "  def parse_example(example_proto):\n",
        "    \"\"\"Extracts relevant fields from the `example_proto`.\n",
        "\n",
        "    Args:\n",
        "      example_proto: An instance of `tf.train.Example`.\n",
        "\n",
        "    Returns:\n",
        "      A pair whose first value is a dictionary containing relevant features\n",
        "      and whose second value contains the ground truth labels.\n",
        "    \"\"\"\n",
        "    # The 'words' feature is a variable length word ID vector.\n",
        "    feature_spec = {\n",
        "        'words': tf.io.VarLenFeature(tf.int64),\n",
        "        'label': tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
        "    }\n",
        "    # We also extract corresponding neighbor features in a similar manner to\n",
        "    # the features above during training.\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i,\n",
        "                                         NBR_WEIGHT_SUFFIX)\n",
        "        feature_spec[nbr_feature_key] = tf.io.VarLenFeature(tf.int64)\n",
        "\n",
        "        # We assign a default value of 0.0 for the neighbor weight so that\n",
        "        # graph regularization is done on samples based on their exact number\n",
        "        # of neighbors. In other words, non-existent neighbors are discounted.\n",
        "        feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
        "            [1], tf.float32, default_value=tf.constant([0.0]))\n",
        "\n",
        "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "\n",
        "    # Since the 'words' feature is a variable length word vector, we pad it to a\n",
        "    # constant maximum length based on HPARAMS.max_seq_length\n",
        "    features['words'] = pad_sequence(features['words'], HPARAMS.max_seq_length)\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        features[nbr_feature_key] = pad_sequence(features[nbr_feature_key],\n",
        "                                                 HPARAMS.max_seq_length)\n",
        "\n",
        "    labels = features.pop('label')\n",
        "    return features, labels\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset([file_path])\n",
        "  if training:\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.map(parse_example)\n",
        "  dataset = dataset.batch(HPARAMS.batch_size)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = make_dataset('/tmp/imdb/nsl_train_data.tfr', True)\n",
        "test_dataset = make_dataset('/tmp/imdb/test_data.tfr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "### Criação do modelo\n",
        "\n",
        "Uma rede neural é criada empilhando-se camadas. Isto requer duas decisões de arquitetura principais:\n",
        "\n",
        "- Quantas camadas usar no modelo?\n",
        "- Quantas *unidades ocultas* usar em cada camada?\n",
        "\n",
        "Neste exemplo, os dados de entrada consistem num array de índices de palavras. Os rótulos a serem previstos são 0 ou 1.\n",
        "\n",
        "Usaremos um LSTM bidirecional como nosso modelo de referência neste tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpKOoWgu-llD"
      },
      "outputs": [],
      "source": [
        "# This function exists as an alternative to the bi-LSTM model used in this\n",
        "# notebook.\n",
        "def make_feed_forward_model():\n",
        "  \"\"\"Builds a simple 2 layer feed forward neural network.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "  embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size, 16)(inputs)\n",
        "  pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
        "  dense_layer = tf.keras.layers.Dense(16, activation='relu')(pooling_layer)\n",
        "  outputs = tf.keras.layers.Dense(1)(dense_layer)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "def make_bilstm_model():\n",
        "  \"\"\"Builds a bi-directional LSTM model.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "  embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size,\n",
        "                                              HPARAMS.num_embedding_dims)(\n",
        "                                                  inputs)\n",
        "  lstm_layer = tf.keras.layers.Bidirectional(\n",
        "      tf.keras.layers.LSTM(HPARAMS.num_lstm_dims))(\n",
        "          embedding_layer)\n",
        "  dense_layer = tf.keras.layers.Dense(\n",
        "      HPARAMS.num_fc_units, activation='relu')(\n",
        "          lstm_layer)\n",
        "  outputs = tf.keras.layers.Dense(1)(dense_layer)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "# Feel free to use an architecture of your choice.\n",
        "model = make_bilstm_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "As camadas são empilhadas sequencialmente para construir o classificador:\n",
        "\n",
        "1. A primeira camada é uma camada `Input` que utiliza o vocabulário codificado por inteiros.\n",
        "2. A camada seguinte é uma camada `Embedding`, que pega o vocabulário codificado por inteiros e procura o vetor de incorporação para cada índice de palavras. Esses vetores adicionam uma dimensão ao array de saída. As dimensões resultantes são: `(batch, sequence, embedding)`.\n",
        "3. Em seguida, uma camada LSTM bidirecional retorna um vetor de saída de comprimento fixo para cada exemplo.\n",
        "4. O vetor de saída com tamanho fixo é passado através de uma camada (`Dense`) totalmente conectada com 64 unidades ocultas.\n",
        "5. A última camada está densamente conectada a um único nó de saída. Usando a função de ativação `sigmoid`, esse valor é um valor flutuante entre 0 e 1, representando uma probabilidade ou nível de confiança."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XMwnDOp-llH"
      },
      "source": [
        "### Unidades ocultas\n",
        "\n",
        "O modelo acima possui duas camadas intermediárias ou \"ocultas\", entre a entrada e a saída, e excluindo a camada `Embedding`. O número de saídas (unidades, nós ou neurônios) é a dimensão do espaço representacional da camada. Em outras palavras, a quantidade de liberdade permitida à rede ao aprender uma representação interna.\n",
        "\n",
        "Se um modelo tiver mais unidades ocultas (um espaço de representação de dimensão superior) e/ou mais camadas, a rede poderá aprender representações mais complexas. No entanto, isso deixa a rede mais cara do ponto de vista computacional e pode levar ao aprendizado de padrões indesejados – padrões que melhoram o desempenho nos dados de treinamento, mas não nos dados de teste. Isto é chamado de *overfitting*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### Função de perda e otimizador\n",
        "\n",
        "Um modelo precisa de uma função de perda e um otimizador para o treinamento. Como este é um problema de classificação binária e o modelo gera como saída uma probabilidade (uma camada de unidade única com uma ativação sigmóide), usaremos a função de perda `binary_crossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCWYwkug-llQ"
      },
      "source": [
        "### Crie um conjunto de validação\n",
        "\n",
        "Durante o treinamento, queremos verificar a precisão do modelo em dados que ele não viu antes. Crie um *dataset de validação* separando uma fração dos dados de treinamento originais. (Por que não usar o dataset de testes agora? Nosso objetivo é desenvolver e ajustar nosso modelo usando apenas os dados de treinamento e, em seguida, usar os dados de teste apenas uma vez para avaliar nossa exatidão).\n",
        "\n",
        "Neste tutorial, consideramos aproximadamente 10% das amostras de treinamento inicial (10% de 25.000) como dados rotulados para treinamento e o restante como dados de validação. Como a divisão inicial de treinamento/teste foi 50/50 (25.000 amostras cada), a divisão efetiva de treinamento/validação/teste que temos agora é 5/45/50.\n",
        "\n",
        "Observe que 'train_dataset' já foi agrupado e embaralhado. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYTf7zkZQ-Dl"
      },
      "outputs": [],
      "source": [
        "validation_fraction = 0.9\n",
        "validation_size = int(validation_fraction *\n",
        "                      int(training_samples_count / HPARAMS.batch_size))\n",
        "print(validation_size)\n",
        "validation_dataset = train_dataset.take(validation_size)\n",
        "train_dataset = train_dataset.skip(validation_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "### Treine o modelo\n",
        "\n",
        "Treine o modelo em minilotes. Durante o treinamento, monitore a perda e a precisão do modelo no dataset de validação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLWzgfF1xpDu"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "### Avalie o modelo\n",
        "\n",
        "Vamos conferir o desempenho do modelo. Serão retornados dois valores: perda (um número que representa o erro; quanto menor, melhor) e exatidão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q7CoDfoCJ5h"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KggXVeL-llZ"
      },
      "source": [
        "### Crie um grafo de precisão/perda ao longo do tempo\n",
        "\n",
        "`model.fit()` retorna um objeto `History` que contém um dicionário com tudo o que aconteceu durante o treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcvSXvhp-llb"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRKsqL40-lle"
      },
      "source": [
        "Há quatro entradas: uma para cada métrica monitorada durante o treinamento e a validação. Você usará esses valores para plotar a perda do treinamento e da validação para fins comparativos, além da exatidão do treinamento e da validação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGoYf2Js-lle"
      },
      "outputs": [],
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hXx-xOv-llh"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEmZ5zq-llk"
      },
      "source": [
        "Observe que a perda do treinamento *diminui* a cada época, e a exatidão do treinamento *aumenta* a cada época. Isso é o esperado ao usar uma otimização do método do gradiente descendente, que deve minimizar a quantidade desejada em cada iteração."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SymtYWWiMUum"
      },
      "source": [
        "## Regularização de grafos\n",
        "\n",
        "Agora estamos prontos para tentar a regularização de grafos usando o modelo base que construímos acima. Usaremos a classe wrapper `GraphRegularization` fornecida pela framework Neural Structured Learning para encapsular o modelo de referência (bi-LSTM) para incluir a regularização do grafo. As demais etapas para treinar e avaliar o modelo regularizado por grafos são semelhantes às do modelo de referência."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCIkVe_QFX38"
      },
      "source": [
        "### Crie um modelo regularizado por grafo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIGN8KQH0jR"
      },
      "source": [
        "Para avaliar o benefício incremental da regularização de grafos, criaremos uma nova instância do modelo de referência. Isto é necessário porque `model` já foi treinado por algumas iterações, e reutilizar esse modelo treinado para criar um modelo regularizado por grafo não produziria uma comparação justa para `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOEElnbtPzSr"
      },
      "outputs": [],
      "source": [
        "# Build a new base LSTM model.\n",
        "base_reg_model = make_bilstm_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGaDeyjEOMLC"
      },
      "outputs": [],
      "source": [
        "# Wrap the base model with graph regularization.\n",
        "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
        "    max_neighbors=HPARAMS.num_neighbors,\n",
        "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
        "    distance_type=HPARAMS.distance_type,\n",
        "    sum_over_axis=-1)\n",
        "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
        "                                                graph_reg_config)\n",
        "graph_reg_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSZSqJOKFdgX"
      },
      "source": [
        "### Treine o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aONZhwc9FWoo"
      },
      "outputs": [],
      "source": [
        "graph_reg_history = graph_reg_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD1oHiGHFjPB"
      },
      "source": [
        "### Avalie o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdFMEfe2e5JY"
      },
      "outputs": [],
      "source": [
        "graph_reg_results = graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(graph_reg_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BshURAbF49R"
      },
      "source": [
        "### Crie um grafo de precisão/perda ao longo do tempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHxshrYLah9v"
      },
      "outputs": [],
      "source": [
        "graph_reg_history_dict = graph_reg_history.history\n",
        "graph_reg_history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBrp0Y0jHu5k"
      },
      "source": [
        "Há cinco entradas no total no dicionário: perda de treinamento, precisão de treinamento, perda de grafo de treinamento, perda de validação e exatidão de validação. Podemos representá-los todos juntos para comparação. Observe que a perda do grafo só é calculada durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhjhH4n_aprb"
      },
      "outputs": [],
      "source": [
        "acc = graph_reg_history_dict['accuracy']\n",
        "val_acc = graph_reg_history_dict['val_accuracy']\n",
        "loss = graph_reg_history_dict['loss']\n",
        "graph_loss = graph_reg_history_dict['scaled_graph_loss']\n",
        "val_loss = graph_reg_history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.clf()   # clear figure\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-gD\" is for solid green line with diamond markers.\n",
        "plt.plot(epochs, graph_loss, '-gD', label='Training graph loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE0vcDiqa1Id"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su1TOgT3mgrk"
      },
      "source": [
        "## O poder da aprendizagem semissupervisionada\n",
        "\n",
        "O aprendizado semissupervisionado e, mais especificamente, a regularização de grafos no contexto deste tutorial, pode ser muito poderoso quando a quantidade de dados de treinamento é pequena. A falta de dados de treinamento é compensada pelo aproveitamento da similaridade entre as amostras de treinamento, o que não é possível no aprendizado supervisionado tradicional.\n",
        "\n",
        "Definimos a ***taxa de supervisão*** como a proporção entre amostras de treinamento e o número total de amostras que inclui amostras de treinamento, validação e teste. Neste notebook, usamos uma taxa de supervisão de 0,05 (ou seja, 5% dos dados rotulados) para treinar tanto o modelo de referência quanto o modelo regularizado por grafo. Ilustramos o impacto da taxa de supervisão na exatidão do modelo na célula abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWWa384R5vSm"
      },
      "outputs": [],
      "source": [
        "# Accuracy values for both the Bi-LSTM model and the feed forward NN model have\n",
        "# been precomputed for the following supervision ratios.\n",
        "\n",
        "supervision_ratios = [0.3, 0.15, 0.05, 0.03, 0.02, 0.01, 0.005]\n",
        "\n",
        "model_tags = ['Bi-LSTM model', 'Feed Forward NN model']\n",
        "base_model_accs = [[84, 84, 83, 80, 65, 52, 50], [87, 86, 76, 74, 67, 52, 51]]\n",
        "graph_reg_model_accs = [[84, 84, 83, 83, 65, 63, 50],\n",
        "                        [87, 86, 80, 75, 67, 52, 50]]\n",
        "\n",
        "plt.clf()  # clear figure\n",
        "\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "fig.set_size_inches((12, 5))\n",
        "\n",
        "for ax, model_tag, base_model_acc, graph_reg_model_acc in zip(\n",
        "    axes, model_tags, base_model_accs, graph_reg_model_accs):\n",
        "\n",
        "  # \"-r^\" is for solid red line with triangle markers.\n",
        "  ax.plot(base_model_acc, '-r^', label='Base model')\n",
        "  # \"-gD\" is for solid green line with diamond markers.\n",
        "  ax.plot(graph_reg_model_acc, '-gD', label='Graph-regularized model')\n",
        "  ax.set_title(model_tag)\n",
        "  ax.set_xlabel('Supervision ratio')\n",
        "  ax.set_ylabel('Accuracy(%)')\n",
        "  ax.set_ylim((25, 100))\n",
        "  ax.set_xticks(range(len(supervision_ratios)))\n",
        "  ax.set_xticklabels(supervision_ratios)\n",
        "  ax.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tukoIryKugX_"
      },
      "source": [
        "Pode-se observar que à medida que o índice de supervisão diminui, a exatidão do modelo também diminui. Isso é verdade tanto para o modelo de referência quanto para o modelo regularizado por grafos, independentemente da arquitetura do modelo utilizada. No entanto, observe que o modelo regularizado por grafos tem desempenho melhor que o modelo de referência para ambas as arquiteturas. Em particular, para o modelo Bi-LSTM, quando o índice de supervisão é 0,01, a exatidão do modelo regularizado por grafos é **~20%** maior que a do modelo de referência. Isso se deve principalmente ao aprendizado semissupervisionado para o modelo regularizado por grafos, onde a similaridade estrutural entre as amostras de treinamento é usada além das próprias amostras de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4zCEyPhIp-"
      },
      "source": [
        "## Conclusão\n",
        "\n",
        "Demonstramos o uso da regularização de grafos usando o framework Neural Structured Learning (NSL) mesmo quando a entrada não contém um grafo explícito. Consideramos a tarefa de classificação de sentimentos de avaliações de filmes do IMDB, para a qual sintetizamos um grafo de similaridade baseado em embeddings de avaliações. Encorajamos os usuários a experimentar mais variando hiperparâmetros, alterando a quantidade de supervisão e usando diferentes arquiteturas de modelo."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "24gYiJcWNlpA"
      ],
      "name": "graph_keras_lstm_imdb.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
