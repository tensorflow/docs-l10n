{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EheA5_j_cEwc"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Probability Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YCriMWd-pRTP"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvRwFTkqcp1e"
      },
      "source": [
        "# Otimizadores no TensorFlow Probability\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/probability/examples/Optimizers_in_TensorFlow_Probability\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/probability/examples/Optimizers_in_TensorFlow_Probability.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/probability/examples/Optimizers_in_TensorFlow_Probability.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/probability/examples/Optimizers_in_TensorFlow_Probability.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiBI9YkYoVBO"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Neste Colab, vamos demonstrar como usar os diversos otimizadores implementados no TensorFlow Probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWQZUqnMf-3A"
      },
      "source": [
        "## Dependências e pré-requisitos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nA2FSdTgcEM"
      },
      "outputs": [],
      "source": [
        "#@title Import { display-mode: \"form\" }\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import contextlib\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from six.moves import urllib\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ec_JW02g-Q"
      },
      "source": [
        "## Otimizadores BFGS e L-BFGS\n",
        "\n",
        "Métodos quase-Newton são uma classe popular de algoritmos de otimização de primeira ordem. Esses métodos usam uma aproximação definida positiva para a hessiana exata com o objetivo de encontrar a direção de pesquisa.\n",
        "\n",
        "O algoritmo Broyden-Fletcher-Goldfarb-Shanno ([BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)) é uma implementação específica dessa ideia geral. É aplicável a problemas de tamanho médio (e o método preferencial para isso), em que o gradiente é contínuo em todo lugar (por exemplo, regressão linear com penalidade $L_2$).\n",
        "\n",
        "[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) é uma versão do BFGS com memória limitada para resolver problemas maiores cujas matrizes hessianas não podem ser computadas a um custo razoável ou não são esparsas. Em vez de armazenar aproximações $n \\times n$ totalmente densas de matrizes hessianas, são salvos somente alguns vetores de tamanho $n$ que representam essas aproximações implicitamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Tm6BS93hQ9Ym"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions\n",
        "\n",
        "CACHE_DIR = os.path.join(os.sep, 'tmp', 'datasets')\n",
        "\n",
        "\n",
        "def make_val_and_grad_fn(value_fn):\n",
        "  @functools.wraps(value_fn)\n",
        "  def val_and_grad(x):\n",
        "    return tfp.math.value_and_gradient(value_fn, x)\n",
        "  return val_and_grad\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def timed_execution():\n",
        "  t0 = time.time()\n",
        "  yield\n",
        "  dt = time.time() - t0\n",
        "  print('Evaluation took: %f seconds' % dt)\n",
        "\n",
        "\n",
        "def np_value(tensor):\n",
        "  \"\"\"Get numpy value out of possibly nested tuple of tensors.\"\"\"\n",
        "  if isinstance(tensor, tuple):\n",
        "    return type(tensor)(*(np_value(t) for t in tensor))\n",
        "  else:\n",
        "    return tensor.numpy()\n",
        "\n",
        "def run(optimizer):\n",
        "  \"\"\"Run an optimizer and measure it's evaluation time.\"\"\"\n",
        "  optimizer()  # Warmup.\n",
        "  with timed_execution():\n",
        "    result = optimizer()\n",
        "  return np_value(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7iEPlp7FkN"
      },
      "source": [
        "### L-BFGS em uma função quadrática simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJdXP5E828aP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.014586 seconds\n",
            "L-BFGS Results\n",
            "Converged: True\n",
            "Location of the minimum: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Number of iterations: 10\n"
          ]
        }
      ],
      "source": [
        "# Fix numpy seed for reproducibility\n",
        "np.random.seed(12345)\n",
        "\n",
        "# The objective must be supplied as a function that takes a single\n",
        "# (Tensor) argument and returns a tuple. The first component of the\n",
        "# tuple is the value of the objective at the supplied point and the\n",
        "# second value is the gradient at the supplied point. The value must\n",
        "# be a scalar and the gradient must have the same shape as the\n",
        "# supplied argument.\n",
        "\n",
        "# The `make_val_and_grad_fn` decorator helps transforming a function\n",
        "# returning the objective value into one that returns both the gradient\n",
        "# and the value. It also works for both eager and graph mode.\n",
        "\n",
        "dim = 10\n",
        "minimum = np.ones([dim])\n",
        "scales = np.exp(np.random.randn(dim))\n",
        "\n",
        "@make_val_and_grad_fn\n",
        "def quadratic(x):\n",
        "  return tf.reduce_sum(scales * (x - minimum) ** 2, axis=-1)\n",
        "\n",
        "# The minimization routine also requires you to supply an initial\n",
        "# starting point for the search. For this example we choose a random\n",
        "# starting point.\n",
        "start = np.random.randn(dim)\n",
        "\n",
        "# Finally an optional argument called tolerance let's you choose the\n",
        "# stopping point of the search. The tolerance specifies the maximum\n",
        "# (supremum) norm of the gradient vector at which the algorithm terminates.\n",
        "# If you don't have a specific need for higher or lower accuracy, leaving\n",
        "# this parameter unspecified (and hence using the default value of 1e-8)\n",
        "# should be good enough.\n",
        "tolerance = 1e-10\n",
        "\n",
        "@tf.function\n",
        "def quadratic_with_lbfgs():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "    quadratic,\n",
        "    initial_position=tf.constant(start),\n",
        "    tolerance=tolerance)\n",
        "\n",
        "results = run(quadratic_with_lbfgs)\n",
        "\n",
        "# The optimization results contain multiple pieces of information. The most\n",
        "# important fields are: 'converged' and 'position'.\n",
        "# Converged is a boolean scalar tensor. As the name implies, it indicates\n",
        "# whether the norm of the gradient at the final point was within tolerance.\n",
        "# Position is the location of the minimum found. It is important to check\n",
        "# that converged is True before using the value of the position.\n",
        "\n",
        "print('L-BFGS Results')\n",
        "print('Converged:', results.converged)\n",
        "print('Location of the minimum:', results.position)\n",
        "print('Number of iterations:', results.num_iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dnp7Nm161KY"
      },
      "source": [
        "### O mesmo problema com BFGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1k6n3_n4W2K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.010468 seconds\n",
            "BFGS Results\n",
            "Converged: True\n",
            "Location of the minimum: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Number of iterations: 10\n"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def quadratic_with_bfgs():\n",
        "  return tfp.optimizer.bfgs_minimize(\n",
        "    quadratic,\n",
        "    initial_position=tf.constant(start),\n",
        "    tolerance=tolerance)\n",
        "\n",
        "results = run(quadratic_with_bfgs)\n",
        "\n",
        "print('BFGS Results')\n",
        "print('Converged:', results.converged)\n",
        "print('Location of the minimum:', results.position)\n",
        "print('Number of iterations:', results.num_iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT1GJU3s1LAW"
      },
      "source": [
        "## Regressão linear com penalidade L1 – Dados de câncer de próstata\n",
        "\n",
        "Exemplo do livro *The Elements of Statistical Learning, Data Mining, Inference, and Prediction* (Os elementos do aprendizado estatístico, mineração de dados, inferência e previsão) dos autores Trevor Hastie, Robert Tibshirani e Jerome Friedman.\n",
        "\n",
        "Esse é um problema de otimização com penalidade L1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RLvid3kqi4L"
      },
      "source": [
        "### Obtenção do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlMkFHomqyxu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data to /tmp/datasets/prostate.data.\n"
          ]
        }
      ],
      "source": [
        "def cache_or_download_file(cache_dir, url_base, filename):\n",
        "  \"\"\"Read a cached file or download it.\"\"\"\n",
        "  filepath = os.path.join(cache_dir, filename)\n",
        "  if tf.io.gfile.exists(filepath):\n",
        "    return filepath\n",
        "  if not tf.io.gfile.exists(cache_dir):\n",
        "    tf.io.gfile.makedirs(cache_dir)\n",
        "  url = url_base + filename\n",
        "  print(\"Downloading {url} to {filepath}.\".format(url=url, filepath=filepath))\n",
        "  urllib.request.urlretrieve(url, filepath)\n",
        "  return filepath\n",
        "\n",
        "def get_prostate_dataset(cache_dir=CACHE_DIR):\n",
        "  \"\"\"Download the prostate dataset and read as Pandas dataframe.\"\"\"\n",
        "  url_base = 'http://web.stanford.edu/~hastie/ElemStatLearn/datasets/'\n",
        "  return pd.read_csv(\n",
        "      cache_or_download_file(cache_dir, url_base, 'prostate.data'),\n",
        "      delim_whitespace=True, index_col=0)\n",
        "\n",
        "prostate_df = get_prostate_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY4JVbrZqpZ-"
      },
      "source": [
        "### Definição do problema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7d6oBnYFZwh"
      },
      "outputs": [],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "feature_names = ['lcavol', 'lweight',\t'age',\t'lbph',\t'svi', 'lcp',\t\n",
        "                 'gleason',\t'pgg45']\n",
        "\n",
        "# Normalize features\n",
        "scalar = preprocessing.StandardScaler()\n",
        "prostate_df[feature_names] = pd.DataFrame(\n",
        "    scalar.fit_transform(\n",
        "        prostate_df[feature_names].astype('float64')))\n",
        "\n",
        "# select training set\n",
        "prostate_df_train = prostate_df[prostate_df.train == 'T'] \n",
        "\n",
        "# Select features and labels \n",
        "features = prostate_df_train[feature_names]\n",
        "labels =  prostate_df_train[['lpsa']]\n",
        "\n",
        "# Create tensors\n",
        "feat = tf.constant(features.values, dtype=tf.float64)\n",
        "lab = tf.constant(labels.values, dtype=tf.float64)\n",
        "\n",
        "dtype = feat.dtype\n",
        "\n",
        "regularization = 0 # regularization parameter\n",
        "dim = 8 # number of features\n",
        "\n",
        "# We pick a random starting point for the search\n",
        "start = np.random.randn(dim + 1)\n",
        "\n",
        "def regression_loss(params):\n",
        "  \"\"\"Compute loss for linear regression model with L1 penalty\n",
        "\n",
        "  Args:\n",
        "    params: A real tensor of shape [dim + 1]. The zeroth component\n",
        "      is the intercept term and the rest of the components are the\n",
        "      beta coefficients.\n",
        "\n",
        "  Returns:\n",
        "    The mean square error loss including L1 penalty.\n",
        "  \"\"\"\n",
        "  params = tf.squeeze(params)\n",
        "  intercept, beta  = params[0], params[1:]\n",
        "  pred = tf.matmul(feat, tf.expand_dims(beta, axis=-1)) + intercept\n",
        "  mse_loss = tf.reduce_sum(\n",
        "      tf.cast(\n",
        "        tf.losses.mean_squared_error(y_true=lab, y_pred=pred), tf.float64))\n",
        "  l1_penalty = regularization * tf.reduce_sum(tf.abs(beta))\n",
        "  total_loss = mse_loss + l1_penalty\n",
        "  return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyRl2uksnAY0"
      },
      "source": [
        "### Resolução com L-BFGS\n",
        "\n",
        "Faça o ajuste usando L-BFGS. Embora a penalidade L1 acarrete descontinuidades derivativas, na prática, o L-BFGS ainda funciona muito bem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXkJbrYVqNSW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.017987 seconds\n",
            "L-BFGS Results\n",
            "Converged: True\n",
            "Intercept: Fitted (2.3879985744556484)\n",
            "Beta:      Fitted [ 0.68626215  0.28193532 -0.17030254  0.10799274  0.33634988 -0.24888523\n",
            "  0.11992237  0.08689026]\n"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def l1_regression_with_lbfgs():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "    make_val_and_grad_fn(regression_loss),\n",
        "    initial_position=tf.constant(start),\n",
        "    tolerance=1e-8)\n",
        "\n",
        "results = run(l1_regression_with_lbfgs)\n",
        "minimum = results.position\n",
        "fitted_intercept = minimum[0]\n",
        "fitted_beta = minimum[1:]\n",
        "\n",
        "print('L-BFGS Results')\n",
        "print('Converged:', results.converged)\n",
        "print('Intercept: Fitted ({})'.format(fitted_intercept))\n",
        "print('Beta:      Fitted {}'.format(fitted_beta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cieJV7D7gIpU"
      },
      "source": [
        "### Resolução com Nelder Mead\n",
        "\n",
        "O [método Nelder Mead](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method) é um dos métodos de minimização livre derivativa mais populares. Esse otimizador não usa informações de gradiente e não faz suposições sobre a capacidade de diferenciação da função alvo. Portanto, ele é adequado para funções objetivas não suaves, por exemplo, problemas de otimização com penalidade L1.\n",
        "\n",
        "Para um problema de otimização com $n$ dimensões, ele mantém um conjunto de $n+1$ soluções candidatas que abarcam um simplex não degenerativo. Ele modifica com êxito o simplex baseado em um conjunto de movimentos (reflexo, expansão, encolhimento e contração) usando os valores da função em cada um dos vértices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8Dg-siSdrsV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.325643 seconds\n",
            "Nelder Mead Results\n",
            "Converged: True\n",
            "Intercept: Fitted (2.387998456121595)\n",
            "Beta:      Fitted [ 0.68626266  0.28193456 -0.17030291  0.10799375  0.33635132 -0.24888703\n",
            "  0.11992244  0.08689023]\n"
          ]
        }
      ],
      "source": [
        "# Nelder mead expects an initial_vertex of shape [n + 1, 1].\n",
        "initial_vertex = tf.expand_dims(tf.constant(start, dtype=dtype), axis=-1)\n",
        "\n",
        "@tf.function\n",
        "def l1_regression_with_nelder_mead():\n",
        "  return tfp.optimizer.nelder_mead_minimize(\n",
        "      regression_loss,\n",
        "      initial_vertex=initial_vertex,\n",
        "      func_tolerance=1e-10,\n",
        "      position_tolerance=1e-10)\n",
        "\n",
        "results = run(l1_regression_with_nelder_mead)\n",
        "minimum = results.position.reshape([-1])\n",
        "fitted_intercept = minimum[0]\n",
        "fitted_beta = minimum[1:]\n",
        "\n",
        "print('Nelder Mead Results')\n",
        "print('Converged:', results.converged)\n",
        "print('Intercept: Fitted ({})'.format(fitted_intercept))\n",
        "print('Beta:      Fitted {}'.format(fitted_beta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntLeQCtFZizJ"
      },
      "source": [
        "## Regressão logística com penalidade L2\n",
        "\n",
        "Para este exemplo, criamos um dataset sintético para classificação e usamos o otimizador L-BFGS para ajustar os parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_uCJjyDZiVM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.056751 seconds\n",
            "Converged: True\n",
            "Intercept: Fitted (1.4111415084244365), Actual (1.3934058329729904)\n",
            "Beta:\n",
            "\tFitted [-0.18016612  0.53121578 -0.56420632 -0.5336374   2.00499675],\n",
            "\tActual [-0.20470766  0.47894334 -0.51943872 -0.5557303   1.96578057]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "dim = 5  # The number of features\n",
        "n_obs = 10000  # The number of observations\n",
        "\n",
        "betas = np.random.randn(dim)  # The true beta\n",
        "intercept = np.random.randn()  # The true intercept\n",
        "\n",
        "features = np.random.randn(n_obs, dim)  # The feature matrix\n",
        "probs = sp.special.expit(\n",
        "    np.matmul(features, np.expand_dims(betas, -1)) + intercept)\n",
        "\n",
        "labels = sp.stats.bernoulli.rvs(probs)  # The true labels\n",
        "\n",
        "regularization = 0.8\n",
        "feat = tf.constant(features)\n",
        "lab = tf.constant(labels, dtype=feat.dtype)\n",
        "\n",
        "@make_val_and_grad_fn\n",
        "def negative_log_likelihood(params):\n",
        "  \"\"\"Negative log likelihood for logistic model with L2 penalty\n",
        "\n",
        "  Args:\n",
        "    params: A real tensor of shape [dim + 1]. The zeroth component\n",
        "      is the intercept term and the rest of the components are the\n",
        "      beta coefficients.\n",
        "\n",
        "  Returns:\n",
        "    The negative log likelihood plus the penalty term. \n",
        "  \"\"\"\n",
        "  intercept, beta  = params[0], params[1:]\n",
        "  logit = tf.matmul(feat, tf.expand_dims(beta, -1)) + intercept\n",
        "  log_likelihood = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "      labels=lab, logits=logit))\n",
        "  l2_penalty = regularization * tf.reduce_sum(beta ** 2)\n",
        "  total_loss = log_likelihood + l2_penalty\n",
        "  return total_loss\n",
        "\n",
        "start = np.random.randn(dim + 1)\n",
        "\n",
        "@tf.function\n",
        "def l2_regression_with_lbfgs():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "      negative_log_likelihood,\n",
        "      initial_position=tf.constant(start),\n",
        "      tolerance=1e-8)\n",
        "\n",
        "results = run(l2_regression_with_lbfgs)\n",
        "minimum = results.position\n",
        "fitted_intercept = minimum[0]\n",
        "fitted_beta = minimum[1:]\n",
        "\n",
        "print('Converged:', results.converged)\n",
        "print('Intercept: Fitted ({}), Actual ({})'.format(fitted_intercept, intercept))\n",
        "print('Beta:\\n\\tFitted {},\\n\\tActual {}'.format(fitted_beta, betas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVbdnfYu8cYh"
      },
      "source": [
        "## Suporte a divisão em lotes\n",
        "\n",
        "Tanto o BFGS quanto o L-BFGS têm suporte à computação em lotes, por exemplo, para otimizar uma única função a partir de diferentes pontos de partida ou múltiplas funções paramétricas a partir de um único ponto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVAO1lit8zzK"
      },
      "source": [
        "### Única função, múltiplos pontos de partida\n",
        "\n",
        "A função de Himmelblau é um caso de teste de otimização padrão. A função é dada por:\n",
        "\n",
        "```\n",
        "$$f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$\n",
        "```\n",
        "\n",
        "A função tem quatro mínimos localizados em:\n",
        "\n",
        "- (3, 2)\n",
        "- (-2,805118, 3,131312)\n",
        "- (-3,779310, -3,283186)\n",
        "- (3,584428, -1,848126)\n",
        "\n",
        "Todos esses mínimos podem ser alcançados a partir dos pontos de partida apropriados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RhP1VON7tO_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.019095 seconds\n",
            "Converged: [ True  True  True  True]\n",
            "Minima: [[ 3.          2.        ]\n",
            " [-2.80511809  3.13131252]\n",
            " [-3.77931025 -3.28318599]\n",
            " [ 3.58442834 -1.84812653]]\n"
          ]
        }
      ],
      "source": [
        "# The function to minimize must take as input a tensor of shape [..., n]. In\n",
        "# this n=2 is the size of the domain of the input and [...] are batching\n",
        "# dimensions. The return value must be of shape [...], i.e. a batch of scalars\n",
        "# with the objective value of the function evaluated at each input point.\n",
        "\n",
        "@make_val_and_grad_fn\n",
        "def himmelblau(coord):\n",
        "  x, y = coord[..., 0], coord[..., 1]\n",
        "  return (x * x + y - 11) ** 2 + (x + y * y - 7) ** 2\n",
        "\n",
        "starts = tf.constant([[1, 1],\n",
        "                      [-2, 2],\n",
        "                      [-1, -1],\n",
        "                      [1, -2]], dtype='float64')\n",
        "\n",
        "# The stopping_condition allows to further specify when should the search stop.\n",
        "# The default, tfp.optimizer.converged_all, will proceed until all points have\n",
        "# either converged or failed. There is also a tfp.optimizer.converged_any to\n",
        "# stop as soon as the first point converges, or all have failed.\n",
        "\n",
        "@tf.function\n",
        "def batch_multiple_starts():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "      himmelblau, initial_position=starts,\n",
        "      stopping_condition=tfp.optimizer.converged_all,\n",
        "      tolerance=1e-8)\n",
        "\n",
        "results = run(batch_multiple_starts)\n",
        "print('Converged:', results.converged)\n",
        "print('Minima:', results.position)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W73lxbpD_UEs"
      },
      "source": [
        "### Múltiplas funções\n",
        "\n",
        "Para fins de demonstração, neste exemplo, otimizamos simultaneamente um grande número de tigelas quadráticas de alta dimensão geradas aleatoriamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy9sPmO1_3w5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 1.994132 seconds\n",
            "All converged: True\n",
            "Largest error: 4.4131473142527966e-08\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "dim = 100\n",
        "batches = 500\n",
        "minimum = np.random.randn(batches, dim)\n",
        "scales = np.exp(np.random.randn(batches, dim))\n",
        "\n",
        "@make_val_and_grad_fn\n",
        "def quadratic(x):\n",
        "  return tf.reduce_sum(input_tensor=scales * (x - minimum)**2, axis=-1)\n",
        "\n",
        "# Make all starting points (1, 1, ..., 1). Note not all starting points need\n",
        "# to be the same.\n",
        "start = tf.ones((batches, dim), dtype='float64')\n",
        "\n",
        "@tf.function\n",
        "def batch_multiple_functions():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "      quadratic, initial_position=start,\n",
        "      stopping_condition=tfp.optimizer.converged_all,\n",
        "      max_iterations=100,\n",
        "      tolerance=1e-8)\n",
        "\n",
        "results = run(batch_multiple_functions)\n",
        "print('All converged:', np.all(results.converged))\n",
        "print('Largest error:', np.max(results.position - minimum))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Optimizers_in_TensorFlow_Probability.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
