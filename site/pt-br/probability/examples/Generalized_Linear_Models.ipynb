{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2J3nB-ZrRv1"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Probability Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9qDhTJmprPnm"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfPtIQ3DdZ8r"
      },
      "source": [
        "# Modelos lineares generalizados\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/probability/examples/Generalized_Linear_Models\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Veja em TensorFlow.org</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/probability/examples/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/probability/examples/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte em GitHub</a>   </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/probability/examples/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOfH1_F9YsOG"
      },
      "source": [
        "Neste notebook, apresentamos os modelos lineares generalizados usando um exemplo funcional, que será resolvido de duas formas diferentes usando dois algoritmos para ajustar de forma eficiente modelos lineares generalizados no TensorFlow Probability: pontuação de Fisher para dados densos e método do gradiente proximal com coordenadas. Comparamos os coeficientes ajustados aos coeficientes verdadeiros e, no caso do método do gradiente proximal com coordenadas, à saída do algoritmo `glmnet` similar do R. Por fim, fornecemos maiores detalhes matemáticos e derivações de diversas propriedades essenciais dos modelos lineares generalizados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsfQ6vLb5I0"
      },
      "source": [
        "# Contexto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdMX-QKagFnY"
      },
      "source": [
        "Um modelo linear generalizado (GLM, na sigla em inglês) é um modelo linear ($\\eta = x^\\top \\beta$) encapsulado em uma transformação (função de ligação) que conta com uma distribuição de respostas a partir de uma família de exponenciais. A escolha da função de ligação e da distribuição de respostas é bastante flexível, proporcionando grande capacidade de expressão para os modelos lineares generalizados. Confira os detalhes completos, incluindo uma apresentação sequencial de todas as definições e resultados até a construção dos modelos lineares generalizados em notação não ambígua, na seção \"Derivação de fatos sobre modelos lineares generalizados\" abaixo. Vamos resumir:\n",
        "\n",
        "Em um modelo linear generalizado, uma distribuição preditiva para a variável de resposta $Y$ está associada a um vetor de preditores observados $x$. A distribuição tem a seguinte forma:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  p(y \\, |\\, x)\n",
        "&=\n",
        "  m(y, \\phi) \\exp\\left(\\frac{\\theta\\, T(y) - A(\\theta)}{\\phi}\\right)\n",
        "\\\\\n",
        "  \\theta\n",
        "&:=\n",
        "  h(\\eta)\n",
        "\\\\\n",
        "  \\eta\n",
        "&:=\n",
        "  x^\\top \\beta\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Aqui, $\\beta$ são os parâmetros (\"pesos\"), $\\phi$ é um hiperparâmetro que representa a dispersão (\"variância\"), e $m$, $h$, $T$, $A$ são caracterizados pela família de modelos especificados pelo usuário.\n",
        "\n",
        "A média de $Y$ depende de $x$ pela composição da **resposta linear** $\\eta$ e pela função de ligação (inversa), isto é:\n",
        "\n",
        "$$\n",
        "\\mu := g^{-1}(\\eta)\n",
        "$$\n",
        "\n",
        "em que $g$ é a **função de ligação**. No TFP, a função de ligação e a família de modelos são especificadas conjuntamente por uma subclasse de `tfp.glm.ExponentialFamily`. Confira alguns exemplos:\n",
        "\n",
        "- `tfp.glm.Normal`, também chamada de \"regressão linear\"\n",
        "- `tfp.glm.Bernoulli`, também chamada de \"regressão logística\"\n",
        "- `tfp.glm.Poisson`, também chamada de \"regressão de Poisson\"\n",
        "- `tfp.glm.BernoulliNormalCDF`, também chamada de \"regressão probit\"\n",
        "\n",
        "O TFP prefere nomear as famílias de modelos de acordo com a distribuição ao longo de `Y` em vez da função de ligação, já que as distribuições `tfp.Distribution` são funções de primeira classe. Se o nome da subclasse de `tfp.glm.ExponentialFamily` contiver uma segunda palavra, isso indica uma [função de ligação não canônica](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oGScpRnqH_b"
      },
      "source": [
        "Os modelos lineares generalizados têm propriedades marcantes que permitem uma implementação eficiente do estimador de máxima verossimilhança. Uma dessas propriedades são fórmulas simples para o gradiente da log-verossimilhança $\\ell$ e para a matriz de informação de Fisher, que é o valor esperado da Hessiana da log-verossimilhança negativa ao fazer uma reamostragem da resposta para os mesmos preditores, isto é:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\nabla_\\beta\\, \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{y})\n",
        "&=\n",
        "  \\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\\frac{\n",
        "      {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta)\n",
        "    }\\right)\n",
        "  \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta)\\right)\n",
        "\\\\\n",
        "  \\mathbb{E}_{Y_i \\sim \\text{GLM} | x_i} \\left[\n",
        "    \\nabla_\\beta^2\\, \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{Y})\n",
        "  \\right]\n",
        "&=\n",
        "  -\\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\n",
        "    \\frac{\n",
        "      \\phi\\, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)^2\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta)\n",
        "    }\\right)\\,\n",
        "  \\mathbf{x}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "em que $\\mathbf{x}$ é a matriz cuja $i$ésima linha é o vetor de preditores para a $i$ésima amostra de dados, e $\\mathbf{y}$ é um vetor cuja $i$ésima coordenada é a resposta observada para a $i$ésima amostra de dados. Aqui (grosso modo), ${\\text{Mean}_T}(\\eta) := \\mathbb{E}[T(Y),|,\\eta]$ e ${\\text{Var}_T}(\\eta) := \\text{Var}[T(Y),|,\\eta]$, e o negrito denota a vetorização dessas funções. Confira os detalhes completos sobre as expectativas e variâncias dessas distribuições na seção \"Derivação de fatos sobre modelos lineares generalizados\" abaixo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNDwfwBObKl"
      },
      "source": [
        "# Exemplo\n",
        "\n",
        "Nesta seção, vamos descrever e demonstrar brevemente dois algoritmos integrados de ajuste de modelos lineares generalizados no TensorFlow Probability: pontuação de Fisher (`tfp.glm.fit`) e método do gradiente proximal com coordenadas (`tfp.glm.fit_sparse`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4phryMfsP4Sn"
      },
      "source": [
        "## Dataset sintético\n",
        "\n",
        "Vamos fingir o carregamento de um dataset de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA2Rf9PPgMAD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEVnTz2hh9RN"
      },
      "outputs": [],
      "source": [
        "def make_dataset(n, d, link, scale=1., dtype=np.float32):\n",
        "  model_coefficients = tfd.Uniform(\n",
        "      low=-1., high=np.array(1, dtype)).sample(d, seed=42)\n",
        "  radius = np.sqrt(2.)\n",
        "  model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n",
        "  mask = tf.random.shuffle(tf.range(d)) < int(0.5 * d)\n",
        "  model_coefficients = tf.where(\n",
        "      mask, model_coefficients, np.array(0., dtype))\n",
        "  model_matrix = tfd.Normal(\n",
        "      loc=0., scale=np.array(1, dtype)).sample([n, d], seed=43)\n",
        "  scale = tf.convert_to_tensor(scale, dtype)\n",
        "  linear_response = tf.linalg.matvec(model_matrix, model_coefficients)\n",
        "  \n",
        "  if link == 'linear':\n",
        "    response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n",
        "  elif link == 'probit':\n",
        "    response = tf.cast(\n",
        "        tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n",
        "                   dtype)\n",
        "  elif link == 'logit':\n",
        "    response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n",
        "  else:\n",
        "    raise ValueError('unrecognized true link: {}'.format(link))\n",
        "  return model_matrix, response, model_coefficients, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Fk5XZKbvi4"
      },
      "source": [
        "### Observação: estabeleça conexão a um runtime local.\n",
        "\n",
        "Neste notebook, compartilhamos dados entre os kernels do Python e do R usando arquivos locais. Para permitir esse compartilhamento, use runtimes na mesma máquina na qual você tenha permissão de ler e gravar arquivos locais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EAQjTrZJqKx"
      },
      "outputs": [],
      "source": [
        "x, y, model_coefficients_true, _ = [t.numpy() for t in make_dataset(\n",
        "    n=int(1e5), d=100, link='probit')]\n",
        "\n",
        "DATA_DIR = '/tmp/glm_example'\n",
        "tf.io.gfile.makedirs(DATA_DIR)\n",
        "with tf.io.gfile.GFile('{}/x.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, x, delimiter=',')\n",
        "with tf.io.gfile.GFile('{}/y.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, y.astype(np.int32) + 1, delimiter=',', fmt='%d')\n",
        "with tf.io.gfile.GFile(\n",
        "    '{}/model_coefficients_true.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, model_coefficients_true, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P5I-aJdN6GZ"
      },
      "source": [
        "## Sem regularização L1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN6HfiH3bAb0"
      },
      "source": [
        "A função `tfp.glm.fit` implementa a pontuação de Fisher, que recebe como alguns de seus argumentos:\n",
        "\n",
        "- `model_matrix` = $\\mathbf{x}$\n",
        "- `response` = $\\mathbf{y}$\n",
        "- `model` = callable que, dado o argumento $\\boldsymbol{\\eta}$, retorna a tupla $\\left( {\\textbf{Mean}_T}(\\boldsymbol{\\eta}), {\\textbf{Var}_T}(\\boldsymbol{\\eta}), {\\textbf{Mean}_T}'(\\boldsymbol{\\eta}) \\right)$.\n",
        "\n",
        "Recomendamos que `model` seja uma instância da classe `tfp.glm.ExponentialFamily`. Existem diversas implementações pré-criadas disponíveis, então, para a maioria dos modelos lineares generalizados comuns, não é necessário escrever código personalizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXkxVBSmesjn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_converged: True\n",
            "    num_iter: 6\n",
            "    accuracy: 0.75241\n",
            "    deviance: -0.992436110973\n",
            "||w0-w1||_2 / (1+||w0||_2): 0.0231555201462\n"
          ]
        }
      ],
      "source": [
        "@tf.function(autograph=False)\n",
        "def fit_model():\n",
        "  model_coefficients, linear_response, is_converged, num_iter = tfp.glm.fit(\n",
        "      model_matrix=x, response=y, model=tfp.glm.BernoulliNormalCDF())\n",
        "  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(y, linear_response)\n",
        "  return (model_coefficients, linear_response, is_converged, num_iter,\n",
        "          log_likelihood)\n",
        " \n",
        "[model_coefficients, linear_response, is_converged, num_iter,\n",
        " log_likelihood] = [t.numpy() for t in fit_model()]\n",
        "\n",
        "print(('is_converged: {}\\n'\n",
        "       '    num_iter: {}\\n'\n",
        "       '    accuracy: {}\\n'\n",
        "       '    deviance: {}\\n'\n",
        "       '||w0-w1||_2 / (1+||w0||_2): {}'\n",
        "      ).format(\n",
        "    is_converged,\n",
        "    num_iter,\n",
        "    np.mean((linear_response > 0.) == y),\n",
        "    2. * np.mean(log_likelihood),\n",
        "    np.linalg.norm(model_coefficients_true - model_coefficients, ord=2) /\n",
        "        (1. + np.linalg.norm(model_coefficients_true, ord=2))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6qexoHAJzEF"
      },
      "source": [
        "### Detalhes matemáticos\n",
        "\n",
        "A pontuação de Fisher é uma modificação do método de Newton para encontrar a estimativa da máxima verossimilhança:\n",
        "\n",
        "$$\n",
        "\\hat\\beta\n",
        ":= \\underset{\\beta}{\\text{arg max}}\\ \\ \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}).\n",
        "$$\n",
        "\n",
        "O método padrão de Newton, que é a procura por zeros do gradiente da log-verossimilhança, seguiria a regra de atualização:\n",
        "\n",
        "$$\n",
        "  \\beta^{(t+1)}_{\\text{Newton}}\n",
        ":=\n",
        "  \\beta^{(t)}\n",
        "  -\n",
        "  \\alpha\n",
        "  \\left(\n",
        "    \\nabla^2_\\beta\\, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}}^{-1}\n",
        "  \\left(\n",
        "    \\nabla_\\beta\\, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}}\n",
        "$$\n",
        "\n",
        "em que $\\alpha \\in (0, 1]$ é uma taxa de aprendizado usada para controlar o tamanho do passo.\n",
        "\n",
        "Na pontuação de Fisher, substituímos a Hessiana pela matriz de informação de Fisher negativa:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\beta^{(t+1)}\n",
        "&:=\n",
        "  \\beta^{(t)}\n",
        "  -\n",
        "  \\alpha\\,\n",
        "  \\mathbb{E}_{\n",
        "    Y_i \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta^{(t)}), \\phi)\n",
        "  }\n",
        "  \\left[\n",
        "    \\left(\n",
        "      \\nabla^2_\\beta\\, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{Y})\n",
        "    \\right)_{\\beta = \\beta^{(t)}}\n",
        "  \\right]^{-1}\n",
        "  \\left(\n",
        "    \\nabla_\\beta\\, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}} \\\\[3mm]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "[Observe que, aqui,  $\\mathbf{Y} = (Y_i)_{i=1}^{n}$ é aleatório, enquanto $\\mathbf{y}$ ainda é o vetor de respostas observadas.]\n",
        "\n",
        "Segundo as fórmulas na seção \"Ajuste dos parâmetros de modelos lineares generalizados de acordo com os dados\" abaixo, isso é simplificado para:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\beta^{(t+1)}\n",
        "&=\n",
        "  \\beta^{(t)}\n",
        "  +\n",
        "  \\alpha\n",
        "  \\left(\n",
        "    \\mathbf{x}^\\top\n",
        "    \\text{diag}\\left(\n",
        "      \\frac{\n",
        "        \\phi\\, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t)})^2\n",
        "      }{\n",
        "        {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t)})\n",
        "      }\\right)\\,\n",
        "    \\mathbf{x}\n",
        "  \\right)^{-1}\n",
        "  \\left(\n",
        "    \\mathbf{x}^\\top\n",
        "    \\text{diag}\\left(\\frac{\n",
        "        {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t)})\n",
        "      }{\n",
        "        {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t)})\n",
        "      }\\right)\n",
        "    \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta^{(t)})\\right)\n",
        "  \\right).\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076quM7tN8_1"
      },
      "source": [
        "## Com regularização L1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnP3jeZOk7Y5"
      },
      "source": [
        "`tfp.glm.fit_sparse` implementa um ajustador de modelos lineares generalizados mais adequado para datasets esparsos baseado no algoritmo em [Yuan, Ho e Lin, 2012](#1). Confira alguns recursos:\n",
        "\n",
        "- Regularização L1\n",
        "- Sem inversão de matrizes\n",
        "- Poucas avaliações do gradiente e da Hessiana\n",
        "\n",
        "Primeiro, vamos apresentar um exemplo de uso do código. Os detalhes do algoritmo são mais bem explicados na seção \"Detalhes do algoritmo para `tfp.glm.fit_sparse`\" abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Oky1X4ijfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_converged: True\n",
            "    num_iter: 1\n",
            "\n",
            "Coefficients:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learned</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.216240</td>\n",
              "      <td>0.220758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.043702</td>\n",
              "      <td>0.063950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.145379</td>\n",
              "      <td>-0.153256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.024382</td>\n",
              "      <td>0.046572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.242985</td>\n",
              "      <td>-0.242609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.106168</td>\n",
              "      <td>-0.123367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.039745</td>\n",
              "      <td>-0.067560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.217717</td>\n",
              "      <td>-0.222169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.016553</td>\n",
              "      <td>-0.041692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.018959</td>\n",
              "      <td>0.049624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.057686</td>\n",
              "      <td>-0.078299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.234406</td>\n",
              "      <td>-0.240482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.232209</td>\n",
              "      <td>0.225448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.144485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>-0.178534</td>\n",
              "      <td>-0.186722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.218493</td>\n",
              "      <td>0.229656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.195579</td>\n",
              "      <td>0.200442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.031153</td>\n",
              "      <td>0.050457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.229065</td>\n",
              "      <td>0.231451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.006512</td>\n",
              "      <td>-0.039516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-0.107947</td>\n",
              "      <td>-0.119896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.149419</td>\n",
              "      <td>0.171693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.047955</td>\n",
              "      <td>0.063434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.083171</td>\n",
              "      <td>-0.107145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.084615</td>\n",
              "      <td>0.101221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-0.168431</td>\n",
              "      <td>-0.175473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.138411</td>\n",
              "      <td>0.152623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.061161</td>\n",
              "      <td>0.081945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-0.083348</td>\n",
              "      <td>-0.104929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.141154</td>\n",
              "      <td>-0.153871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Learned      True\n",
              "0   0.216240  0.220758\n",
              "1   0.000000  0.000000\n",
              "2   0.000000  0.000000\n",
              "3   0.000000  0.000000\n",
              "4   0.000000  0.000000\n",
              "5   0.043702  0.063950\n",
              "6  -0.145379 -0.153256\n",
              "7   0.000000  0.000000\n",
              "8   0.000000  0.000000\n",
              "9   0.000000  0.000000\n",
              "10  0.000000  0.000000\n",
              "11  0.000000  0.000000\n",
              "12  0.000000  0.000000\n",
              "13  0.024382  0.046572\n",
              "14 -0.242985 -0.242609\n",
              "15 -0.106168 -0.123367\n",
              "16  0.000000  0.000000\n",
              "17 -0.039745 -0.067560\n",
              "18 -0.217717 -0.222169\n",
              "19  0.000000  0.000000\n",
              "20  0.000000  0.000000\n",
              "21 -0.016553 -0.041692\n",
              "22  0.018959  0.049624\n",
              "23 -0.057686 -0.078299\n",
              "24  0.003642  0.035682\n",
              "25  0.000000  0.000000\n",
              "26  0.000000  0.000000\n",
              "27 -0.234406 -0.240482\n",
              "28  0.000000  0.000000\n",
              "29  0.232209  0.225448\n",
              "..       ...       ...\n",
              "70  0.000000  0.000000\n",
              "71  0.130166  0.144485\n",
              "72  0.000000  0.000000\n",
              "73  0.000000  0.000000\n",
              "74  0.000000  0.000000\n",
              "75 -0.178534 -0.186722\n",
              "76  0.000000  0.000000\n",
              "77  0.218493  0.229656\n",
              "78  0.000000  0.000000\n",
              "79  0.000000  0.000000\n",
              "80  0.195579  0.200442\n",
              "81  0.000000  0.000000\n",
              "82  0.000000  0.000000\n",
              "83  0.031153  0.050457\n",
              "84  0.229065  0.231451\n",
              "85 -0.006512 -0.039516\n",
              "86 -0.107947 -0.119896\n",
              "87  0.000000  0.000000\n",
              "88  0.149419  0.171693\n",
              "89  0.000000  0.000000\n",
              "90  0.047955  0.063434\n",
              "91  0.000000  0.003592\n",
              "92 -0.083171 -0.107145\n",
              "93  0.084615  0.101221\n",
              "94 -0.168431 -0.175473\n",
              "95  0.138411  0.152623\n",
              "96  0.000000  0.000000\n",
              "97  0.061161  0.081945\n",
              "98 -0.083348 -0.104929\n",
              "99 -0.141154 -0.153871\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = tfp.glm.Bernoulli()\n",
        "model_coefficients_start = tf.zeros(x.shape[-1], np.float32)\n",
        "@tf.function(autograph=False)\n",
        "def fit_model():\n",
        "  return tfp.glm.fit_sparse(\n",
        "    model_matrix=tf.convert_to_tensor(x),\n",
        "    response=tf.convert_to_tensor(y),\n",
        "    model=model,\n",
        "    model_coefficients_start=model_coefficients_start,\n",
        "    l1_regularizer=800.,\n",
        "    l2_regularizer=None,\n",
        "    maximum_iterations=10,\n",
        "    maximum_full_sweeps_per_iteration=10,\n",
        "    tolerance=1e-6,\n",
        "    learning_rate=None)\n",
        "\n",
        "model_coefficients, is_converged, num_iter = [t.numpy() for t in fit_model()]\n",
        "coefs_comparison = pd.DataFrame({\n",
        "  'Learned': model_coefficients,\n",
        "  'True': model_coefficients_true,\n",
        "})\n",
        "  \n",
        "print(('is_converged: {}\\n'\n",
        "       '    num_iter: {}\\n\\n'\n",
        "       'Coefficients:').format(\n",
        "    is_converged,\n",
        "    num_iter))\n",
        "coefs_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrJC2J1YbR5L"
      },
      "source": [
        "Observe que os coeficientes aprendidos têm o mesmo padrão de esparsidade que os coeficientes verdadeiros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ7SzrPZMpke"
      },
      "outputs": [],
      "source": [
        "# Save the learned coefficients to a file.\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_prox.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, model_coefficients, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW9NgB1Zisqh"
      },
      "source": [
        "### Comparação com `glmnet` do R\n",
        "\n",
        "Vamos comparar a saída do método do gradiente proximal com coordenadas ao `glmnet` do R, que usa um algoritmo similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aptz7SWwkd5v"
      },
      "source": [
        "#### OBSERVAÇÃO: para executar o código nesta seção, você precisa mudar para um runtime de Colab do R."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS1H3n53h9qc"
      },
      "outputs": [],
      "source": [
        "suppressMessages({\n",
        "  library('glmnet')\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X6zKSaxie7I"
      },
      "outputs": [],
      "source": [
        "data_dir <- '/tmp/glm_example'\n",
        "x <- as.matrix(read.csv(paste(data_dir, '/x.csv', sep=''),\n",
        "                        header=FALSE))\n",
        "y <- as.matrix(read.csv(paste(data_dir, '/y.csv', sep=''),\n",
        "                        header=FALSE, colClasses='integer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb31LbhRjsSz"
      },
      "outputs": [],
      "source": [
        "fit <- glmnet(\n",
        "x = x,\n",
        "y = y,\n",
        "family = \"binomial\",  # Logistic regression\n",
        "alpha = 1,  # corresponds to l1_weight = 1, l2_weight = 0\n",
        "standardize = FALSE,\n",
        "intercept = FALSE,\n",
        "thresh = 1e-30,\n",
        "type.logistic = \"Newton\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTN4RKQbhlCm"
      },
      "outputs": [],
      "source": [
        "write.csv(as.matrix(coef(fit, 0.008)),\n",
        "          paste(data_dir, '/model_coefficients_glmnet.csv', sep=''),\n",
        "          row.names=FALSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsrEKgUGjGjf"
      },
      "source": [
        "#### Comparação entre R, TFP e coeficientes verdadeiros (observação: volte para o kernel do Python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCOlGo_4i2sb"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/tmp/glm_example'\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_glmnet.csv'.format(DATA_DIR),\n",
        "                       'r') as f:\n",
        "  model_coefficients_glmnet = np.loadtxt(f,\n",
        "                                   skiprows=2  # Skip column name and intercept\n",
        "                               )\n",
        "\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_prox.csv'.format(DATA_DIR),\n",
        "                       'r') as f:\n",
        "  model_coefficients_prox = np.loadtxt(f)\n",
        "\n",
        "with tf.io.gfile.GFile(\n",
        "    '{}/model_coefficients_true.csv'.format(DATA_DIR), 'r') as f:\n",
        "  model_coefficients_true = np.loadtxt(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l-SZ85lnKg5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R</th>\n",
              "      <th>TFP</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.281080</td>\n",
              "      <td>0.216240</td>\n",
              "      <td>0.220758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.056625</td>\n",
              "      <td>0.043702</td>\n",
              "      <td>0.063950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.188771</td>\n",
              "      <td>-0.145379</td>\n",
              "      <td>-0.153256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.030112</td>\n",
              "      <td>0.024382</td>\n",
              "      <td>0.046572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.316488</td>\n",
              "      <td>-0.242985</td>\n",
              "      <td>-0.242609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.139214</td>\n",
              "      <td>-0.106168</td>\n",
              "      <td>-0.123367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.050239</td>\n",
              "      <td>-0.039745</td>\n",
              "      <td>-0.067560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.283372</td>\n",
              "      <td>-0.217717</td>\n",
              "      <td>-0.222169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.021815</td>\n",
              "      <td>-0.016553</td>\n",
              "      <td>-0.041692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.024070</td>\n",
              "      <td>0.018959</td>\n",
              "      <td>0.049624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.074039</td>\n",
              "      <td>-0.057686</td>\n",
              "      <td>-0.078299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.005321</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.304958</td>\n",
              "      <td>-0.234406</td>\n",
              "      <td>-0.240482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301562</td>\n",
              "      <td>0.232209</td>\n",
              "      <td>0.225448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.169291</td>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.144485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>-0.231294</td>\n",
              "      <td>-0.178534</td>\n",
              "      <td>-0.186722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.284215</td>\n",
              "      <td>0.218493</td>\n",
              "      <td>0.229656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.254524</td>\n",
              "      <td>0.195579</td>\n",
              "      <td>0.200442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.040716</td>\n",
              "      <td>0.031153</td>\n",
              "      <td>0.050457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.297475</td>\n",
              "      <td>0.229065</td>\n",
              "      <td>0.231451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.008569</td>\n",
              "      <td>-0.006512</td>\n",
              "      <td>-0.039516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-0.141028</td>\n",
              "      <td>-0.107947</td>\n",
              "      <td>-0.119896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.194130</td>\n",
              "      <td>0.149419</td>\n",
              "      <td>0.171693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.062601</td>\n",
              "      <td>0.047955</td>\n",
              "      <td>0.063434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.107693</td>\n",
              "      <td>-0.083171</td>\n",
              "      <td>-0.107145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.109381</td>\n",
              "      <td>0.084615</td>\n",
              "      <td>0.101221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-0.218831</td>\n",
              "      <td>-0.168431</td>\n",
              "      <td>-0.175473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.180662</td>\n",
              "      <td>0.138411</td>\n",
              "      <td>0.152623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.078815</td>\n",
              "      <td>0.061161</td>\n",
              "      <td>0.081945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-0.108332</td>\n",
              "      <td>-0.083348</td>\n",
              "      <td>-0.104929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.183284</td>\n",
              "      <td>-0.141154</td>\n",
              "      <td>-0.153871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           R       TFP      True\n",
              "0   0.281080  0.216240  0.220758\n",
              "1   0.000000  0.000000  0.000000\n",
              "2   0.000000  0.000000  0.000000\n",
              "3   0.000000  0.000000  0.000000\n",
              "4   0.000000  0.000000  0.000000\n",
              "5   0.056625  0.043702  0.063950\n",
              "6  -0.188771 -0.145379 -0.153256\n",
              "7   0.000000  0.000000  0.000000\n",
              "8   0.000000  0.000000  0.000000\n",
              "9   0.000000  0.000000  0.000000\n",
              "10  0.000000  0.000000  0.000000\n",
              "11  0.000000  0.000000  0.000000\n",
              "12  0.000000  0.000000  0.000000\n",
              "13  0.030112  0.024382  0.046572\n",
              "14 -0.316488 -0.242985 -0.242609\n",
              "15 -0.139214 -0.106168 -0.123367\n",
              "16  0.000000  0.000000  0.000000\n",
              "17 -0.050239 -0.039745 -0.067560\n",
              "18 -0.283372 -0.217717 -0.222169\n",
              "19  0.000000  0.000000  0.000000\n",
              "20  0.000000  0.000000  0.000000\n",
              "21 -0.021815 -0.016553 -0.041692\n",
              "22  0.024070  0.018959  0.049624\n",
              "23 -0.074039 -0.057686 -0.078299\n",
              "24  0.005321  0.003642  0.035682\n",
              "25  0.000000  0.000000  0.000000\n",
              "26  0.000000  0.000000  0.000000\n",
              "27 -0.304958 -0.234406 -0.240482\n",
              "28  0.000000  0.000000  0.000000\n",
              "29  0.301562  0.232209  0.225448\n",
              "..       ...       ...       ...\n",
              "70  0.000000  0.000000  0.000000\n",
              "71  0.169291  0.130166  0.144485\n",
              "72  0.000000  0.000000  0.000000\n",
              "73  0.000000  0.000000  0.000000\n",
              "74  0.000000  0.000000  0.000000\n",
              "75 -0.231294 -0.178534 -0.186722\n",
              "76  0.000000  0.000000  0.000000\n",
              "77  0.284215  0.218493  0.229656\n",
              "78  0.000000  0.000000  0.000000\n",
              "79  0.000000  0.000000  0.000000\n",
              "80  0.254524  0.195579  0.200442\n",
              "81  0.000000  0.000000  0.000000\n",
              "82  0.000000  0.000000  0.000000\n",
              "83  0.040716  0.031153  0.050457\n",
              "84  0.297475  0.229065  0.231451\n",
              "85 -0.008569 -0.006512 -0.039516\n",
              "86 -0.141028 -0.107947 -0.119896\n",
              "87  0.000000  0.000000  0.000000\n",
              "88  0.194130  0.149419  0.171693\n",
              "89  0.000000  0.000000  0.000000\n",
              "90  0.062601  0.047955  0.063434\n",
              "91  0.000000  0.000000  0.003592\n",
              "92 -0.107693 -0.083171 -0.107145\n",
              "93  0.109381  0.084615  0.101221\n",
              "94 -0.218831 -0.168431 -0.175473\n",
              "95  0.180662  0.138411  0.152623\n",
              "96  0.000000  0.000000  0.000000\n",
              "97  0.078815  0.061161  0.081945\n",
              "98 -0.108332 -0.083348 -0.104929\n",
              "99 -0.183284 -0.141154 -0.153871\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coefs_comparison = pd.DataFrame({\n",
        "    'TFP': model_coefficients_prox,\n",
        "    'R': model_coefficients_glmnet,\n",
        "    'True': model_coefficients_true,\n",
        "})\n",
        "coefs_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfv0GVXqY74Y"
      },
      "source": [
        "# Detalhes do algoritmo para `tfp.glm.fit_sparse`\n",
        "\n",
        "Apresentamos o algoritmo como uma sequência de três modificações do método de Newton. Em cada uma delas, a regra de atualização de $\\beta$ é baseada em um vetor $s$ e em uma matriz $H$, que faz a aproximação do gradiente e da Hessiana da log-verossimilhança. No passo $t$, escolhemos uma coordenada $j^{(t)}$ a ser alterada e atualizamos $\\beta$ de acordo com a regra de atualização:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  u^{(t)}\n",
        "&:=\n",
        "  \\frac{\n",
        "    \\left(\n",
        "      s^{(t)}\n",
        "    \\right)_{j^{(t)}}\n",
        "  }{\n",
        "    \\left(\n",
        "      H^{(t)}\n",
        "    \\right)_{j^{(t)},\\, j^{(t)}}\n",
        "  }\n",
        "\\\\[3mm]\n",
        "  \\beta^{(t+1)}\n",
        "&:=\n",
        "  \\beta^{(t)}\n",
        "  -\n",
        "  \\alpha\\,\n",
        "  u^{(t)}\n",
        "  \\,\\text{onehot}(j^{(t)})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Essa atualização é um passo similar ao Newton com taxa de aprendizado $\\alpha$. Exceto pela parte final (regularização L1), as modificações abaixo diferem apenas na forma como $s$ e $H$ são atualizados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH7C1xBWUV7_"
      },
      "source": [
        "## Ponto de partida: método de Newton com coordenadas\n",
        "\n",
        "No método de Newton com coordenadas, definimos $s$ e $H$ como o gradiente verdadeiro e a Hessiana da log-verossimilhança:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  s^{(t)}_{\\text{vanilla}}\n",
        "&:=\n",
        "  \\left(\n",
        "    \\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}}\n",
        "\\\\\n",
        "  H^{(t)}_{\\text{vanilla}}\n",
        "&:=\n",
        "  \\left(\n",
        "    \\nabla^2_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "  \\right)_{\\beta = \\beta^{(t)}}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rJZD6iyUl0v"
      },
      "source": [
        "## Menos avaliações do gradiente e da Hessiana\n",
        "\n",
        "Geralmente, é caro computar o gradiente e a Hessiana da log-verossimilhança, então costuma valer a pena fazer a aproximação deles da seguinte forma:\n",
        "\n",
        "- Geralmente, fazemos a aproximação da Hessiana como constante localmente e fazemos a aproximação do gradiente para a primeira ordem usando a Hessiana (aproximada):\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  H_{\\text{approx}}^{(t+1)}\n",
        "&:=\n",
        "  H^{(t)}\n",
        "\\\\\n",
        "  s_{\\text{approx}}^{(t+1)}\n",
        "&:=\n",
        "  s^{(t)}\n",
        "  +\n",
        "  H^{(t)}\n",
        "  \\left(\n",
        "    \\beta^{(t+1)} - \\beta^{(t)}\n",
        "  \\right)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "- Ocasionalmente, faça um passo de atualização padrão conforme acima, definindo $s^{(t+1)}$ como o gradiente exato e $H^{(t+1)}$ como a Hessiana exata da log-verossimilhança, avaliada em $\\beta^{(t+1)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvvyaVnUqIQ"
      },
      "source": [
        "## Substituta a informação de Fisher negativa pela Hessiana\n",
        "\n",
        "Para reduzir ainda mais o custo dos passos de atualização padrão, podemos definir $H$ como a matriz de informação de Fisher negativa (eficiente do ponto de vista computacional ao usar as fórmulas da seção \"Ajuste dos parâmetros de modelos lineares generalizados de acordo com os dados\" abaixo) em vez da Hessiana exata:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  H_{\\text{Fisher}}^{(t+1)}\n",
        "&:=\n",
        "  \\mathbb{E}_{Y_i \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta^{(t+1)}), \\phi)}\n",
        "  \\left[\n",
        "    \\left(\n",
        "      \\nabla_\\beta^2\\, \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{Y})\n",
        "    \\right)_{\\beta = \\beta^{(t+1)}}\n",
        "  \\right]\n",
        "\\\\\n",
        "&=\n",
        "  -\\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\n",
        "    \\frac{\n",
        "      \\phi\\, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t+1)})^2\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t+1)})\n",
        "    }\\right)\\,\n",
        "  \\mathbf{x}\n",
        "\\\\\n",
        "  s_{\\text{Fisher}}^{(t+1)}\n",
        "&:=\n",
        "  s_{\\text{vanilla}}^{(t+1)}\n",
        "\\\\\n",
        "&=\n",
        "  \\left(\n",
        "    \\mathbf{x}^\\top\n",
        "    \\,\\text{diag}\\left(\\frac{\n",
        "        {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t+1)})\n",
        "      }{\n",
        "        {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t+1)})\n",
        "      }\\right)\n",
        "    \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta^{(t+1)})\\right)\n",
        "  \\right)\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTH07xYpWGcR"
      },
      "source": [
        "## Regularização L1 via método do gradiente proximal\n",
        "\n",
        "Para incorporar a regularização L1, substituímos a regra de atualização\n",
        "\n",
        "$$\n",
        "  \\beta^{(t+1)}\n",
        ":=\n",
        "  \\beta^{(t)}\n",
        "  -\n",
        "  \\alpha\\,\n",
        "  u^{(t)}\n",
        "  \\,\\text{onehot}(j^{(t)})\n",
        "$$\n",
        "\n",
        "pela regra de atualização mais geral\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\gamma^{(t)}\n",
        "&:=\n",
        "  -\\frac{\\alpha\\, r_{\\text{L1}}}{\\left(H^{(t)}\\right)_{j^{(t)},\\, j^{(t)}}}\n",
        "\\\\[2mm]\n",
        "  \\left(\\beta_{\\text{reg}}^{(t+1)}\\right)_j\n",
        "&:=\n",
        "  \\begin{cases}\n",
        "    \\beta^{(t+1)}_j\n",
        "      &\\text{if } j \\neq j^{(t)} \\\\\n",
        "    \\text{SoftThreshold} \\left(\n",
        "      \\beta^{(t)}_j - \\alpha\\, u^{(t)}\n",
        "      ,\\ \n",
        "      \\gamma^{(t)}\n",
        "    \\right)\n",
        "      &\\text{if } j = j^{(t)}\n",
        "  \\end{cases}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "em que $r_{\text{L1}} > 0$ é uma constante fornecida (o coeficiente de regularização L1), e $\\text{SoftThreshold}$ é o operador de limiarização suave (soft threshold), definido por:\n",
        "\n",
        "$$\n",
        "\\text{SoftThreshold}(\\beta, \\gamma)\n",
        ":=\n",
        "\\begin{cases}\n",
        "\\beta + \\gamma\n",
        "  &\\text{if } \\beta < -\\gamma\n",
        "\\\\\n",
        "0\n",
        "  &\\text{if } -\\gamma \\leq \\beta \\leq \\gamma\n",
        "\\\\\n",
        "\\beta - \\gamma\n",
        "  &\\text{if } \\beta > \\gamma.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Essa regra de atualização tem as duas propriedades inspiradoras abaixo:\n",
        "\n",
        "1. No caso limitante $r_{\\text{L1}} \\to 0$ (ou seja, sem regularização L1), essa regra de atualização é idêntica à original.\n",
        "\n",
        "2. Essa regra de atualização pode ser interpretada como a aplicação de um operador de proximidade cujo ponto fixo é a solução do problema de minimização com regularização L1\n",
        "\n",
        "$$\n",
        "\\underset{\\beta - \\beta^{(t)} \\in \\text{span}\\{ \\text{onehot}(j^{(t)}) \\}}{\\text{arg min}}\n",
        "\\left(\n",
        "  -\\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "  + r_{\\text{L1}} \\left\\lVert \\beta \\right\\rVert_1\n",
        "\\right).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSs7_osNPLVt"
      },
      "source": [
        "### O caso degenerado $r_{\\text{L1}} = 0$ recupera a regra de atualização original\n",
        "\n",
        "Para entender (1), observe que, se $r_{\\text{L1}} = 0$, então $\\gamma^{(t)} = 0$, portanto:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\left(\\beta_{\\text{reg}}^{(t+1)}\\right)_{j^{(t)}}\n",
        "&=\n",
        "  \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}} - \\alpha\\, u^{(t)}\n",
        "    ,\\ \n",
        "    0\n",
        "  \\right)\n",
        "\\\\\n",
        "&=\n",
        "  \\beta^{(t)}_{j^{(t)}} - \\alpha\\, u^{(t)}.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Portanto:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\beta_{\\text{reg}}^{(t+1)}\n",
        "&=\n",
        "  \\beta^{(t)} - \\alpha\\, u^{(t)} \\,\\text{onehot}(j^{(t)})\n",
        "\\\\\n",
        "&=\n",
        "  \\beta^{(t+1)}.\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiHy_0NIPT5f"
      },
      "source": [
        "### Operador de proximidade cujo ponto fixo é a estimativa de máxima verossimilhança regularizada\n",
        "\n",
        "Para entender (2), primeiro observe (confira a [Wikipedia](#3)) que, para qualquer $\\gamma > 0$, a regra de atualização\n",
        "\n",
        "$$\n",
        "  \\left(\\beta_{\\text{exact-prox}, \\gamma}^{(t+1)}\\right)_{j^{(t)}}\n",
        ":=\n",
        "  \\text{prox}_{\\gamma \\lVert \\cdot \\rVert_1}\n",
        "  \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}} + \\frac{\\gamma}{r_{\\text{L1}}}\n",
        "    \\left(\n",
        "      \\left(\n",
        "        \\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "      \\right)_{\\beta = \\beta^{(t)}}\n",
        "    \\right)_{j^{(t)}}\n",
        "  \\right)\n",
        "$$\n",
        "\n",
        "satisfaz (2), em que $\\text{prox}$ é o operador de proximidade (confira <a>Yu</a>, em que esse operador é denotado como $\\mathsf{P}$). O lado direito da equação acima é computado [aqui](#4):\n",
        "\n",
        "$$\n",
        "  \\left(\\beta_{\\text{exact-prox}, \\gamma}^{(t+1)}\\right)_{j^{(t)}}\n",
        "=\n",
        "  \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}}\n",
        "    +\n",
        "    \\frac{\\gamma}{r_{\\text{L1}}}\n",
        "    \\left(\n",
        "      \\left(\n",
        "        \\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "      \\right)_{\\beta = \\beta^{(t)}}\n",
        "    \\right)_{j^{(t)}}\n",
        "    ,\\ \n",
        "    \\gamma\n",
        "  \\right).\n",
        "$$\n",
        "\n",
        "Especificamente, ao definir $\\gamma = \\gamma^{(t)} = -\\frac{\\alpha\\, r_{\\text{L1}}}{\\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}}}$ (observe que (note that $\\gamma^{(t)} > 0$, desde que a log-verossimilhança negativa seja convexa), obtemos a regra de atualização:\n",
        "\n",
        "$$\n",
        "  \\left(\\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\\right)_{j^{(t)}}\n",
        "=\n",
        "  \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}}\n",
        "    -\n",
        "    \\alpha\n",
        "    \\frac{\n",
        "      \\left(\n",
        "        \\left(\n",
        "          \\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y})\n",
        "        \\right)_{\\beta = \\beta^{(t)}}\n",
        "      \\right)_{j^{(t)}}\n",
        "    }{\n",
        "      \\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}}\n",
        "    }\n",
        "    ,\\ \n",
        "    \\gamma^{(t)}\n",
        "  \\right).\n",
        "$$\n",
        "\n",
        "Em seguida, substitua o gradiente exato $\\left( \\nabla_\\beta\\, \\ell(\\beta \\,;\\, \\mathbf{x}, \\mathbf{y}) \\right)_{\\beta = \\beta^{(t)}}$ por sua aproximação $s^{(t)}$, obtendo:\n",
        "\n",
        "\\begin{align*}\n",
        "  \\left(\\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\\right)_{j^{(t)}}\n",
        "&\\approx\n",
        "  \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}}\n",
        "    -\n",
        "    \\alpha\n",
        "    \\frac{\n",
        "      \\left(s^{(t)}\\right)_{j^{(t)}}\n",
        "    }{\n",
        "      \\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}}\n",
        "    }\n",
        "    ,\\ \n",
        "    \\gamma^{(t)}\n",
        "  \\right)\n",
        "\\\\\n",
        "&=\n",
        "    \\text{SoftThreshold} \\left(\n",
        "    \\beta^{(t)}_{j^{(t)}}\n",
        "    -\n",
        "    \\alpha\\,\n",
        "    u^{(t)}\n",
        "    ,\\ \n",
        "    \\gamma^{(t)}\n",
        "  \\right).\n",
        "\\end{align*}\n",
        "\n",
        "Portanto:\n",
        "\n",
        "$$\n",
        "  \\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\n",
        "\\approx\n",
        "  \\beta_{\\text{reg}}^{(t+1)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7YOOrmI8j0L"
      },
      "source": [
        "# Derivação de fatos sobre modelos lineares generalizados\n",
        "\n",
        "Nesta seção, apresentamos maiores detalhes e derivamos os resultados sobre modelos lineares generalizados que foram usados nas seções anteriores. Em seguida, usamos `gradients` do TensorFlow para verificar numericamente as fórmulas derivadas para o gradiente da log-verossimilhança e da informação de Fisher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkHZyhuAIW-p"
      },
      "source": [
        "## Pontuação e informação de Fisher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbyYy0bE8pOK"
      },
      "source": [
        "Considere uma família de distribuições de probabilidade parametrizadas pelo vetor de parâmetros $\\theta$, com densidades de probabilidade $\\left\\{p(\\cdot | \\theta)\\right\\}_{\\theta \\in \\mathcal{T}}$. A <strong>pontuação</strong> de um resultado $y$ no vetor de parâmetros $\\theta_0$ é definida como o gradiente da log-verossimilhança de $y$ (avaliada em $\\theta_0$), isto é:\n",
        "\n",
        "$$\n",
        "\\text{score}(y, \\theta_0) := \\left[\\nabla_\\theta\\, \\log p(y | \\theta)\\right]_{\\theta=\\theta_0}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYGaMPIx8uOc"
      },
      "source": [
        "### Afirmação: a expectativa da pontuação é zero\n",
        "\n",
        "Sob condições de regularidade fracas (permitindo passar a diferenciação sob a integral),\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\text{score}(Y, \\theta_0)\\right] = 0.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3H-wNmJ800R"
      },
      "source": [
        "#### Prova\n",
        "\n",
        "Temos:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\text{score}(Y, \\theta_0)\\right]\n",
        "&:=\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\left(\\nabla_\\theta \\log p(Y|\\theta)\\right)_{\\theta=\\theta_0}\\right] \\\\\n",
        "&\\stackrel{\\text{(1)}}{=} \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\frac{\\left(\\nabla_\\theta p(Y|\\theta)\\right)_{\\theta=\\theta_0}}{p(Y|\\theta=\\theta_0)}\\right] \\\\\n",
        "&\\stackrel{\\text{(2)}}{=} \\int_{\\mathcal{Y}} \\left[\\frac{\\left(\\nabla_\\theta p(y|\\theta)\\right)_{\\theta=\\theta_0}}{p(y|\\theta=\\theta_0)}\\right] p(y | \\theta=\\theta_0)\\, dy \\\\\n",
        "&= \\int_{\\mathcal{Y}} \\left(\\nabla_\\theta p(y|\\theta)\\right)_{\\theta=\\theta_0}\\, dy \\\\\n",
        "&\\stackrel{\\text{(3)}}{=} \\left[\\nabla_\\theta \\left(\\int_{\\mathcal{Y}} p(y|\\theta)\\, dy\\right) \\right]_{\\theta=\\theta_0} \\\\\n",
        "&\\stackrel{\\text{(4)}}{=} \\left[\\nabla_\\theta\\, 1 \\right]_{\\theta=\\theta_0} \\\\\n",
        "&= 0,\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "em que usamos: (1) regra da cadeia para diferenciação, (2) definição da expectativa, (3) passagem da diferenciação sob o sinal da integral (usando as condições de regularidade), (4) a integral de uma densidade de probabilidade é igual a 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1DPVOI9OT2"
      },
      "source": [
        "### Afirmação (informação de Fisher): a variância da pontuação é igual à Hessiana esperada negativa da log-verossimilhança\n",
        "\n",
        "Sob condições de regularidade fracas (permitindo passar a diferenciação sob a integral),\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "\\text{score}(Y, \\theta_0) \\text{score}(Y, \\theta_0)^\\top\n",
        "\\right]\n",
        "=\n",
        "-\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "\\left(\\nabla_\\theta^2 \\log p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "em que $\\nabla_\\theta^2 F$ denota a matriz Hessiana, cuja entrada $(i, j)$ é $\\frac{\\partial^2 F}{\\partial \\theta_i \\partial \\theta_j}$.\n",
        "\n",
        "O lado esquerdo dessa equação é chamado de **informação de Fisher** da família $\\left\\{p(\\cdot | \\theta)\\right\\}_{\\theta \\in \\mathcal{T}}$ no vetor de parâmetros $\\theta_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF-ac0Bk-HmR"
      },
      "source": [
        "#### Prova da afirmação\n",
        "\n",
        "Temos:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "\\left(\\nabla_\\theta^2 \\log p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "\\right]\n",
        "&\\stackrel{\\text{(1)}}{=} \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\left(\\nabla_\\theta^\\top \\frac{\n",
        "    \\nabla_\\theta p(Y | \\theta)\n",
        "  }{\n",
        "    p(Y|\\theta)\n",
        "  }\\right)_{\\theta=\\theta_0}\n",
        "\\right] \\\\\n",
        "&\\stackrel{\\text{(2)}}{=} \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\n",
        "  -\n",
        "  \\left(\\frac{\n",
        "    \\left(\\nabla_\\theta\\, p(Y|\\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\\right)\n",
        "  \\left(\\frac{\n",
        "    \\left(\\nabla_\\theta\\, p(Y|\\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\\right)^\\top\n",
        "\\right] \\\\\n",
        "&\\stackrel{\\text{(3)}}{=} \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\n",
        "  -\n",
        "  \\text{score}(Y, \\theta_0)\n",
        "  \\,\\text{score}(Y, \\theta_0)^\\top\n",
        "\\right],\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "em que usamos: (1) regra da cadeia para diferenciação, (2) regra do quociente para diferenciação, (3) regra da cadeia novamente, mas reversa.\n",
        "\n",
        "Para completar a prova, é suficiente demonstrar que:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\n",
        "\\right]\n",
        "\\stackrel{\\text{?}}{=}\n",
        "0.\n",
        "$$\n",
        "\n",
        "Para fazer isso, passamos a diferenciação sob o sinal de integral duas vezes:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(Y|\\theta=\\theta_0)\n",
        "  }\n",
        "\\right]\n",
        "&= \\int_{\\mathcal{Y}}\n",
        "  \\left[\n",
        "  \\frac{\n",
        "    \\left(\\nabla^2_\\theta p(y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  }{\n",
        "    p(y|\\theta=\\theta_0)\n",
        "  }\n",
        "  \\right]\n",
        "  \\, p(y | \\theta=\\theta_0)\\, dy \\\\\n",
        "&= \\int_{\\mathcal{Y}}\n",
        "  \\left(\\nabla^2_\\theta p(y | \\theta)\\right)_{\\theta=\\theta_0}\n",
        "  \\, dy \\\\\n",
        "&= \\left[\n",
        "    \\nabla_\\theta^2 \\left(\n",
        "      \\int_{\\mathcal{Y}} p(y | \\theta) \\, dy\n",
        "    \\right)\n",
        "  \\right]_{\\theta=\\theta_0} \\\\\n",
        "&= \\left[\n",
        "    \\nabla_\\theta^2 \\, 1\n",
        "  \\right]_{\\theta=\\theta_0} \\\\\n",
        "&= 0.\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAIJfX7IX_lP"
      },
      "source": [
        "### Lema sobre a derivada da função de partição logarítmica\n",
        "\n",
        "Se $a$, $b$ e $c$ são funções de valor escalar, $c$ sendo diferenciável duas vezes, de tal forma que a família de distribuições $\\left\\{p(\\cdot | \\theta)\\right\\}_{\\theta \\in \\mathcal{T}}$ definida por\n",
        "\n",
        "$$\n",
        "p(y|\\theta) = a(y) \\exp\\left(b(y)\\, \\theta - c(\\theta)\\right)\n",
        "$$\n",
        "\n",
        "satisfaça as condições de regularidade fracas que permitem passar a diferenciação com relação a $\\theta$ sob uma integral com relação a $y$, então:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right]\n",
        "= c'(\\theta_0)\n",
        "$$\n",
        "\n",
        "e\n",
        "\n",
        "$$\n",
        "\\text{Var}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right]\n",
        "= c''(\\theta_0).\n",
        "$$\n",
        "\n",
        "(Aqui, $'$ denota a diferenciação, então $c'$ e $c''$ são a primeira e segunda derivadas de $c$. )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYBH-KwpfWhr"
      },
      "source": [
        "#### Prova\n",
        "\n",
        "Para essa família de distribuições, temos $\\text{score}(y, \\theta_0) = b(y) - c'(\\theta_0)$. A primeira equação vem então do fato de que $\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\text{score}(y, \\theta_0) \\right] = 0$. Em seguida, temos:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Var}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right]\n",
        "&= \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\left(b(Y) - c'(\\theta_0)\\right)^2 \\right] \\\\\n",
        "&= \\text{the one entry of } \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\text{score}(y, \\theta_0) \\text{score}(y, \\theta_0)^\\top \\right] \\\\\n",
        "&= \\text{the one entry of } -\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\left(\\nabla_\\theta^2 \\log p(\\cdot | \\theta)\\right)_{\\theta=\\theta_0} \\right] \\\\\n",
        "&= -\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ -c''(\\theta_0) \\right] \\\\\n",
        "&= c''(\\theta_0).\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYpWUvvKcX-e"
      },
      "source": [
        "## Família de exponenciais superdispersadas\n",
        "\n",
        "Uma **família de exponenciais superdispersadas** (escalares) é uma família cujas densidades assumem a forma de:\n",
        "\n",
        "$$\n",
        "p_{\\text{OEF}(m,  T)}(y\\, |\\, \\theta, \\phi) = m(y, \\phi) \\exp\\left(\\frac{\\theta\\, T(y) - A(\\theta)}{\\phi}\\right),\n",
        "$$\n",
        "\n",
        "em que $m$ e $T$ são funções de valor escalar conhecidas, e $\\theta$ e $\\phi$ são parâmetros escalares.\n",
        "\n",
        "*[Observe que $A$ é sobredeterminada: para qualquer $\\phi_0$, a função $A$ é completamente determinada pela restrição de que $\\int p_{\\text{OEF}(m, T)}(y\\ |\\ \\theta, \\phi=\\phi_0)\\, dy = 1$ para todos os $\\theta$. Todos os $A$ gerados por diferentes valores de $\\phi_0$ devem ser iguais, o que coloca uma restrição nas funções $m$ e $T$.]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgpoijwPf7TV"
      },
      "source": [
        "### Média e variância da estatística suficiente\n",
        "\n",
        "Sob as mesmas condições que em \"Lema sobre a derivada da função de partição logarítmica\" acima, temos:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[\n",
        "T(Y)\n",
        "\\right]\n",
        "=\n",
        "A'(\\theta)\n",
        "$$\n",
        "\n",
        "e\n",
        "\n",
        "$$\n",
        "\\text{Var}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[\n",
        "T(Y)\n",
        "\\right]\n",
        "=\n",
        "\\phi A''(\\theta).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyf51flphGOK"
      },
      "source": [
        "#### Prova\n",
        "\n",
        "De acordo com \"Lema sobre a derivada da função de partição logarítmica\", temos:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[\n",
        "\\frac{T(Y)}{\\phi}\n",
        "\\right]\n",
        "=\n",
        "\\frac{A'(\\theta)}{\\phi}\n",
        "$$\n",
        "\n",
        "e\n",
        "\n",
        "$$\n",
        "\\text{Var}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[\n",
        "\\frac{T(Y)}{\\phi}\n",
        "\\right]\n",
        "=\n",
        "\\frac{A''(\\theta)}{\\phi}.\n",
        "$$\n",
        "\n",
        "O resultado vem então do fato de que a expectativa é linear ($\\mathbb{E}[aX] = a\\mathbb{E}[X]$), e a variância é homogênea de grau 2 ($\\text{Var}[aX] = a^2 \\,\\text{Var}[X]$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYOnAZv9d4XH"
      },
      "source": [
        "## Modelo linear generalizado\n",
        "\n",
        "Em um modelo linear generalizado, uma distribuição preditiva para a variável de resposta $Y$ está associada a um vetor de preditores observados $x$. A distribuição é membro de uma família de exponenciais superdispersas, e o parâmetro $\\theta$ é substituído por $h(\\eta)$, em que $h$ é uma função conhecida, $\\eta := x^\\top \\beta$ é a <strong>resposta linear</strong>, e $\\beta$ é um vetor de parâmetros (coeficientes de regressão) a serem aprendidos. Em geral, o parâmetro de dispersão $\\phi$ também pode ser aprendido, mas, em nossa configuração, tratamos $\\phi$ como conhecido. Então, temos:\n",
        "\n",
        "$$\n",
        "Y \\sim p_{\\text{OEF}(m, T)}(\\cdot\\, |\\, \\theta = h(\\eta), \\phi)\n",
        "$$\n",
        "\n",
        "em que a estrutura do modelo é caracterizada pela distribuição $p_{\\text{OEF}(m, T)}$ e pela função $h$, que converte a resposta linear em parâmetros.\n",
        "\n",
        "Tradicionalmente, o mapeamento da resposta linear $\\eta$  para a média $\\mu := \\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot\\, |\\, \\theta = h(\\eta), \\phi)}\\left[ Y\\right]$ é denotado assim:\n",
        "\n",
        "$$\n",
        "\\mu = g^{-1}(\\eta).\n",
        "$$\n",
        "\n",
        "Esse mapeamento precisa ser um-para-um obrigatoriamente, e seu mapeamento inverso, $g$, é chamado de **função de ligação** para esse modelo linear generalizado. Tipicamente, descrevemos um modelo linear generalizado nomeando sua função de ligação e sua família de distribuições – por exemplo: um \"modelo linear generalizado com distribuição de Bernoulli e função de ligação logit\" (também conhecido como modelo de regressão logística). Para caracterizar totalmente o modelo linear generalizado, a função $h$ também precisa ser especificada. Se $h$ é a identidade, então dizemos que $g$ é a **função de ligação canônica**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-mrWHH2-wtv"
      },
      "source": [
        "### Afirmação: expressando $h'$ em termos da estatística suficiente\n",
        "\n",
        "Definir\n",
        "\n",
        "$$\n",
        "{\\text{Mean}_T}(\\eta)\n",
        ":=\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(\\eta), \\phi)} \\left[\n",
        "  T(Y)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "e\n",
        "\n",
        "$$\n",
        "{\\text{Var}_T}(\\eta)\n",
        ":=\n",
        "\\text{Var}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(\\eta), \\phi)} \\left[\n",
        "  T(Y)\n",
        "\\right].\n",
        "$$\n",
        "\n",
        "Então, temos:\n",
        "\n",
        "$$\n",
        "h'(\\eta) = \\frac{\\phi\\, {\\text{Mean}_T}'(\\eta)}{{\\text{Var}_T}(\\eta)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z36iGKlf_-3F"
      },
      "source": [
        "#### Prova\n",
        "\n",
        "De acordo com \"Média e variância da estatística suficiente\", temos:\n",
        "\n",
        "$$\n",
        "{\\text{Mean}_T}(\\eta) = A'(h(\\eta)).\n",
        "$$\n",
        "\n",
        "Fazendo a diferenciação com a regra da cadeia, obtemos $$ {\\text{Mean}_T}'(\\eta) = A''(h(\\eta))\\, h'(\\eta), $$\n",
        "\n",
        "e, de acordo com \"Média e variância da estatística suficiente\", temos:\n",
        "\n",
        "$$\n",
        "\\cdots = \\frac{1}{\\phi} {\\text{Var}_T}(\\eta)\\ h'(\\eta).\n",
        "$$\n",
        "\n",
        "A conclusão vem daí."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8LV_QHPx-wV"
      },
      "source": [
        "## Ajuste dos parâmetros de modelos lineares generalizados de acordo com os dados\n",
        "\n",
        "As propriedades derivadas acima são excelentes para ajustar os parâmetros $\\beta$ de modelos lineares generalizados de acordo com um dataset. Os métodos quase-Newton, como a pontuação de Fisher, dependem do gradiente da log-verossimilhança e da informação de Fisher, que agora podemos computar de uma forma particularmente eficiente para um modelo linear generalizado.\n",
        "\n",
        "Digamos que tenhamos os vetores de preditores observados $x_i$ e as respostas escalares associadas $y_i$. Na forma de matriz, vamos dizer que observamos os preditores $\\mathbf{x}$ e a resposta $\\mathbf{y}$, em que $\\mathbf{x}$ é a matriz cuja $i$ésima linha é $x_i^\\top$, e $\\mathbf{y}$ é o vetor cujo $i$ésimo elemento é $y_i$. A log-verossimilhança dos parâmetros $\\beta$ é, portanto:\n",
        "\n",
        "$$\n",
        "\\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{N} \\log p_{\\text{OEF}(m, T)}(y_i\\, |\\, \\theta = h(x_i^\\top \\beta), \\phi).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aghNxiO_HFW1"
      },
      "source": [
        "### Para uma única amostra de dados\n",
        "\n",
        "Para simplificar a notação, vamos considerar primeiro o caso de um único ponto de dados, $N=1$; em seguida, vamos estender para o caso geral por aditividade.\n",
        "\n",
        "#### Gradiente\n",
        "\n",
        "Temos:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\ell(\\beta\\, ;\\, x, y)\n",
        "&= \\log p_{\\text{OEF}(m, T)}(y\\, |\\, \\theta = h(x^\\top \\beta), \\phi) \\\\\n",
        "&= \\log m(y, \\phi) + \\frac{\\theta\\, T(y) - A(\\theta)}{\\phi}, \\quad\\text{where}\\  \\theta = h(x^\\top \\beta).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Portanto, pela regra da cadeia:\n",
        "\n",
        "$$\n",
        "\\nabla_\\beta \\ell(\\beta\\, ; \\, x, y) = \\frac{T(y) - A'(\\theta)}{\\phi}\\, h'(x^\\top \\beta)\\, x.\n",
        "$$\n",
        "\n",
        "Separadamente, de acordo com \"Média e variância da estatística suficiente\", temos $A'(\\theta) = {\\text{Mean}_T}(x^\\top \\beta)$. Portanto, de acordo com \"Afirmação: expressando $h'$ em termos da estatística suficiente\", temos:\n",
        "\n",
        "$$\n",
        "\\cdots =\n",
        "  \\left(T(y) - {\\text{Mean}_T}(x^\\top \\beta)\\right)\n",
        "  \\frac{{\\text{Mean}_T}'(x^\\top \\beta)}{{\\text{Var}_T}(x^\\top \\beta)}\n",
        "  \\,x.\n",
        "$$\n",
        "\n",
        "#### Hessiana\n",
        "\n",
        "Fazendo a diferenciação uma segunda vez pela regra do produto, obtemos:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla_\\beta^2 \\ell(\\beta\\, ;\\, x, y)\n",
        "&=\n",
        "  \\left[\n",
        "    -A''(h(x^\\top \\beta))\\, h'(x^\\top \\beta)\n",
        "  \\right]\n",
        "  h'(x^\\top \\beta)\\, x x^\\top\n",
        "  +\n",
        "  \\left[\n",
        "    T(y) - A'(h(x^\\top \\beta))\n",
        "  \\right]\n",
        "  h''(x^\\top \\beta)\\, xx^\\top\n",
        "  ] \\\\\n",
        "&=\n",
        "  \\left(\n",
        "    -{\\text{Mean}_T}'(x^\\top \\beta)\\, h'(x^\\top \\beta)\n",
        "    +\n",
        "    \\left[T(y) - A'(h(x^\\top \\beta))\\right]\n",
        "  \\right)\\, x x^\\top.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "#### Informação de Fisher\n",
        "\n",
        "De acordo com \"Média e variância da estatística suficiente\", temos:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x^\\top \\beta), \\phi)} \\left[\n",
        "T(y) - A'(h(x^\\top \\beta))\n",
        "\\right] = 0.\n",
        "$$\n",
        "\n",
        "Portanto:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x^\\top \\beta), \\phi)} \\left[\n",
        "  \\nabla_\\beta^2 \\ell(\\beta\\, ;\\, x, y)\n",
        "\\right]\n",
        "&=\n",
        "  -{\\text{Mean}_T}'(x^\\top \\beta)\\, h'(x^\\top \\beta) x x^\\top \\\\\n",
        "&=\n",
        "  -\\frac{\\phi\\, {\\text{Mean}_T}'(x^\\top \\beta)^2}{{\\text{Var}_T}(x^\\top \\beta)}\\, x x^\\top.\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrA1A583HOng"
      },
      "source": [
        "### Para diversas amostras de dados\n",
        "\n",
        "Agora, estendemos o caso de $N=1$ para o caso geral. Seja $\\boldsymbol{\\eta} := \\mathbf{x} \\beta$ o vetor cuja $i$ésima coordenada é a resposta linear da $i$ésima amostra de dados. Seja $\\mathbf{T}$ (respectivamente, ${\\textbf{Mean}_T}$, respectivamente, ${\\textbf{Var}_T}$) a função (vetorizada) difundida que aplica a função de valor escalar $T$ (respectivamente, ${\\text{Mean}_T}$, respectivamente, ${\\text{Var}_T}$) a cada coordenada. Então, temos:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla_\\beta \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{y})\n",
        "&= \\sum_{i=1}^{N} \\nabla_\\beta \\ell(\\beta\\, ;\\, x_i, y_i) \\\\\n",
        "&= \\sum_{i=1}^{N}\n",
        "  \\left(T(y) - {\\text{Mean}_T}(x_i^\\top \\beta)\\right)\n",
        "  \\frac{{\\text{Mean}_T}'(x_i^\\top \\beta)}{{\\text{Var}_T}(x_i^\\top \\beta)}\n",
        "  \\, x_i \\\\\n",
        "&=\n",
        "  \\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\\frac{\n",
        "      {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta)\n",
        "    }\\right)\n",
        "  \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta)\\right) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "e\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{Y_i \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta), \\phi)} \\left[\n",
        "  \\nabla_\\beta^2 \\ell(\\beta\\, ;\\, \\mathbf{x}, \\mathbf{Y})\n",
        "\\right]\n",
        "&= \\sum_{i=1}^{N} \\mathbb{E}_{Y_i \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta), \\phi)} \\left[\n",
        "  \\nabla_\\beta^2 \\ell(\\beta\\, ;\\, x_i, Y_i)\n",
        "\\right] \\\\\n",
        "&= \\sum_{i=1}^{N}\n",
        "  -\\frac{\\phi\\, {\\text{Mean}_T}'(x_i^\\top \\beta)^2}{{\\text{Var}_T}(x_i^\\top \\beta)}\\, x_i x_i^\\top \\\\\n",
        "&=\n",
        "  -\\mathbf{x}^\\top\n",
        "  \\,\\text{diag}\\left(\n",
        "    \\frac{\n",
        "      \\phi\\, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)^2\n",
        "    }{\n",
        "      {\\textbf{Var}_T}(\\mathbf{x} \\beta)\n",
        "    }\\right)\\,\n",
        "  \\mathbf{x},\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "em que as frações denotam a divisão com elementos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUrOmdt395hZ"
      },
      "source": [
        "## Verificação das fórmulas numericamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVp59IBW-TK6"
      },
      "source": [
        "Agora, verificamos numericamente a fórmula acima para o gradiente da log-verossimilhança usando `tf.gradients`, e verificamos a fórmula para a informação de Fisher com uma estimativa de Monte Carlo usando `tf.hessians`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM-HDPdPepE-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coordinatewise relative error between naively computed gradients and formula-based gradients (should be zero):\n",
            "[[2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]]\n",
            "\n",
            "Coordinatewise relative error between average of naively computed Hessian and formula-based FIM (should approach zero as num_trials -> infinity):\n",
            "[[0.00072369 0.00072369 0.00072369]\n",
            " [0.00072369 0.00072369 0.00072369]\n",
            " [0.00072369 0.00072369 0.00072369]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def VerifyGradientAndFIM():\n",
        "  model = tfp.glm.BernoulliNormalCDF()\n",
        "  model_matrix = np.array([[1., 5, -2],\n",
        "                           [8, -1, 8]])\n",
        "\n",
        "  def _naive_grad_and_hessian_loss_fn(x, response):\n",
        "    # Computes gradient and Hessian of negative log likelihood using autodiff.\n",
        "    predicted_linear_response = tf.linalg.matvec(model_matrix, x)\n",
        "    log_probs = model.log_prob(response, predicted_linear_response)\n",
        "    grad_loss = tf.gradients(-log_probs, [x])[0]\n",
        "    hessian_loss = tf.hessians(-log_probs, [x])[0]\n",
        "    return [grad_loss, hessian_loss]\n",
        "\n",
        "  def _grad_neg_log_likelihood_and_fim_fn(x, response):\n",
        "    # Computes gradient of negative log likelihood and Fisher information matrix\n",
        "    # using the formulas above.\n",
        "    predicted_linear_response = tf.linalg.matvec(model_matrix, x)\n",
        "    mean, variance, grad_mean = model(predicted_linear_response)\n",
        "\n",
        "    v = (response - mean) * grad_mean / variance\n",
        "    grad_log_likelihood = tf.linalg.matvec(model_matrix, v, adjoint_a=True)\n",
        "    w = grad_mean**2 / variance\n",
        "\n",
        "    fisher_info = tf.linalg.matmul(\n",
        "        model_matrix,\n",
        "        w[..., tf.newaxis] * model_matrix,\n",
        "        adjoint_a=True)\n",
        "    return [-grad_log_likelihood, fisher_info]\n",
        "\n",
        "  @tf.function(autograph=False)\n",
        "  def compute_grad_hessian_estimates():\n",
        "    # Monte Carlo estimate of E[Hessian(-LogLikelihood)], where the expectation is\n",
        "    # as written in \"Claim (Fisher information)\" above.\n",
        "    num_trials = 20\n",
        "    trial_outputs = []\n",
        "    np.random.seed(10)\n",
        "    model_coefficients_ = np.random.random(size=(model_matrix.shape[1],))\n",
        "    model_coefficients = tf.convert_to_tensor(model_coefficients_)\n",
        "    for _ in range(num_trials):\n",
        "      # Sample from the distribution of `model`\n",
        "      response = np.random.binomial(\n",
        "          1,\n",
        "          scipy.stats.norm().cdf(np.matmul(model_matrix, model_coefficients_))\n",
        "      ).astype(np.float64)\n",
        "      trial_outputs.append(\n",
        "          list(_naive_grad_and_hessian_loss_fn(model_coefficients, response)) +\n",
        "          list(\n",
        "              _grad_neg_log_likelihood_and_fim_fn(model_coefficients, response))\n",
        "      )\n",
        "\n",
        "    naive_grads = tf.stack(\n",
        "        list(naive_grad for [naive_grad, _, _, _] in trial_outputs), axis=0)\n",
        "    fancy_grads = tf.stack(\n",
        "        list(fancy_grad for [_, _, fancy_grad, _] in trial_outputs), axis=0)\n",
        "\n",
        "    average_hess = tf.reduce_mean(tf.stack(\n",
        "        list(hess for [_, hess, _, _] in trial_outputs), axis=0), axis=0)\n",
        "    [_, _, _, fisher_info] = trial_outputs[0]\n",
        "    return naive_grads, fancy_grads, average_hess, fisher_info\n",
        "  \n",
        "  naive_grads, fancy_grads, average_hess, fisher_info = [\n",
        "      t.numpy() for t in compute_grad_hessian_estimates()]\n",
        "\n",
        "  print(\"Coordinatewise relative error between naively computed gradients and\"\n",
        "        \" formula-based gradients (should be zero):\\n{}\\n\".format(\n",
        "            (naive_grads - fancy_grads) / naive_grads))\n",
        "\n",
        "  print(\"Coordinatewise relative error between average of naively computed\"\n",
        "        \" Hessian and formula-based FIM (should approach zero as num_trials\"\n",
        "        \" -> infinity):\\n{}\\n\".format(\n",
        "                (average_hess - fisher_info) / average_hess))\n",
        "    \n",
        "VerifyGradientAndFIM()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAiNubQ-WDHN"
      },
      "source": [
        "# Referências\n",
        "\n",
        "<a name=\"1\"></a>[1]: Guo-Xun Yuan, Chia-Hua Ho e Chih-Jen Lin. An Improved GLMNET for L1-regularized Logistic Regression (GLMNET aprimorada para regressão logística com regularização L1). *Journal of Machine Learning Research*, 13, 2012. http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf\n",
        "\n",
        "<a name=\"2\"></a>[2]: skd. Derivation of Soft Thresholding Operator (Derivação de operador de soft threshold).  2018. https://math.stackexchange.com/q/511106\n",
        "\n",
        "<a name=\"3\"></a>[3]: Contribuidores da Wikipedia. Proximal gradient methods for learning (Métodos do gradiente proximal para aprendizado). *Wikipedia, The Free Encyclopedia*, 2018. https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n",
        "\n",
        "<a name=\"4\"></a>[4]: Yao-Liang Yu. The Proximity Operator (Operador de proximidade). https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Generalized_Linear_Models.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
