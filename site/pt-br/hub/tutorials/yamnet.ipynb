{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laa9tRjJ59bl"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "T4ZHtBpK6Dom"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk5u_9KN1m-t"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/yamnet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/hub/tutorials/yamnet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/hub/tutorials/yamnet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/hub/tutorials/yamnet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelo do TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ep-q7k_5R-"
      },
      "source": [
        "# Classificação de áudio com o YAMNet\n",
        "\n",
        "O YAMNet é uma rede profunda que prevê 521 [classes](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv) de eventos de áudio a partir do [corpus AudioSet-YouTube](http://g.co/audioset) com o qual foi treinado. Ele emprega a arquitetura de convolução separável com reconhecimento de profundidade [Mobilenet_v1](https://arxiv.org/pdf/1704.04861.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bteu7pfkpt_f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSVs3zRrrYmY"
      },
      "source": [
        "Vamos carregar o modelo a partir do TensorFlow Hub.\n",
        "\n",
        "Observação: para ler a documentação, basta acessar a [URL](https://tfhub.dev/google/yamnet/1) do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX8Vzs6EpwMo"
      },
      "outputs": [],
      "source": [
        "# Load the model.\n",
        "model = hub.load('https://tfhub.dev/google/yamnet/1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxWx6tOdtdBP"
      },
      "source": [
        "O arquivo de rótulos será carregado a partir dos ativos dos modelos e está presente em `model.class_map_path()`. Você vai carregá-lo na variável `class_names`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHSToAW--o4U"
      },
      "outputs": [],
      "source": [
        "# Find the name of the class with the top score when mean-aggregated across frames.\n",
        "def class_names_from_csv(class_map_csv_text):\n",
        "  \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
        "  class_names = []\n",
        "  with tf.io.gfile.GFile(class_map_csv_text) as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      class_names.append(row['display_name'])\n",
        "\n",
        "  return class_names\n",
        "\n",
        "class_map_path = model.class_map_path().numpy()\n",
        "class_names = class_names_from_csv(class_map_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSFjRwkZ59lU"
      },
      "source": [
        "Adicione um método para verificar se o áudio tem a taxa de amostragem adequada (16 kHz) e convertê-lo. Caso contrário, os resultados do modelo seriam afetados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LizGwWjc5w6A"
      },
      "outputs": [],
      "source": [
        "def ensure_sample_rate(original_sample_rate, waveform,\n",
        "                       desired_sample_rate=16000):\n",
        "  \"\"\"Resample waveform if required.\"\"\"\n",
        "  if original_sample_rate != desired_sample_rate:\n",
        "    desired_length = int(round(float(len(waveform)) /\n",
        "                               original_sample_rate * desired_sample_rate))\n",
        "    waveform = scipy.signal.resample(waveform, desired_length)\n",
        "  return desired_sample_rate, waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZEgCobA9bWl"
      },
      "source": [
        "## Download e preparação do arquivo de áudio\n",
        "\n",
        "Você vai baixar um arquivo .wav e ouvi-lo. Caso você já tenha um arquivo disponível, basta fazer upload no Colab e usá-lo.\n",
        "\n",
        "Observação: o arquivo de áudio esperado deve ser um arquivo .wav mono com taxa de amostragem de 16 kHz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzZHvyTtsJrc"
      },
      "outputs": [],
      "source": [
        "!curl -O https://storage.googleapis.com/audioset/speech_whistling2.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8LKmqvGzZzr"
      },
      "outputs": [],
      "source": [
        "!curl -O https://storage.googleapis.com/audioset/miaow_16k.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo9KJb-5zuz1"
      },
      "outputs": [],
      "source": [
        "# wav_file_name = 'speech_whistling2.wav'\n",
        "wav_file_name = 'miaow_16k.wav'\n",
        "sample_rate, wav_data = wavfile.read(wav_file_name, 'rb')\n",
        "sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
        "\n",
        "# Show some basic information about the audio.\n",
        "duration = len(wav_data)/sample_rate\n",
        "print(f'Sample rate: {sample_rate} Hz')\n",
        "print(f'Total duration: {duration:.2f}s')\n",
        "print(f'Size of the input: {len(wav_data)}')\n",
        "\n",
        "# Listening to the wav file.\n",
        "Audio(wav_data, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9I290COsMBm"
      },
      "source": [
        "O `wav_data` precisa ser normalizado para valores no intervalo `[-1.0, 1.0]` (conforme indicado na [documentação](https://tfhub.dev/google/yamnet/1) do modelo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKr78aCBsQo3"
      },
      "outputs": [],
      "source": [
        "waveform = wav_data / tf.int16.max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Xwd4GPuMsB"
      },
      "source": [
        "## Execução do modelo\n",
        "\n",
        "Agora a parte fácil: usando os dados já preparados, basta chamar o modelo e obter: as pontuações, o embedding e o espectrograma.\n",
        "\n",
        "A pontuação é o principal resultado que você usará. O espectrograma será usado para visualizações posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJGP6r-At_Jc"
      },
      "outputs": [],
      "source": [
        "# Run the model, check the output.\n",
        "scores, embeddings, spectrogram = model(waveform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmo7griQprDk"
      },
      "outputs": [],
      "source": [
        "scores_np = scores.numpy()\n",
        "spectrogram_np = spectrogram.numpy()\n",
        "infered_class = class_names[scores_np.mean(axis=0).argmax()]\n",
        "print(f'The main sound is: {infered_class}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj2xLf-P_ndS"
      },
      "source": [
        "## Visualização\n",
        "\n",
        "O YAMNet também retorna informações adicionais que podemos usar para visualização. Vamos conferir a forma de onda, o espectrograma e as principais classes inferidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QSTkmv7wr2M"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the waveform.\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(waveform)\n",
        "plt.xlim([0, len(waveform)])\n",
        "\n",
        "# Plot the log-mel spectrogram (returned by the model).\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.imshow(spectrogram_np.T, aspect='auto', interpolation='nearest', origin='lower')\n",
        "\n",
        "# Plot and label the model output scores for the top-scoring classes.\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "top_n = 10\n",
        "top_class_indices = np.argsort(mean_scores)[::-1][:top_n]\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.imshow(scores_np[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
        "\n",
        "# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n",
        "# values from the model documentation\n",
        "patch_padding = (0.025 / 2) / 0.01\n",
        "plt.xlim([-patch_padding-0.5, scores.shape[0] + patch_padding-0.5])\n",
        "# Label the top_N classes.\n",
        "yticks = range(0, top_n, 1)\n",
        "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
        "_ = plt.ylim(-0.5 + np.array([top_n, 0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "yamnet.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
