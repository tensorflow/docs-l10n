{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic4_occAAiAT"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "ioaprt5q5US7"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yCl0eTNH5RS3"
      },
      "outputs": [],
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet                                  # IGNORE_COPYRIGHT: cleared by OSS licensing\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# Classificação de texto com avaliações de filmes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf2_text_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/hub/tutorials/tf2_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/hub/tutorials/tf2_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/hub/tutorials/tf2_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/collections/nnlm/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelos do TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "Este notebook classifica avaliações de filmes como *positivas* ou *negativas*, com base no texto da avaliação. Este é um exemplo de classificação *binária*, ou de duas classes, um tipo de problema de aprendizado de máquina importante, com diversas aplicações.\n",
        "\n",
        "Usaremos o [Large Movie Review Dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb), que contém o texto de 50 mil avaliações de filmes do [Internet Movie Database](https://www.imdb.com/). Elas são divididas em 25 mil avaliações para treinamento e 25 mil para teste. Os conjuntos de treinamento e teste são *equilibrados*, ou seja, contêm a mesma quantidade de avaliações positivas e negativas.\n",
        "\n",
        "Este notebook usa [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras), uma API de alto nível para criar e treinar modelos no TensorFlow, e o  [TensorFlow Hub](https://www.tensorflow.org/hub), uma biblioteca e plataforma para aprendizado por transferência. Para ver um tutorial de classificação de texto mais avançado usando `tf.keras`, confira o [Guia de Classificação de texto do MLCC](https://developers.google.com/machine-learning/guides/text-classification/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrk8NjzhSBh-"
      },
      "source": [
        "### Outros modelos\n",
        "\n",
        "Você encontra [aqui](https://tfhub.dev/s?module-type=text-embedding) modelos mais expressivos e com melhor desempenho que podem ser usados para gerar o embedding de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4DN769E2O_R"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ew7HTbPpCJH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "source": [
        "## Baixe o dataset do IMDB\n",
        "\n",
        "O dataset do IMDB está disponível no [TensorFlow Datasets](https://github.com/tensorflow/datasets). O seguinte código baixa o dataset do IMDB para sua máquina (ou para o runtime do Colab):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXXx5Oc3pOmN"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], \n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l50X3GfjpU4r"
      },
      "source": [
        "## Explore os dados\n",
        "\n",
        "Vamos reservar um momento para compreender o formato dos dados. Cada exemplo é uma frase representando a avaliação do filme e um rótulo correspondente. A frase não é pré-processada de nenhuma forma. O rótulo é um número inteiro igual a 0 ou 1, em que 0 é uma avaliação negativa e 1, uma avaliação positiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8qCnve_-lkO"
      },
      "outputs": [],
      "source": [
        "print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnKvHWW4-lkW"
      },
      "source": [
        "Vamos exibir os primeiros 10 exemplos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtTS4kpEpjbi"
      },
      "outputs": [],
      "source": [
        "train_examples[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFtaCHTdc-GY"
      },
      "source": [
        "Vamos exibir também os primeiros 10 rótulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvAjVXOWc6Mj"
      },
      "outputs": [],
      "source": [
        "train_labels[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "## Criação do modelo\n",
        "\n",
        "A rede neural é criada empilhando-se camadas, o que requer três decisões de arquitetura principais:\n",
        "\n",
        "- Como representar o texto?\n",
        "- Quantas camadas usar no modelo?\n",
        "- Quantas *unidades ocultas* usar em cada camada?\n",
        "\n",
        "Neste exemplo, os dados de entrada são frases. Os rótulos a serem previstos são 0 ou 1.\n",
        "\n",
        "Uma maneira de representar o texto é converter as frases em vetores de embeddings. Use um embedding de texto pré-treinado como a primeira camada, o que traz duas vantagens:\n",
        "\n",
        "- Não precisamos nos preocupar com o pré-processamento do texto.\n",
        "- Podemos nos beneficiar do aprendizado por transferência.\n",
        "\n",
        "Neste exemplo, você usará um modelo do [TensorFlow Hub](https://www.tensorflow.org/hub) chamado [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2).\n",
        "\n",
        "Existem dois outros modelos a serem testados neste tutorial:\n",
        "\n",
        "- [google/nnlm-en-dim50-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim50-with-normalization/2) – igual a [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2), mas com uma normalização de texto adicional para remover pontuações.  Isso pode ajudar a ter uma melhor cobertura dos embeddings do vocabulário para tokens do texto de entrada.\n",
        "- [google/nnlm-en-dim128-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2) – um modelo maior, com uma dimensão de embedding igual a 128, em vez de 50 do modelo menor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In2nDpTLkgKa"
      },
      "source": [
        "Primeiro, vamos criar uma camada do Keras que use um modelo do TensorFlow Hub para fazer o embedding das frases e testar em alguns exemplos de entrada. O formato da saída dos embeddings gerados esperado é: `(num_examples, embedding_dimension)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NUbzVeYkgcO"
      },
      "outputs": [],
      "source": [
        "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfSbV6igl1EH"
      },
      "source": [
        "Agora, vamos criar o modelo completo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpKOoWgu-llD"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "As camadas são empilhadas sequencialmente para construir o classificador:\n",
        "\n",
        "1. A primeira é uma camada do TensorFlow Hub. Essa camada usa um SavedModel pré-treinado para mapear uma frase em seu vetor de embeddings. O modelo que estamos usando ([google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2)) divide a frase em tokens, faz o embedding de cada token e depois combina o embedding. As dimensões resultantes são: `(num_examples, embedding_dimension)`.\n",
        "2. O vetor de saída com tamanho fixo é passado por uma camada (`Dense`) totalmente conectada com 16 unidades ocultas.\n",
        "3. A última camada é densamente conectada com apenas um nó de saída. Isso gera como saída os logits: as log-probabilidades da classe verdadeira, segundo o modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XMwnDOp-llH"
      },
      "source": [
        "### Unidades ocultas\n",
        "\n",
        "O modelo acima possui duas camadas intermediárias ou \"ocultas\", entre a entrada e a saída. O número de saídas (unidades, nós ou neurônios) é a dimensão do espaço representacional da camada. Em outras palavras, a quantidade de liberdade permitida à rede ao aprender uma representação interna.\n",
        "\n",
        "Se um modelo tiver mais unidades ocultas (um espaço de representação de dimensão superior) e/ou mais camadas, a rede poderá aprender representações mais complexas. No entanto, isso deixa a rede mais cara do ponto de vista computacional e pode levar ao aprendizado de padrões indesejados – padrões que melhoram o desempenho para os dados de treinamento, mas não para os dados de teste. Isto é chamado de *overfitting*, e falaremos mais sobre o tema posteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### Função de perda e otimizador\n",
        "\n",
        "Um modelo precisa de uma função de perda e um otimizador para o treinamento. Como este é um problema de classificação binária e o modelo gera como saída uma probabilidade (uma camada de unidade única com uma ativação sigmoide), usaremos a função de perda `binary_crossentropy`.\n",
        "\n",
        "Essa não é a única opção para a função de perda. Por exemplo, você pode optar por `mean_squared_error`. Porém, de forma geral, `binary_crossentropy` lida melhor com probabilidades, pois mede a \"distância\" entre distribuições de probabilidade ou, no nosso caso, entre a distribuição de verdade fundamental (ground-truth) e as previsões.\n",
        "\n",
        "Depois, quando lidarmos com problemas de regressão (por exemplo, para prever o preço de uma casa), veremos como usar outra função de perda chamada Erro Quadrático Médio.\n",
        "\n",
        "Agora, configure o modelo para usar um otimizador e uma função de perda:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCWYwkug-llQ"
      },
      "source": [
        "## Crie um conjunto de validação\n",
        "\n",
        "Durante o treinamento, queremos verificar a exatidão do modelo para dados que ele nunca viu. Crie um *dataset de validação* separando 10 mil exemplos dos dados de treinamento originais. (Por que não usar o dataset de testes agora? Nosso objetivo é desenvolver e ajustar o modelo usando apenas os dados de treinamento e, em seguida, usar os dados de teste apenas uma vez para avaliar a exatidão)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NpcXY9--llS"
      },
      "outputs": [],
      "source": [
        "x_val = train_examples[:10000]\n",
        "partial_x_train = train_examples[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "## Treine o modelo\n",
        "\n",
        "Treine o modelo com 10 épocas em minilotes com 512 amostras. São feitas 10 iterações em todas as amostras nos tensores `x_train` e `y_train`. Durante o treinamento, monitore a perda e a exatidão do modelo para as 10 mil amostras do conjunto de validação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXSGrjWZ-llW"
      },
      "outputs": [],
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "## Avalie o modelo\n",
        "\n",
        "Vamos conferir o desempenho do modelo. Serão retornados dois valores: perda (um número que representa o erro; quanto menor, melhor) e exatidão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(test_examples, test_labels)\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1iEXVTR0Z2t"
      },
      "source": [
        "Essa estratégia bem simples atinge uma exatidão de cerca de 87%. Com estratégias mais avançadas, a exatidão do modelo deve se aproximar de 95%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KggXVeL-llZ"
      },
      "source": [
        "## Crie um gráfico de exatidão e perda ao longo do tempo\n",
        "\n",
        "`model.fit()` retorna um objeto `History` que contém um dicionário com tudo o que aconteceu durante o treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcvSXvhp-llb"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRKsqL40-lle"
      },
      "source": [
        "Há quatro entradas: uma para cada métrica monitorada durante o treinamento e a validação. Você usará esses valores para plotar a perda do treinamento e da validação para fins comparativos, além da exatidão do treinamento e da validação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGoYf2Js-lle"
      },
      "outputs": [],
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hXx-xOv-llh"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEmZ5zq-llk"
      },
      "source": [
        "Neste gráfico, os pontos representam a perda e exatidão do treinamento, enquanto as linhas sólidas representam a perda e exatidão da validação.\n",
        "\n",
        "Observe que a perda do treinamento *diminui* a cada época, e a exatidão do treinamento *aumenta* a cada época. Isso é o esperado ao usar uma otimização do método do gradiente descendente, que deve minimizar a quantidade desejada em cada iteração.\n",
        "\n",
        "Esse não é o caso para a perda e exatidão de validação, que parecem atingir o pico após 20 épocas. Este é um exemplo de  overfitting: o modelo tem desempenho melhor com os dados de treinamento em comparação a dados nunca vistos. Após esse ponto, o modelo sofre uma sobreotimização e aprende representações *específicas* dos dados de treinamento que não oferecem boas *generalizações* para os dados de teste.\n",
        "\n",
        "Para este caso específico, podemos evitar o overfitting simplesmente interrompendo o treinamento após aproximadamente 20 épocas. Mais para a frente, você verá como fazer isso automaticamente com um callback."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tf2_text_classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
