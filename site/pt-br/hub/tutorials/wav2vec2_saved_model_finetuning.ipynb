{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCs7P9JTMlzV"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqn-HYw-Mkea"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stRetE8gMlmZ"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/wav2vec2_saved_model_finetuning\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/wav2vec2_saved_model_finetuning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/hub/tutorials/wav2vec2_saved_model_finetuning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/hub/tutorials/wav2vec2_saved_model_finetuning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelo do TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndG8MjmJeicp"
      },
      "source": [
        "# Ajustes finos de Wav2Vec2 com um head de modelagem de linguagem\n",
        "\n",
        "Neste notebook, vamos carregar o modelo Wav2Vec2 pré-treinado a partir do [TF Hub](https://tfhub.dev) e fazer os ajustes finos com o  [dataset LibriSpeech](https://huggingface.co/datasets/librispeech_asr), anexando um head de modelagem de linguagem (LM, na sigla em inglês) por cima do modelo pré-treinado. A tarefa subjacente é criar um modelo para **reconhecimento automático de fala**, isto é, dada uma fala, o modelo deve conseguir transcrevê-la em texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWk8nL6Ui-_0"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Antes de executar este notebook, confirme se você está usando o runtime de GPU (`Runtime` &gt; `Change runtime type` &gt; `GPU`). A célula abaixo instala o pacote [`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2) e suas dependências."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seqTlMyeZvM4"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main\n",
        "!sudo apt-get install -y libsndfile1-dev\n",
        "!pip3 install -q SoundFile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuJL8-f0zn5"
      },
      "source": [
        "## Configuração do modelo usando o `TF Hub`\n",
        "\n",
        "Vamos começar importando alguns módulos/bibliotecas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3_fgx4eZvM7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from wav2vec2 import Wav2Vec2Config\n",
        "\n",
        "config = Wav2Vec2Config()\n",
        "\n",
        "print(\"TF version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0rVUxyWsS5f"
      },
      "source": [
        "Primeiro, vamos baixar o modelo do TF Hub e encapsular a assinatura do modelo com [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer)  para podermos usar este modelo como uma camada do Keras. Felizmente, `hub.KerasLayer` consegue fazer isso com apenas uma linha.\n",
        "\n",
        "**Observação:** ao carregar o modelo com `hub.KerasLayer`, ele se torna um pouco opaco, mas, às vezes, precisamos de controles mais finos do modelo, então podemos carregá-lo com  `tf.keras.models.load_model(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO6QRC7KZvM9"
      },
      "outputs": [],
      "source": [
        "pretrained_layer = hub.KerasLayer(\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\", trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCputyVBv2e9"
      },
      "source": [
        "Caso tenha interesse no script de exportação do modelo, confira  [aqui](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/export2hub.py). O objeto `pretrained_layer` é a versão congelada de [`Wav2Vec2Model`](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/wav2vec2/modeling.py). Esses pesos pré-treinados foram convertidos a partir dos [pesos pré-treinados](https://huggingface.co/facebook/wav2vec2-base) de HuggingFace PyTorch usando [este script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/convert_torch_to_tf.py).\n",
        "\n",
        "Originalmente, Wav2Vec2 foi pré-treinado com uma estratégia de modelagem de linguagem mascarada, com o objetivo de identificar a representação de fala latente quantizada verdadeira para o timestep mascarado. Leia mais sobre o objetivo do treinamento no artigo  [wav2vec 2.0 – Framework para aprendizado autossupervisionado de representações de fala](https://arxiv.org/abs/2006.11477)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SseDnCr7hyhC"
      },
      "source": [
        "Agora, vamos definir algumas constantes e hiperparâmetros, que serão úteis nas próximas células.  `AUDIO_MAXLEN` é definido intencionalmente como `246000`, pois a assinatura do modelo aceita somente um tamanho de sequência estático igual a `246000`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiILuMBERxlO"
      },
      "outputs": [],
      "source": [
        "AUDIO_MAXLEN = 246000\n",
        "LABEL_MAXLEN = 256\n",
        "BATCH_SIZE = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V4gTgGLgXvO"
      },
      "source": [
        "Na célula abaixo, vamos encapsular `pretrained_layer` e uma camada densa (head da LM) com a [API Functional do Keras](https://www.tensorflow.org/guide/keras/functional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3CUN1KEB10Q"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(AUDIO_MAXLEN,))\n",
        "hidden_states = pretrained_layer(inputs)\n",
        "outputs = tf.keras.layers.Dense(config.vocab_size)(hidden_states)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zDXuoMXhDMo"
      },
      "source": [
        "A camada densa (definida acima) tem uma dimensão de saída igual a `vocab_size`, pois queremos prever as probabilidades de cada token do vocabulário em cada timestep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPp18ZHRtnq-"
      },
      "source": [
        "## Configuração do estado do treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATQy1ZK3vFr7"
      },
      "source": [
        "No TensorFlow, os pesos do modelo são criados somente quando `model.call` ou `model.build` é chamado pela primeira vez. Portanto, a célula abaixo cria os pesos do modelo. Além disso, vamos executar  `model.summary()` para verificar o número total de parâmetros treináveis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgL5wyaXZvM-"
      },
      "outputs": [],
      "source": [
        "model(tf.random.uniform(shape=(BATCH_SIZE, AUDIO_MAXLEN)))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxxA4Fevp7m"
      },
      "source": [
        "Agora, precisamos definir a função de perda  (`loss_fn`) e o otimizador para podermos treinar o modelo. A célula abaixo faz isso. Por questões de simplicidade, usaremos o otimizador `Adam`. `CTCLoss` é um tipo de perda comum usado para tarefas (como `ASR`) em que as subpartes da entrada não podem ser alinhadas facilmente às subpartes da saída. Saiba mais sobre CTC-loss nesta incrível [postagem de blog](https://distill.pub/2017/ctc/).\n",
        "\n",
        "`CTCLoss` (do pacote  [`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2)) aceita três argumentos: `config`, `model_input_shape` e `division_factor`. Se `division_factor=1`, então a perda será simplesmente somada; portanto, passe `division_factor` para obter a média para o lote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glDepVEHZvM_"
      },
      "outputs": [],
      "source": [
        "from wav2vec2 import CTCLoss\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "loss_fn = CTCLoss(config, (BATCH_SIZE, AUDIO_MAXLEN), division_factor=BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mvTuOXpwsQe"
      },
      "source": [
        "## Carregamento e pré-processamento dos dados\n",
        "\n",
        "Agora, vamos baixar o dataset LibriSpeech no [site oficial](http://www.openslr.org/12) e configurá-lo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4kIEC77cBCM"
      },
      "outputs": [],
      "source": [
        "!wget https://www.openslr.org/resources/12/dev-clean.tar.gz -P ./data/train/\n",
        "!tar -xf ./data/train/dev-clean.tar.gz -C ./data/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsQpmpn6jrMI"
      },
      "source": [
        "**Observação:** estamos usando a configuração `dev-clean`, pois este notebook foi criado apenas para fins de demonstração, então precisamos de uma pequena quantidade de dados. Os dados de treinamento completos podem  ser baixados no [site do LibriSpeech](http://www.openslr.org/12)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynxAjtGHGFpM"
      },
      "outputs": [],
      "source": [
        "ls ./data/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBMiORo0xJD0"
      },
      "source": [
        "O dataset fica no diretório LibriSpeech. Vamos explorar os arquivos dentro dele."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkIu_Wt4ZvNA"
      },
      "outputs": [],
      "source": [
        "data_dir = \"./data/train/LibriSpeech/dev-clean/2428/83705/\"\n",
        "all_files = os.listdir(data_dir)\n",
        "\n",
        "flac_files = [f for f in all_files if f.endswith(\".flac\")]\n",
        "txt_files = [f for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "print(\"Transcription files:\", txt_files, \"\\nSound files:\", flac_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEObi_Apk3ZD"
      },
      "source": [
        "Cada subdiretório tem diversos arquivos `.flac` e um arquivo `.txt`. O arquivo `.txt` contém as transcrições de texto para todas as amostras de fala (isto é, os  arquivos `.flac`) presentes nesse subdiretório."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYW6WKJflO2e"
      },
      "source": [
        "Podemos carregar os dados de texto da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEBKxQblHPwq"
      },
      "outputs": [],
      "source": [
        "def read_txt_file(f):\n",
        "  with open(f, \"r\") as f:\n",
        "    samples = f.read().split(\"\\n\")\n",
        "    samples = {s.split()[0]: \" \".join(s.split()[1:]) for s in samples if len(s.split()) > 2}\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldkf_ceb0_YW"
      },
      "source": [
        "De maneira similar, vamos definir uma função para carregar uma amostra de fala a partir de um arquivo `.flac`.\n",
        "\n",
        "`REQUIRED_SAMPLE_RATE` é definido como `16000`, pois o Wav2Vec2 foi pré-treinado com frequência de `16K`, e recomenda-se fazer os  ajustes finos sem uma grande mudança na distribuição dos dados devido à frequência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOJ3OzPsTyXv"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "REQUIRED_SAMPLE_RATE = 16000\n",
        "\n",
        "def read_flac_file(file_path):\n",
        "  with open(file_path, \"rb\") as f:\n",
        "      audio, sample_rate = sf.read(f)\n",
        "  if sample_rate != REQUIRED_SAMPLE_RATE:\n",
        "      raise ValueError(\n",
        "          f\"sample rate (={sample_rate}) of your files must be {REQUIRED_SAMPLE_RATE}\"\n",
        "      )\n",
        "  file_id = os.path.split(file_path)[-1][:-len(\".flac\")]\n",
        "  return {file_id: audio}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sxDN8P4nWkW"
      },
      "source": [
        "Agora, vamos escolher algumas amostras aleatórias e tentar visualizá-las."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI5J-2Dfm_wT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "import random\n",
        "\n",
        "file_id = random.choice([f[:-len(\".flac\")] for f in flac_files])\n",
        "flac_file_path, txt_file_path = os.path.join(data_dir, f\"{file_id}.flac\"), os.path.join(data_dir, \"2428-83705.trans.txt\")\n",
        "\n",
        "print(\"Text Transcription:\", read_txt_file(txt_file_path)[file_id], \"\\nAudio:\")\n",
        "Audio(filename=flac_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8jJ7Ed81p_A"
      },
      "source": [
        "Agora, vamos combinar todas as amostras de fala e texto, e também vamos definir a função (na próxima célula) para essa finalidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI-5YCzaTsei"
      },
      "outputs": [],
      "source": [
        "def fetch_sound_text_mapping(data_dir):\n",
        "  all_files = os.listdir(data_dir)\n",
        "\n",
        "  flac_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".flac\")]\n",
        "  txt_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "  txt_samples = {}\n",
        "  for f in txt_files:\n",
        "    txt_samples.update(read_txt_file(f))\n",
        "\n",
        "  speech_samples = {}\n",
        "  for f in flac_files:\n",
        "    speech_samples.update(read_flac_file(f))\n",
        "\n",
        "  assert len(txt_samples) == len(speech_samples)\n",
        "\n",
        "  samples = [(speech_samples[file_id], txt_samples[file_id]) for file_id in speech_samples.keys() if len(speech_samples[file_id]) < AUDIO_MAXLEN]\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx95Lxvu0nT4"
      },
      "source": [
        "Chegou a hora de verificarmos algumas amostras..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ls7X_jqIz4R"
      },
      "outputs": [],
      "source": [
        "samples = fetch_sound_text_mapping(data_dir)\n",
        "samples[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjhSWfsnlCL"
      },
      "source": [
        "Observação: estamos carregando esses dados na memória porque estamos trabalhando com um dataset pequeno neste notebook. Porém, para treinamento usando o dataset completo (cerca de 300 GB), você precisará carregar os dados de maneira lazy. Para mais informações, confira  [este script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/data_utils.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg8Zia1kzw0J"
      },
      "source": [
        "Vamos pré-processar os dados agora.\n",
        "\n",
        "Primeiro, vamos definir o tokenizador e o processador usando o pacote `gsoc-wav2vec2`. Em seguida, vamos fazer um pré-processamento bem simples. O `processor` normalizará a fala bruta em relação ao eixo de frames, e o  `tokenizer` converterá as saídas do modelo em string (usando o vocabulário definido) e cuidará da remoção de tokens especiais (dependendo da configuração do tokenizador)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaat_hMLNVHF"
      },
      "outputs": [],
      "source": [
        "from wav2vec2 import Wav2Vec2Processor\n",
        "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
        "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  label = tokenizer(text)\n",
        "  return tf.constant(label, dtype=tf.int32)\n",
        "\n",
        "def preprocess_speech(audio):\n",
        "  audio = tf.constant(audio, dtype=tf.float32)\n",
        "  return processor(tf.transpose(audio))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyKl8QP-zRFC"
      },
      "source": [
        "Agora, vamos definir o gerador Python para chamar as funções de pré-processamento definidas na células acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoQrRalwMpQ6"
      },
      "outputs": [],
      "source": [
        "def inputs_generator():\n",
        "  for speech, text in samples:\n",
        "    yield preprocess_speech(speech), preprocess_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vlm3ySFULsG"
      },
      "source": [
        "## Configuração do `tf.data.Dataset`\n",
        "\n",
        "A célula abaixo configura o objeto  `tf.data.Dataset` usando seu método `.from_generator(...)`. Usaremos o objeto `generator` definido na célula acima.\n",
        "\n",
        "**Observação:** para treinamento distribuído (especialmente em TPUs), atualmente `.from_generator(...)` não funciona, e recomenda-se treinar usando dados  armazenados no formato `.tfrecord` (idealmente, os TFRecords devem ser armazenados dentro de um bucket do GCS para que as TPUs sejam usadas ao máximo).\n",
        "\n",
        "Confira mais detalhes de como converter dados LibriSpeech em TFRecords [neste script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/make_tfrecords.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbQ_dMwGO62h"
      },
      "outputs": [],
      "source": [
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None),  dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        ")\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(inputs_generator, output_signature=output_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXBbNsRyPyw3"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(flac_files)\n",
        "SEED = 42\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DAUmns3pXfr"
      },
      "source": [
        "Vamos passar o dataset para diversos lotes, então vamos preparar os lotes na célula abaixo. Todas as sequências em um lote devem ser preenchidas até um tamanho constante. Usaremos o método  `.padded_batch(...)` para isso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okhko1IWRida"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(AUDIO_MAXLEN, LABEL_MAXLEN), padding_values=(0.0, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A45CjQG5qSbV"
      },
      "source": [
        "Os aceleradores (como GPUs/TPUs) são muito rápidos e, geralmente, o carregamento dos dados (e o pré-processamento) se torna o gargalo durante o treinamento, já que o carregamento ocorre em CPUs, o que pode aumentar bastante o tempo de treinamento, especialmente quando há muito pré-processamento online ou quando os dados são transmitidos online a partir de buckets do GCS. Para lidar com esses problemas,  `tf.data.Dataset` conta com o método `.prefetch(...)`, que ajuda a preparar os próximos lotes paralelamente (em CPUs) enquanto o modelo está fazendo previsões (em GPUs/TPUs) no lote atual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-bKu2YjRior"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqk2cs6LxVIh"
      },
      "source": [
        "Como este notebook foi criado para fins de demonstração, pegaremos somente os primeiros `num_train_batches` para fazer o treinamento. Porém, sugerimos que você faça o treinamento usando todo o dataset. De maneira similar, vamos avaliar somente `num_val_batches`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6GO5oYUxXtz"
      },
      "outputs": [],
      "source": [
        "num_train_batches = 10\n",
        "num_val_batches = 4\n",
        "\n",
        "train_dataset = dataset.take(num_train_batches)\n",
        "val_dataset = dataset.skip(num_train_batches).take(num_val_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzAOI78tky08"
      },
      "source": [
        "## Treinamento do modelo\n",
        "\n",
        "Para o treinamento do modelo, vamos chamar o método `.fit(...)` diretamente após compilar o modelo com  `.compile(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuBY2sZElgwg"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer, loss=loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qswxafSl0HjO"
      },
      "source": [
        "A célula acima configura o estado do treinamento. Agora, podemos iniciar o treinamento com o método `.fit(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtuSfnj1l-I_"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
        "history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySvp8r2E1q_V"
      },
      "source": [
        "Vamos salvar o modelo com o método `.save(...)` para podermos fazer a inferência posteriormente. Você também pode exportar esse SavedModel para o TF Hub de acordo com a [documentação do TF Hub](https://www.tensorflow.org/hub/publish)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0KEYcwydwjF"
      },
      "outputs": [],
      "source": [
        "save_dir = \"finetuned-wav2vec2\"\n",
        "model.save(save_dir, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkOpp9rZ211t"
      },
      "source": [
        "Observação: definimos `include_optimizer=False`, pois queremos usar este modelo somente para inferência."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJfPlTgezD0i"
      },
      "source": [
        "## Avaliação\n",
        "\n",
        "Agora, vamos computar a Taxa de Erro de Palavras para o dataset de validação.\n",
        "\n",
        "A **Taxa de Erro de Palavras** (WER, na sigla em inglês) é uma métrica comum para mensurar o desempenho de um sistema de reconhecimento automático de fala. O WER é derivado da distância de Levenshtein no nível de palavra. Essa taxa pode ser computada da seguinte forma: WER = (S + D + I) / N =  (S + D + I) / (S + D + C), em que S é o número de substituições, D é o número de exclusões, I é o número de inserções, C é o número de palavras corretas, e N é o número de palavras na referência (N=S+D+C). Esse valor indica a porcentagem de palavras que foram previstas incorretamente.\n",
        "\n",
        "Para saber mais sobre WER, confira [este artigo](https://www.isca-speech.org/archive_v0/interspeech_2004/i04_2765.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_91Y7-r3xu"
      },
      "source": [
        "Vamos usar a função `load_metric(...)` da biblioteca [HuggingFace datasets](https://huggingface.co/docs/datasets/). Primeiro, vamos instalar a biblioteca `datasets` usando `pip` e definir o objeto `metric`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW9F_oVDU1TZ"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q datasets\n",
        "\n",
        "from datasets import load_metric\n",
        "metric = load_metric(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssWXWc7CZvNB"
      },
      "outputs": [],
      "source": [
        "@tf.function(jit_compile=True)\n",
        "def eval_fwd(batch):\n",
        "  logits = model(batch, training=False)\n",
        "  return tf.argmax(logits, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFh1myg1x4ua"
      },
      "source": [
        "Agora está na hora de executar a avaliação para os dados de validação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQTFVjZghckJ"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for speech, labels in tqdm(val_dataset, total=num_val_batches):\n",
        "    predictions  = eval_fwd(speech)\n",
        "    predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "    references = [tokenizer.decode(label, group_tokens=False) for label in labels.numpy().tolist()]\n",
        "    metric.add_batch(references=references, predictions=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCc8qBesv3e"
      },
      "source": [
        "Usamos o método `tokenizer.decode(...)` para decodificar as previsões e os rótulos de volta para texto e adicionamos à métrica para a computação de `WER` posteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI_URj8Wtb2g"
      },
      "source": [
        "Agora, vamos calcular o valor da métrica na célula abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a83wekLgWMod"
      },
      "outputs": [],
      "source": [
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cD1OgVEjl4"
      },
      "source": [
        "**Observação:** aqui, o valor da métrica não faz nenhum sentido, já que o modelo é treinado com poucos dados, e tarefas de reconhecimento automático de fala costumam exigir  muitos dados para aprender um mapeamento de fala para texto. Para conseguir bons resultados, provavelmente você precisará treinar com muitos dados. Este notebook fornece um template para fazer os ajustes finos de um modelo de fala pré-treinado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G14o706kdTE1"
      },
      "source": [
        "## Inferência\n",
        "\n",
        "Agora que estamos satisfeitos com o processo de treinamento e salvamos o modelo em `save_dir`, veremos como ele pode ser usado para inferência.\n",
        "\n",
        "Primeiro, vamos carregar o modelo usando `tf.keras.models.load_model(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrTrExiUdaED"
      },
      "outputs": [],
      "source": [
        "finetuned_model = tf.keras.models.load_model(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luodSroz20SR"
      },
      "source": [
        "Vamos baixar algumas amostras de fala para fazer a inferência. Você pode substituir a amostra abaixo pela sua amostra de fala."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUE0shded6Ej"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/SA2.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycBjU_U53FjL"
      },
      "source": [
        "Agora, vamos ler a amostra de fala usando  `soundfile.read(...)` e preenchê-la até `AUDIO_MAXLEN` para atender à assinatura do modelo. Em seguida, vamos normalizar essa amostra de fala usando a instância de `Wav2Vec2Processor` e vamos alimentá-la ao modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7CARje4d5_H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "speech, _ = sf.read(\"SA2.wav\")\n",
        "speech = np.pad(speech, (0, AUDIO_MAXLEN - len(speech)))\n",
        "speech = tf.expand_dims(processor(tf.constant(speech)), 0)\n",
        "\n",
        "outputs = finetuned_model(speech)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUSttSPa30qP"
      },
      "source": [
        "Vamos decodificar os números de volta na sequência de texto usando a instância de `Wav2Vec2tokenizer` definida acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYdJqxQ4llgI"
      },
      "outputs": [],
      "source": [
        "predictions = tf.argmax(outputs, axis=-1)\n",
        "predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXC757bztJc"
      },
      "source": [
        "Essa previsão é bem aleatória, pois o modelo nunca foi treinado com muitos dados neste notebook (não é objetivo deste notebook fazer um treinamento completo). Você conseguirá boas previsões se treinar este modelo usando o dataset LibriSpeech completo.\n",
        "\n",
        "Chegamos ao fim deste notebook, mas não é o fim do aprendizado de tarefas relacionadas a fala no TensorFlow. Este [repositório](https://github.com/tulasiram58827/TTS_TFLite) contém alguns tutoriais incríveis. Caso você encontre algum bug neste notebook, pedimos que crie um issue  [aqui](https://github.com/vasudevgupta7/gsoc-wav2vec2/issues)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rWk8nL6Ui-_0",
        "wvuJL8-f0zn5",
        "oPp18ZHRtnq-",
        "1mvTuOXpwsQe",
        "7Vlm3ySFULsG",
        "CzAOI78tky08",
        "SJfPlTgezD0i",
        "G14o706kdTE1"
      ],
      "name": "wav2vec2_saved_model_finetuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
