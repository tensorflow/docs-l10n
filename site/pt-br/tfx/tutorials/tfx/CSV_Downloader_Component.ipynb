{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl4XCJN9g8Bc"
      },
      "source": [
        "Copyright 2023 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIUc9Zh3hM6H"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU-hMBZVmyCo"
      },
      "source": [
        "# Tutorial de pipeline TFX para LLM usando o dataset CNN Daily\n",
        "\n",
        "Neste codelab, usaremos o KerasNLP para carregar um Large Language Model (LLM) pré-treinado, modelo GPT-2, e ajustá-lo a um dataset. O dataset usado nesta demonstração é o dataset CNN Daily. Observe que o GPT-2 é usado aqui apenas para demonstrar o processo de ponta a ponta; as técnicas e ferramentas apresentadas neste codelab são potencialmente transferíveis para outros modelos de linguagem generativa, como o Google T5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJAp-HxKiKsE"
      },
      "source": [
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/CSV_Downloader_Component\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "<td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tfx/tutorials/tfx/CSV_Downloader_Component.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "<td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tfx/tutorials/tfx/CSV_Downloader_Component.ipynb\"><img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "<td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tfx/tutorials/tfx/CSV_Downloader_Component.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MK3ryPikKtj"
      },
      "source": [
        "# Antes de começar\n",
        "\n",
        "O Colab oferece diferentes tipos de sistemas de runtime. Certifique-se de acessar **Runtime -&gt; Change runtime type** e escolher o runtime GPU Hardware Accelerator (que deve ter mais de 12GB de RAM de sistema e aproximadamente 15GB de RAM de GPU), pois você fará ajustes finos no modelo GPT-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMmMNdV1jZAS"
      },
      "source": [
        "# Configuração\n",
        "\n",
        "Primeiro instalaremos o pacote TFX Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C23ItymvmVth"
      },
      "source": [
        "## Atualize o Pip\n",
        "\n",
        "Para evitar a atualização do Pip num sistema ao executar localmente, garanta que estamos executando no Colab. Os sistemas locais podem, claro, ser atualizados separadamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfSG5IFamUq7"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te56mTWomdLq"
      },
      "source": [
        "## Instale o TFX\n",
        "\n",
        "Atualmente, o TFX está tendo alguns problemas com Python 3.10 no Colab. Portanto, simplesmente executar o comando\n",
        "\n",
        "```\n",
        "!pip install -U tfx\n",
        "```\n",
        "\n",
        "para instalar o tfx vai **falhar**. Portanto, siga o código abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGlfiX4PmcjZ"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 3\n",
        "curl -O https://bootstrap.pypa.io/get-pip.py\n",
        "python get-pip.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYHRZQjQEcS7"
      },
      "outputs": [],
      "source": [
        "# 1) TFX relies on an old version of google-api-core so we let google-auth float\n",
        "# for the install. We grep it out below:\n",
        "!grep -v google-auth /etc/requirements.core.in > requirements.txt\n",
        "\n",
        "# 2) httplib2 should be included in /etc/requirements.core.in but it's not for\n",
        "# reasons. We ensure it's included:\n",
        "!grep httplib2 /etc/requirements.user.in >> requirements.txt\n",
        "\n",
        "# 3) google.colab package is not available as a wheel. We symlink that in so\n",
        "# it's on the sys.path of Python 3.8:\n",
        "!mkdir /usr/local/lib/python3.8/dist-packages/google\n",
        "!ln -s /usr/local/lib/python3.10/dist-packages/google/colab /usr/local/lib/python3.8/dist-packages/google/colab\n",
        "\n",
        "# Now with those pre-requisites out of the way:\n",
        "!pip install tfx==1.13.0 -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MiV2iFkiqbL"
      },
      "outputs": [],
      "source": [
        "!pip install keras_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZo6NOYQEcS7"
      },
      "source": [
        "# Importações\n",
        "\n",
        "Vamos primeiro tirar nossas importações do caminho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDhX6vgUEcS7"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tfx.types import Channel\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVMFdYDtmgPX"
      },
      "source": [
        "## Desinstale o shapely\n",
        "\n",
        "TODO(b/263441833) Esta é uma solução temporária para evitar um ImportError. Em última análise, isto deverá ser resolvido com suporte a uma versão mais recente do Bigquery, em vez de desinstalar outras dependências extras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4AKpWUiEcS7"
      },
      "outputs": [],
      "source": [
        "!pip uninstall shapely -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJaN_u_8tEwi"
      },
      "source": [
        "## Você reiniciou o runtime?\n",
        "\n",
        "Se você estiver usando o Google Colab, na primeira vez que executar a célula acima, você deve reiniciar o runtime clicando no botão \"RESTART RUNTIME\" acima ou usando o menu \"Runtime &gt; Restart runtime ...\". Isso é necessário devido à maneira como o Colab carrega os pacotes.\n",
        "\n",
        "Verifique as versões do TensorFlow e TFX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fac1XkwrnXW6"
      },
      "source": [
        "Vamos verificar as versões das bibliotecas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNwD6G4TXrlq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnvgEYNwtMhJ"
      },
      "source": [
        "## Configuração de variáveis\n",
        "\n",
        "Existem algumas variáveis ​​usadas para definir um pipeline. Você pode personalizar essas variáveis ​​como desejar. Por padrão, toda a saída do pipeline será gerada no diretório atual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVFcsQhWkbkw"
      },
      "source": [
        "# Componente CSV Downloader\n",
        "\n",
        "Para tornar o pipeline mais eficiente e possível de automação, é útil ter um componente que receba um link de download para o arquivo CSV a ser baixado. Além disso, um objetivo importante do pipeline de ML em produção do TFX é coletar metadados contendo informações sobre os componentes do pipeline, suas execuções e artefatos resultantes. Em outras palavras, o objetivo dos metadados é analisar a linhagem dos componentes do pipeline e depurar problemas, e o componente CSV Downloader ajudaria os usuários a registrar e rastrear informações sobre a origem dos dados e as etapas de pré-processamento pelas quais os dados foram submetidos antes de entrar no pipeline. Nesta seção, declaramos um novo artefato chamado CSVdoc e desenvolvemos um componente personalizado – o CSV Downloader – que armazena informações sobre o dataset e baixa o arquivo CSV na URI do artefato CSVdoc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc1JTbjjo0bd"
      },
      "outputs": [],
      "source": [
        "from tfx.types import artifact\n",
        "from tfx import types\n",
        "\n",
        "Property = artifact.Property\n",
        "PropertyType = artifact.PropertyType\n",
        "\n",
        "URL_PROPERTY = Property(type=PropertyType.STRING)\n",
        "PATH_PROPERTY = Property(type=PropertyType.STRING)\n",
        "\n",
        "class CsvDoc(types.Artifact):\n",
        "  \"\"\" Artifact that contains the CSV dataset.\n",
        "\n",
        "     - 'url' : saves the source of the original data.\n",
        "     - 'path': saves the path to the CSV file.\n",
        "  \"\"\"\n",
        "\n",
        "  TYPE_NAME = 'CsvDoc'\n",
        "  PROPERTIES = {\n",
        "      'url' : URL_PROPERTY,\n",
        "      'path': PATH_PROPERTY,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qks2al5X1Us"
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "import requests\n",
        "import os\n",
        "import tfx.v1 as tfx\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "\n",
        "@tfx.dsl.components.component\n",
        "def CsvDownloaderComponent(\n",
        "    url: tfx.dsl.components.Parameter[str],\n",
        "    file_name: tfx.dsl.components.Parameter[str],\n",
        "    saved_file: tfx.dsl.components.OutputArtifact[CsvDoc],\n",
        ") -> None:\n",
        "  response = requests.get(url)\n",
        "  saved_file.url = url\n",
        "  if response.status_code == 200:\n",
        "    file_path = os.path.join(saved_file.uri, file_name)\n",
        "    saved_file.path = file_path\n",
        "    url_content = response.content\n",
        "    with open(file_path, 'wb') as csv_file:\n",
        "      csv_file.write(url_content)\n",
        "    logging.info(f\"CSV file saved successfully at {file_path}\")\n",
        "  else:\n",
        "    raise Exception(\"CSV file failed to be saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D3O4L6hYBBt"
      },
      "outputs": [],
      "source": [
        "downloader = CsvDownloaderComponent(\n",
        "  url = 'https://drive.google.com/uc?id=1YdZsJlRafqxiNSl0nHQkwR7rzrNlN9LI&export=download', file_name ='testing_doc.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGm5cG6cYE10"
      },
      "outputs": [],
      "source": [
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "context = InteractiveContext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHpBtrduYG7U"
      },
      "outputs": [],
      "source": [
        "context.run(downloader, enable_cache = False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CSV_Downloader_Component.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
