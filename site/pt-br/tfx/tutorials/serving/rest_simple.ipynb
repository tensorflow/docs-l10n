{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhoQ0WE77laV"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Treine e sirva um modelo do TensorFlow com o TensorFlow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6FwTNtl3S4v"
      },
      "source": [
        "**Importante: este notebook foi projetado para ser executado apenas num Google Colab**. Ele instala pacotes no sistema e requer acesso root. Se você quiser executá-lo num notebook Jupyter local, proceda com cuidado.\n",
        "\n",
        "Observação: você pode executar este exemplo agora mesmo num notebook estilo Jupyter, sem necessidade de configuração! Basta clicar em \"Executar no Google Colab\"\n",
        "\n",
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<tr>\n",
        "<td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/serving/rest_simple\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "<td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tfx/tutorials/serving/rest_simple.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "<td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tfx/tutorials/serving/rest_simple.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"> Ver fonte no GitHub</a>\n",
        "</td>\n",
        "<td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tfx/tutorials/serving/rest_simple.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</tr>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbVhjPpzn6BM"
      },
      "source": [
        "Este guia treina um modelo de rede neural para classificar [imagens de roupas, como tênis e camisas](https://github.com/zalandoresearch/fashion-mnist), salva o modelo treinado e depois o disponibiliza com o [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving). O foco está no TensorFlow Serving, em vez da modelagem e treinamento no TensorFlow, portanto, para obter um exemplo completo dedicado à modelagem e treinamento, veja o [Exemplo Básico de Classificação](https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/r1/tutorials/keras/basic_classification.ipynb).\n",
        "\n",
        "Este guia usa o [tf.keras](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/keras.ipynb), uma API de alto nível, para criar e treinar modelos no TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWkuJabJSKGB"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Confirm that we're using Python 3\n",
        "assert sys.version_info.major == 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "# TensorFlow and tf.keras\n",
        "print(\"Installing dependencies for Colab environment\")\n",
        "!pip install -Uq grpcio==1.26.0\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "print('TensorFlow version: {}'.format(tf.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jAk1ZXqTJqN"
      },
      "source": [
        "## Crie seu modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "### Importe o dataset Fashion MNIST\n",
        "\n",
        "Este guia usa o dataset [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) que contém 70.000 imagens em tons de cinza em 10 categorias. As imagens mostram peças de roupa individuais em baixa resolução (28 por 28 pixels), como pode ser visto aqui:\n",
        "\n",
        "<table>\n",
        "  <tr><td>     <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\" width=\"600\" alt=\"Fashion MNIST sprite\">\n",
        "</td></tr>\n",
        "  <tr><td align=\"center\">     <b>Figura 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Amostras Fashion-MNIST</a> (por Zalando, licença MIT).<br>\n",
        "</td></tr>\n",
        "</table>\n",
        "\n",
        "O Fashion MNIST pretende ser um substituto imediato para o dataset clássico [MNIST](http://yann.lecun.com/exdb/mnist/) - frequentemente usado como o \"Olá, mundo\" em programas de aprendizado de máquina para visão computacional. Você pode acessar o Fashion MNIST diretamente do TensorFlow, basta importar e carregar os dados.\n",
        "\n",
        "Observação: Embora sejam realmente imagens, elas são carregadas como matrizes NumPy e não como objetos de imagem binária."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MqDQO0KCaWS"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# scale the values to 0.0 to 1.0\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# reshape for feeding into the model\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
        "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDu7OX8Nf5PY"
      },
      "source": [
        "### Treine e avalie seu modelo\n",
        "\n",
        "Vamos usar a CNN mais simples possível, já que não estamos focados na parte de modelagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTNN0ANGgA36"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
        "                      strides=2, activation='relu', name='Conv1'),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10, name='Dense')\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "testing = False\n",
        "epochs = 5\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "model.fit(train_images, train_labels, epochs=epochs)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('\\nTest accuracy: {}'.format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwGPItyphqXT"
      },
      "source": [
        "## Salve seu modelo\n",
        "\n",
        "Para carregar nosso modelo treinado no TensorFlow Serving, primeiro precisamos salvá-lo no formato [SavedModel](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model). Isto criará um arquivo protobuf numa hierarquia de diretórios bem definida e incluirá um número de versão. [O TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) nos permite selecionar qual versão de um modelo, ou \"servable\" (\"servível\"), queremos usar quando fazemos solicitações de inferência. Cada versão será exportada para um subdiretório diferente no caminho fornecido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w5Rq8SsgWE6"
      },
      "outputs": [],
      "source": [
        "# Fetch the Keras session and save the model\n",
        "# The signature definition is defined by the input and output tensors,\n",
        "# and stored with the default serving key\n",
        "import tempfile\n",
        "\n",
        "MODEL_DIR = tempfile.gettempdir()\n",
        "version = 1\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "print('export_path = {}\\n'.format(export_path))\n",
        "\n",
        "tf.keras.models.save_model(\n",
        "    model,\n",
        "    export_path,\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        ")\n",
        "\n",
        "print('\\nSaved model:')\n",
        "!ls -l {export_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM7B_RuDYoIj"
      },
      "source": [
        "## Examine seu modelo salvo\n",
        "\n",
        "Usaremos o utilitário de linha de comando `saved_model_cli` para examinar os [MetaGraphDefs](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef) (os modelos) e [SignatureDefs](https://www.tensorflow.org/tfx/serving/signature_defs) (os métodos que você pode chamar) em nosso SavedModel. Veja [esta discussão sobre a CLI SavedModel](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel) no Guia do TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU4GDF_aYtfQ"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir {export_path} --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSPWuegUb7Eo"
      },
      "source": [
        "Isto nos diz bastante sobre o nosso modelo! Neste caso acabamos de treinar nosso modelo, então já conhecemos as entradas e saídas, mas se não o fizéssemos, esta seria uma informação importante. Isto não nos diz tudo, como o fato de que se trata de dados de imagens em tons de cinza, por exemplo, mas é um ótimo começo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBgsyhytS6KD"
      },
      "source": [
        "## Sirva seu modelo com o TensorFlow Serving\n",
        "\n",
        "**Importante: Se você NÃO estiver executando isso num Google Colab,** as células a seguir instalarão pacotes no sistema com acesso root. Se você quiser executá-lo num notebook Jupyter local, proceda com cuidado.\n",
        "\n",
        "### Adicione a URI de distribuição do TensorFlow Serving como origem do pacote:\n",
        "\n",
        "Estamos nos preparando para instalar o TensorFlow Serving usando o [Aptitude](https://wiki.debian.org/Aptitude), já que este Colab é executado em um ambiente Debian. Adicionaremos o pacote `tensorflow-model-server` à lista de pacotes que o Aptitude conhece. Observe que estamos executando como root.\n",
        "\n",
        "Observação: este exemplo executa o TensorFlow Serving de forma nativa, mas [você também pode executá-lo num container Docker](https://www.tensorflow.org/tfx/serving/docker), que é uma das maneiras mais fáceis de começar a usar o TensorFlow Serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2hF_ChoOrEd"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# We need sudo prefix if not on a Google Colab.\n",
        "if 'google.colab' not in sys.modules:\n",
        "  SUDO_IF_NEEDED = 'sudo'\n",
        "else:\n",
        "  SUDO_IF_NEEDED = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWg9X2QHlbGS"
      },
      "outputs": [],
      "source": [
        "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
        "# You would instead do:\n",
        "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
        "\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | {SUDO_IF_NEEDED} tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | {SUDO_IF_NEEDED} apt-key add -\n",
        "!{SUDO_IF_NEEDED} apt update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1ZVp_VOU7Wu"
      },
      "source": [
        "### Instale o TensorFlow Serving\n",
        "\n",
        "Isso é tudo que você precisa: uma única linha de comando!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygwa9AgRloYy"
      },
      "outputs": [],
      "source": [
        "# TODO: Use the latest model server version when colab supports it.\n",
        "#!{SUDO_IF_NEEDED} apt-get install tensorflow-model-server\n",
        "# We need to install Tensorflow Model server 2.8 instead of latest version\n",
        "# Tensorflow Serving >2.9.0 required `GLIBC_2.29` and `GLIBCXX_3.4.26`. Currently colab environment doesn't support latest version of`GLIBC`,so workaround is to use specific version of Tensorflow Serving `2.8.0` to mitigate issue.\n",
        "!wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-2.8.0/t/tensorflow-model-server/tensorflow-model-server_2.8.0_all.deb'\n",
        "!dpkg -i tensorflow-model-server_2.8.0_all.deb\n",
        "!pip3 install tensorflow-serving-api==2.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5NrYdQeVm52"
      },
      "source": [
        "### Comece a executar o TensorFlow Serving\n",
        "\n",
        "É aqui que começamos a executar o TensorFlow Serving e carregamos nosso modelo. Após carregar, podemos começar a fazer solicitações de inferência usando REST. Existem alguns parâmetros importantes:\n",
        "\n",
        "- `rest_api_port`: a porta que você usará para solicitações REST.\n",
        "- `model_name`: você usará isto na URL das solicitações REST. Pode ser qualquer coisa.\n",
        "- `model_base_path`: Este é o caminho para o diretório onde você salvou seu modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUgp3vUdU5GS"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJDhHNJVnaLN"
      },
      "outputs": [],
      "source": [
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=fashion_model \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxbeiOCUUs2z"
      },
      "outputs": [],
      "source": [
        "!tail server.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwg1JKaGXWAg"
      },
      "source": [
        "## Faça uma solicitação ao seu modelo no TensorFlow Serving\n",
        "\n",
        "Primeiro, vamos dar uma olhada num exemplo aleatório de nossos dados de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luqm_Jyff9iR"
      },
      "outputs": [],
      "source": [
        "def show(idx, title):\n",
        "  plt.figure()\n",
        "  plt.imshow(test_images[idx].reshape(28,28))\n",
        "  plt.axis('off')\n",
        "  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})\n",
        "\n",
        "import random\n",
        "rando = random.randint(0,len(test_images)-1)\n",
        "show(rando, 'An Example Image: {}'.format(class_names[test_labels[rando]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKnEHeTrbh3L"
      },
      "source": [
        "Ok, isto parece interessante. Você acha difícil reconhecer isso? Agora vamos criar o objeto JSON para um lote de três solicitações de inferência e ver como nosso modelo reconhece as coisas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dsD7KQG1m-R"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()})\n",
        "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReQd4QESIwXN"
      },
      "source": [
        "### Faça solicitações REST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT3J-lHrhOYQ"
      },
      "source": [
        "#### Versão mais recente do servable\n",
        "\n",
        "Enviaremos uma solicitação de previsão como uma solicitação POST para o endpoint REST do nosso servidor e passaremos três exemplos. Pediremos ao nosso servidor que nos forneça a versão mais recente do nosso servable, não especificando uma versão específica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGvFyuIzW6n6"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "!pip install -q requests\n",
        "\n",
        "import requests\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://localhost:8501/v1/models/fashion_model:predict', data=data, headers=headers)\n",
        "predictions = json.loads(json_response.text)['predictions']\n",
        "\n",
        "show(0, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
        "  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[0]], test_labels[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJH8LtM4XELp"
      },
      "source": [
        "#### Uma versão específica do servable\n",
        "\n",
        "Agora vamos especificar uma versão específica do nosso servable. Como só temos um, vamos selecionar a versão 1. Também veremos todos os três resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRftRxeR1tZx"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://localhost:8501/v1/models/fashion_model/versions/1:predict', data=data, headers=headers)\n",
        "predictions = json.loads(json_response.text)['predictions']\n",
        "\n",
        "for i in range(0,3):\n",
        "  show(i, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
        "    class_names[np.argmax(predictions[i])], np.argmax(predictions[i]), class_names[test_labels[i]], test_labels[i]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rest_simple.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
