{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBFXQGKYUc4X"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1z4xy2gTUc4a"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE7KNzPPVrVV"
      },
      "source": [
        "# Classificação de imagens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwQtSOz0VrVX"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/images/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/images/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/images/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN7G9GFmVrVY"
      },
      "source": [
        "Este tutorial mostra como classificar imagens de flores usando um modelo `tf.keras.Sequential` e carregar dados usando `tf.keras.utils.image_dataset_from_directory`. São demonstrados os seguintes conceitos:\n",
        "\n",
        "- Carregamento de um dataset fora do disco de forma eficiente.\n",
        "- Identificação de overfitting e aplicação de técnicas para mitigá-lo, incluindo ampliação de dados e dropout.\n",
        "\n",
        "Este tutorial segue um workflow básico de aprendizado de máquina:\n",
        "\n",
        "1. Analisar e entender os dados\n",
        "2. Criar um pipeline de entrada\n",
        "3. Criar o modelo\n",
        "4. Treinar o modelo\n",
        "5. Testar o modelo\n",
        "6. Melhorar o modelo e repetir o processo\n",
        "\n",
        "Além disso, o notebook demonstra como converter um [modelo salvo](../../../guide/saved_model.ipynb) em um modelo do [TensorFlow Lite](https://www.tensorflow.org/lite/) para aprendizado de máquina em dispositivos móveis, embarcados e IoT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF9uvbXNVrVY"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Importe o TensorFlow e outras bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1WtoaOHVrVh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZZI6lNkVrVm"
      },
      "source": [
        "## Baixar e explorar o dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPHx8-t-VrVo"
      },
      "source": [
        "Este tutorial usa um dataset com cerca de 3.700 fotos de flores. O dataset contém cinco subdiretórios, um por classe:\n",
        "\n",
        "```\n",
        "flower_photo/\n",
        "  daisy/\n",
        "  dandelion/\n",
        "  roses/\n",
        "  sunflowers/\n",
        "  tulips/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57CcilYSG0zv"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
        "data_dir = pathlib.Path(data_dir).with_suffix('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpmywIlsVrVx"
      },
      "source": [
        "Após baixá-lo, você terá uma cópia do dataset disponível. Há 3.670 imagens no total:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbtTDYhOHZb6"
      },
      "outputs": [],
      "source": [
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVmwkOSdHZ5A"
      },
      "source": [
        "Veja algumas rosas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1loMlbYHeiJ"
      },
      "outputs": [],
      "source": [
        "roses = list(data_dir.glob('roses/*'))\n",
        "PIL.Image.open(str(roses[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQbZBOTLHiUP"
      },
      "outputs": [],
      "source": [
        "PIL.Image.open(str(roses[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGEqiBbRHnyI"
      },
      "source": [
        "E algumas tulipas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyQkfPGdHilw"
      },
      "outputs": [],
      "source": [
        "tulips = list(data_dir.glob('tulips/*'))\n",
        "PIL.Image.open(str(tulips[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtlhWJPAHivf"
      },
      "outputs": [],
      "source": [
        "PIL.Image.open(str(tulips[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIjgz7_JIo_m"
      },
      "source": [
        "## Carregar os dados usando um utilitário do Keras\n",
        "\n",
        "Agora, carregue essas imagens fora do disco, utilizando o utilitário conveniente `tf.keras.utils.image_dataset_from_directory`. Isso levará o diretório de flores do disco para um `tf.data.Dataset` com apenas algumas linhas de código. Se você quiser, também pode escrever seu próprio código de carregamento de dados do zero – confira o tutorial [Carregar e pré-processar imagens](../load_data/images.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyDNn9MbIzfT"
      },
      "source": [
        "### Criar um dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anqiK_AGI086"
      },
      "source": [
        "Defina alguns parâmetros para o loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H74l2DoDI2XD"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFBhRrrEI49z"
      },
      "source": [
        "É recomendável usar uma divisão de validação ao desenvolver seu modelo. Use 80% das imagens para treinamento e 20% para validação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIR0kRZiI_AT"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iscU3UoVJBXj"
      },
      "outputs": [],
      "source": [
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLQULyAvJC3X"
      },
      "source": [
        "Os nomes das classes estão disponíveis no atributo `class_names` desses datasets. Eles correspondem aos nomes dos diretórios, em ordem alfabética."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHAxkHX5JD3k"
      },
      "outputs": [],
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uoVvxSLJW9m"
      },
      "source": [
        "## Visualizar os dados\n",
        "\n",
        "Aqui estão as primeiras nove imagens do dataset de treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBmEA9c0JYes"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6BXtXFJdW0"
      },
      "source": [
        "Você passará esses datasets para o método `Model.fit` do Keras para fazer o treinamento mais adiante neste treinamento. Se você quiser, também pode fazer a iteração do dataset manualmente e recuperar lotes de imagens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-MfMoenJi8s"
      },
      "outputs": [],
      "source": [
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj4FrKxxJkoW"
      },
      "source": [
        "O `image_batch` é um tensor de formato `(32, 180, 180, 3)`. É um lote de 32 imagens de formato `180x180x3` (a última dimensão refere-se aos canais de cores RGB). O `label_batch` é um tensor de formato `(32,)`; são os rótulos correspondentes às 32 imagens.\n",
        "\n",
        "Você pode chamar `.numpy()` nos tensores `image_batch` e `labels_batch` para convertê-los em um `numpy.ndarray`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dr0at41KcAU"
      },
      "source": [
        "## Configurar o dataset para melhor desempenho\n",
        "\n",
        "Utilize a pré-busca em buffer para gerar dados a partir do disco sem o bloqueio de I/O. Veja dois métodos importantes que você deve usar ao carregar os dados:\n",
        "\n",
        "- `Dataset.cache` mantém a imagem na memória após o carregamento fora do disco durante a primeira época. Isso garantirá que o dataset não se torne um gargalo ao treinar seu modelo. Se o dataset for muito grande para a memória, você também pode usar esse método para criar um cache no disco com bom desempenho.\n",
        "- `Dataset.prefetch` sobrepõe o pré-processamento de dados e a execução do modelo durante o treinamento.\n",
        "\n",
        "Os leitores interessados podem saber mais sobre ambos os métodos, além de como armazenar os dados em cache no disco, na seção *Pré-busca* do guia [Melhor desempenho com a API tf.data](../../guide/data_performance.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOjJSm7DKoZA"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GUnmPF4JvEf"
      },
      "source": [
        "## Padronizar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e56VXHMWJxYT"
      },
      "source": [
        "Os valores dos canais RGB estão no intervalo `[0, 255]`. Isso não é ideal para uma rede neural. Em geral, você deve buscar diminuir seus valores de entrada.\n",
        "\n",
        "Aqui, você padronizará os valores para colocá-los no intervalo `[0, 1]` usando `tf.keras.layers.Rescaling`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEYxo2CTJvY9"
      },
      "outputs": [],
      "source": [
        "normalization_layer = layers.Rescaling(1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl4RmanbJ4g0"
      },
      "source": [
        "Há duas maneiras de usar essa camada. Você pode aplicá-la ao dataset ao chamar `Dataset.map`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9o9ESaJJ502"
      },
      "outputs": [],
      "source": [
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "first_image = image_batch[0]\n",
        "# Notice the pixel values are now in `[0,1]`.\n",
        "print(np.min(first_image), np.max(first_image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWEOmRSBJ9J8"
      },
      "source": [
        "Ou pode incluir a camada dentro da definição do modelo para simplificar a implantação. Use a segunda estratégia aqui."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsRk1xCwKZR4"
      },
      "source": [
        "Observação: você redimensionou imagens anteriormente usando o argumento `image_size` de `tf.keras.utils.image_dataset_from_directory`. Se quiser incluir a lógica de redimensionamento no seu modelo também, pode usar a camada `tf.keras.layers.Resizing`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcUTyDOPKucd"
      },
      "source": [
        "## Modelo básico do Keras\n",
        "\n",
        "### Criar o modelo\n",
        "\n",
        "O modelo [Sequential](https://www.tensorflow.org/guide/keras/sequential_model) consiste em três blocos de convolução (`tf.keras.layers.Conv2D`) com uma camada de pooling máximo (`tf.keras.layers.MaxPooling2D`) em cada um. Há uma camada totalmente conectada (`tf.keras.layers.Dense`) com 128 unidades sobre ela, que é ativada por uma função de ativação ReLU (`'relu'`). Esse modelo não foi ajustado para alta exatidão — o objetivo deste tutorial é mostrar estratégia padrão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR6argA1K074"
      },
      "outputs": [],
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "model = Sequential([\n",
        "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaKFzz72Lqpg"
      },
      "source": [
        "### Compilar o modelo\n",
        "\n",
        "Neste tutorial, escolha o otimizador `tf.keras.optimizers.Adam` e a função de perda `tf.keras.losses.SparseCategoricalCrossentropy`. Para ver a exatidão do treinamento e da validação para cada época de treinamento, passe o argumento `metrics` para `Model.compile`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jloGNS1MLx3A"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMJ4DnuJL55A"
      },
      "source": [
        "### Resumo do modelo\n",
        "\n",
        "Veja todas as camadas da rede usando o método `Model.summary` do Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llLYH-BXL7Xe"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiYHcbvaL9H-"
      },
      "source": [
        "### Treinar o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j30F69T4sIVN"
      },
      "source": [
        "Treine o modelo com 10 épocas usando o método `Model.fit` do Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fWToCqYMErH"
      },
      "outputs": [],
      "source": [
        "epochs=10\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyFKdQpXMJT4"
      },
      "source": [
        "## Visualizar os resultados do treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFvOvmAmMK9w"
      },
      "source": [
        "Crie gráficos da perda e da exatidão para os conjuntos de treinamento e avaliação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWnopEChMMCn"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO_jT7HwMrEn"
      },
      "source": [
        "Os gráficos mostram que a exatidão do treinamento e a exatidão da validação estão bem distantes entre si, e o modelo atingiu somente cerca de 60% de exatidão para o conjunto de validação.\n",
        "\n",
        "As próximas seções do tutorial mostram como avaliar o que deu errado e tentar aumentar o desempenho geral do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqtyGodAMvNV"
      },
      "source": [
        "## Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixsz9XFfMxcu"
      },
      "source": [
        "Nos gráficos acima, a exatidão do treinamento fica cada vez mais linear ao longo do tempo, enquanto a exatidão da validação fica empacada em 60% no processo de treinamento. Além disso, a diferença de exatidão entre o treinamento e a validação é perceptível, um sinal de [overfitting](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit).\n",
        "\n",
        "Quando há uma pequena quantidade de exemplos de treinamento, às vezes o modelo aprende com ruídos ou detalhes indesejados dos exemplos de treinamento, ao ponto de impactar negativamente o desempenho do modelo para novos exemplos. Esse fenômeno é conhecido como overfitting e significa que o modelo terá dificuldades de generalizar para um novo dataset.\n",
        "\n",
        "Existem diversas formas de combater o overfitting no processo de treinamento. Neste tutorial, usaremos a *ampliação de dados* e adicionaremos *dropout* ao seu modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDMfYqwmM1C-"
      },
      "source": [
        "## Ampliação de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxYwix81M2YO"
      },
      "source": [
        "Geralmente o overfitting ocorre quando há uma pequena quantidade de exemplos de treinamento. A [ampliação de dados](./data_augmentation.ipynb) usa a estratégia de gerar dados adicionais de treinamento a partir dos exemplos existentes ampliando-os por meio do uso de transformações aleatórias que geram imagens que parecem críveis. Isso ajuda a expor o modelo a mais aspectos dos dados e a generalizar de forma melhor.\n",
        "\n",
        "Você implementará a ampliação de dados usando as seguintes camadas de pré-processamento do Keras: `tf.keras.layers.RandomFlip`, `tf.keras.layers.RandomRotation` e `tf.keras.layers.RandomZoom`. Elas podem ser incluídas dentro do seu modelo da mesma forma que outras camadas e podem ser executadas na GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J80BAbIMs21"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(img_height,\n",
        "                                  img_width,\n",
        "                                  3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN4k1dK3S6eV"
      },
      "source": [
        "Veja alguns exemplos ampliados por meio da ampliação de dados na mesma imagem diversas vezes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z90k539S838"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    augmented_images = data_augmentation(images)\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsjXCBLYYNs5"
      },
      "source": [
        "Você adicionará a ampliação de dados ao seu modelo antes do treinamento no próximo passo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeD3bXepYKXs"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "Outra técnica para reduzir o overfitting é acrescentar regularização de [dropout](https://developers.google.com/machine-learning/glossary#dropout_regularization){:.external} à rede.\n",
        "\n",
        "Ao aplicar dropout em uma camada, ela descarta aleatoriamente (definindo a ativação como zero) uma quantidade de unidades de saída da camada durante o processo de treinamento. O dropout recebe um número fracionário como valor de entrada, como 0.1, 0.2, 0.4, etc. Isso significa descartar 10%, 20% ou 40%, respectivamente, das unidades de saída da camada aplicada, de forma aleatória.\n",
        "\n",
        "Crie uma nova rede neural com `tf.keras.layers.Dropout` antes de treiná-la usando as imagens ampliadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Zeg8zsqXCsm"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "  data_augmentation,\n",
        "  layers.Rescaling(1./255),\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes, name=\"outputs\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4nEcuqgZLbi"
      },
      "source": [
        "## Compilar e treinar o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvyAINs9ZOmJ"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWLkKoKjZSoC"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWS-vvNaZDag"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkdl8VsBbZOu"
      },
      "source": [
        "## Visualizar os resultados do treinamento\n",
        "\n",
        "Após aplicar a ampliação de dados e o `tf.keras.layers.Dropout`, há menos overfitting do que antes, e a exatidão do treinamento e da validação fica mais alinhada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dduoLfKsZVIA"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtv5VbaVb-3W"
      },
      "source": [
        "## Fazer previsões com base em novos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10buWpJbcCQz"
      },
      "source": [
        "Use seu modelo para classificar uma imagem que não estava incluída nos conjuntos de treinamento e validação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKgMZ4bDcHf7"
      },
      "source": [
        "Observação: as camadas de ampliação de dados e de dropout ficam inativas no momento da inferência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC40sRITBSsQ"
      },
      "outputs": [],
      "source": [
        "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
        "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
        "\n",
        "img = tf.keras.utils.load_img(\n",
        "    sunflower_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOc3PZ2N2r18"
      },
      "source": [
        "## Usar o TensorFlow Lite\n",
        "\n",
        "O TensorFlow Lite é um conjunto de ferramentas que permite aprendizado de máquina em dispositivos, pois ajuda os desenvolvedores e executarem os modelos em dispositivos móveis, embarcados e de borda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cThu25rh4LPP"
      },
      "source": [
        "### Converter o modelo Sequential do Keras em um modelo do TensorFlow Lite\n",
        "\n",
        "Para usar o modelo treinado em aplicações em dispositivos, primeiro [converta-o](https://www.tensorflow.org/lite/models/convert) em um formato de modelo menor e mais eficiente, chamado de modelo do [TensorFlow Lite](https://www.tensorflow.org/lite/).\n",
        "\n",
        "Neste exemplo, pegue o modelo Sequential do Keras treinado e use `tf.lite.TFLiteConverter.from_keras_model` para gerar um modelo do [TensorFlow Lite](https://www.tensorflow.org/lite/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXo6ftuL2ufx"
      },
      "outputs": [],
      "source": [
        "# Convert the model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R26OU4gGKhh"
      },
      "source": [
        "O modelo do TensorFlow Lite que você salvou no passo anterior pode conter diversas assinaturas de função. A API de conversão de modelos do Keras usa a assinatura padrão automaticamente. Saiba mais sobre as [assinaturas do TensorFlow Lite](https://www.tensorflow.org/lite/guide/signatures)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fjQfXaV2l-5"
      },
      "source": [
        "### Executar o modelo do TensorFlow Lite\n",
        "\n",
        "No Python, você pode acessar as assinaturas do modelo do TensorFlow salvo usando a classe `tf.lite.Interpreter`.\n",
        "\n",
        "Carregue o modelo com o `Interpreter`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHYcip_FOaHq"
      },
      "outputs": [],
      "source": [
        "TF_MODEL_FILE_PATH = 'model.tflite' # The default path to the saved TensorFlow Lite model\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPUXY6BdHDHo"
      },
      "source": [
        "Exiba as assinaturas do modelo convertido para obter os nomes das entradas (e das saídas):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdDl00E2OaHq"
      },
      "outputs": [],
      "source": [
        "interpreter.get_signature_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eVFqT0je3YG"
      },
      "source": [
        "Neste exemplo, você tem uma assinatura padrão chamada `serving_default`. Além disso, o nome das  `'inputs'` (entradas) é `'sequential_1_input'`, enquanto as `'outputs'` (saídas) são chamadas `'outputs'`. Você pode conferir os nomes dessa primeira e última camadas do Keras ao executar `Model.summary`, conforme demonstrado anteriormente neste tutorial.\n",
        "\n",
        "Agora, você pode testar o modelo do TensorFlow carregado realizando a inferência em uma imagem de amostra com `tf.lite.Interpreter.get_signature_runner`, passando o nome da assinatura da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFoT_7W_OaHq"
      },
      "outputs": [],
      "source": [
        "classify_lite = interpreter.get_signature_runner('serving_default')\n",
        "classify_lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1mfRcBOnEx0"
      },
      "source": [
        "Similar ao que você fez anteriormente neste tutorial, é possível usar o modelo do TensorFlow Lite para classificar imagens que não estavam incluídas nos testes de treinamento e validação.\n",
        "\n",
        "Você já transformou essa imagem em um tensor e a salvou como `img_array`. Agora, passe-a ao primeiro argumento (o nome das `'inputs'`) do modelo do TensorFlow Lite carregado (`predictions_lite`), compute ativações softmax e depois exiba a previsão para a classe com a probabilidade mais alta computada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEqR27YcnFvc"
      },
      "outputs": [],
      "source": [
        "predictions_lite = classify_lite(sequential_1_input=img_array)['outputs']\n",
        "score_lite = tf.nn.softmax(predictions_lite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKP_GFeKUWb5"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(class_names[np.argmax(score_lite)], 100 * np.max(score_lite))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Poz_iYgeUg_U"
      },
      "source": [
        "A previsão gerada pelo modelo do Lite deve ser quase idêntica às previsões geradas pelo modelo original:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InXXDJL8UYC1"
      },
      "outputs": [],
      "source": [
        "print(np.max(np.abs(predictions - predictions_lite)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hJzY8XijM7N"
      },
      "source": [
        "Das cinco classes — `'daisy'` (margarida), `'dandelion'` (dente-de-leão), `'roses'` (rosa), `'sunflowers'` (girassol) e `'tulips'` (tulipa) — o modelo deve prever que a imagem pertence aos girassóis, que é o mesmo resultado de antes da conversão em um modelo do TensorFlow Lite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RlfCY9v2_ir"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "Este tutorial mostrou como treinar um modelo para fazer classificação de imagens, testá-lo, convertê-lo no formato TensorFlow Lite para aplicações em dispositivos (como um aplicativo de classificação de imagens) e realizar inferência com o modelo TensorFlow Lite usando a API do Python.\n",
        "\n",
        "Saiba mais sobre o TensorFlow Lite consultando os [tutoriais](https://www.tensorflow.org/lite/tutorials) e [guias](https://www.tensorflow.org/lite/guide)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
