{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZCM65CBt1CJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JOgMcEajtkmg"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCSP-dbMw88x"
      },
      "source": [
        "# Segmentação de imagens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEWs8JXRuGex"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/segmentation\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/images/segmentation.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"> Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/images/segmentation.ipynb\"> <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"> Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMP7mglMuGT2"
      },
      "source": [
        "O foco deste tutorial é a tarefa de segmentação de imagens usando uma <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\"external\">U-Net</a> modificada.\n",
        "\n",
        "## O que é segmentação de imagens?\n",
        "\n",
        "Em uma tarefa de classificação de imagens, a rede atribui um rótulo (ou classe) a cada imagem de entrada. Entretanto, vamos supor que você queira saber o formato desse objeto, qual pixel pertence a qual objeto, etc. Nesse caso, você precisa atribuir uma classe a cada pixel da imagem. Essa tarefa é conhecida como segmentação. Um modelo de segmentação retorna informações mais detalhadas sobre a imagem. A segmentação de imagens tem diversas aplicações nos exames de imagens médicos, em carros autônomos e em imagens de satélites, apenas para citar algumas.\n",
        "\n",
        "Este tutorial usa o [dataset Oxford-IIIT Pets](https://www.robots.ox.ac.uk/~vgg/data/pets/) ([Parkhi et al, 2012](https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)). O dataset é composto por imagens de 37 raças de animais domésticos, com 200 imagens por raça (cerca de 100 para o treinamento e 100 para o teste). Cada imagem inclui os rótulos correspondentes e as máscaras de pixels. As máscaras são rótulos (classes) para cada pixel, e é atribuída uma categoria a cada pixel:\n",
        "\n",
        "- Classe 1: pixels pertencentes ao animal doméstico.\n",
        "- Classe 2: pixels no contorno do animal doméstico.\n",
        "- Classe 3: nenhuma das opções acima/pixels nos arredores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQmKthrSBCld"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/tensorflow/examples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQX7R4bhZy5h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g87--n2AtyO_"
      },
      "outputs": [],
      "source": [
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWe0_rQM4JbC"
      },
      "source": [
        "## Baixar o dataset Oxford-IIIT Pets\n",
        "\n",
        "O dataset está [disponível no TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet). As máscaras de segmentação estão incluídas na versão 3 e posteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ITeStwDwZb"
      },
      "outputs": [],
      "source": [
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJcVdj_U4vzf"
      },
      "source": [
        "Além disso, os valores de cor das imagens são normalizados no intervalo `[0, 1]`. Por fim, conforme mencionado acima, os pixels na máscara de segmentação são rotulados como {1, 2, 3}. Por questões de conveniência, subtraia 1 da máscara de segmentação, e os rótulos resultantes serão: {0, 1, 2}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD60EbcAQqov"
      },
      "outputs": [],
      "source": [
        "def normalize(input_image, input_mask):\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image, input_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf0S67hJRp3D"
      },
      "outputs": [],
      "source": [
        "def load_image(datapoint):\n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
        "  input_mask = tf.image.resize(\n",
        "    datapoint['segmentation_mask'],\n",
        "    (128, 128),\n",
        "    method = tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
        "  )\n",
        "\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65-qHTjX5VZh"
      },
      "source": [
        "O dataset já contém as divisões de treinamento e teste necessárias, então continue usando as mesmas divisões:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHwj2-8SaQli"
      },
      "outputs": [],
      "source": [
        "TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39fYScNz9lmo"
      },
      "outputs": [],
      "source": [
        "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9hGHyg8L3Y1"
      },
      "source": [
        "A classe abaixo faz uma ampliação simples invertendo uma imagem aleatoriamente. Saiba mais no tutorial [Ampliação de imagens](data_augmentation.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUWdDJRTL0PP"
      },
      "outputs": [],
      "source": [
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=42):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same random changes.\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "  \n",
        "  def call(self, inputs, labels):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTIbNIBdcgL3"
      },
      "source": [
        "Crie o pipeline de entrada, aplicando a ampliação após dividir as entradas em lotes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPscskQcNCx4"
      },
      "outputs": [],
      "source": [
        "train_batches = (\n",
        "    train_images\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .repeat()\n",
        "    .map(Augment())\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "test_batches = test_images.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa3gMAE_9qNa"
      },
      "source": [
        "Veja uma imagem de exemplo e sua máscara correspondente do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N2RPAAW9q4W"
      },
      "outputs": [],
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6u_Rblkteqb"
      },
      "outputs": [],
      "source": [
        "for images, masks in train_batches.take(2):\n",
        "  sample_image, sample_mask = images[0], masks[0]\n",
        "  display([sample_image, sample_mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "## Definir o modelo\n",
        "\n",
        "O modelo em uso aqui é uma  [U-Net](https://arxiv.org/abs/1505.04597) modificada. Uma U-Net consiste de um encoder (downsampler) e decoder (upsampler). Para aprender recursos robustos e reduzir o número de parâmetros treináveis, use um modelo pré-treinado, [MobileNetV2](https://arxiv.org/abs/1801.04381), como encoder. Para o decoder, você usará o bloco de upsample, que já está implementado no exemplo [Pix2Pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) do repositório de exemplos do TensorFlow (confira o tutorial [Pix2Pix: Conversão imagem-para-imagem com uma GAN condicional](../generative/pix2pix.ipynb) em um notebook).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4mQle3lthit"
      },
      "source": [
        "Conforme mencionado, o encoder é um modelo MobileNetV2 pré-treinado. Você usará o modelo de `tf.keras.applications`. O encoder consiste de saídas específicas das camadas intermediárias do modelo. É importante salientar que o encoder não será treinado durante o processo de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liCeLH0ctjq7"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPw8Lzra5_T9"
      },
      "source": [
        "O decoder/upsampler é simplesmente uma série de blocos de upsample implementados nos exemplos do TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0ZbfywEbZpJ"
      },
      "outputs": [],
      "source": [
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45HByxpVtrPF"
      },
      "outputs": [],
      "source": [
        "def unet_model(output_channels:int):\n",
        "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      filters=output_channels, kernel_size=3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRsjdZuEnZfA"
      },
      "source": [
        "O número de filtros na última camada é definido como o número de `output_channels` (canais de saída). Será um canal de saída por classe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "## Treinar o modelo\n",
        "\n",
        "Agora, só falta compilar e treinar o modelo.\n",
        "\n",
        "Como este é um problema de classificação multiclasse, use a função de perda do `tf.keras.losses.SparseCategoricalCrossentropy` com o argumento `from_logits` definido como `True`, já que os rótulos são inteiros escalares em vez de vetores de pontuações para cada pixel de cada classe.\n",
        "\n",
        "Ao executar a instância, o rótulo atribuído a cada pixel é o canal com o valor mais alto. É isso que a função `create_mask` está fazendo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6he36HK5uKAc"
      },
      "outputs": [],
      "source": [
        "OUTPUT_CLASSES = 3\n",
        "\n",
        "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVMzbIZLcyEF"
      },
      "source": [
        "Plote a arquitetura do modelo resultante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw82qF1Gcovr"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3MiEO2twLS"
      },
      "source": [
        "Teste o modelo para verificar o que ele prevê antes do treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwvIKLZPtxV_"
      },
      "outputs": [],
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLNsrynNtx4d"
      },
      "outputs": [],
      "source": [
        "def show_predictions(dataset=None, num=1):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      display([image[0], mask[0], create_mask(pred_mask)])\n",
        "  else:\n",
        "    display([sample_image, sample_mask,\n",
        "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_1CC0T4dho3"
      },
      "outputs": [],
      "source": [
        "show_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22AyVYWQdkgk"
      },
      "source": [
        "O callback definido abaixo é usado para observar como o modelo melhora à medida que é treinado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHrHsqijdmL6"
      },
      "outputs": [],
      "source": [
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    clear_output(wait=True)\n",
        "    show_predictions()\n",
        "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StKDH_B9t4SD"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "VAL_SUBSPLITS = 5\n",
        "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=test_batches,\n",
        "                          callbacks=[DisplayCallback()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_mu0SAbt40Q"
      },
      "outputs": [],
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
        "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unP3cnxo_N72"
      },
      "source": [
        "## Fazer previsões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BVXldSo-0mW"
      },
      "source": [
        "Agora, faça previsões. Para poupar tempo, o número de épocas foi mantido baixo, mas você pode usar um valor mais alto para conseguir resultados mais precisos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikrzoG24qwf5"
      },
      "outputs": [],
      "source": [
        "show_predictions(test_batches, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAwvlgSNoK3o"
      },
      "source": [
        "## Opcional: classes desequilibradas e pesos de classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqtFPqqu2kxP"
      },
      "source": [
        "Os datasets de segmentação semântica podem ficar muito desequilibrados, ou seja, pixels de uma classe específica podem estar mais presentes dentro das imagens do que de outras classes. Como os problemas de segmentação podem ser tratados como problemas de classificação por pixel, você pode lidar com o problema de desequilíbrio ponderando a função de perda para levar isso em conta. É uma forma simples e elegante de lidar com o problema. Confira mais detalhes no tutorial [Classificação ao usar dados desequilibrados](../structured_data/imbalanced_data.ipynb).\n",
        "\n",
        "Para [evitar ambiguidade](https://github.com/keras-team/keras/issues/3653#issuecomment-243939748), `Model.fit` não tem suporte ao argumento `class_weight` para alvos com mais de três dimensões."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHt90UEQsZDn"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  model_history = model.fit(train_batches, epochs=EPOCHS,\n",
        "                            steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                            class_weight = {0:2.0, 1:2.0, 2:1.0})\n",
        "  assert False\n",
        "except Exception as e:\n",
        "  print(f\"Expected {type(e).__name__}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brbhYODCsvbe"
      },
      "source": [
        "Portanto, nesse caso, você é que precisa implementar os pesos. Para fazer isso, utilize pesos de amostra: além de pares `(data, label)`, `Model.fit` também aceita tuplas `(data, label, sample_weight)`.\n",
        "\n",
        "`Model.fit` do Keras propaga `sample_weight` para as perdas e métricas, que também aceitam um argumento `sample_weight`. O peso da amostra é multiplicado pelo valor da amostra antes do passo de redução. Por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmHtImJn5Kk-"
      },
      "outputs": [],
      "source": [
        "label = [0,0]\n",
        "prediction = [[-3., 0], [-3, 0]] \n",
        "sample_weight = [1, 10] \n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                               reduction=tf.keras.losses.Reduction.NONE)\n",
        "loss(label, prediction, sample_weight).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbwo3DZ-9TxM"
      },
      "source": [
        "Portanto, para fazer pesos de amostra para este tutorial, você precisa de uma função que receba um par `(data, label)` e retorne uma tupla `(data, label, sample_weight)`, em que `sample_weight` é uma imagem de um canal contendo o peso de classes para cada pixel.\n",
        "\n",
        "A implementação mais simples possível é usar o rótulo como índice em uma lista `class_weight`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlG-n2Ugo8Jc"
      },
      "outputs": [],
      "source": [
        "def add_sample_weights(image, label):\n",
        "  # The weights for each class, with the constraint that:\n",
        "  #     sum(class_weights) == 1.0\n",
        "  class_weights = tf.constant([2.0, 2.0, 1.0])\n",
        "  class_weights = class_weights/tf.reduce_sum(class_weights)\n",
        "\n",
        "  # Create an image of `sample_weights` by using the label at each pixel as an \n",
        "  # index into the `class weights` .\n",
        "  sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
        "\n",
        "  return image, label, sample_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLH_NvH2UrXU"
      },
      "source": [
        "Cada elemento do dataset resultante contém três imagens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE_ezRSFRCnE"
      },
      "outputs": [],
      "source": [
        "train_batches.map(add_sample_weights).element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc-EpIzaRbSL"
      },
      "source": [
        "Agora, você pode treinar um modelo com esse dataset ponderado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDWipedAoOQe"
      },
      "outputs": [],
      "source": [
        "weighted_model = unet_model(OUTPUT_CLASSES)\n",
        "weighted_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btEFKc1xodGR"
      },
      "outputs": [],
      "source": [
        "weighted_model.fit(\n",
        "    train_batches.map(add_sample_weights),\n",
        "    epochs=1,\n",
        "    steps_per_epoch=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R24tahEqmSCk"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "Agora que você sabe o que é segmentação de imagens e como ela funciona, pode tentar usar este tutorial com diferentes saídas de camadas intermediárias ou até mesmo para diferentes modelos pré-treinados. Além disso, se quiser se desafiar, você pode experimentar o desafio de mascaramento de imagens [Carvana](https://www.kaggle.com/c/carvana-image-masking-challenge/overview), disponível no Kaggle.\n",
        "\n",
        "Talvez também seja interessante conferir a [API de Detecção de Objetos do Tensorflow](https://github.com/tensorflow/models/blob/master/research/object_detection/README.md) para ver outro modelo que você pode treinar novamente usando seus próprios dados. Estão disponíveis modelos pré-treinados no [TensorFlow Hub](https://www.tensorflow.org/hub/tutorials/tf2_object_detection#optional)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "segmentation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
