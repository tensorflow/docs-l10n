{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNdWfPXCjTjY"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I1dUQ0GejU8N"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c05P9g5WjizZ"
      },
      "source": [
        "# Classificar dados estruturados com colunas de características"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zofH_gCzgplN"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/structured_data/feature_columns\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/structured_data/feature_columns.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/structured_data/feature_columns.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/structured_data/feature_columns.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1y4OHpGgss7"
      },
      "source": [
        "> Aviso: o módulo `tf.feature_columns` descrito neste tutorial não é recomendado para novos códigos. As [camadas de pré-processamento do Keras](../../guide/migrate/migrating_feature_columns.ipynb) abrangem essa funcionalidade. Para instruções de como migrar, confira o [Guia de migração das colunas de características](../../guide/migrate/migrating_feature_columns.ipynb). O módulo `tf.feature_columns` foi feito para ser usado com `Estimadores` do TF1. Ele é abarcado pelas [garantias de compatibilidade](https://tensorflow.org/guide/versions), mas não recebe mais correções, exceto para vulnerabilidades de segurança.\n",
        "\n",
        "Este tutorial demonstra como classificar dados estruturados (por exemplo, dados tabulados em um CSV). Usaremos o [Keras](https://www.tensorflow.org/guide/keras) para definir o modelo e `tf.feature_column` como uma ponte para mapear as colunas em um CSV para características usadas a fim de treinar o modelo. Este tutorial contém o código completo para:\n",
        "\n",
        "- Carregar um arquivo CSV usando o [Pandas](https://pandas.pydata.org/).\n",
        "- Criar um pipeline de entrada para dividir as linhas em lotes e misturá-las usando [tf.data](https://www.tensorflow.org/guide/datasets).\n",
        "- Mapear as colunas no CSV para características usadas a fim de treinar o modelo utilizando colunas de características.\n",
        "- Criar, treinar e avaliar um modelo usando o Keras.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "Usaremos uma versão simplificada do [dataset PetFinder](https://www.kaggle.com/c/petfinder-adoption-prediction). Há milhares de linhas no CSV. Cada linha representa um animal doméstico, e cada coluna representa um atributo. Usaremos essas informações para prever com que rapidez o animal doméstico será adotado.\n",
        "\n",
        "Veja abaixo uma descrição do dataset. Observe que existem colunas numéricas e de categorias. Há uma coluna de texto livre, que não será usada neste tutorial.\n",
        "\n",
        "Coluna | Descrição | Tipo de característica | Tipo de dado\n",
        "--- | --- | --- | ---\n",
        "Type | Tipo de animal (cachorro, gato) | Categoria | string\n",
        "Age | Idade do animal doméstico | Número | inteiro\n",
        "Breed1 | Raça principal do animal doméstico | Categoria | string\n",
        "Color1 | Cor 1 do animal doméstico | Categoria | string\n",
        "Color2 | Cor 2 do animal doméstico | Categoria | string\n",
        "MaturitySize | Tamanho quando adulto | Categoria | string\n",
        "FurLength | Tamanho dos pelos | Categoria | string\n",
        "Vaccinated | Se o animal doméstico foi vacinado | Categoria | string\n",
        "Sterilized | Se o animal doméstico foi castrado | Categoria | string\n",
        "Health | Estado de saúde | Categoria | string\n",
        "Fee | Taxa de adoção | Número | inteiro\n",
        "Description | Descrição do perfil deste animal doméstico | Texto | string\n",
        "PhotoAmt | Total de fotos carregadas deste animal doméstico | Número | inteiro\n",
        "AdoptionSpeed | Rapidez de adoção | Classificação | inteiro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxyBFc_kKazA"
      },
      "source": [
        "## Importar o TensorFlow e outras bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuOWVJBz8a6G"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dEreb4QKizj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCEhSZcULZ9n"
      },
      "source": [
        "## Usar o Pandas para criar um dataframe\n",
        "\n",
        "O [Pandas](https://pandas.pydata.org/) é uma biblioteca do Python com diversos utilitários muito úteis para carregar dados estruturados e trabalhar com eles. Usaremos o Pandas para baixar o dataset de uma URL e carregá-lo em um dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REZ57BXCLdfG"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
        "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
        "\n",
        "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
        "                        extract=True, cache_dir='.')\n",
        "dataframe = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8QIi0_jT5LM"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awGiBeBWbQC8"
      },
      "source": [
        "## Criar a variável alvo\n",
        "\n",
        "A tarefa do dataset original é prever a rapidez de adoção de um animal doméstico (por exemplo, na primeira semana, no primeiro mês, nos primeiros três meses, e assim por diante). Vamos simplificar para este tutorial – vamos transformar em um problema de classificação binário e apenas prever se o animal doméstico foi adotado ou não.\n",
        "\n",
        "Após modificarmos a coluna de rótulo, 0 indicará que o animal doméstico não foi adotado, e 1 indicará que foi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcbTpEXWbMDz"
      },
      "outputs": [],
      "source": [
        "# In the original dataset \"4\" indicates the pet was not adopted.\n",
        "dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
        "\n",
        "# Drop un-used columns.\n",
        "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0zhLtQqMPem"
      },
      "source": [
        "## Dividir o dataframe em treinamento, validação e teste\n",
        "\n",
        "O dataset baixado é um único arquivo CSV. Vamos dividi-lo em conjuntos de treinamento, validação e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEOpw7LhMYsI"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ef46LXMfvu"
      },
      "source": [
        "## Criar um pipeline de entrada usando tf.data\n",
        "\n",
        "Agora, vamos encapsular os dataframes usando [tf.data](https://www.tensorflow.org/guide/datasets), que nos permitirá usar as colunas de características como uma ponte para mapear as colunas no dataframe do Pandas para características usadas para treinar o modelo. Se estivéssemos usando um arquivo CSV muito grande (tão grande que não coubesse na memória), usaríamos tf.data para ler no disco diretamente. Esse processo não é discutido neste tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkcaMYP-MsRe"
      },
      "outputs": [],
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXbbXkJvMy34"
      },
      "outputs": [],
      "source": [
        "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRLGSMDzM-dl"
      },
      "source": [
        "## Sobre o pipeline de entrada\n",
        "\n",
        "Agora que criamos o pipeline de entrada, vamos chamá-lo para ver o formato dos dados retornados. Usamos um tamanho pequeno para o lote a fim de manter a saída fácil de ler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSBo3dUVNFc9"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  print('Every feature:', list(feature_batch.keys()))\n",
        "  print('A batch of ages:', feature_batch['Age'])\n",
        "  print('A batch of targets:', label_batch )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5N6Se-NQsC"
      },
      "source": [
        "Podemos ver que o dataset retorna um dicionário de nomes de colunas (do dataframe) que faz o mapeamento das linhas para valores de coluna no dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttIvgLRaNoOQ"
      },
      "source": [
        "## Demonstração de vários tipos de coluna de características\n",
        "\n",
        "O TensorFlow fornece diversos tipos de coluna de característica. Nesta seção, vamos criar vários tipos e demonstrar como eles transformam uma coluna do dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxwiHFHuNhmf"
      },
      "outputs": [],
      "source": [
        "# We will use this batch to demonstrate several types of feature columns\n",
        "example_batch = next(iter(train_ds))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wfLB8Q3N3UH"
      },
      "outputs": [],
      "source": [
        "# A utility method to create a feature column\n",
        "# and to transform a batch of data\n",
        "def demo(feature_column):\n",
        "  feature_layer = layers.DenseFeatures(feature_column)\n",
        "  print(feature_layer(example_batch).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7OEKe82N-Qb"
      },
      "source": [
        "### Colunas numéricas\n",
        "\n",
        "A saída de uma coluna de característica se torna a entrada do modelo (usando a função de demonstração definida acima, conseguiremos ver exatamente como cada coluna do dataframe é transformada). Uma [coluna numérica](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column) é o tipo mais simples e é usada para representar características com valores reais. Ao usar essa coluna, seu modelo receberá o valor da coluna do dataframe sem alterações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZTZ0HnHOCxC"
      },
      "outputs": [],
      "source": [
        "photo_count = feature_column.numeric_column('PhotoAmt')\n",
        "demo(photo_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a6ddSyzOKpq"
      },
      "source": [
        "No dataset PetFinder, a maioria das colunas do dataframe são de categoria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcSxUoYgOlA1"
      },
      "source": [
        "### Coluna em buckets\n",
        "\n",
        "Geralmente, não alimentamos um número diretamente no modelo. Em vez disso, dividimos o valor em diferentes categorias com base nos intervalos numéricos. Considere os dados brutos que representem a idade de uma pessoa. Em vez de representar a idade como uma coluna numérica, poderíamos dividir a idade em diversos buckets usando uma [coluna em buckets](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column). Observe que os valores one-hot abaixo descrevem a qual intervalo de idade cada linha corresponde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ4Wt3SAOpTQ"
      },
      "outputs": [],
      "source": [
        "age = feature_column.numeric_column('Age')\n",
        "age_buckets = feature_column.bucketized_column(age, boundaries=[1, 3, 5])\n",
        "demo(age_buckets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tArzewPb-b"
      },
      "source": [
        "### Colunas de categoria\n",
        "\n",
        "Neste dataset, Type (Tipo) é representado como uma string (por exemplo, \"dog\" – cachorro – ou \"cat\" – gato). Não podemos alimentar strings diretamente no modelo. Em vez disso, primeiro precisamos mapeá-las para valores numéricos. As colunas de vocabulário de categorias são uma forma de representar strings como um vetor one-hot (similar ao que vimos acima com buckets de idade). O vocabulário pode ser passado como uma lista usando [categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list) ou carregado de um arquivo usando [categorical_column_with_vocabulary_file](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ6QnSHkPtOC"
      },
      "outputs": [],
      "source": [
        "animal_type = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Type', ['Cat', 'Dog'])\n",
        "\n",
        "animal_type_one_hot = feature_column.indicator_column(animal_type)\n",
        "demo(animal_type_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEFPjUr6QmwS"
      },
      "source": [
        "### Colunas de embedding\n",
        "\n",
        "Suponha que, em vez de ter somente algumas strings possíveis, tivéssemos milhares (ou mais) de valores por categoria. Por diversos motivos, à medida que o número de categorias cresce, fica inviável treinar uma rede neural usando codificações one-hot. Podemos usar uma coluna de embedding para superar essa limitação. Em vez de representar os dados como um vetor one-hot com diversas dimensões, uma [coluna de embedding](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column) representa os dados como um vetor Dense de baixa dimensão, em que cada célula pode conter qualquer número, não apenas 0 ou 1. O tamanho do embedding (8, no exemplo abaixo) é um parâmetro que precisa ser ajustado.\n",
        "\n",
        "Ponto-chave: usar uma coluna de embedding é a melhor opção quando uma coluna de categoria tiver diversos valores possíveis. Estamos usando uma aqui para fins de demonstração. Dessa forma, você terá um exemplo completo que pode ser modificado para diferentes datasets no futuro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSlohmr2Q_UU"
      },
      "outputs": [],
      "source": [
        "# Notice the input to the embedding column is the categorical column\n",
        "# we previously created\n",
        "breed1 = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Breed1', dataframe.Breed1.unique())\n",
        "breed1_embedding = feature_column.embedding_column(breed1, dimension=8)\n",
        "demo(breed1_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urFCAvTVRMpB"
      },
      "source": [
        "### Colunas de características com hash\n",
        "\n",
        "Outra maneira de representar uma coluna de categorias com um grande número de valores é usando [categorical_column_with_hash_bucket](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket). Essa coluna de categorias calcula um valor de hash da entrada, depois seleciona um dos buckets `hash_bucket_size` para codificar uma string. Ao usar essa coluna, você não precisa fornecer o vocabulário e pode optar por deixar o número de hash_buckets consideravelmente menor do que o número de categorias reais para economizar espaço.\n",
        "\n",
        "Ponto-chave: uma desvantagem importante desta técnica é que poderá haver colisões, em que strings diferentes são mapeadas para o mesmo bucket. Mesmo assim, na prática, isso pode funcionar bem para alguns datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHU_Aj2nRRDC"
      },
      "outputs": [],
      "source": [
        "breed1_hashed = feature_column.categorical_column_with_hash_bucket(\n",
        "      'Breed1', hash_bucket_size=10)\n",
        "demo(feature_column.indicator_column(breed1_hashed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB94M27DRXtZ"
      },
      "source": [
        "### Colunas de características cruzadas\n",
        "\n",
        "Com a combinação de características em uma só, mais conhecida como [cruzamentos de características](https://developers.google.com/machine-learning/glossary/#feature_cross), um modelo pode aprender pesos separados para cada combinação de características. Aqui, criaremos uma nova característica proveniente do cruzamento de Age (Idade) e Type (Tipo). Observe que `crossed_column` não cria a tabela completa de todas as possíveis combinações (que seria muito grande). Em vez disso, é utilizada uma coluna `hashed_column` para você poder escolher o tamanho da tabela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaPVERd9Rep6"
      },
      "outputs": [],
      "source": [
        "crossed_feature = feature_column.crossed_column([age_buckets, animal_type], hash_bucket_size=10)\n",
        "demo(feature_column.indicator_column(crossed_feature))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypkI9zx6Rj1q"
      },
      "source": [
        "## Escolher quais colunas usar\n",
        "\n",
        "Vimos como usar diversos tipos de coluna de características. Agora, vamos usá-los para treinar um modelo. O objetivo deste tutorial é mostrar o código completo (ou seja, a mecânica) necessário para trabalhar com colunas de características. Selecionamos algumas colunas arbitrariamente para treinar o modelo abaixo.\n",
        "\n",
        "Ponto-chave: se o seu objetivo é criar um modelo exato, experimente usar um dataset maior e pense com cuidado em quais características são as mais importantes a serem incluídas e como devem ser representadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PlLY7fORuzA"
      },
      "outputs": [],
      "source": [
        "feature_columns = []\n",
        "\n",
        "# numeric cols\n",
        "for header in ['PhotoAmt', 'Fee', 'Age']:\n",
        "  feature_columns.append(feature_column.numeric_column(header))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdF4rXkcDmBl"
      },
      "outputs": [],
      "source": [
        "# bucketized cols\n",
        "age = feature_column.numeric_column('Age')\n",
        "age_buckets = feature_column.bucketized_column(age, boundaries=[1, 2, 3, 4, 5])\n",
        "feature_columns.append(age_buckets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsteO7FGDmNc"
      },
      "outputs": [],
      "source": [
        "# indicator_columns\n",
        "indicator_column_names = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
        "                          'FurLength', 'Vaccinated', 'Sterilized', 'Health']\n",
        "for col_name in indicator_column_names:\n",
        "  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
        "      col_name, dataframe[col_name].unique())\n",
        "  indicator_column = feature_column.indicator_column(categorical_column)\n",
        "  feature_columns.append(indicator_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MhdqQ5uDmYU"
      },
      "outputs": [],
      "source": [
        "# embedding columns\n",
        "breed1 = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Breed1', dataframe.Breed1.unique())\n",
        "breed1_embedding = feature_column.embedding_column(breed1, dimension=8)\n",
        "feature_columns.append(breed1_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkzRNfCLDsQf"
      },
      "outputs": [],
      "source": [
        "# crossed columns\n",
        "age_type_feature = feature_column.crossed_column([age_buckets, animal_type], hash_bucket_size=100)\n",
        "feature_columns.append(feature_column.indicator_column(age_type_feature))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-nDp8krS_ts"
      },
      "source": [
        "### Criar uma camada de características\n",
        "\n",
        "Agora que definimos as colunas de características, vamos usar uma camada [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) como entrada do modelo do Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o-El1R2TGQP"
      },
      "outputs": [],
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cf6vKfgTH0U"
      },
      "source": [
        "Anteriormente, usamos um tamanho pequeno para o lote a fim de demonstrar o funcionamento das colunas de características. Criamos um novo pipeline de entrada com um tamanho maior para o lote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcemszoGSse_"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBx4Xu0eTXWq"
      },
      "source": [
        "## Criar, compilar e treinar o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YJPPb3xTPeZ"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dropout(.1),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds,\n",
        "          validation_data=val_ds,\n",
        "          epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnFmMOW0Tcaa"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdfbq20V6zu"
      },
      "source": [
        "Ponto-chave: geralmente, você observará os melhores resultados com aprendizado profundo ao usar datasets muito maiores e mais complexos. Ao utilizar um dataset pequeno como este, recomendamos usar uma árvore de decisão ou floresta aleatória como linha de base. O objetivo deste tutorial não é treinar um modelo preciso, mas demonstrar a mecânica ao trabalhar com dados estruturados para que você tenha um código que possa ser usado como ponto de partida ao trabalhar com seus próprios datasets no futuro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SotnhVWuHQCw"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "A melhor forma de aprender mais sobre a classificação de dados estruturados é botando a mão na massa. Sugerimos que você encontre outro dataset e treine um modelo para classificá-lo usando um código parecido com o deste tutorial. Para aumentar a exatidão, pense com cuidado em quais características você incluirá no modelo e como elas serão representadas."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "feature_columns.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
