{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg02FZzDyEqd"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2mapZ9afGJ69"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMYQvJuBi7MS"
      },
      "source": [
        "# Classificar dados estruturados usando camadas de pré-processamento do Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FaL4wnr22oy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/structured_data/preprocessing_layers.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/structured_data/preprocessing_layers.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/structured_data/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nna1tOKxyEqe"
      },
      "source": [
        "Este tutorial demonstra como classificar dados estruturados, como dados tabulados, usando uma versão simplificada do <a href=\"https://www.kaggle.com/c/petfinder-adoption-prediction\" class=\"external\">dataset PetFinder de uma competição do Kaggle</a> armazenado em um arquivo CSV.\n",
        "\n",
        "Você usará o [Keras](https://www.tensorflow.org/guide/keras) para definir o modelo e [camadas de pré-processamento do Keras](https://www.tensorflow.org/guide/keras/preprocessing_layers) como uma ponte a fim de mapear colunas de um arquivo CSV para características usadas para treinar o modelo. O objetivo é prever se um animal doméstico será adotado.\n",
        "\n",
        "Este tutorial contém o código completo para:\n",
        "\n",
        "- Carregar um arquivo CSV em um <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\" class=\"external\">DataFrame</a> usando o <a href=\"https://pandas.pydata.org/\" class=\"external\">pandas</a>.\n",
        "- Criar um pipeline de entrada para dividir as linhas em lotes e misturá-las usando `tf.data`. (Confira mais detalhes em [tf.data: Criar pipelines de entrada do TensorFlow](../../guide/data.ipynb).)\n",
        "- Mapear as colunas no arquivo CSV para características usadas a fim de treinar o modelo utilizando as camadas de pré-processamento do Keras.\n",
        "- Criar, treinar e avaliar um modelo usando os métodos integrados do Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5xkXCicjFQD"
      },
      "source": [
        "Observação: este tutorial é similar ao tutorial [Classificar dados estruturados com colunas de características](../structured_data/feature_columns.ipynb). Esta versão usa as [camadas de pré-processamento do Keras](https://www.tensorflow.org/guide/keras/preprocessing_layers) em vez da API `tf.feature_column`, pois elas são mais intuitivas e é fácil incluí-las dentro do modelo para simplificar o desenvolvimento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHxU1FMNpomc"
      },
      "source": [
        "## Dataset PetFinder.my reduzido\n",
        "\n",
        "Há milhares de linhas no arquivo CSV do dataset PetFinder.my reduzido, em que cada linha representa um animal doméstico (cachorro ou gato), e cada coluna representa um atributo, como idade, raça, cor e assim por diante.\n",
        "\n",
        "No resumo do dataset abaixo, observe que há colunas numéricas e de categoria, em sua maioria. Neste tutorial, usaremos somente esses dois tipos de característica, descartando `Description` (descrição, uma característica com texto livre) e `AdoptionSpeed` (rapidez de adoção, uma característica de classificação) durante o pré-processamento dos dados.\n",
        "\n",
        "Coluna | Descrição do animal doméstico | Tipo de característica | Tipo de dado\n",
        "--- | --- | --- | ---\n",
        "`Type` | Tipo de animal (`Dog` – cachorro, `Cat` – gato) | Categoria | String\n",
        "`Age` | Idade | Número | Inteiro\n",
        "`Breed1` | Raça principal | Categoria | String\n",
        "`Color1` | Cor 1 | Categoria | String\n",
        "`Color2` | Cor 2 | Categoria | String\n",
        "`MaturitySize` | Tamanho quando adulto | Categoria | String\n",
        "`FurLength` | Tamanho dos pelos | Categoria | String\n",
        "`Vaccinated` | Se o animal doméstico foi vacinado | Categoria | String\n",
        "`Sterilized` | Se o animal doméstico foi castrado | Categoria | String\n",
        "`Health` | Estado de saúde | Categoria | String\n",
        "`Fee` | Taxa de adoção | Número | Inteiro\n",
        "`Description` | Descrição do perfil | Texto | String\n",
        "`PhotoAmt` | Total de fotos carregadas | Número | Inteiro\n",
        "`AdoptionSpeed` | Rapidez de adoção | Classificação | Inteiro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjFbdBldyEqf"
      },
      "source": [
        "## Importar o TensorFlow e outras bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LklnLlt6yEqf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKU7RyoQGVKB"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXvBvobayEqi"
      },
      "source": [
        "## Carregar o dataset em um DataFrame do pandas\n",
        "\n",
        "O <a href=\"https://pandas.pydata.org/\" class=\"external\">pandas</a> é uma biblioteca do Python com diversos utilitários muito úteis para carregar dados estruturados e trabalhar com eles. Use `tf.keras.utils.get_file` para baixar e extrair o arquivo CSV com o dataset PetFinder.my reduzido e carregá-lo em um <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\" class=\"external\">DataFrame</a> com <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\" class=\"external\"><code>pandas.read_csv</code></a>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ4Ajn-YyEqj"
      },
      "outputs": [],
      "source": [
        "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
        "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
        "\n",
        "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
        "                        extract=True, cache_dir='.')\n",
        "dataframe = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efa6910dfa5f"
      },
      "source": [
        "Verifique as primeiras cinco linhas do DataFrame para avaliar o dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uiq4hoIGyXI"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3zDbrozyEqq"
      },
      "source": [
        "## Criar uma variável alvo\n",
        "\n",
        "A tarefa original da <a href=\"https://www.kaggle.com/c/petfinder-adoption-prediction\" class=\"external\">competição de previsão de adoção</a> do Kaggle era prever a rapidez de adoção de um animal doméstico (por exemplo, na primeira semana, no primeiro mês, nos primeiros três meses, e assim por diante).\n",
        "\n",
        "Neste tutorial, simplificaremos a tarefa para um problema de classificação binária, em que você somente precisa prever se um animal doméstico foi adotado ou não.\n",
        "\n",
        "Após modificarmos a coluna `AdoptionSpeed`, `0` indicará que o animal doméstico não foi adotado, e `1` indicará que foi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmMDc46-yEqq"
      },
      "outputs": [],
      "source": [
        "# In the original dataset, `'AdoptionSpeed'` of `4` indicates\n",
        "# a pet was not adopted.\n",
        "dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
        "\n",
        "# Drop unused features.\n",
        "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp0NCbswyEqs"
      },
      "source": [
        "## Dividir o DataFrame em conjuntos de treinamento, validação e teste\n",
        "\n",
        "O dataset é um único DataFrame do pandas. Vamos dividi-lo em conjuntos de treinamento, validação e teste usando, por exemplo, uma proporção de 80/10/10, respectivamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvSinthO8oMj"
      },
      "outputs": [],
      "source": [
        "train, val, test = np.split(dataframe.sample(frac=1), [int(0.8*len(dataframe)), int(0.9*len(dataframe))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U02Q1moWoPwQ"
      },
      "outputs": [],
      "source": [
        "print(len(train), 'training examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_7uVu-xyEqv"
      },
      "source": [
        "## Criar um pipeline de entrada usando tf.data\n",
        "\n",
        "Agora, crie uma função utilitária que converta cada DataFrame de treinamento, validação e teste em um `tf.data.Dataset`, depois misture os dados e divida-os em lotes.\n",
        "\n",
        "Observação: ao usar um arquivo CSV muito grande (tão grande que não caiba na memória), usaríamos a API `tf.data` para ler no disco diretamente. Esse processo não é discutido neste tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r4j-1lRyEqw"
      },
      "outputs": [],
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  df = dataframe.copy()\n",
        "  labels = df.pop('target')\n",
        "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYxIXH579uS9"
      },
      "source": [
        "Agora, use a função (`df_to_dataset`) recém-criada para verificar o formato dos dados retornados pela função helper do pipeline de entrada, chamando-a nos dados de treinamento, e use um tamanho pequeno para o lote a fim de manter a saída fácil de ler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYiNH-QI96Jo"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFYir6S8HgIJ"
      },
      "outputs": [],
      "source": [
        "[(train_features, label_batch)] = train_ds.take(1)\n",
        "print('Every feature:', list(train_features.keys()))\n",
        "print('A batch of ages:', train_features['Age'])\n",
        "print('A batch of targets:', label_batch )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geqHWW54Hmte"
      },
      "source": [
        "Conforme demonstrado pela saída, o conjunto de treinamento retorna um dicionário de nomes de colunas (do DataFrame) que faz o mapeamento das linhas para valores de coluna."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v50jBIuj4gb"
      },
      "source": [
        "## Aplicar as camadas de pré-processamento do Keras\n",
        "\n",
        "Com as camadas de pré-processamento do Keras, você pode criar pipelines de entrada nativos do Keras que podem ser usados ​​como código de pré-processamento independente em outros workflows (que não usam Keras), combinados diretamente com modelos Keras e exportados como parte de um Keras SavedModel.\n",
        "\n",
        "Neste tutorial, você usará as quatro camadas de pré-processamento abaixo para demonstrar como fazer o pré-processamento, a codificação de dados estruturados e a engenharia de características:\n",
        "\n",
        "- `tf.keras.layers.Normalization`: executa uma normalização das características de entrada.\n",
        "- `tf.keras.layers.CategoryEncoding`: transforma características de categoria representadas por inteiros em representações one-hot, multi-hot ou <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" class=\"external\">tf-idf</a> densas.\n",
        "- `tf.keras.layers.StringLookup`: transforma valores de categoria representados por strings em índices inteiros.\n",
        "- `tf.keras.layers.IntegerLookup`: transforma valores de categoria representados por inteiros em índices inteiros.\n",
        "\n",
        "Saiba mais sobre as camadas disponíveis no guia [Como usar camadas de pré-processamento](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
        "\n",
        "- Para *características numéricas* do dataset PetFinder.my reduzido, você usará uma camada `tf.keras.layers.Normalization` para padronizar a distribuição dos dados.\n",
        "- Para *características de categoria*, como `Type` (Tipo) (strings `Dog` – cachorro e `Cat` – gato), você as transformará em tensores com codificação multi-hot usando `tf.keras.layers.CategoryEncoding`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twXBSxnT66o8"
      },
      "source": [
        "### Colunas numéricas\n",
        "\n",
        "Para cada característica numérica do dataset PetFinder.my reduzido, você usará uma camada `tf.keras.layers.Normalization` para padronizar a distribuição dos dados.\n",
        "\n",
        "Defina uma nova função utilitária que retorne uma camada que aplique uma normalização de características numéricas usando a camada de pré-processamento do Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6OuEKMMyEq1"
      },
      "outputs": [],
      "source": [
        "def get_normalization_layer(name, dataset):\n",
        "  # Create a Normalization layer for the feature.\n",
        "  normalizer = layers.Normalization(axis=None)\n",
        "\n",
        "  # Prepare a Dataset that only yields the feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the statistics of the data.\n",
        "  normalizer.adapt(feature_ds)\n",
        "\n",
        "  return normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL4TRreQCPjV"
      },
      "source": [
        "Em seguida, para testar a nova a função, chame-a nas características de total de fotos do animal doméstico carregadas para normalizar `'PhotoAmt'`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpKgUDyk69bM"
      },
      "outputs": [],
      "source": [
        "photo_count_col = train_features['PhotoAmt']\n",
        "layer = get_normalization_layer('PhotoAmt', train_ds)\n",
        "layer(photo_count_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foWY00YBUx9N"
      },
      "source": [
        "Observação: se houver muitas características numéricas (centenas ou mais), é mais eficiente concatená-las primeiro e usar uma única camada `tf.keras.layers.Normalization`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVD--2WZ7vmh"
      },
      "source": [
        "### Colunas de categoria\n",
        "\n",
        "O `Type` (Tipo) de animal doméstico é representado como string no dataset – `Dog` (cachorro) e `Cat` (gato), que precisa sofrer codificação multi-hot antes de ser alimentado no modelo. A característica `Age` (idade)\n",
        "\n",
        "Defina mais uma função utilitária que retorne um camada que mapeie valores de um vocabulário para índices inteiros e faça a codificação multi-hot das características usando as camadas de pré-processamento `tf.keras.layers.StringLookup`, `tf.keras.layers.IntegerLookup` e `tf.keras.CategoryEncoding`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmgaeRjlDoUO"
      },
      "outputs": [],
      "source": [
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a layer that turns strings into integer indices.\n",
        "  if dtype == 'string':\n",
        "    index = layers.StringLookup(max_tokens=max_tokens)\n",
        "  # Otherwise, create a layer that turns integer values into integer indices.\n",
        "  else:\n",
        "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
        "\n",
        "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Encode the integer indices.\n",
        "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
        "\n",
        "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
        "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
        "  return lambda feature: encoder(index(feature))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b3DwtTeCPjX"
      },
      "source": [
        "Para testar a função `get_category_encoding_layer`, chame-a nas características `'Type'` do animal doméstico para transformá-las em tensores com codificação multi-hot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2t2ff9K8PcT"
      },
      "outputs": [],
      "source": [
        "test_type_col = train_features['Type']\n",
        "test_type_layer = get_category_encoding_layer(name='Type',\n",
        "                                              dataset=train_ds,\n",
        "                                              dtype='string')\n",
        "test_type_layer(test_type_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6eDongw8knz"
      },
      "source": [
        "Repita o processo para as características `'Age'` do animal doméstico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FjBioQ38oNE"
      },
      "outputs": [],
      "source": [
        "test_age_col = train_features['Age']\n",
        "test_age_layer = get_category_encoding_layer(name='Age',\n",
        "                                             dataset=train_ds,\n",
        "                                             dtype='int64',\n",
        "                                             max_tokens=5)\n",
        "test_age_layer(test_age_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiE0glOPkMyh"
      },
      "source": [
        "## Pré-processar determinadas características para treinar o modelo\n",
        "\n",
        "Você aprendeu a usar diversos tipos de camada de pré-processamento do Keras. Agora você vai:\n",
        "\n",
        "- Aplicar as funções utilitárias de pré-processamento definidas anteriormente em 13 características numéricas e de categoria do dataset PetFinder.my reduzido.\n",
        "- Adicionar todas as características de entrada a uma lista.\n",
        "\n",
        "Conforme mencionado no começo, para treinar o modelo, usaremos as características numéricas (`'PhotoAmt'`, `'Fee'`) e de categoria (`'Age'`, `'Type'`, `'Color1'`, `'Color2'`, `'Gender'`, `'MaturitySize'`, `'FurLength'`, `'Vaccinated'`, `'Sterilized'`, `'Health'`, `'Breed1'`) do dataset PetFinder.my reduzido.\n",
        "\n",
        "Observação: se o seu objetivo é criar um modelo exato, experimente usar um dataset maior e pense com cuidado em quais características são as mais importantes a serem incluídas e como devem ser representadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj1GoHSZ9R3H"
      },
      "source": [
        "Anteriormente, usamos um tamanho pequeno para o lote a fim de demonstrar o pipeline de entrada. Agora, vamos criar um novo pipeline de entrada com um tamanho maior para o lote, igual a 256:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rcv2kQTTo23h"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bIGNYN2V7iR"
      },
      "source": [
        "Normalize as características numéricas (o número de fotos e a taxa de adoção do animal doméstico) e adicione-as a uma lista de entradas chamada `encoded_features`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3RBa51VkaAn"
      },
      "outputs": [],
      "source": [
        "all_inputs = []\n",
        "encoded_features = []\n",
        "\n",
        "# Numerical features.\n",
        "for header in ['PhotoAmt', 'Fee']:\n",
        "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "  normalization_layer = get_normalization_layer(header, train_ds)\n",
        "  encoded_numeric_col = normalization_layer(numeric_col)\n",
        "  all_inputs.append(numeric_col)\n",
        "  encoded_features.append(encoded_numeric_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVcUAFd6bvlT"
      },
      "source": [
        "Transforme os valores inteiros de categoria do dataset (a idade do animal doméstico) em índices inteiros, faça a codificação multi-hot e adicione as entradas de características resultantes a `encoded_features`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FOMGfZflhoA"
      },
      "outputs": [],
      "source": [
        "age_col = tf.keras.Input(shape=(1,), name='Age', dtype='int64')\n",
        "\n",
        "encoding_layer = get_category_encoding_layer(name='Age',\n",
        "                                             dataset=train_ds,\n",
        "                                             dtype='int64',\n",
        "                                             max_tokens=5)\n",
        "encoded_age_col = encoding_layer(age_col)\n",
        "all_inputs.append(age_col)\n",
        "encoded_features.append(encoded_age_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYzynk6wdqKe"
      },
      "source": [
        "Repita o mesmo passo para os valores de categoria representados por strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8C8xyiXm-Ie"
      },
      "outputs": [],
      "source": [
        "categorical_cols = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
        "                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1']\n",
        "\n",
        "for header in categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(name=header,\n",
        "                                               dataset=train_ds,\n",
        "                                               dtype='string',\n",
        "                                               max_tokens=5)\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHSnhz2fyEq3"
      },
      "source": [
        "## Criar, compilar e treinar o modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDGyN_wpo0XS"
      },
      "source": [
        "O próximo passo é criar um modelo usando a [API Functional do Keras](https://www.tensorflow.org/guide/keras/functional). Para a primeira camada do modelo, combine a lista de entradas de característica — `encoded_features` — em um vetor por meio da concatenação com `tf.keras.layers.concatenate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yrj-_pr6jyL"
      },
      "outputs": [],
      "source": [
        "all_features = tf.keras.layers.concatenate(encoded_features)\n",
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "model = tf.keras.Model(all_inputs, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRLDRcYAefTA"
      },
      "source": [
        "Configure o modelo com `Model.compile` do Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZDb_lJdelSg"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6mNMfG6yEq5"
      },
      "source": [
        "Vamos conferir o grafo de conectividade:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7Bkx4c7yEq5"
      },
      "outputs": [],
      "source": [
        "# Use `rankdir='LR'` to make the graph horizontal.\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CED6OStLyEq7"
      },
      "source": [
        "Agora, vamos treinar e testar o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQfE3PC6yEq8"
      },
      "outputs": [],
      "source": [
        "model.fit(train_ds, epochs=10, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8N2uAdU2Cni"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmZMnTKaCZda"
      },
      "source": [
        "## Fazer inferência\n",
        "\n",
        "Agora, o modelo desenvolvido pode classificar uma linha de um arquivo CSV diretamente após a inclusão das camadas de pré-processamento dentro do modelo.\n",
        "\n",
        "Agora você pode [salvar e recarregar o modelo do Keras](../keras/save_and_load.ipynb) usando `Model.save` e `Model.load_model` antes de fazer a inferência para novos dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH9Zy1sBvwOH"
      },
      "outputs": [],
      "source": [
        "model.save('my_pet_classifier.keras')\n",
        "reloaded_model = tf.keras.models.load_model('my_pet_classifier.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D973plJrdwQ9"
      },
      "source": [
        "Para fazer uma previsão para uma nova amostra, basta chamar o método `Model.predict` do Keras. Você só precisa fazer duas ações:\n",
        "\n",
        "1. Encapsular escalares em uma lista para ter uma dimensão de lote (`Model` processa somente lotes de dados, não amostras individuais).\n",
        "2. Chamar `tf.convert_to_tensor` em cada característica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKq4pxtdDa7i"
      },
      "outputs": [],
      "source": [
        "sample = {\n",
        "    'Type': 'Cat',\n",
        "    'Age': 3,\n",
        "    'Breed1': 'Tabby',\n",
        "    'Gender': 'Male',\n",
        "    'Color1': 'Black',\n",
        "    'Color2': 'White',\n",
        "    'MaturitySize': 'Small',\n",
        "    'FurLength': 'Short',\n",
        "    'Vaccinated': 'No',\n",
        "    'Sterilized': 'No',\n",
        "    'Health': 'Healthy',\n",
        "    'Fee': 100,\n",
        "    'PhotoAmt': 2,\n",
        "}\n",
        "\n",
        "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "predictions = reloaded_model.predict(input_dict)\n",
        "prob = tf.nn.sigmoid(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This particular pet had a %.1f percent probability \"\n",
        "    \"of getting adopted.\" % (100 * prob)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJQQZEiH2FaB"
      },
      "source": [
        "Observação: geralmente, você terá resultados melhores com aprendizado profundo ao usar datasets muito maiores e mais complexos. Ao utilizar um dataset pequeno, como o PetFinder.my reduzido, você pode usar uma <a href=\"https://developers.google.com/machine-learning/glossary#decision-tree\" class=\"external\">árvore de decisão</a> ou uma <a href=\"https://developers.google.com/machine-learning/glossary#random-forest\" class=\"external\">floresta aleatória</a> como linha de base. O objetivo deste tutorial é demonstrar a mecânica ao trabalhar com dados estruturados para que você tenha um ponto de partida ao trabalhar com seus próprios datasets no futuro.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0QAY2Tb2HYG"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "Para saber mais sobre a classificação de dados estruturados, use outros datasets. Para aumentar a exatidão durante o treinamento e teste dos modelos, pense com cuidado em quais características você incluirá no modelo e como elas serão representadas.\n",
        "\n",
        "Sugerimos alguns datasets:\n",
        "\n",
        "- [Datasets do TensorFlow: MovieLens](https://www.tensorflow.org/datasets/catalog/movie_lens): conjunto de avaliações de filmes de um serviço de recomendação de filmes.\n",
        "- [Datasets do TensorFlow: Wine Quality](https://www.tensorflow.org/datasets/catalog/wine_quality): dois datasets relacionados a vinhos tinto e branco da vinícola portuguesa \"Vinho Verde\". O dataset Red Wine Quality (qualidade de vinhos tinto) também está disponível no <a href=\"https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\" class=\"external\">Kaggle</a>.\n",
        "- <a href=\"https://www.kaggle.com/Cornell-University/arxiv\" class=\"external\">Kaggle: dataset arXiv</a>: corpus com 1,7 milhão de artigos acadêmicos do arXiv, que abrangem física, ciência da computação, matemática, estatística, engenharia elétrica, biologia quantitativa e economia.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocessing_layers.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
