{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9teObmxrP0FE"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mz8tfSwOP4fW"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBwpoARnQ3H-"
      },
      "source": [
        "# Aprendizado profundo com SNGP com reconhecimento de incerteza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dL6_obQRBGQ"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/understanding/sngp\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/understanding/sngp.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/understanding/sngp.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/understanding/sngp.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvW1QEMtP7Gy"
      },
      "source": [
        "Em aplicações de inteligência artificial críticas, como tomada de decisão médica e veículos autônomos, ou quando os dados têm ruído inerente (por exemplo, compreensão de língua natural), é importante que um classificador profundo quantifique a incerteza com confiança. O classificador profundo deve conseguir reconhecer suas próprias limitações e quando deve passar o controle para especialistas humanos. Este tutorial mostra como melhorar a capacidade de um classificador profundo de quantificar a incerteza usando uma técnica chamada **Processo Gaussiano Neural normalizado espectral ([SNGP, na sigla em inglês](https://arxiv.org/abs/2006.10108){.external})**.\n",
        "\n",
        "A ideia principal do SNGP é melhorar o ***reconhecimento de distância*** de um classificador profundo por meio de modificações simples da rede. O *reconhecimento de distância* de um modelo é uma medida de como sua probabilidade preditiva reflete a distância entre o exemplo de teste e os dados de treinamento. É uma propriedade desejável comum aos modelos probabilísticos padrão-ouro (por exemplo, o [processo gaussiano](https://en.wikipedia.org/wiki/Gaussian_process){.external} com kernels RBF), mas que falta nos modelos com redes neurais profundas. O SNGP oferece uma maneira simples de incorporar esse comportamento de processo gaussiano a um classificador profundo, mantendo sua exatidão preditiva.\n",
        "\n",
        "Este tutorial implementa um modelo de SNGP de rede residual profunda baseado em ResNet no dataset [two moons do scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html){.external} (duas Luas) e compara sua superfície de incerteza com a de duas outras estratégias de incerteza comuns: [dropout de Monte Carlo](https://arxiv.org/abs/1506.02142){.external} e [Ensemble profundo](https://arxiv.org/abs/1612.01474){.external}.\n",
        "\n",
        "Este tutorial ilustra o modelo de SNGP em um dataset bidimensional de brinquedo. Para ver um exemplo de como aplicar o SNGP a uma tarefa real de compreensão de língua natural usando uma base BERT, confira o [tutorial SNGP-BERT](https://www.tensorflow.org/text/tutorials/uncertainty_quantification_with_sngp_bert). Para ver implementações de alta qualidade de um modelo de SNGP (e muitos outros métodos de incerteza) em uma grande variedade de conjuntos referenciais (como [CIFAR-100](https://www.tensorflow.org/datasets/catalog/cifar100), [ImageNet](https://www.tensorflow.org/datasets/catalog/imagenet2012), [Jigsaw toxicity detection](https://www.tensorflow.org/datasets/catalog/wikipedia_toxicity_subtypes), etc), confira o referencial [Linhas de base de incerteza](https://github.com/google/uncertainty-baselines){.external}."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-tZzAIlvMv_"
      },
      "source": [
        "## Sobre o SNGP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysyslHCyvYi-"
      },
      "source": [
        "O SNGP é uma estratégia simples para melhorar a qualidade de incerteza de um classificador profundo mantendo um nível similar de exatidão e latência. Dada uma rede residual profunda, o SNGP faz duas alterações simples no modelo:\n",
        "\n",
        "- Aplica a normalização espectral às camadas residuais ocultas.\n",
        "- Substitui a camada de saída Dense por uma camada de processo gaussiano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffU8rJ_eWHou"
      },
      "source": [
        "> ![SNGP](http://tensorflow.org/tutorials/understanding/images/sngp.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L88PoKr6XaE"
      },
      "source": [
        "Comparado a outras estratégias de incerteza (como dropout de Monte Carlo e Ensemble profundo), o SNGP tem diversas vantagens:\n",
        "\n",
        "- Funciona com diversas arquiteturas residuais de última geração (por exemplo, (Wide) ResNet, DenseNet ou BERT).\n",
        "- É um método com um único modelo (não depende da média do ensemble). Portanto, o SNGP tem um nível similar de latência de uma única rede determinística e pode ser facilmente dimensionado para datasets grandes, como [ImageNet](https://github.com/google/uncertainty-baselines/tree/main/baselines/imagenet){.external} e [Jigsaw Toxic Comments classification](https://github.com/google/uncertainty-baselines/tree/main/baselines/toxic_comments){.external}.\n",
        "- Tem um bom desempenho de detecção fora do domínio devido à propriedade de *reconhecimento de distância*.\n",
        "\n",
        "As desvantagens deste método são:\n",
        "\n",
        "- A incerteza preditiva do SNGP é computada usando-se o [método de aproximação de Laplace](http://www.gaussianprocess.org/gpml/chapters/RW3.pdf){.external}. Portanto, teoricamente, a incerteza posterior do SNGP é diferente da de um processo gaussiano exato.\n",
        "\n",
        "- O treinamento do SNGP requer uma etapa de redefinição da covariância no começo de uma nova época, o que pode adicionar uma pequena complexidade extra a um pipeline de treinamento. Este tutorial mostra uma forma simples de implementar isso usando callbacks do Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck_O7S8r1boS"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOS9qFlW2o3J"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q --use-deprecated=legacy-resolver tf-models-official tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCoGSYz-PIR7"
      },
      "outputs": [],
      "source": [
        "# refresh pkg_resources so it takes the changes into account.\n",
        "import pkg_resources\n",
        "import importlib\n",
        "importlib.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJKtMbtYNXHn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "\n",
        "import sklearn.datasets\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import official.nlp.modeling.layers as nlp_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RCyTrQO4ZXo"
      },
      "source": [
        "Defina as macros de visualização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5VTrcxo3rI-"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.dpi'] = 140\n",
        "\n",
        "DEFAULT_X_RANGE = (-3.5, 3.5)\n",
        "DEFAULT_Y_RANGE = (-2.5, 2.5)\n",
        "DEFAULT_CMAP = colors.ListedColormap([\"#377eb8\", \"#ff7f00\"])\n",
        "DEFAULT_NORM = colors.Normalize(vmin=0, vmax=1,)\n",
        "DEFAULT_N_GRID = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL5HzdYT5x5J"
      },
      "source": [
        "## Dataset two moon (duas Luas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqazrSzhd24R"
      },
      "source": [
        "Crie os datasets de treinamento e avaliação a partir do [dataset two moon do scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html){.external}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ_ul9Ii52mh"
      },
      "outputs": [],
      "source": [
        "def make_training_data(sample_size=500):\n",
        "  \"\"\"Create two moon training dataset.\"\"\"\n",
        "  train_examples, train_labels = sklearn.datasets.make_moons(\n",
        "      n_samples=2 * sample_size, noise=0.1)\n",
        "\n",
        "  # Adjust data position slightly.\n",
        "  train_examples[train_labels == 0] += [-0.1, 0.2]\n",
        "  train_examples[train_labels == 1] += [0.1, -0.2]\n",
        "\n",
        "  return train_examples, train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goQkrxR_fFGd"
      },
      "source": [
        "Avalie o comportamento preditivo do modelo para todo o espaço de entrada bidimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj7ldkNw5-cT"
      },
      "outputs": [],
      "source": [
        "def make_testing_data(x_range=DEFAULT_X_RANGE, y_range=DEFAULT_Y_RANGE, n_grid=DEFAULT_N_GRID):\n",
        "  \"\"\"Create a mesh grid in 2D space.\"\"\"\n",
        "  # testing data (mesh grid over data space)\n",
        "  x = np.linspace(x_range[0], x_range[1], n_grid)\n",
        "  y = np.linspace(y_range[0], y_range[1], n_grid)\n",
        "  xv, yv = np.meshgrid(x, y)\n",
        "  return np.stack([xv.flatten(), yv.flatten()], axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9BYe4yqfeFa"
      },
      "source": [
        "Para avaliar a incerteza do modelo, adicione um dataset fora do domínio (OOD, na sigla em inglês) que pertença a uma terceira classe. O modelo nunca observa esses exemplos OOD durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UHz2SU4feSI"
      },
      "outputs": [],
      "source": [
        "def make_ood_data(sample_size=500, means=(2.5, -1.75), vars=(0.01, 0.01)):\n",
        "  return np.random.multivariate_normal(\n",
        "      means, cov=np.diag(vars), size=sample_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1-jmJb45_at"
      },
      "outputs": [],
      "source": [
        "# Load the train, test and OOD datasets.\n",
        "train_examples, train_labels = make_training_data(\n",
        "    sample_size=500)\n",
        "test_examples = make_testing_data()\n",
        "ood_examples = make_ood_data(sample_size=500)\n",
        "\n",
        "# Visualize\n",
        "pos_examples = train_examples[train_labels == 0]\n",
        "neg_examples = train_examples[train_labels == 1]\n",
        "\n",
        "plt.figure(figsize=(7, 5.5))\n",
        "\n",
        "plt.scatter(pos_examples[:, 0], pos_examples[:, 1], c=\"#377eb8\", alpha=0.5)\n",
        "plt.scatter(neg_examples[:, 0], neg_examples[:, 1], c=\"#ff7f00\", alpha=0.5)\n",
        "plt.scatter(ood_examples[:, 0], ood_examples[:, 1], c=\"red\", alpha=0.1)\n",
        "\n",
        "plt.legend([\"Positive\", \"Negative\", \"Out-of-Domain\"])\n",
        "\n",
        "plt.ylim(DEFAULT_Y_RANGE)\n",
        "plt.xlim(DEFAULT_X_RANGE)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlzxsnBBkybB"
      },
      "source": [
        "Aqui, azul e laranja representam as classes positivas e negativas, enquanto vermelho representa os dados OOD. Espera-se que um modelo que quantifique bem a incerteza tenha confiança quando próximo dos dados de treinamento (ou seja, $p(x_{test})$ próximo de 0 ou 1), e tenha incerteza quando distante das regiões dos dados de treinamento (ou seja, $p(x_{test})$ próximo de 0,5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ3i4n8li-Mv"
      },
      "source": [
        "## Modelo determinístico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utncgxs3lc4u"
      },
      "source": [
        "### Defina o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z83-nq6jPlb"
      },
      "source": [
        "Comece pelo modelo determinístico (linha de base): uma rede (ResNet) multicamada com regularização de dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wCBRm8fc6CgY"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class DeepResNet(tf.keras.Model):\n",
        "  \"\"\"Defines a multi-layer residual network.\"\"\"\n",
        "  def __init__(self, num_classes, num_layers=3, num_hidden=128,\n",
        "               dropout_rate=0.1, **classifier_kwargs):\n",
        "    super().__init__()\n",
        "    # Defines class meta data.\n",
        "    self.num_hidden = num_hidden\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.classifier_kwargs = classifier_kwargs\n",
        "\n",
        "    # Defines the hidden layers.\n",
        "    self.input_layer = tf.keras.layers.Dense(self.num_hidden, trainable=False)\n",
        "    self.dense_layers = [self.make_dense_layer() for _ in range(num_layers)]\n",
        "\n",
        "    # Defines the output layer.\n",
        "    self.classifier = self.make_output_layer(num_classes)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Projects the 2d input data to high dimension.\n",
        "    hidden = self.input_layer(inputs)\n",
        "\n",
        "    # Computes the ResNet hidden representations.\n",
        "    for i in range(self.num_layers):\n",
        "      resid = self.dense_layers[i](hidden)\n",
        "      resid = tf.keras.layers.Dropout(self.dropout_rate)(resid)\n",
        "      hidden += resid\n",
        "\n",
        "    return self.classifier(hidden)\n",
        "\n",
        "  def make_dense_layer(self):\n",
        "    \"\"\"Uses the Dense layer as the hidden layer.\"\"\"\n",
        "    return tf.keras.layers.Dense(self.num_hidden, activation=\"relu\")\n",
        "\n",
        "  def make_output_layer(self, num_classes):\n",
        "    \"\"\"Uses the Dense layer as the output layer.\"\"\"\n",
        "    return tf.keras.layers.Dense(\n",
        "        num_classes, **self.classifier_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u870GAen2aO"
      },
      "source": [
        "Este tutorial utiliza uma ResNet de seis camadas, com 128 unidades ocultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWL9wCnGpc4h"
      },
      "outputs": [],
      "source": [
        "resnet_config = dict(num_classes=2, num_layers=6, num_hidden=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I47RY26wurgg"
      },
      "outputs": [],
      "source": [
        "resnet_model = DeepResNet(**resnet_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okQXf2F1ur16"
      },
      "outputs": [],
      "source": [
        "resnet_model.build((None, 2))\n",
        "resnet_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZzueLoImW0t"
      },
      "source": [
        "### Treine o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfQ1k-IUukLt"
      },
      "source": [
        "Configure os parâmetros de treinamento para usar `SparseCategoricalCrossentropy` como a função de perda e o otimizador Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ZkfNXAnumV"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy(),\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n",
        "\n",
        "train_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEWzfHvf6A5_"
      },
      "source": [
        "Treine o modelo com 100 épocas e tamanho de lote igual a 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fx6EtGXpzVr"
      },
      "outputs": [],
      "source": [
        "fit_config = dict(batch_size=128, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFwkbKIuqj7Y"
      },
      "outputs": [],
      "source": [
        "resnet_model.compile(**train_config)\n",
        "resnet_model.fit(train_examples, train_labels, **fit_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo6tKKd1rvBh"
      },
      "source": [
        "### Visualize a incerteza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HZDMX7gZrZ-5"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def plot_uncertainty_surface(test_uncertainty, ax, cmap=None):\n",
        "  \"\"\"Visualizes the 2D uncertainty surface.\n",
        "  \n",
        "  For simplicity, assume these objects already exist in the memory:\n",
        "\n",
        "    test_examples: Array of test examples, shape (num_test, 2).\n",
        "    train_labels: Array of train labels, shape (num_train, ).\n",
        "    train_examples: Array of train examples, shape (num_train, 2).\n",
        "  \n",
        "  Arguments:\n",
        "    test_uncertainty: Array of uncertainty scores, shape (num_test,).\n",
        "    ax: A matplotlib Axes object that specifies a matplotlib figure.\n",
        "    cmap: A matplotlib colormap object specifying the palette of the\n",
        "      predictive surface.\n",
        "\n",
        "  Returns:\n",
        "    pcm: A matplotlib PathCollection object that contains the palette\n",
        "      information of the uncertainty plot.\n",
        "  \"\"\"\n",
        "  # Normalize uncertainty for better visualization.\n",
        "  test_uncertainty = test_uncertainty / np.max(test_uncertainty)\n",
        "\n",
        "  # Set view limits.\n",
        "  ax.set_ylim(DEFAULT_Y_RANGE)\n",
        "  ax.set_xlim(DEFAULT_X_RANGE)\n",
        "\n",
        "  # Plot normalized uncertainty surface.\n",
        "  pcm = ax.imshow(\n",
        "      np.reshape(test_uncertainty, [DEFAULT_N_GRID, DEFAULT_N_GRID]),\n",
        "      cmap=cmap,\n",
        "      origin=\"lower\",\n",
        "      extent=DEFAULT_X_RANGE + DEFAULT_Y_RANGE,\n",
        "      vmin=DEFAULT_NORM.vmin,\n",
        "      vmax=DEFAULT_NORM.vmax,\n",
        "      interpolation='bicubic',\n",
        "      aspect='auto')\n",
        "\n",
        "  # Plot training data.\n",
        "  ax.scatter(train_examples[:, 0], train_examples[:, 1],\n",
        "             c=train_labels, cmap=DEFAULT_CMAP, alpha=0.5)\n",
        "  ax.scatter(ood_examples[:, 0], ood_examples[:, 1], c=\"red\", alpha=0.1)\n",
        "\n",
        "  return pcm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1age2y0339T"
      },
      "source": [
        "Agora, visualize as predições do modelo determinístico. Primeiro plote a probabilidade da classe: $$p(x) = softmax(logit(x))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aqFgQOD40lb"
      },
      "outputs": [],
      "source": [
        "resnet_logits = resnet_model(test_examples)\n",
        "resnet_probs = tf.nn.softmax(resnet_logits, axis=-1)[:, 0]  # Take the probability for class 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpflH2Qj33oN"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(7, 5.5))\n",
        "\n",
        "pcm = plot_uncertainty_surface(resnet_probs, ax=ax)\n",
        "\n",
        "plt.colorbar(pcm, ax=ax)\n",
        "plt.title(\"Class Probability, Deterministic Model\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ShGAB7FNYgU"
      },
      "source": [
        "Neste gráfico, amarelo e roxo são as probabilidades preditivas das duas classes. O modelo determinístico teve bom desempenho ao classificar as duas classes desconhecidas, azul e laranja, com um limite de decisão não linear. Entretanto, ele não tem **reconhecimento de distância** e classificou os exemplos fora do domínio (OOD) vermelhos, nunca observados antes, como a classe laranja com confiança.\n",
        "\n",
        "Para visualizar a incerteza do modelo, calcule a [variância preditiva](https://en.wikipedia.org/wiki/Bernoulli_distribution#Variance): $$var(x) = p(x) * (1 - p(x))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxMce5D15Qwt"
      },
      "outputs": [],
      "source": [
        "resnet_uncertainty = resnet_probs * (1 - resnet_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwdEfb5_6woh"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(7, 5.5))\n",
        "\n",
        "pcm = plot_uncertainty_surface(resnet_uncertainty, ax=ax)\n",
        "\n",
        "plt.colorbar(pcm, ax=ax)\n",
        "plt.title(\"Predictive Uncertainty, Deterministic Model\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alVunBEXhD-1"
      },
      "source": [
        "Neste gráfico, amarelo indica incerteza alta, e roxo indica incerteza baixa. A incerteza de uma ResNet determinística depende somente da distância entre os exemplos de teste e o limite de decisão. Isso faz o modelo ter uma confiança alta demais quando fora do domínio de treinamento. A próxima seção mostra como o SNGP se comporta de maneira diferente para este dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwDa80iOh32J"
      },
      "source": [
        "## Modelo de SNGP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbjNrmLu8oXv"
      },
      "source": [
        "### Defina o modelo de SNGP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urtuCrk1Hf5P"
      },
      "source": [
        "Agora, vamos implementar o modelo de SNGP. Os dois componentes do SNGP, `SpectralNormalization` e `RandomFeatureGaussianProcess`, estão disponíveis nas [camadas integradas](https://github.com/tensorflow/models/tree/master/official/nlp/modeling/layers) do modelo do TensorFlow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IlQwKCEGwpk"
      },
      "source": [
        "> ![SNGP](http://tensorflow.org/tutorials/understanding/images/sngp.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2O2iv8LSke"
      },
      "source": [
        "Vamos avaliar esses dois componentes com mais detalhes (você também pode ir para a seção [modelo de SNGP completo](#full-sngp-model) para ver como o SNGP é implementado)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n4NIt3QjKwl"
      },
      "source": [
        "#### Encapsulador `SpectralNormalization`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE-Va7J2jR2X"
      },
      "source": [
        "[`SpectralNormalization`](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/spectral_normalization.py){.external} é um encapsulador de camadas do Keras e pode ser aplicado a uma camada Dense existente da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp8vqJBWLSq3"
      },
      "outputs": [],
      "source": [
        "dense = tf.keras.layers.Dense(units=10)\n",
        "dense = nlp_layers.SpectralNormalization(dense, norm_multiplier=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9q25_6fRJRh"
      },
      "source": [
        "A normalização espectral normaliza o peso oculto $W$ ao levar gradualmente sua norma espectral (ou seja, o autovalor mais alto de $W$) em direção ao valor alvo `norm_multiplier`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqt-DVzvqyAE"
      },
      "source": [
        "Observação: geralmente, é preferível definir `norm_multiplier` como um valor menor do que 1. Entretanto, na prática, também pode ser definido como um valor mais alto para garantir que a rede profunda tenha potência expressiva suficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqvxJUXBjBhV"
      },
      "source": [
        "#### Camada do processo gaussiano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rYfIgtrjHnB"
      },
      "source": [
        "[`RandomFeatureGaussianProcess`](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/gaussian_process.py){.external} implementa uma [aproximação baseada em característica aleatória](https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf){.external} de um modelo de processo gaussiano que pode ser treinado do começo ao fim com uma rede neural profunda. Em segundo plano, a camada do processo gaussiano implementa uma rede de duas camadas:\n",
        "\n",
        "$$logits(x) = \\Phi(x) \\beta, \\quad \\Phi(x)=\\sqrt{\\frac{2}{M}} * cos(Wx + b)$$\n",
        "\n",
        "Aqui, $x$ é a entrada, e $W$ e $b$ são pesos congelados inicializados aleatoriamente a partir das distribuições gaussiana e uniforme, respectivamente (portanto, $\\Phi(x)$ são chamadas de \"características aleatórias\"). $\\beta$ é o peso de kernel que pode ser aprendido, similar ao de uma camada Dense. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqnU39ui3wAE"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "input_dim = 1024\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrlVd-foRJno"
      },
      "outputs": [],
      "source": [
        "gp_layer = nlp_layers.RandomFeatureGaussianProcess(units=num_classes,\n",
        "                                               num_inducing=1024,\n",
        "                                               normalize_input=False,\n",
        "                                               scale_random_features=True,\n",
        "                                               gp_cov_momentum=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxb8sSAg5AGf"
      },
      "source": [
        "Os principais parâmetros das camadas do processo gaussiano são:\n",
        "\n",
        "- `units`: dimensão dos logits de saída.\n",
        "- `num_inducing`: dimensão $M$ do peso oculto $W$. O padrão é 1024.\n",
        "- `normalize_input`: define se a normalização da camada deve ser aplicada à entrada $x$.\n",
        "- `scale_random_features`: define se a escala $\\sqrt{2/M}$ deve ser aplicada à saída oculta.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgzw09gS03ae"
      },
      "source": [
        "Observação: para uma rede neural profunda sensível à taxa de aprendizado (por exemplo, ResNet-50 e ResNet-110), geralmente recomenda-se definir `normalize_input=True` para estabilizar o treinamento e definir `scale_random_features=False` para evitar que a taxa de aprendizado seja modificada de formas inesperadas ao passar pela camada do processo gaussiano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZkcKw-u7XRp"
      },
      "source": [
        "- `gp_cov_momentum` controla como a covariância do modelo é calculada. Se definido como um valor positivo (por exemplo, `0.999`), a matriz de covariância é calculada usando-se a atualização da média móvel baseada no momento (similar à normalização de lotes). Se definido como `-1`, a matriz de covariância é atualizada sem usar o momento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P13X7Adt-c2d"
      },
      "source": [
        "Observação: o método de avaliação com base no momento pode ser sensível ao tamanho do lote. Portanto, geralmente recomenda-se definir `gp_cov_momentum=-1` para calcular o valor exato da covariância. Para que isso funcione corretamente, o estimador da matriz de covariância precisa ser redefinido no começo de cada época para evitar a contabilização dos mesmos dados duas vezes. Para `RandomFeatureGaussianProcess`, isso pode ser feito realizando uma chamada a `reset_covariance_matrix()`. A próxima seção mostra uma implementação fácil usando a API integrada do Keras.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AA492qA1biZ"
      },
      "source": [
        "Dada uma entrada de lote com formato `(batch_size, input_dim)`, a camada do processo gaussiano retorna um tensor `logits` (formato `(batch_size, num_classes)`) para a previsão, além de um tensor `covmat` (formato `(batch_size, batch_size)`), que é a matriz de covariância posterior dos logits do lote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOXxBYnMi1v4"
      },
      "outputs": [],
      "source": [
        "embedding = tf.random.normal(shape=(batch_size, input_dim))\n",
        "\n",
        "logits, covmat = gp_layer(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALBqcAtwDNiO"
      },
      "source": [
        "Observação: com esta implementação do modelo de SNGP, os logits preditivos $logit(x_{test})$ de todas as classes compartilham a mesma matriz de covariância $var(x_{test})$, que descreve a distância entre $x_{test}$ e os dados de treinamento.\n",
        "\n",
        "Teoricamente, é possível estender o algoritmo para calcular os diferentes valores de covariância para as diferentes classes (conforme discutido no [artigo original sobre SNGP](https://arxiv.org/abs/2006.10108){.external}). Entretanto, é difícil fazer isso para problemas com espaços de saída grandes (como classificação com ImageNet ou modelagem de língua)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II3GVzjJhu5Z"
      },
      "source": [
        "<a name=\"full-sngp-model\"></a>\n",
        "\n",
        "#### Modelo de SNGP completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Fm0NUlLTHd"
      },
      "source": [
        "Dada a classe base `DeepResNet`, é fácil implementar o modelo de SNGP por meio da modificação das camadas ocultas e da saída da rede residual. Para fins de compatibilidade com a API `model.fit()` do Keras, modifique também o método `call()` do modelo para que gere somente os `logits` durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzx4FsO97QZv"
      },
      "outputs": [],
      "source": [
        "class DeepResNetSNGP(DeepResNet):\n",
        "  def __init__(self, spec_norm_bound=0.9, **kwargs):\n",
        "    self.spec_norm_bound = spec_norm_bound\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def make_dense_layer(self):\n",
        "    \"\"\"Applies spectral normalization to the hidden layer.\"\"\"\n",
        "    dense_layer = super().make_dense_layer()\n",
        "    return nlp_layers.SpectralNormalization(\n",
        "        dense_layer, norm_multiplier=self.spec_norm_bound)\n",
        "\n",
        "  def make_output_layer(self, num_classes):\n",
        "    \"\"\"Uses Gaussian process as the output layer.\"\"\"\n",
        "    return nlp_layers.RandomFeatureGaussianProcess(\n",
        "        num_classes,\n",
        "        gp_cov_momentum=-1,\n",
        "        **self.classifier_kwargs)\n",
        "\n",
        "  def call(self, inputs, training=False, return_covmat=False):\n",
        "    # Gets logits and a covariance matrix from the GP layer.\n",
        "    logits, covmat = super().call(inputs)\n",
        "\n",
        "    # Returns only logits during training.\n",
        "    if not training and return_covmat:\n",
        "      return logits, covmat\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02SvlnDFyszm"
      },
      "source": [
        "Utilize a mesma arquitetura usada no modelo determinístico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiDcC5ipyqMU"
      },
      "outputs": [],
      "source": [
        "resnet_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9-imQtDyZL5"
      },
      "outputs": [],
      "source": [
        "sngp_model = DeepResNetSNGP(**resnet_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12P9UxlCyq6k"
      },
      "outputs": [],
      "source": [
        "sngp_model.build((None, 2))\n",
        "sngp_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlCgMRrwkXgN"
      },
      "source": [
        "<a name=\"covariance-reset-callback\"></a> Implemente um callback do Keras para redefinir a matriz de covariância no começo de uma nova época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6Wr8D_0n-cQ"
      },
      "outputs": [],
      "source": [
        "class ResetCovarianceCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    \"\"\"Resets covariance matrix at the beginning of the epoch.\"\"\"\n",
        "    if epoch > 0:\n",
        "      self.model.classifier.reset_covariance_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMiz_mYawEPh"
      },
      "source": [
        "Adicione esse callback à classe do modelo `DeepResNetSNGP`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrSi0ZD5zaDf"
      },
      "outputs": [],
      "source": [
        "class DeepResNetSNGPWithCovReset(DeepResNetSNGP):\n",
        "  def fit(self, *args, **kwargs):\n",
        "    \"\"\"Adds ResetCovarianceCallback to model callbacks.\"\"\"\n",
        "    kwargs[\"callbacks\"] = list(kwargs.get(\"callbacks\", []))\n",
        "    kwargs[\"callbacks\"].append(ResetCovarianceCallback())\n",
        "\n",
        "    return super().fit(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asIwYqGlwJcP"
      },
      "source": [
        "### Treine o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YRzayOCopt9"
      },
      "source": [
        "Use `tf.keras.model.fit` para treinar o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coazo53nwJqv"
      },
      "outputs": [],
      "source": [
        "sngp_model = DeepResNetSNGPWithCovReset(**resnet_config)\n",
        "sngp_model.compile(**train_config)\n",
        "sngp_model.fit(train_examples, train_labels, **fit_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTONd8vowgEP"
      },
      "source": [
        "### Visualize a incerteza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhkpsiy10l9d"
      },
      "source": [
        "Primeiro, calcule as variâncias e os logits preditivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqRPqeEavi4Z"
      },
      "outputs": [],
      "source": [
        "sngp_logits, sngp_covmat = sngp_model(test_examples, return_covmat=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7w0iW_L0cU8"
      },
      "outputs": [],
      "source": [
        "sngp_variance = tf.linalg.diag_part(sngp_covmat)[:, None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLbz_EZF1Gay"
      },
      "source": [
        "<a name=\"mean-field-logits\"></a> Agora, calcule a probabilidade preditiva posterior. O método clássico para calcular a probabilidade preditiva de um modelo probabilístico é usando a amostragem de Monte Carlo:\n",
        "\n",
        "$$E(p(x)) = \\frac{1}{M} \\sum_{m=1}^M logit_m(x), $$\n",
        "\n",
        "em que $M$ é o tamanho da amostra, e $logit_m(x)$ são amostras aleatórias do $MultivariateNormal$(`sngp_logits` posterior do SNGP, `sngp_covmat`). Entretanto, essa estratégia pode ser lenta demais para aplicações sensíveis à latência, como veículos autônomos ou leilão em tempo real. Em vez disso, você pode aproximar $E(p(x))$ usando o [método do campo médio](https://arxiv.org/abs/2006.07584){.external}:\n",
        "\n",
        "$$E(p(x)) \\approx softmax(\\frac{logit(x)}{\\sqrt{1+ \\lambda * \\sigma^2(x)}})$$\n",
        "\n",
        "em que $\\sigma^2(x)$ é a variância do SNGP, e $\\lambda$ geralmente é escolhido como $\\pi/8$ ou $3/\\pi^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A9NYMhd0iZ-"
      },
      "outputs": [],
      "source": [
        "sngp_logits_adjusted = sngp_logits / tf.sqrt(1. + (np.pi / 8.) * sngp_variance)\n",
        "sngp_probs = tf.nn.softmax(sngp_logits_adjusted, axis=-1)[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNVs_KO-5HdL"
      },
      "source": [
        "Observação: em vez de fixar $\\lambda$, você também pode tratá-lo como um hiperparâmetro para otimizar o desempenho de calibração do modelo. Isso é conhecido como [Temperature Scaling](http://proceedings.mlr.press/v70/guo17a.html){.external} (dimensionamento de temperatura) na literatura sobre incerteza em aprendizado profundo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlYPlUbJfBFa"
      },
      "source": [
        "Esse método do campo médio é implementado como uma função integrada `layers.gaussian_process.mean_field_logits`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgb3WSaY8iQY"
      },
      "outputs": [],
      "source": [
        "def compute_posterior_mean_probability(logits, covmat, lambda_param=np.pi / 8.):\n",
        "  # Computes uncertainty-adjusted logits using the built-in method.\n",
        "  logits_adjusted = nlp_layers.gaussian_process.mean_field_logits(\n",
        "      logits, covmat, mean_field_factor=lambda_param)\n",
        "  \n",
        "  return tf.nn.softmax(logits_adjusted, axis=-1)[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVToZpG7QqS3"
      },
      "outputs": [],
      "source": [
        "sngp_logits, sngp_covmat = sngp_model(test_examples, return_covmat=True)\n",
        "sngp_probs = compute_posterior_mean_probability(sngp_logits, sngp_covmat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVi_Whpwe3O4"
      },
      "source": [
        "### Resumo do SNGP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dRmmxuO41BV4"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "def plot_predictions(pred_probs, model_name=\"\"):\n",
        "  \"\"\"Plot normalized class probabilities and predictive uncertainties.\"\"\"\n",
        "  # Compute predictive uncertainty.\n",
        "  uncertainty = pred_probs * (1. - pred_probs)\n",
        "\n",
        "  # Initialize the plot axes.\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "  # Plots the class probability.\n",
        "  pcm_0 = plot_uncertainty_surface(pred_probs, ax=axs[0])\n",
        "  # Plots the predictive uncertainty.\n",
        "  pcm_1 = plot_uncertainty_surface(uncertainty, ax=axs[1])\n",
        "\n",
        "  # Adds color bars and titles.\n",
        "  fig.colorbar(pcm_0, ax=axs[0])\n",
        "  fig.colorbar(pcm_1, ax=axs[1])\n",
        "\n",
        "  axs[0].set_title(f\"Class Probability, {model_name}\")\n",
        "  axs[1].set_title(f\"(Normalized) Predictive Uncertainty, {model_name}\")\n",
        "\n",
        "  plt.show()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9kY5dJg8fEi"
      },
      "source": [
        "Agora você pode juntar tudo. Todo o procedimento — treinamento, avaliação e cálculo da incerteza — pode ser feito em apenas cinco linhas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQtUAG4-ftqe"
      },
      "outputs": [],
      "source": [
        "def train_and_test_sngp(train_examples, test_examples):\n",
        "  sngp_model = DeepResNetSNGPWithCovReset(**resnet_config)\n",
        "\n",
        "  sngp_model.compile(**train_config)\n",
        "  sngp_model.fit(train_examples, train_labels, verbose=0, **fit_config)\n",
        "\n",
        "  sngp_logits, sngp_covmat = sngp_model(test_examples, return_covmat=True)\n",
        "  sngp_probs = compute_posterior_mean_probability(sngp_logits, sngp_covmat)\n",
        "\n",
        "  return sngp_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl3N7kHJ283w"
      },
      "outputs": [],
      "source": [
        "sngp_probs = train_and_test_sngp(train_examples, test_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAUGrXSv6k3R"
      },
      "source": [
        "Visualize a probabilidade das classes (à esquerda) e a incerteza preditiva (à direita) do modelo de SNGP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxt3CY51A_jq"
      },
      "outputs": [],
      "source": [
        "plot_predictions(sngp_probs, model_name=\"SNGP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raHP5Vuiuku9"
      },
      "source": [
        "Lembre-se de que, no gráfico de probabilidades de classes (à esquerda), amarelo e roxo são as probabilidades das classes. Quando próximo do domínio dos dados de treinamento, o SNGP classifica corretamente os exemplos com confiança alta (ou seja, atribuição perto da probabilidade 0 ou 1). Quando distante dos dados de treinamento, o SNGP fica cada vez mais menos confiante, e sua probabilidade preditiva se aproxima de 0,5, enquanto a incerteza do modelo (normalizada) sobe para 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4_VRi9w7Km3"
      },
      "source": [
        "Compare com a superfície de incerteza do modelo determinístico: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMAdstYZ7T_w"
      },
      "outputs": [],
      "source": [
        "plot_predictions(resnet_probs, model_name=\"Deterministic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mao9L-LYE1Nl"
      },
      "source": [
        "Conforme mencionado anteriormente, um modelo determinístico não tem *reconhecimento de distância*. Sua incerteza é definida pela distância entre o exemplo de teste e o limite de decisão. Isso faz o modelo gerar previsões com confiança alta demais para os exemplos fora do domínio (vermelhos)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKURpzOf0oNq"
      },
      "source": [
        "## Comparação com outras estratégias de incerteza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1DPELWE6LL8"
      },
      "source": [
        "Esta seção compara a incerteza do SNGP com [Dropout de Monte Carlo](https://arxiv.org/abs/1506.02142){.external} e [Ensemble profundo](https://arxiv.org/abs/1612.01474){.external}.\n",
        "\n",
        "Os dois métodos são baseados na média de Monte Carlo de diversos passos para frente de modelos determinísticos. Primeiro, defina o tamanho do ensemble $M$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLqkihbk8Dey"
      },
      "outputs": [],
      "source": [
        "num_ensemble = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM5AQmtIAatd"
      },
      "source": [
        "### Dropout de Monte Carlo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBzp2LBt7-kj"
      },
      "source": [
        "Dada uma rede neural treinada com camadas de dropout, o dropout de Monte Carlo calcula a probabilidade preditiva média:\n",
        "\n",
        "$$E(p(x)) = \\frac{1}{M}\\sum_{m=1}^M softmax(logit_m(x))$$\n",
        "\n",
        "fazendo a média de diversos passos para frente com dropout ${logit_m(x)}_{m=1}^M$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7R2WBgq4-OC"
      },
      "outputs": [],
      "source": [
        "def mc_dropout_sampling(test_examples):\n",
        "  # Enable dropout during inference.\n",
        "  return resnet_model(test_examples, training=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6oXgaDZAiD0"
      },
      "outputs": [],
      "source": [
        "# Monte Carlo dropout inference.\n",
        "dropout_logit_samples = [mc_dropout_sampling(test_examples) for _ in range(num_ensemble)]\n",
        "dropout_prob_samples = [tf.nn.softmax(dropout_logits, axis=-1)[:, 0] for dropout_logits in dropout_logit_samples]\n",
        "dropout_probs = tf.reduce_mean(dropout_prob_samples, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oUNtbVG-YuI"
      },
      "outputs": [],
      "source": [
        "dropout_probs = tf.reduce_mean(dropout_prob_samples, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-mhyp8hAiPn"
      },
      "outputs": [],
      "source": [
        "plot_predictions(dropout_probs, model_name=\"MC Dropout\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtj2vTB75cF"
      },
      "source": [
        "### Ensemble profundo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Z2veGJ9ZgY"
      },
      "source": [
        "O ensemble profundo é um método de última geração (porém caro) para incerteza em aprendizado profundo. Para treinar um ensemble profundo, primeiro treine os membros do ensemble $M$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a43hxiJC3Kla"
      },
      "outputs": [],
      "source": [
        "# Deep ensemble training\n",
        "resnet_ensemble = []\n",
        "for _ in range(num_ensemble):\n",
        "  resnet_model = DeepResNet(**resnet_config)\n",
        "  resnet_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "  resnet_model.fit(train_examples, train_labels, verbose=0, **fit_config)\n",
        "\n",
        "  resnet_ensemble.append(resnet_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al7uM-fn_ZE1"
      },
      "source": [
        "Colete os logits e calcule a probabilidade preditiva média $E(p(x)) = \\frac{1}{M}\\sum_{m=1}^M softmax(logit_m(x))$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6E9PntV3Mue"
      },
      "outputs": [],
      "source": [
        "# Deep ensemble inference\n",
        "ensemble_logit_samples = [model(test_examples) for model in resnet_ensemble]\n",
        "ensemble_prob_samples = [tf.nn.softmax(logits, axis=-1)[:, 0] for logits in ensemble_logit_samples]\n",
        "ensemble_probs = tf.reduce_mean(ensemble_prob_samples, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_JhYftG-NKR"
      },
      "outputs": [],
      "source": [
        "plot_predictions(ensemble_probs, model_name=\"Deep ensemble\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH33oVvV5-ez"
      },
      "source": [
        "Tanto o método de dropout de Monte Carlo quanto o de ensemble profundo melhoram a capacidade de incerteza do modelo ao diminuir a certeza do limite de decisão. Entretanto, ambos herdam a limitação de redes profundas determinísticas: a falta de reconhecimento de distância."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9nAaWuYfD03"
      },
      "source": [
        "## Resumo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ryMkRllFHOC"
      },
      "source": [
        "Neste tutorial, você:\n",
        "\n",
        "- Implementou o modelo de SNGP em um classificador profundo para melhorar seu reconhecimento de distância.\n",
        "- Treinou o modelo de SNGP do começo ao fim usando a API `Model.fit` do Keras.\n",
        "- Visualizou o comportamento da incerteza do SNGP.\n",
        "- Comparou o comportamento da incerteza entre os modelos de SNGP, dropout de Monte Carlo e ensemble profundo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoTekiQmkZXF"
      },
      "source": [
        "## Recursos e leitura adicional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoIikRybke-b"
      },
      "source": [
        "- Confira o [tutorial SNGP-BERT](https://www.tensorflow.org/text/tutorials/uncertainty_quantification_with_sngp_bert) para ver um exemplo de como aplicar o SNGP a um modelo BERT para a compreensão de língua natural com reconhecimento de incerteza.\n",
        "- Acesse o [repositório do GitHub Linhas de base de incerteza](https://github.com/google/uncertainty-baselines){.external} para ver a implementação do modelo de SNGP (e muitos outros métodos de incerteza) para uma variedade de datasets referenciais (por exemplo, [CIFAR](https://www.tensorflow.org/datasets/catalog/cifar100), [ImageNet](https://www.tensorflow.org/datasets/catalog/imagenet2012), [Jigsaw toxicity detection](https://www.tensorflow.org/datasets/catalog/wikipedia_toxicity_subtypes), etc).\n",
        "- Para ter uma compreensão mais profunda do método do SNGP, confira o artigo [Estimativa simples do princípio da incerteza com aprendizado profundo determinístico via reconhecimento de distância](https://arxiv.org/abs/2006.10108){.external}.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sngp.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
