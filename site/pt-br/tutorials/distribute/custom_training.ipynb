{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhoQ0WE77laV"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Treinamento personalizado com tf.distribute.Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/distribute/custom_training\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/distribute/custom_training.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/distribute/custom_training.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/distribute/custom_training.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbVhjPpzn6BM"
      },
      "source": [
        "Este tutorial demonstra como usar o `tf.distribute.Strategy`, uma API do TensorFlow que fornece uma abstração para [distribuir o treinamento](../../guide/distributed_training.ipynb) em diversas unidades de processamento (GPUs, diversas máquinas ou TPUs), com loops de treinamento personalizado. Neste exemplo, você treinará uma rede neural convolucional simples no [dataset Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist), que contém mais de 70 mil imagens de tamanho 28x28 pixels.\n",
        "\n",
        "[Os loops de treinamento personalizados](../customization/custom_training_walkthrough.ipynb) oferecem flexibilidade e um controle maior do treinamento. Além disso, facilitam a depuração do modelo e do loop de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM6W__qraV55"
      },
      "source": [
        "## Download do dataset Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MqDQO0KCaWS"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Add a dimension to the array -> new shape == (28, 28, 1)\n",
        "# This is done because the first layer in our model is a convolutional\n",
        "# layer and it requires a 4D input (batch_size, height, width, channels).\n",
        "# batch_size dimension will be added later on.\n",
        "train_images = train_images[..., None]\n",
        "test_images = test_images[..., None]\n",
        "\n",
        "# Scale the images to the [0, 1] range.\n",
        "train_images = train_images / np.float32(255)\n",
        "test_images = test_images / np.float32(255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AXoHhrsbdF3"
      },
      "source": [
        "## Criação de uma estratégia para distribuir as variáveis e o grafo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mVuLZhbem8d"
      },
      "source": [
        "Como a estratégia `tf.distribute.MirroredStrategy` funciona?\n",
        "\n",
        "- Todas as variáveis e o modelo de grafos são replicados em todas as réplicas.\n",
        "- As entradas são distribuídas para as réplicas de maneira uniforme.\n",
        "- Cada réplica calcula a perda e os gradientes para a entrada recebida.\n",
        "- Os gradientes são sincronizados em todas as réplicas através da **soma**.\n",
        "- Após a sincronização, a mesma atualização é feita nas cópias das variáveis em cada réplica.\n",
        "\n",
        "Observação: você pode colocar todo o código abaixo em um único escopo. Este exemplo divide-o em diversas células de código para fins de exemplificação.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2VeZUWUj5S4"
      },
      "outputs": [],
      "source": [
        "# If the list of devices is not specified in\n",
        "# `tf.distribute.MirroredStrategy` constructor, they will be auto-detected.\n",
        "strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZngeM_2o0_JO"
      },
      "outputs": [],
      "source": [
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k53F5I_IiGyI"
      },
      "source": [
        "## Configuração de um pipeline de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwJtsCQhHK-E"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(train_images)\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7fj3GskHC8g"
      },
      "source": [
        "Crie e distribua os datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYrMNNDhAvVl"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAXAo_wWbWSb"
      },
      "source": [
        "## Criação do modelo\n",
        "\n",
        "Crie um modelo usando `tf.keras.Sequential`. Também é possível usar a [API de subclasse de modelos](https://www.tensorflow.org/guide/keras/custom_layers_and_models) ou a [API funcional](https://www.tensorflow.org/guide/keras/functional) para fazer isso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ODch-OFCaW4"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  regularizer = tf.keras.regularizers.L2(1e-5)\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3,\n",
        "                             activation='relu',\n",
        "                             kernel_regularizer=regularizer),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Conv2D(64, 3,\n",
        "                             activation='relu',\n",
        "                             kernel_regularizer=regularizer),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64,\n",
        "                            activation='relu',\n",
        "                            kernel_regularizer=regularizer),\n",
        "      tf.keras.layers.Dense(10, kernel_regularizer=regularizer)\n",
        "    ])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iagoTBfijUz"
      },
      "outputs": [],
      "source": [
        "# Create a checkpoint directory to store the checkpoints.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-VVTqDEICrl"
      },
      "source": [
        "## Definição da função de perda\n",
        "\n",
        "Lembre-se de que a função de perda consiste em uma ou duas partes:\n",
        "\n",
        "- A **perda de previsão** mede a distância entre as previsões do modelo e os rótulos de treinamento para um lote de exemplos de treinamento. Ela é computada para cada exemplo rotulado e depois reduzida em todo o lote ao computar o valor médio.\n",
        "- Opcionalmente, termos de **perda de regularização** podem ser adicionados à perda de previsão, para evitar que o modelo faça overfitting dos dados de treinamento. Uma escolha comum é a regularização L2, que adiciona um pequeno múltiplo fixo da soma dos quadrados de todos os pesos do modelo, independente do número de exemplos. O modelo acima usa regularização L2 para demonstrar seu tratamento no loop de treinamento abaixo.\n",
        "\n",
        "Para treinamento numa única máquina com uma única GPU/CPU, isto funciona da seguinte maneira:\n",
        "\n",
        "- A perda de previsão é computada para cada exemplo no lote, somada ao lote e depois dividida pelo tamanho do lote.\n",
        "- A perda de regularização é adicionada à perda de previsão.\n",
        "- O gradiente da perda total é computado com base em cada peso de modelo, e o otimizador atualiza cada peso de modelo a partir do gradiente correspondente.\n",
        "\n",
        "Com `tf.distribute.Strategy`, o lote de entrada é dividido entre réplicas. Por exemplo, digamos que você tenha 4 GPUs, cada uma com uma réplica do modelo. Um lote de 256 exemplos de entrada é distribuído uniformemente pelas 4 réplicas, então cada réplica recebe um lote de tamanho 64: Temos `256 = 4*64`, ou geralmente `GLOBAL_BATCH_SIZE = num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`.\n",
        "\n",
        "Cada réplica computa a perda dos exemplos de treinamento que obtém e calcula os gradientes da perda em relação ao peso de cada modelo. O otimizador cuida para que esses **gradientes sejam somados nas réplicas** antes de usá-los para atualizar as cópias dos pesos do modelo em cada réplica.\n",
        "\n",
        "*Então, como a perda deve ser calculada ao usar um `tf.distribute.Strategy`?*\n",
        "\n",
        "- Cada réplica computa a perda de previsão para todos os exemplos distribuídos a ela, soma os resultados e os divide por `num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`, ou equivalentemente, `GLOBAL_BATCH_SIZE`.\n",
        "- Cada réplica computa as perdas de regularização e as divide por `num_replicas_in_sync`.\n",
        "\n",
        "Em comparação com o treinamento não distribuído, todos os termos de perda por réplica são reduzidos por um fator de `1/num_replicas_in_sync`. Por outro lado, todos os termos de perda – ou melhor, seus gradientes – são somados nesse número de réplicas antes que o otimizador os aplique. Na verdade, o otimizador em cada réplica usa os mesmos gradientes como se uma computação não distribuída com `GLOBAL_BATCH_SIZE` tivesse acontecido. Isso é consistente com o comportamento distribuído e não distribuído de Keras `Model.fit`. Consulte o tutorial [Treinamento distribuído com Keras](./keras.ipynb) sobre como um tamanho de lote global maior permite aumentar a taxa de aprendizado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-wlFFZbP33n"
      },
      "source": [
        "*Como fazer isso no TensorFlow?*\n",
        "\n",
        "- O escalonamento e a redução da perda são feitos automaticamente no `Model.compile` e `Model.fit` do Keras.\n",
        "\n",
        "- Se você estiver escrevendo um loop de treinamento personalizado, como neste tutorial, você deve somar as perdas por exemplo e dividir a soma pelo tamanho global do lote usando `tf.nn.compute_average_loss`, que leva as perdas por exemplo e pesos de amostra opcionais como argumentos e retorna a perda escalonada.\n",
        "\n",
        "- Se estiver usando classes `tf.keras.losses` (como no exemplo abaixo), a redução de perda precisa ser especificada explicitamente como `NONE` ou `SUM`. Os padrões `AUTO` e `SUM_OVER_BATCH_SIZE` não são permitidos fora de `Model.fit`.\n",
        "\n",
        "    - `AUTO` não é permitido porque o usuário deve pensar explicitamente sobre qual redução deseja para ter certeza de que está correta no caso distribuído.\n",
        "    - `SUM_OVER_BATCH_SIZE` não é permitido porque atualmente ele dividiria apenas pelo tamanho do lote por réplica e deixaria a divisão pelo número de réplicas para o usuário, o que pode ser fácil de esquecer. Então, em vez disso, você mesmo precisa fazer a redução explicitamente.\n",
        "\n",
        "- Se você estiver escrevendo um loop de treinamento personalizado para um modelo com uma lista não vazia de `Model.losses` (por exemplo, regularizadores de peso), você deve resumi-los e dividir a soma pelo número de réplicas. Você pode fazer isso usando a função `tf.nn.scale_regularization_loss`. O código do modelo permanece alheio ao número de réplicas.\n",
        "\n",
        "No entanto, os modelos podem definir perdas de regularização dependentes de entrada com APIs Keras, como `Layer.add_loss(...)` e `Layer(activity_regularizer=...)`. Para `Layer.add_loss(...)`, cabe ao código de modelagem realizar a divisão dos termos somados por exemplo pelo tamanho do lote por réplica(!), por exemplo, usando `tf.math.reduce_mean()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R144Wci782ix"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  # Set reduction to `NONE` so you can do the reduction yourself.\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True,\n",
        "      reduction=tf.keras.losses.Reduction.NONE)\n",
        "  def compute_loss(labels, predictions, model_losses):\n",
        "    per_example_loss = loss_object(labels, predictions)\n",
        "    loss = tf.nn.compute_average_loss(per_example_loss)\n",
        "    if model_losses:\n",
        "      loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pM96bqQY52D"
      },
      "source": [
        "### Casos especiais\n",
        "\n",
        "Usuários avançados também devem considerar os seguintes casos especiais.\n",
        "\n",
        "- Lotes de entrada menores que `GLOBAL_BATCH_SIZE` criam casos específicos e desagradáveis ​​em diversos lugares. Na prática, muitas vezes funciona melhor evitá-los, permitindo que os lotes abranjam os limites da época usando `Dataset.repeat().batch()` e definindo épocas aproximadas por contagens de passos, e não pelas pontas do dataset. Alternativamente, `Dataset.batch(drop_remainder=True)` mantém a noção de época, mas descarta os últimos exemplos.\n",
        "\n",
        "Como ilustração, considere este exemplo que segue o caminho mais difícil e permite lotes curtos, de modo que cada época de treinamento contenha cada exemplo de treinamento exatamente uma vez.\n",
        "\n",
        "Qual denominador deve ser usado por `tf.nn.compute_average_loss()`?\n",
        "\n",
        "```\n",
        "* By default, in the example code above and equivalently in `Keras.fit()`, the sum of prediction losses is divided by `num_replicas_in_sync` times the actual batch size seen on the replica (with empty batches silently ignored). This preserves the balance between the prediction loss on the one hand and the regularization losses on the other hand. It is particularly appropriate for models that use input-dependent regularization losses. Plain L2 regularization just superimposes weight decay onto the gradients of the prediction loss and is less in need of such a balance.\n",
        "* In practice, many custom training loops pass as a constant Python value into `tf.nn.compute_average_loss(..., global_batch_size=GLOBAL_BATCH_SIZE)` to use it as the denominator. This preserves the relative weighting of training examples between batches. Without it, the smaller denominator in short batches effectively upweights the examples in those. (Before TensorFlow 2.13, this was also needed to avoid NaNs in case some replica received an actual batch size of zero.)\n",
        "```\n",
        "\n",
        "Ambas as opções são equivalentes se forem evitados lotes curtos, conforme sugerido acima.\n",
        "\n",
        "- `labels` multidimensionais exigem que você calcule a média da `per_example_loss` entre o número de previsões em cada exemplo. Considere uma tarefa de classificação para todos os pixels de uma imagem de entrada, com `predictions` de formato `(batch_size, H, W, n_classes)` e `labels` de formato `(batch_size, H, W)`. Você precisará atualizar `per_example_loss` como: `per_example_loss /= tf.cast(tf.reduce_prod(tf.shape(labels)[1:]), tf.float32)`\n",
        "\n",
        "Cuidado: **verifique o formato da sua perda**. As funções de perda em `tf.losses`/`tf.keras.losses` costumam retornar a média da última dimensão da entrada. As classes de perda encapsulam essas funções. Se for passado `reduction=Reduction.NONE` ao criar uma instância de uma classe de perda, significa \"nenhuma redução **adicional**\". Para perdas categóricas com um formato de entrada do exemplo igual `[batch, W, H, n_classes]`, a dimensão `n_classes` é reduzida. Para perdas pontuais, como `losses.mean_squared_error` ou `losses.binary_crossentropy`, inclua um eixo fictício para que `[batch, W, H, 1]` seja reduzido para `[batch, W, H]`. Sem o eixo fictício, `[batch, W, H]` será reduzido incorretamente para `[batch, W]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8y54-o9T2Ni"
      },
      "source": [
        "## Definição das métricas para monitorar a perda e a exatidão\n",
        "\n",
        "Estas métricas monitoram a perda do teste e a precisão do treinamento e do teste. É possível usar `.result()` para obter as estatísticas acumuladas a qualquer momento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt3AHb46Tr3w"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      name='train_accuracy')\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      name='test_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuKuNXPORfqJ"
      },
      "source": [
        "## Loop de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrMmakq5EqeQ"
      },
      "outputs": [],
      "source": [
        "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\n",
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UX43wUu04EL"
      },
      "outputs": [],
      "source": [
        "def train_step(inputs):\n",
        "  images, labels = inputs\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images, training=True)\n",
        "    loss = compute_loss(labels, predictions, model.losses)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_accuracy.update_state(labels, predictions)\n",
        "  return loss\n",
        "\n",
        "def test_step(inputs):\n",
        "  images, labels = inputs\n",
        "\n",
        "  predictions = model(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss.update_state(t_loss)\n",
        "  test_accuracy.update_state(labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX975dMSNw0e"
      },
      "outputs": [],
      "source": [
        "# `run` replicates the provided computation and runs it\n",
        "# with the distributed input.\n",
        "@tf.function\n",
        "def distributed_train_step(dataset_inputs):\n",
        "  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
        "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                         axis=None)\n",
        "\n",
        "@tf.function\n",
        "def distributed_test_step(dataset_inputs):\n",
        "  return strategy.run(test_step, args=(dataset_inputs,))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # TRAIN LOOP\n",
        "  total_loss = 0.0\n",
        "  num_batches = 0\n",
        "  for x in train_dist_dataset:\n",
        "    total_loss += distributed_train_step(x)\n",
        "    num_batches += 1\n",
        "  train_loss = total_loss / num_batches\n",
        "\n",
        "  # TEST LOOP\n",
        "  for x in test_dist_dataset:\n",
        "    distributed_test_step(x)\n",
        "\n",
        "  if epoch % 2 == 0:\n",
        "    checkpoint.save(checkpoint_prefix)\n",
        "\n",
        "  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
        "              \"Test Accuracy: {}\")\n",
        "  print(template.format(epoch + 1, train_loss,\n",
        "                         train_accuracy.result() * 100, test_loss.result(),\n",
        "                         test_accuracy.result() * 100))\n",
        "\n",
        "  test_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_accuracy.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1YvXqOpwy08"
      },
      "source": [
        "### Observações sobre o exemplo acima:\n",
        "\n",
        "- Faça a iteração de `train_dist_dataset` e `test_dist_dataset` usando um constructo `for x in ...`.\n",
        "- A perda escalonada é o valor de retorno de `distributed_train_step`. Este valor é agregado entre as réplicas usando a chamada `tf.distribute.Strategy.reduce` e depois entre os lotes somando o valor de retorno das chamadas feitas a `tf.distribute.Strategy.reduce`.\n",
        "- `tf.keras.Metrics` deve ser atualizado dentro de `train_step` e `test_step`, que é executado por `tf.distribute.Strategy.run`.\n",
        "- `tf.distribute.Strategy.run` retorna resultados de cada réplica local da estratégia, e há diversas formas de usar esse resultado. Você pode usar `tf.distribute.Strategy.reduce` para obter um valor agregado. Além disso, pode usar `tf.distribute.Strategy.experimental_local_results` para obter a lista de valores contidos no resultado, um por réplica local.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q5qp31IQD8t"
      },
      "source": [
        "## Restauração do último checkpoint e teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNW2P00bkMGJ"
      },
      "source": [
        "É possível restaurar um modelo com `tf.distribute.Strategy` que tenha um checkpoint, com ou sem uma estratégia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg3B-Cw_cn3a"
      },
      "outputs": [],
      "source": [
        "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      name='eval_accuracy')\n",
        "\n",
        "new_model = create_model()\n",
        "new_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qYii7KUYiSM"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def eval_step(images, labels):\n",
        "  predictions = new_model(images, training=False)\n",
        "  eval_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeZ6eeWRoUNq"
      },
      "outputs": [],
      "source": [
        "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "for images, labels in test_dataset:\n",
        "  eval_step(images, labels)\n",
        "\n",
        "print('Accuracy after restoring the saved model without strategy: {}'.format(\n",
        "    eval_accuracy.result() * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbcI87EEzhzg"
      },
      "source": [
        "## Maneiras alternativas de fazer a iteração de um dataset\n",
        "\n",
        "### Uso de iteradores\n",
        "\n",
        "Se você deseja fazer a iteração de um determinado número de passos e não do dataset inteiro, pode criar um iterador usando a chamada `iter` e chamando explicitamente `next` no iterador. Você pode optar por fazer a iteração do dataset tanto dentro como fora de `tf.function`. Veja um pequeno trecho de código que demonstra a iteração do dataset fora de `tf.function` usando um iterador.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c73wGC00CzN"
      },
      "outputs": [],
      "source": [
        "for _ in range(EPOCHS):\n",
        "  total_loss = 0.0\n",
        "  num_batches = 0\n",
        "  train_iter = iter(train_dist_dataset)\n",
        "\n",
        "  for _ in range(10):\n",
        "    total_loss += distributed_train_step(next(train_iter))\n",
        "    num_batches += 1\n",
        "  average_train_loss = total_loss / num_batches\n",
        "\n",
        "  template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
        "  print(template.format(epoch + 1, average_train_loss, train_accuracy.result() * 100))\n",
        "  train_accuracy.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxVp48Oy0m6y"
      },
      "source": [
        "### Iteração dentro de `tf.function`\n",
        "\n",
        "Além disso, você pode fazer a iteração de toda a entrada `train_dist_dataset` dentro de `tf.function` usando o constructo `for x in ...` ou criando iteradores, conforme feito acima, O exemplo abaixo demonstra o encapsulamento de uma época de treinamento com um decorador `@tf.function` e fazendo a iteração de `train_dist_dataset` dentro da função."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-REzmcXv00qm"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def distributed_train_epoch(dataset):\n",
        "  total_loss = 0.0\n",
        "  num_batches = 0\n",
        "  for x in dataset:\n",
        "    per_replica_losses = strategy.run(train_step, args=(x,))\n",
        "    total_loss += strategy.reduce(\n",
        "      tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "    num_batches += 1\n",
        "  return total_loss / tf.cast(num_batches, dtype=tf.float32)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = distributed_train_epoch(train_dist_dataset)\n",
        "\n",
        "  template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
        "  print(template.format(epoch + 1, train_loss, train_accuracy.result() * 100))\n",
        "\n",
        "  train_accuracy.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuZGXiyC7ABR"
      },
      "source": [
        "### Monitoramento da perda de treinamento nas réplicas\n",
        "\n",
        "Observação: como regra geral, você deve usar `tf.keras.Metrics` para monitorar os valores por amostra e evitar valores que foram agregados dentro de uma réplica.\n",
        "\n",
        "Devido à computação de escalonamento de perda realizada, não é recomendável usar `tf.keras.metrics.Mean` para monitorar a perda de treinamento nas diferentes réplicas.\n",
        "\n",
        "Por exemplo, se você executar um trabalho de treinamento com as seguintes características:\n",
        "\n",
        "- Duas réplicas\n",
        "- Duas amostras processadas em cada réplica\n",
        "- Valores de perda resultante: [2,  3] e [4,  5] em cada réplica\n",
        "- Tamanho global do lote = 4\n",
        "\n",
        "Com o escalonamento da perda, você calcula o valor da perda por amostra em cada réplica somando os valores de perda e depois dividindo pelo tamanho global do lote. Neste caso: `(2 + 3) / 4 = 1,25` e `(4 + 5) / 4 = 2,25`.\n",
        "\n",
        "Se você utilizar `tf.keras.metrics.Mean` para monitorar a perda nas duas réplicas, o resultado será diferente. Neste exemplo, você fica com um `total` de 3,50 e uma `count` de 2, que resulta em `total`/`count` = 1,75 quando `result()` é chamado na métrica. A perda calculada com `tf.keras.Metrics` é escalonada por um fator adicional, que é igual ao número de réplicas na sincronização."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xisYJaV9KZTN"
      },
      "source": [
        "### Guia e exemplos\n",
        "\n",
        "Veja alguns exemplos de como usar a estratégia de distribuição com loops de treinamento personalizado:\n",
        "\n",
        "1. [Guia de treinamento distribuído](../../guide/distributed_training)\n",
        "2. Exemplo de [DenseNet](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/densenet/distributed_train.py) usando `MirroredStrategy`.\n",
        "3. Exemplo de [BERT](https://github.com/tensorflow/models/blob/master/official/legacy/bert/run_classifier.py) treinado usando `MirroredStrategy` e `TPUStrategy`. Este exemplo é bastante útil para entender como carregar um checkpoint e gerar checkpoints periódicos durante o treinamento distribuído, etc.\n",
        "4. Exemplo de [NCF](https://github.com/tensorflow/models/blob/master/official/recommendation/ncf_keras_main.py) treinado usando `MirroredStrategy`, que pode ser ativado utilizando o sinalizador `keras_use_ctl`.\n",
        "5. Exemplo de [NMT](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/nmt_with_attention/distributed_train.py) treinado usando `MirroredStrategy`.\n",
        "\n",
        "Confira mais exemplos na seção *Exemplos e tutoriais* do [Guia de estratégia de distribuição](../../guide/distributed_training.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hEJNsokjOKs"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "- Experimente a nova API `tf.distribute.Strategy` em seus modelos.\n",
        "- Confira os guias [Desempenho melhor com `tf.function`](../../guide/function.ipynb) e [TensorFlow Profiler](../../guide/profiler.md) para saber mais sobre ferramentas de otimização do desempenho de seus modelos do TensorFlow.\n",
        "- Confira o guia [Treinamento distribuído no TensorFlow](../../guide/distributed_training.ipynb), que apresenta uma visão geral das estratégias de distribuição disponíveis."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "custom_training.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
