{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOgDUDMAG6mn"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B3PsBDmGG_W8"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifkGYxdCHIof"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWxDDkRwLVMC"
      },
      "source": [
        "# Aprendizado por transferência para a classificação de vídeos com MoViNet\n",
        "\n",
        "O MoViNets (Mobile Video Networks – redes de vídeo para dispositivos móveis) oferece uma família de modelos eficientes para a classificação de vídeos, com suporte à inferência de vídeos de streaming. Neste tutorial, usaremos um modelo MoViNet treinado para classificar vídeos, especificamente para uma tarefa de reconhecimento de ações, do [dataset UCF101](https://www.crcv.ucf.edu/data/UCF101.php). Um modelo pré-treinado é uma rede salva treinada anteriormente com um dataset grande. Confira mais detalhes sobre o MoViNets no artigo [MoViNets: Redes de vídeo para dispositivos móveis para reconhecimento eficiente de vídeos](https://arxiv.org/abs/2103.11511) escrito por Kondratyuk, D. et al. (2021). Neste tutorial, você vai:\n",
        "\n",
        "- Aprender a baixar um modelo MoViNet pré-treinado.\n",
        "- Criar um novo modelo usando um modelo pré-treinado com um novo classificador por meio do congelamento da base convolucional do modelo MoViNet.\n",
        "- Substituir o cabeçalho do classificador pelo número de rótulos de um novo dataset.\n",
        "- Fazer o aprendizado por transferência para o [dataset UCF101](https://www.crcv.ucf.edu/data/UCF101.php).\n",
        "\n",
        "O modelo baixado neste tutorial veio de [official/projects/movinet](https://github.com/tensorflow/models/tree/master/official/projects/movinet). Esse repositório contém uma coleção de modelos MoViNet que o TF Hub usa no formato SavedModel do TensorFlow 2.\n",
        "\n",
        "Este tutorial de aprendizado por transferência é a terceira parte de uma série de tutoriais do TensorFlow sobre vídeos. Aqui estão os outros três tutoriais:\n",
        "\n",
        "- [Carregue dados de vídeo](https://www.tensorflow.org/tutorials/load_data/video): este tutorial explica boa parte do código usado neste documento, em especial, é explicado mais detalhadamente como pré-processar e carregar os dados usando a classe `FrameGenerator`.\n",
        "- [Crie um modelo CNN 3D para a classificação de vídeos](https://www.tensorflow.org/tutorials/video/video_classification): observe que este tutorial usa uma CNN (2+1)D que decompõe os aspectos espaciais e temporais dos dados 3D. Se você estiver usando dados volumétricos, como uma ressonância magnética, considere usar uma CNN 3D em vez de uma CNN (2+1)D.\n",
        "- [MoViNet para o reconhecimento de ações de streaming](https://www.tensorflow.org/hub/tutorials/movinet): conheça os modelos MoViNet disponíveis no TF Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GidiisyXwK--"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Comece instalando e importando algumas bibliotecas necessárias, incluindo: [remotezip](https://github.com/gtsystem/python-remotezip) para analisar o conteúdo de um arquivo ZIP, [tqdm](https://github.com/tqdm/tqdm) para usar uma barra de progresso, [OpenCV](https://opencv.org/) para processar arquivos de vídeo (`opencv-python` e `opencv-python-headless` precisam estar na mesma versão) e modelos do TensorFlow ([`tf-models-official`](https://github.com/tensorflow/models/tree/master/official)) para baixar o modelo MoViNet pré-treinado. O pacote de modelos do TensorFlow é uma coleção de modelos que usam as APIs de alto nível do TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nubWhqYdwEXD"
      },
      "outputs": [],
      "source": [
        "!pip install remotezip tqdm opencv-python==4.5.2.52 opencv-python-headless==4.5.2.52 tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QImPsudoK9JI"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w3H4dfOPfnm"
      },
      "source": [
        "## Carregue os dados\n",
        "\n",
        "A célula oculta abaixo define funções helper para baixar um segmento dos dados do dataset UCF-101 e carregá-los em um `tf.data.Dataset`. O [tutorial Carregue dados de vídeos](https://www.tensorflow.org/tutorials/load_data/video) apresenta instruções detalhadas sobre esse código.\n",
        "\n",
        "A classe `FrameGenerator` no final do bloco oculto é o utilitário mais importante que temos aqui, pois cria um objeto iterável que pode alimentar dados no pipeline de dados do TensorFlow. Especificamente, essa classe contém um gerador Python que carrega os quadros do vídeo juntamente com seu rótulo codificado. A função geradora (`__call__`) gera a array de quadros produzida por `frames_from_video_file` e um vetor com codificação one-hot do rótulo associado ao conjunto de quadros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fwEhJ13_PSy6"
      },
      "outputs": [],
      "source": [
        "#@title \n",
        "\n",
        "def list_files_per_class(zip_url):\n",
        "  \"\"\"\n",
        "    List the files in each class of the dataset given the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: URL from which the files can be unzipped. \n",
        "\n",
        "    Return:\n",
        "      files: List of files in each of the classes.\n",
        "  \"\"\"\n",
        "  files = []\n",
        "  with rz.RemoteZip(URL) as zip:\n",
        "    for zip_info in zip.infolist():\n",
        "      files.append(zip_info.filename)\n",
        "  return files\n",
        "\n",
        "def get_class(fname):\n",
        "  \"\"\"\n",
        "    Retrieve the name of the class given a filename.\n",
        "\n",
        "    Args:\n",
        "      fname: Name of the file in the UCF101 dataset.\n",
        "\n",
        "    Return:\n",
        "      Class that the file belongs to.\n",
        "  \"\"\"\n",
        "  return fname.split('_')[-3]\n",
        "\n",
        "def get_files_per_class(files):\n",
        "  \"\"\"\n",
        "    Retrieve the files that belong to each class. \n",
        "\n",
        "    Args:\n",
        "      files: List of files in the dataset.\n",
        "\n",
        "    Return:\n",
        "      Dictionary of class names (key) and files (values).\n",
        "  \"\"\"\n",
        "  files_for_class = collections.defaultdict(list)\n",
        "  for fname in files:\n",
        "    class_name = get_class(fname)\n",
        "    files_for_class[class_name].append(fname)\n",
        "  return files_for_class\n",
        "\n",
        "def download_from_zip(zip_url, to_dir, file_names):\n",
        "  \"\"\"\n",
        "    Download the contents of the zip file from the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: Zip URL containing data.\n",
        "      to_dir: Directory to download data to.\n",
        "      file_names: Names of files to download.\n",
        "  \"\"\"\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for fn in tqdm.tqdm(file_names):\n",
        "      class_name = get_class(fn)\n",
        "      zip.extract(fn, str(to_dir / class_name))\n",
        "      unzipped_file = to_dir / class_name / fn\n",
        "\n",
        "      fn = pathlib.Path(fn).parts[-1]\n",
        "      output_file = to_dir / class_name / fn\n",
        "      unzipped_file.rename(output_file,)\n",
        "\n",
        "def split_class_lists(files_for_class, count):\n",
        "  \"\"\"\n",
        "    Returns the list of files belonging to a subset of data as well as the remainder of\n",
        "    files that need to be downloaded.\n",
        "\n",
        "    Args:\n",
        "      files_for_class: Files belonging to a particular class of data.\n",
        "      count: Number of files to download.\n",
        "\n",
        "    Return:\n",
        "      split_files: Files belonging to the subset of data.\n",
        "      remainder: Dictionary of the remainder of files that need to be downloaded.\n",
        "  \"\"\"\n",
        "  split_files = []\n",
        "  remainder = {}\n",
        "  for cls in files_for_class:\n",
        "    split_files.extend(files_for_class[cls][:count])\n",
        "    remainder[cls] = files_for_class[cls][count:]\n",
        "  return split_files, remainder\n",
        "\n",
        "def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n",
        "  \"\"\"\n",
        "    Download a subset of the UFC101 dataset and split them into various parts, such as\n",
        "    training, validation, and test. \n",
        "\n",
        "    Args:\n",
        "      zip_url: Zip URL containing data.\n",
        "      num_classes: Number of labels.\n",
        "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n",
        "              (value is number of files per split).\n",
        "      download_dir: Directory to download data to.\n",
        "\n",
        "    Return:\n",
        "      dir: Posix path of the resulting directories containing the splits of data.\n",
        "  \"\"\"\n",
        "  files = list_files_per_class(zip_url)\n",
        "  for f in files:\n",
        "    tokens = f.split('/')\n",
        "    if len(tokens) <= 2:\n",
        "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
        "\n",
        "  files_for_class = get_files_per_class(files)\n",
        "\n",
        "  classes = list(files_for_class.keys())[:num_classes]\n",
        "\n",
        "  for cls in classes:\n",
        "    new_files_for_class = files_for_class[cls]\n",
        "    random.shuffle(new_files_for_class)\n",
        "    files_for_class[cls] = new_files_for_class\n",
        "\n",
        "  # Only use the number of classes you want in the dictionary\n",
        "  files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n",
        "\n",
        "  dirs = {}\n",
        "  for split_name, split_count in splits.items():\n",
        "    print(split_name, \":\")\n",
        "    split_dir = download_dir / split_name\n",
        "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
        "    download_from_zip(zip_url, split_dir, split_files)\n",
        "    dirs[split_name] = split_dir\n",
        "\n",
        "  return dirs\n",
        "\n",
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded. \n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame\n",
        "\n",
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))  \n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result\n",
        "\n",
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    \"\"\" Returns a set of frames with their associated label. \n",
        "\n",
        "      Args:\n",
        "        path: Video file paths.\n",
        "        n_frames: Number of frames. \n",
        "        training: Boolean to determine if training dataset is being created.\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.avi'))\n",
        "    classes = [p.parent.name for p in video_paths] \n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames) \n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDHrNLZkPSR9"
      },
      "outputs": [],
      "source": [
        "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\n",
        "download_dir = pathlib.Path('./UCF101_subset/')\n",
        "subset_paths = download_ufc_101_subset(URL, \n",
        "                        num_classes = 10, \n",
        "                        splits = {\"train\": 30, \"test\": 20}, \n",
        "                        download_dir = download_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYYShfhMx9DW"
      },
      "source": [
        "Crie os datasets de treinamento e teste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-twTu3_Bx-iJ"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "num_frames = 8\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n",
        "                                          output_signature = output_signature)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n",
        "                                         output_signature = output_signature)\n",
        "test_ds = test_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7stgmuBCGQT"
      },
      "source": [
        "Os rótulos gerados aqui representam a codificação das classes. Por exemplo, 'ApplyEyeMakeup' é mapeado para o inteiro. Confira os rótulos dos dados de treinamento para garantir que o dataset tenha sido misturado o suficiente. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9L2-toXCOQq"
      },
      "outputs": [],
      "source": [
        "for frames, labels in train_ds.take(10):\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ3qwZnpfy9c"
      },
      "source": [
        "Confira o formato dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6MqP4m2fyQT"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxbhPqXGvc_F"
      },
      "source": [
        "## O que são MoViNets?\n",
        "\n",
        "Conforme mencionado anteriormente, [MoViNets](https://arxiv.org/abs/2103.11511) são modelos para classificação de vídeos usados para inferência online ou de vídeos de streaming em diversas tarefas, como reconhecimento de ações. Você pode considerar o uso de MoViNets para classificar os dados dos seus vídeos a fim de fazer o reconhecimento de ações.\n",
        "\n",
        "É eficiente e simples executar um classificador baseado em quadros 2D em vídeos inteiros ou quadro a quadro. Como o contexto temporal não é levado em conta, a exatidão é limitada, e as saídas geradas podem ser inconsistentes a cada quadro.\n",
        "\n",
        "Uma CNN 3D simples usa contexto temporal bidirecional, o que pode aumentar a exatidão e a consistência temporal. Essas redes poderão exigir mais recursos e, como analisam o futuro, não podem ser usadas para fazer o streaming de dados.\n",
        "\n",
        "![Standard convolution](https://www.tensorflow.org/images/tutorials/video/standard_convolution.png)\n",
        "\n",
        "A arquitetura do MoViNet usa convoluções 3D que são \"casuais\" no eixo do tempo (como `layers.Conv1D` com `padding=\"causal\"`). Isso traz algumas vantagens das duas estratégias, principalmente propiciar um streaming eficiente.\n",
        "\n",
        "![Causal convolution](https://www.tensorflow.org/images/tutorials/video/causal_convolution.png)\n",
        "\n",
        "A convolução casual garante que a saída no tempo *t* seja computada usando somente entradas até o tempo *t*. Para demonstrar como isso pode deixar o streaming mais eficiente, comece com um exemplo mais simples que você já deve conhecer: uma RNN, que passa o estado para frente do tempo:\n",
        "\n",
        "![RNN model](https://www.tensorflow.org/images/tutorials/video/rnn_comparison.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMvDkgfFZC6a"
      },
      "outputs": [],
      "source": [
        "gru = layers.GRU(units=4, return_sequences=True, return_state=True)\n",
        "\n",
        "inputs = tf.random.normal(shape=[1, 10, 8]) # (batch, sequence, channels)\n",
        "\n",
        "result, state = gru(inputs) # Run it all at once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7xyb5C4bTs7"
      },
      "source": [
        "Ao definir o argumento `return_sequences=True` da RNN, você pede que ela retorne o estado no final da computação, o que permite pausar e depois continuar de onde parou, obtendo exatamente o mesmo resultado:\n",
        "\n",
        "![States passing in RNNs](https://www.tensorflow.org/images/tutorials/video/rnn_state_passing.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI8FOPRRXXPa"
      },
      "outputs": [],
      "source": [
        "first_half, state = gru(inputs[:, :5, :])   # run the first half, and capture the state\n",
        "second_half, _ = gru(inputs[:,5:, :], initial_state=state)  # Use the state to continue where you left off.\n",
        "\n",
        "print(np.allclose(result[:, :5,:], first_half))\n",
        "print(np.allclose(result[:, 5:,:], second_half))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM3MArumY_Qk"
      },
      "source": [
        "As convoluções casuais podem ser usadas da mesma forma, se utilizadas com cuidado. Essa técnica foi usada no [Algoritmo de geração de Wavenet rápida](https://arxiv.org/abs/1611.09482) de Le Paine et al. No [artigo do MoViNet](https://arxiv.org/abs/2103.11511), o `state` (estado) é chamado de \"Buffer de stream\".\n",
        "\n",
        "![States passed in causal convolution](https://www.tensorflow.org/images/tutorials/video/causal_conv_states.png)\n",
        "\n",
        "Ao passar esse pequeno estado para frente, é possível evitar o recálculo de todo o campo receptivo exibido acima. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UsxiPs8yA2e"
      },
      "source": [
        "## Baixe um modelo MoViNet pré-treinado\n",
        "\n",
        "Nesta seção, você vai:\n",
        "\n",
        "1. Criar um modelo MoViNet usando o código aberto fornecido em [`official/projects/movinet`](https://github.com/tensorflow/models/tree/master/official/projects/movinet) dos modelos do TensorFlow.\n",
        "2. Carregar os pesos pré-treinados.\n",
        "3. Congelar a base convolucional ou todas as outras camadas, exceto o cabeçalho final do classificador, para acelerar o ajuste fino.\n",
        "\n",
        "Para criar o modelo, você pode começar com a configuração `a0`, pois é a mais rápida para o treinamento em comparação com outros modelos. Confira os [modelos MoViNet disponíveis no TensorFlow Model Garden](https://github.com/tensorflow/models/blob/master/official/projects/movinet/configs/movinet.py) para descobrir o que funcionará para seu caso de uso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhSCM6cee05F"
      },
      "outputs": [],
      "source": [
        "model_id = 'a0'\n",
        "resolution = 224\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(model_id=model_id)\n",
        "backbone.trainable = False\n",
        "\n",
        "# Set num_classes=600 to load the pre-trained weights from the original model\n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "model.build([None, None, None, None, 3])\n",
        "\n",
        "# Load pre-trained weights\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
        "!tar -xvf movinet_a0_base.tar.gz\n",
        "\n",
        "checkpoint_dir = f'movinet_{model_id}_base'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW23HVNtCXff"
      },
      "source": [
        "Para criar um classificador, crie uma função que receba o backbone e o número de classes em um dataset. A função `build_classifier` fará isso. Neste caso, o novo classificador receberá `num_classes` saídas (10 classes para este subconjunto do UCF101)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cfAelbU5Gi3"
      },
      "outputs": [],
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HWSk-u7oPUZ"
      },
      "outputs": [],
      "source": [
        "model = build_classifier(batch_size, num_frames, resolution, backbone, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhbX7qdTN8lc"
      },
      "source": [
        "Neste tutorial, escolha o otimizador `tf.keras.optimizers.Adam` e a função de perda `tf.keras.losses.SparseCategoricalCrossentropy`. Use o argumento metrics para ver a exatidão do desempenho do modelo a cada passo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVqBLrn1tBsd"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2\n",
        "\n",
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VflEr_t6CuQu"
      },
      "source": [
        "Treine o modelo. Após duas épocas, observe uma perda baixa com exatidão alta para o dataset de treinamento e de teste. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZeiYzI0tqQG"
      },
      "outputs": [],
      "source": [
        "results = model.fit(train_ds,\n",
        "                    validation_data=test_ds,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkLl2zF8G9W0"
      },
      "source": [
        "## Avalie o modelo\n",
        "\n",
        "O modelo atingiu uma exatidão alta para o dataset de treinamento. Em seguida, use `Model.evaluate` do Keras para avaliar a exatidão para o conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqgbzOiKuxxT"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkFst2gsHBwD"
      },
      "source": [
        "Para avaliar o desempenho do modelo com mais detalhes, use uma [matriz de confusão](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix), que permite avaliar o desempenho do modelo de classificação além de apenas a exatidão. Para criar a matriz de confusão para este problema de classificação multiclasse, obtenha os valores reais do conjunto de teste e os valores previstos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hssSdW9XHF_j"
      },
      "outputs": [],
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "  \"\"\"\n",
        "    Create a list of actual ground truth values and the predictions from the model.\n",
        "\n",
        "    Args:\n",
        "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
        "\n",
        "    Return:\n",
        "      Ground truth and predicted values for a particular dataset.\n",
        "  \"\"\"\n",
        "  actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TmTue6THGWO"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
        "  sns.set(rc={'figure.figsize':(12, 12)})\n",
        "  sns.set(font_scale=1.4)\n",
        "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
        "  ax.set_xlabel('Predicted Action')\n",
        "  ax.set_ylabel('Actual Action')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RK1A1C1HH6V"
      },
      "outputs": [],
      "source": [
        "fg = FrameGenerator(subset_paths['train'], num_frames, training = True)\n",
        "label_names = list(fg.class_ids_for_name.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4AFi2e5HKEO"
      },
      "outputs": [],
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)\n",
        "plot_confusion_matrix(actual, predicted, label_names, 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddQG9sYxa1Ib"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "Agora que você já conhece um pouco o modelo MoViNet e como usar diversas APIs do TensorFlow (para aprendizado por transferência, por exemplo), tente usar o código neste tutorial com seu próprio dataset. Os dados não precisam estar limitados a vídeos. Dados volumétricos, como tomografias, também podem ser usados com CNNs 3D. Os datasets NUSDAT e IMH mencionados em [Redes Neurais Convolucionais 3D baseadas em tomografias do cérebro para classificação de esquizofrenia e controles](https://arxiv.org/pdf/2003.08818.pdf) podem ser duas fontes de dados de tomografia.\n",
        "\n",
        "Especificamente, você pode usar a classe `FrameGenerator` utilizada neste tutorial e os outros dados de vídeos e tutoriais de classificação para ajudar a carregar os dados em seus modelos.\n",
        "\n",
        "Para saber mais sobre como trabalhar com dados de vídeo no TensorFlow, confira os tutoriais a seguir:\n",
        "\n",
        "- [Carregue dados de vídeo](https://www.tensorflow.org/tutorials/load_data/video)\n",
        "- [Crie um modelo CNN 3D para a classificação de vídeos](https://www.tensorflow.org/tutorials/video/video_classification)\n",
        "- [MoViNet para o reconhecimento de ações de streaming](https://www.tensorflow.org/hub/tutorials/movinet)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transfer_learning_with_movinet.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
