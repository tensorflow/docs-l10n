{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt9dL5dIir8X"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ufPx7EiCiqgR"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4StGz9ynOEL6"
      },
      "source": [
        "# Carregue dados de vídeo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwQtSOz0VrVX"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/video\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/load_data/video.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/load_data/video.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/load_data/video.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-SqCosJ6-0H"
      },
      "source": [
        "Este tutorial mostra como carregar e pré-processar dados de vídeos [AVI](https://en.wikipedia.org/wiki/Audio_Video_Interleave) usando o [dataset de ações humanas UCF101](https://www.tensorflow.org/datasets/catalog/ucf101). Depois de pré-processar os dados, usados para tarefas de vídeo como classificação/reconhecimento, legendagem ou clustering. O dataset original contém vídeos de ações realísticas do YouTube com 101 categorias, como tocar violoncelo, escovar os dentes e maquiar os olhos. Você aprenderá a:\n",
        "\n",
        "- Carregar os dados a partir de um arquivo zip.\n",
        "\n",
        "- Ler sequências de frames dos arquivos de vídeo.\n",
        "\n",
        "- Visualizar os dados dos vídeos.\n",
        "\n",
        "- Empacotar o [`tf.data.Dataset`](https://www.tensorflow.org/guide/data) gerador de frames.\n",
        "\n",
        "Este tutorial de carregamento e pré-processamento de vídeos é a primeira parte de uma série de tutoriais do TensorFlow sobre vídeos. Aqui estão os outros três tutoriais:\n",
        "\n",
        "- [Crie um modelo CNN 3D para a classificação de vídeos](https://www.tensorflow.org/tutorials/video/video_classification): observe que este tutorial usa uma CNN (2+1)D que decompõe os aspectos espaciais e temporais dos dados 3D. Se você estiver usando dados volumétricos como uma ressonância magnética, considere usar uma CNN 3D em vez de uma CNN (2+1)D.\n",
        "- [MoViNet para o reconhecimento de ações de streaming](https://www.tensorflow.org/hub/tutorials/movinet): conheça os modelos MoViNet disponíveis no TF Hub.\n",
        "- [Aprendizado por transferência para a classificação de vídeos com MoViNet](https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet): este tutorial explica como usar um modelo de classificação de vídeos pré-treinado em um dataset diferente com o dataset UCF-101."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnpPjKVD68eH"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Comece instalando e importando algumas bibliotecas necessárias, incluindo: [remotezip](https://github.com/gtsystem/python-remotezip) para inspecionar o conteúdo de um arquivo ZIP, [tqdm](https://github.com/tqdm/tqdm) para usar uma barra de progresso, [OpenCV](https://opencv.org/) para processar arquivos de vídeo e [`tensorflow_docs`](https://github.com/tensorflow/docs/tree/master/tools/tensorflow_docs) para incorporar dados em um notebook Jupyter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjI3AaaO16bd"
      },
      "outputs": [],
      "source": [
        "# The way this tutorial uses the `TimeDistributed` layer requires TF>=2.10\n",
        "!pip install -U \"tensorflow>=2.10.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5SBasQcbwQA"
      },
      "outputs": [],
      "source": [
        "!pip install remotezip tqdm opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RYQIJ9C6BVH"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython import display\n",
        "from urllib import request\n",
        "from tensorflow_docs.vis import embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbhwWLLM7FXo"
      },
      "source": [
        "## Baixe um subconjunto do dataset UCF101\n",
        "\n",
        "O [dataset UCF101](https://www.tensorflow.org/datasets/catalog/ucf101) contém 101 categorias de ações diferentes em vídeo, usadas principalmente no reconhecimento de ações. Você usará um subconjunto dessas categorias nesta demonstração."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVIgj-jIA8U8"
      },
      "outputs": [],
      "source": [
        "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tm8aBzw6Md7"
      },
      "source": [
        "A URL acima contém um arquivo zip com o dataset UCF 101. Crie uma função que usa a biblioteca `remotezip` para examinar o conteúdo do arquivo zip nessa URL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY-x7TaZlK6O"
      },
      "outputs": [],
      "source": [
        "def list_files_from_zip_url(zip_url):\n",
        "  \"\"\" List the files in each class of the dataset given a URL with the zip file.\n",
        "\n",
        "    Args:\n",
        "      zip_url: A URL from which the files can be extracted from.\n",
        "\n",
        "    Returns:\n",
        "      List of files in each of the classes.\n",
        "  \"\"\"\n",
        "  files = []\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for zip_info in zip.infolist():\n",
        "      files.append(zip_info.filename)\n",
        "  return files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYErXAdUr-rk"
      },
      "outputs": [],
      "source": [
        "files = list_files_from_zip_url(URL)\n",
        "files = [f for f in files if f.endswith('.avi')]\n",
        "files[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ4l8D9dFPS7"
      },
      "source": [
        "Comece com alguns vídeos e um número limitado de classes para treinamento. Depois de executar o bloco de código acima, observe que o nome da classe está incluso no nome de arquivo de cada vídeo.\n",
        "\n",
        "Defina a função `get_class` que recupera o nome da classe a partir de um nome de arquivo. Em seguida, crie uma função chamada `get_files_per_class` que converte a lista de todos os arquivos (`files` acima) em um dicionário listando os arquivos para cada classe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyyivOX0sO19"
      },
      "outputs": [],
      "source": [
        "def get_class(fname):\n",
        "  \"\"\" Retrieve the name of the class given a filename.\n",
        "\n",
        "    Args:\n",
        "      fname: Name of the file in the UCF101 dataset.\n",
        "\n",
        "    Returns:\n",
        "      Class that the file belongs to.\n",
        "  \"\"\"\n",
        "  return fname.split('_')[-3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qnH0xKzlyw_"
      },
      "outputs": [],
      "source": [
        "def get_files_per_class(files):\n",
        "  \"\"\" Retrieve the files that belong to each class.\n",
        "\n",
        "    Args:\n",
        "      files: List of files in the dataset.\n",
        "\n",
        "    Returns:\n",
        "      Dictionary of class names (key) and files (values). \n",
        "  \"\"\"\n",
        "  files_for_class = collections.defaultdict(list)\n",
        "  for fname in files:\n",
        "    class_name = get_class(fname)\n",
        "    files_for_class[class_name].append(fname)\n",
        "  return files_for_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxSt5YgSGrWn"
      },
      "source": [
        "Depois de obter a lista de arquivos por classe, você pode escolher o número de classes que quer usar e o número de vídeos desejado por classe para criar seu dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPdURg74uUTk"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "FILES_PER_CLASS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUs0xtXsr9i3"
      },
      "outputs": [],
      "source": [
        "files_for_class = get_files_per_class(files)\n",
        "classes = list(files_for_class.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YqFARvqwon9"
      },
      "outputs": [],
      "source": [
        "print('Num classes:', len(classes))\n",
        "print('Num videos for class[0]:', len(files_for_class[classes[0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFAFqKqE92bQ"
      },
      "source": [
        "Crie uma nova função chamada `select_subset_of_classes` que seleciona um subconjunto de classes presentes no dataset e um número específico de arquivos por classe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3jek4QimIj-"
      },
      "outputs": [],
      "source": [
        "def select_subset_of_classes(files_for_class, classes, files_per_class):\n",
        "  \"\"\" Create a dictionary with the class name and a subset of the files in that class.\n",
        "\n",
        "    Args:\n",
        "      files_for_class: Dictionary of class names (key) and files (values).\n",
        "      classes: List of classes.\n",
        "      files_per_class: Number of files per class of interest.\n",
        "\n",
        "    Returns:\n",
        "      Dictionary with class as key and list of specified number of video files in that class.\n",
        "  \"\"\"\n",
        "  files_subset = dict()\n",
        "\n",
        "  for class_name in classes:\n",
        "    class_files = files_for_class[class_name]\n",
        "    files_subset[class_name] = class_files[:files_per_class]\n",
        "\n",
        "  return files_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cjcz6Gpcb-W"
      },
      "outputs": [],
      "source": [
        "files_subset = select_subset_of_classes(files_for_class, classes[:NUM_CLASSES], FILES_PER_CLASS)\n",
        "list(files_subset.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrlDS1lZx3E"
      },
      "source": [
        "Defina as funções helper que dividem os vídeos em datasets de treinamento, validação e teste. Os vídeos são baixados de uma URL com o arquivo zip e colocados nos respectivos subdiretórios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH9sWS_6nRz3"
      },
      "outputs": [],
      "source": [
        "def download_from_zip(zip_url, to_dir, file_names):\n",
        "  \"\"\" Download the contents of the zip file from the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: A URL with a zip file containing data.\n",
        "      to_dir: A directory to download data to.\n",
        "      file_names: Names of files to download.\n",
        "  \"\"\"\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for fn in tqdm.tqdm(file_names):\n",
        "      class_name = get_class(fn)\n",
        "      zip.extract(fn, str(to_dir / class_name))\n",
        "      unzipped_file = to_dir / class_name / fn\n",
        "\n",
        "      fn = pathlib.Path(fn).parts[-1]\n",
        "      output_file = to_dir / class_name / fn\n",
        "      unzipped_file.rename(output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pejRTChA6mrp"
      },
      "source": [
        "A seguinte função retorna os dados restantes que não foram colocados em um subconjunto de dados. Assim, você pode colocar os dados restantes no próximo subconjunto de dados especificado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ARYc-WLqqNF"
      },
      "outputs": [],
      "source": [
        "def split_class_lists(files_for_class, count):\n",
        "  \"\"\" Returns the list of files belonging to a subset of data as well as the remainder of\n",
        "    files that need to be downloaded.\n",
        "    \n",
        "    Args:\n",
        "      files_for_class: Files belonging to a particular class of data.\n",
        "      count: Number of files to download.\n",
        "\n",
        "    Returns:\n",
        "      Files belonging to the subset of data and dictionary of the remainder of files that need to be downloaded.\n",
        "  \"\"\"\n",
        "  split_files = []\n",
        "  remainder = {}\n",
        "  for cls in files_for_class:\n",
        "    split_files.extend(files_for_class[cls][:count])\n",
        "    remainder[cls] = files_for_class[cls][count:]\n",
        "  return split_files, remainder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlEQ_I0TLd1X"
      },
      "source": [
        "A função `download_ucf_101_subset` permite que você baixe um subconjunto do dataset UCF101 e o divida em datasets de treinamento, validação e teste. Você pode especificar o número de classes que gostaria de usar. O argumento `splits` permite que você passe um dicionário em que os valores-chave são o nome do subconjunto (exemplo: \"trem\") e o número de vídeos desejado por classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHH2Y1M06xoz"
      },
      "outputs": [],
      "source": [
        "def download_ucf_101_subset(zip_url, num_classes, splits, download_dir):\n",
        "  \"\"\" Download a subset of the UCF101 dataset and split them into various parts, such as\n",
        "    training, validation, and test.\n",
        "\n",
        "    Args:\n",
        "      zip_url: A URL with a ZIP file with the data.\n",
        "      num_classes: Number of labels.\n",
        "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n",
        "              (value is number of files per split).\n",
        "      download_dir: Directory to download data to.\n",
        "\n",
        "    Return:\n",
        "      Mapping of the directories containing the subsections of data.\n",
        "  \"\"\"\n",
        "  files = list_files_from_zip_url(zip_url)\n",
        "  for f in files:\n",
        "    path = os.path.normpath(f)\n",
        "    tokens = path.split(os.sep)\n",
        "    if len(tokens) <= 2:\n",
        "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
        "  \n",
        "  files_for_class = get_files_per_class(files)\n",
        "\n",
        "  classes = list(files_for_class.keys())[:num_classes]\n",
        "\n",
        "  for cls in classes:\n",
        "    random.shuffle(files_for_class[cls])\n",
        "    \n",
        "  # Only use the number of classes you want in the dictionary\n",
        "  files_for_class = {x: files_for_class[x] for x in classes}\n",
        "\n",
        "  dirs = {}\n",
        "  for split_name, split_count in splits.items():\n",
        "    print(split_name, \":\")\n",
        "    split_dir = download_dir / split_name\n",
        "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
        "    download_from_zip(zip_url, split_dir, split_files)\n",
        "    dirs[split_name] = split_dir\n",
        "\n",
        "  return dirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuD-xU8Q66Vm"
      },
      "outputs": [],
      "source": [
        "download_dir = pathlib.Path('./UCF101_subset/')\n",
        "subset_paths = download_ucf_101_subset(URL,\n",
        "                                       num_classes = NUM_CLASSES,\n",
        "                                       splits = {\"train\": 30, \"val\": 10, \"test\": 10},\n",
        "                                       download_dir = download_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBMRm9Ub3Zrk"
      },
      "source": [
        "Depois de baixar os dados, você terá uma cópia de um subconjunto do dataset UCF101. Execute o código abaixo para imprimir o número total de vídeos entre todos os seus subconjuntos de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zupvOLYP4D4q"
      },
      "outputs": [],
      "source": [
        "video_count_train = len(list(download_dir.glob('train/*/*.avi')))\n",
        "video_count_val = len(list(download_dir.glob('val/*/*.avi')))\n",
        "video_count_test = len(list(download_dir.glob('test/*/*.avi')))\n",
        "video_total = video_count_train + video_count_val + video_count_test\n",
        "print(f\"Total videos: {video_total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmJG1SlXiOX8"
      },
      "source": [
        "Você também pode visualizar o diretório de arquivos de dados agora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9be0WlDiNM0"
      },
      "outputs": [],
      "source": [
        "!find ./UCF101_subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4uslY4dScyu"
      },
      "source": [
        "## Crie frames a partir de cada arquivo de vídeo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1vvyT0F7JAZ"
      },
      "source": [
        "A função `frames_from_video_file` divide os vídeos em frames, lê um intervalo de `n_frames` escolhidos aleatoriamente de um arquivo de vídeo e os retorna como um `array` do NumPy. Para reduzir a sobrecarga computacional e na memória, escolha um número **pequeno** de frames. Além disso, escolha o **mesmo** número de frames de cada vídeo, o que facilita o trabalho com lotes de dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNBCiV3bMzpD"
      },
      "outputs": [],
      "source": [
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "    \n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded. \n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ujLDC9G7JyE"
      },
      "outputs": [],
      "source": [
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))  \n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ENtlwhxwyTe"
      },
      "source": [
        "## Visualize os dados de vídeo\n",
        "\n",
        "A função `frames_from_video_file` retorna um conjunto de frames como um array do NumPy. Tente usar essa função em um novo vídeo da [Wikimedia](https://commons.wikimedia.org/wiki/Category:Videos_of_sports){:.external}, de Patrick Gillett:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2hgSghlykzA"
      },
      "outputs": [],
      "source": [
        "!curl -O https://upload.wikimedia.org/wikipedia/commons/8/86/End_of_a_jam.ogv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdHvHw3hym-U"
      },
      "outputs": [],
      "source": [
        "video_path = \"End_of_a_jam.ogv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u845YODXyqo5"
      },
      "outputs": [],
      "source": [
        "sample_video = frames_from_video_file(video_path, n_frames = 10)\n",
        "sample_video.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFHGHiFgGjv2"
      },
      "outputs": [],
      "source": [
        "def to_gif(images):\n",
        "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
        "  imageio.mimsave('./animation.gif', converted_images, fps=10)\n",
        "  return embed.embed_file('./animation.gif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hiwUJenEN3p"
      },
      "outputs": [],
      "source": [
        "to_gif(sample_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dktTnDVG7xf"
      },
      "source": [
        "Além de examinar esse vídeo, você também pode mostrar os dados do UCF-101. Para isso, execute o código a seguir:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MghJzJsWme0t"
      },
      "outputs": [],
      "source": [
        "# docs-infra: no-execute\n",
        "ucf_sample_video = frames_from_video_file(next(subset_paths['train'].glob('*/*.avi')), 50)\n",
        "to_gif(ucf_sample_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlvuC5_E7XrF"
      },
      "source": [
        "Em seguida, defina a classe `FrameGenerator` para criar um objeto iterável que possa alimentar o pipeline de dados do TensorFlow. A função (`__call__`) gera o array de frames produzido por `frames_from_video_file` e um vetor de one-hot encoding do rótulo associado ao conjunto de frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVmfLTlw7Ues"
      },
      "outputs": [],
      "source": [
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    \"\"\" Returns a set of frames with their associated label. \n",
        "\n",
        "      Args:\n",
        "        path: Video file paths.\n",
        "        n_frames: Number of frames. \n",
        "        training: Boolean to determine if training dataset is being created.\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.avi'))\n",
        "    classes = [p.parent.name for p in video_paths] \n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames) \n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsvhPIkpzx-r"
      },
      "source": [
        "Teste o objeto `FrameGenerator` antes de empacotá-lo como um objeto do TensorFlow Dataset. Além disso, para os dados de treinamento, ative o modo de treinamento para que os dados sejam misturados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5jwagZxzxOf"
      },
      "outputs": [],
      "source": [
        "fg = FrameGenerator(subset_paths['train'], 10, training=True)\n",
        "\n",
        "frames, label = next(fg())\n",
        "\n",
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7MRRFSks7l1"
      },
      "source": [
        "Por fim, crie um pipeline de entrada de dados do TensorFlow. Esse pipeline criado a partir do objeto gerador permite que você alimente seu modelo de aprendizado profundo com dados. Nesse pipeline de vídeo, cada elemento é um único conjunto de frames e o rótulo associado a ele. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM4NboJr7ck4"
      },
      "outputs": [],
      "source": [
        "# Create the training set\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], 10, training=True),\n",
        "                                          output_signature = output_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oF_8m8IZvcY"
      },
      "source": [
        "Confira se os rótulos foram misturados. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XYVmsgiZsJD"
      },
      "outputs": [],
      "source": [
        "for frames, labels in train_ds.take(10):\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi8-WkOkEXw5"
      },
      "outputs": [],
      "source": [
        "# Create the validation set\n",
        "val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], 10),\n",
        "                                        output_signature = output_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6qXc-6i7eyK"
      },
      "outputs": [],
      "source": [
        "# Print the shapes of the data\n",
        "train_frames, train_labels = next(iter(train_ds))\n",
        "print(f'Shape of training set of frames: {train_frames.shape}')\n",
        "print(f'Shape of training labels: {train_labels.shape}')\n",
        "\n",
        "val_frames, val_labels = next(iter(val_ds))\n",
        "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
        "print(f'Shape of validation labels: {val_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIrFpUIxvTLe"
      },
      "source": [
        "## Configure o dataset para melhor desempenho\n",
        "\n",
        "Utilize a pré-busca em buffer para gerar dados a partir do disco sem o bloqueio de I/O. Veja duas funções importantes para usar ao carregar os dados:\n",
        "\n",
        "- `Dataset.cache` mantém o conjunto de frames na memória após o carregamento fora do disco durante a primeira época. Essa função garante que o dataset não se torne um gargalo ao treinar seu modelo. Se o dataset for muito grande para a memória, você também pode usar esse método para criar um cache no disco eficaz.\n",
        "\n",
        "- `Dataset.prefetch`: sobrepõe o processamento de dados e a execução do modelo durante o treinamento. Confira mais detalhes no guia [Melhor desempenho com o `tf.data`](https://www.tensorflow.org/guide/data_performance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSxjFtxAvY3_"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaY-hyr-Fbfr"
      },
      "source": [
        "Para preparar os dados que alimentarão o modelo, use os lotes conforme mostrado abaixo. Observe que, ao trabalhar com dados de vídeo, como arquivos AVI, eles devem estar no formato de um objeto pentadimensional `[batch_size, number_of_frames, height, width, channels]`. Em comparação, uma imagem teria quatro dimensões: `[batch_size, height, width, channels]`. A imagem abaixo é uma ilustração de como o formato dos dados de vídeo é representado.\n",
        "\n",
        "![Formato dos dados de vídeo](https://www.tensorflow.org/images/tutorials/video/video_data_shape.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp2Qc6XSFmeB"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.batch(2)\n",
        "val_ds = val_ds.batch(2)\n",
        "\n",
        "train_frames, train_labels = next(iter(train_ds))\n",
        "print(f'Shape of training set of frames: {train_frames.shape}')\n",
        "print(f'Shape of training labels: {train_labels.shape}')\n",
        "\n",
        "val_frames, val_labels = next(iter(val_ds))\n",
        "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
        "print(f'Shape of validation labels: {val_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqjXn1FgsMqZ"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "Agora que você criou um `Dataset` do TensorFlow de frames de vídeos com seus rótulos, você pode usá-lo com um modelo de aprendizado profundo. O seguinte modelo de classificação que usa uma [EfficientNet](https://arxiv.org/abs/1905.11946){:.external} pré-treinada realiza o treinamento com alta exatidão em alguns minutos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzqgPBUuForj"
      },
      "outputs": [],
      "source": [
        "net = tf.keras.applications.EfficientNetB0(include_top = False)\n",
        "net.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(scale=255),\n",
        "    tf.keras.layers.TimeDistributed(net),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.GlobalAveragePooling3D()\n",
        "])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds, \n",
        "          epochs = 10,\n",
        "          validation_data = val_ds,\n",
        "          callbacks = tf.keras.callbacks.EarlyStopping(patience = 2, monitor = 'val_loss'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdJm7ojgGxtT"
      },
      "source": [
        "Para saber mais sobre como trabalhar com dados de vídeo no TensorFlow, confira os tutoriais a seguir:\n",
        "\n",
        "- [Crie um modelo CNN 3D para a classificação de vídeos](https://www.tensorflow.org/tutorials/video/video_classification)\n",
        "- [MoViNet para o reconhecimento de ações de streaming](https://www.tensorflow.org/hub/tutorials/movinet)\n",
        "- [Aprendizado por transferência para a classificação de vídeos com MoViNet](https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "video.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
