{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jQ1tEQCxwRx"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V_sgB_5dx1f1"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62G8M_viUJp"
      },
      "source": [
        "# Jogando o desafio CartPole com o método Actor-Critic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mJ2i6jvZ3sK"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/reinforcement_learning/actor_critic.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/reinforcement_learning/actor_critic.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/reinforcement_learning/actor_critic.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgN7h_wiUJq"
      },
      "source": [
        "Este tutorial demonstra como implementar o método [Actor-Critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) usando o TensorFlow para treinar um agente no ambiente [`CartPole-v0`](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) do [Open AI Gym](https://www.gymlibrary.dev/). Presume-se que o leitor tenha alguma familiaridade com os [métodos de gradiente de política](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) do [ aprendizado (profundo) por reforço](https://en.wikipedia.org/wiki/Deep_reinforcement_learning).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kA10ZKRR0hi"
      },
      "source": [
        "**Métodos Actor-Critic**\n",
        "\n",
        "Os métodos Actor-Critic são métodos de [aprendizado de diferença temporal (TD)](https://en.wikipedia.org/wiki/Temporal_difference_learning) que representam a \"função política\" independente da função de valor.\n",
        "\n",
        "Uma política ou função política retorna uma distribuição de probabilidade das ações que o agente pode realizar com base no estado específico. Uma função de valor determina o retorno esperado para um agente que começa em um determinado estado e age sempre de acordo com uma certa política depois disso.\n",
        "\n",
        "No método Actor-Critic, a política refere-se ao *ator* que propõe um conjunto de ações possíveis considerando um estado, e a função de valor estimado refere-se ao *crítico*, que avalia as ações tomadas pelo *ator* com base na política específica.\n",
        "\n",
        "Neste tutorial, ambos o *Actor* e o *Critic* serão representados usando uma rede neural com duas saídas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBfiafKSRs2k"
      },
      "source": [
        "**`CartPole-v0`**\n",
        "\n",
        "No [ambiente `CartPole-v0`](https://www.gymlibrary.dev/environments/classic_control/cart_pole/), um pêndulo é conectado a um carrinho que se move por uma trilha sem atrito. O pêndulo começa na posição vertical e o objetivo do agente é evitar que ele caia ao aplicar uma força de `-1` ou `+1` ao carrinho. Uma recompensa de `+1` é concedida sempre que o pêndulo permanece na vertical. Um episódio termina quando: 1) o pêndulo está mais de 15 graus fora da vertical; ou 2) o carrinho se move mais de 2,4 unidades do centro.\n",
        "\n",
        "<center>\n",
        "  <pre data-md-type=\"custom_pre\">&lt;figure&gt;\n",
        "    &lt;image src=\"https://tensorflow.org/tutorials/reinforcement_learning/images/cartpole-v0.gif\"&gt;\n",
        "    &lt;figcaption&gt;\n",
        "      Trained actor-critic model in Cartpole-v0 environment\n",
        "    &lt;/figcaption&gt;\n",
        "  &lt;/figure&gt;</pre>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSNVK0AeRoJd"
      },
      "source": [
        "O problema é considerado \"solucionado\" quando a recompensa média total para o episódio alcança 195 em 100 testes consecutivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glLwIctHiUJq"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Importe os pacotes necessários e ajuste as configurações globais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13l6BbxKhCKp"
      },
      "outputs": [],
      "source": [
        "!pip install gym[classic_control]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBeQhPi2S4m5"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y python-opengl > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT4N3qYviUJr"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOUCe2D0iUJu"
      },
      "source": [
        "## O modelo\n",
        "\n",
        "O *Actor* e o *Critic* serão modelados usando uma rede neural que gera as probabilidades das ações e o valor do Critic, respectivamente. Este tutorial usa as subclasses de modelo para definir o modelo.\n",
        "\n",
        "Durante a passagem direta, o modelo usará o estado como entrada e gerará as probabilidades das ações e o valor do crítico $V$, que modela a [função de valor](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions) dependente do estado. A meta é treinar um modelo que escolha ações com base em uma política $\\pi$ que maximiza o [retorno](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return) esperado.\n",
        "\n",
        "Para `CartPole-v0`, há quatro valores que representam o estado: a posição do carrinho, a velocidade do carrinho, o ângulo do pêndulo e a velocidade do pêndulo, respectivamente. O agente pode realizar duas ações para empurrar o carrinho para a esquerda (`0`) e a direita (`1`), respectivamente.\n",
        "\n",
        "Consulte a [página com a documentação sobre o Cart Pole do Gym](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) e o artigo [*Neuronlike adaptive elements that can solve difficult learning control problems*](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) (Elementos adaptativos semelhantes a neurônios que podem resolver problemas difíceis de controle de aprendizado) de Barto, Sutton e Anderson (1983) para mais informações.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXKbbMC-kmuv"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions: int, \n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    self.actor = layers.Dense(num_actions)\n",
        "    self.critic = layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.common(inputs)\n",
        "    return self.actor(x), self.critic(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWyxJgjLn68c"
      },
      "outputs": [],
      "source": [
        "num_actions = env.action_space.n  # 2\n",
        "num_hidden_units = 128\n",
        "\n",
        "model = ActorCritic(num_actions, num_hidden_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk92njFziUJw"
      },
      "source": [
        "## Treine o agente\n",
        "\n",
        "Para treinar o agente, siga estas etapas:\n",
        "\n",
        "1. Execute o agente no ambiente para coletar dados de treinamento por episódio.\n",
        "2. Compute o retorno esperado em cada timestep.\n",
        "3. Compute a perda para o modelo Actor-Critic combinado.\n",
        "4. Compute os gradientes e atualize os parâmetros da rede.\n",
        "5. Repita as etapas 1 a 4 até alcançar os critérios de sucesso ou o máximo de episódios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2nde2XDs8Gh"
      },
      "source": [
        "### 1. Colete os dados de treinamento\n",
        "\n",
        "No aprendizado supervisionado, para treinar o modelo Actor-Critic, você precisa ter dados de treinamento. No entanto, para coletar esses dados, o modelo precisaria ser \"executado\" no ambiente.\n",
        "\n",
        "Os dados de treinamento são coletados para cada episódio. Depois, em cada timestep, a passagem direta do modelo será executada no estado do ambiente para gerar as probabilidades das ações e o valor do crítico com base na política atual parametrizada pelos pesos do modelo.\n",
        "\n",
        "A próxima ação será tomada das probabilidades das ações geradas pelo modelo, que depois seria aplicada ao ambiente, fazendo com que o próximo estado e recompensa sejam gerados.\n",
        "\n",
        "Esse processo é implementado na função `run_episode`, que usa as operações do TensorFlow para depois compilar em um grafo do TensorFlow e treinar mais rapidamente. Observe que `tf.TensorArray`s foram usados para apoiar a iteração do Tensor em arrays de comprimentos variados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5URrbGlDSAGx"
      },
      "outputs": [],
      "source": [
        "# Wrap Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, truncated, info = env.step(action)\n",
        "  return (state.astype(np.float32), \n",
        "          np.array(reward, np.int32), \n",
        "          np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action], \n",
        "                           [tf.float32, tf.int32, tf.int32])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4qVRV063Cl9"
      },
      "outputs": [],
      "source": [
        "def run_episode(\n",
        "    initial_state: tf.Tensor,  \n",
        "    model: tf.keras.Model, \n",
        "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  for t in tf.range(max_steps):\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "  \n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "  \n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "    # Store critic values\n",
        "    values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "  \n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "  \n",
        "    # Store reward\n",
        "    rewards = rewards.write(t, reward)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  action_probs = action_probs.stack()\n",
        "  values = values.stack()\n",
        "  rewards = rewards.stack()\n",
        "  \n",
        "  return action_probs, values, rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBnIHdz22dIx"
      },
      "source": [
        "### 2. Compute os retornos esperados\n",
        "\n",
        "A sequência de recompensas para cada timestep $t$, ${r_{t}}^{T}{em0}{t=1}$ coletada durante um episódio é convertida em uma sequência de retornos esperados ${G{/em0}{t}}^{T}_{t=1}$ em que a soma de recompensas é obtida do timestep atual $t$ a $T$ e cada recompensa é multiplicada com um fator de desconto que diminui exponencialmente $\\gamma$:\n",
        "\n",
        "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
        "\n",
        "Desde $\\gamma\\in(0,1)$, as recompensas mais distantes do timestep atual recebem menos peso.\n",
        "\n",
        "Intuitivamente, o retorno esperado simplesmente sugere que as recompensas agora são melhores do que as recompensas depois. Em um sentido matemático, é para garantir a convergência da soma de recompensas.\n",
        "\n",
        "Para estabilizar o treinamento, a sequência resultante de retornos também é padronizada (ou seja, não ter desvio padrão de unidade e média).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpEwFyl315dl"
      },
      "outputs": [],
      "source": [
        "def get_expected_return(\n",
        "    rewards: tf.Tensor, \n",
        "    gamma: float, \n",
        "    standardize: bool = True) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = reward + gamma * discounted_sum\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  if standardize:\n",
        "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
        "               (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "  return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhr50_Czxazw"
      },
      "source": [
        "### 3. A perda Actor-Critic\n",
        "\n",
        "Como você está usando um modelo Actor-Critic híbrido, a função de perda escolhida é uma combinação de perdas de Actor e Critic para treinamento, conforme mostrado abaixo:\n",
        "\n",
        "$$L = L_{actor} + L_{critic}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOQIJuG1xdTH"
      },
      "source": [
        "#### A perda Actor\n",
        "\n",
        "A perda Actor é baseada em [gradientes de política com o Critic como linha de base dependente do estado](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) e computada com estimativas de amostra única (por episódio).\n",
        "\n",
        "$$L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
        "\n",
        "em que:\n",
        "\n",
        "- $T$: é o número de timesteps por episódio, que pode variar de acordo com o episódio\n",
        "- $s_{t}$: é o estado do timestep $t$\n",
        "- $a_{t}$: é a ação escolhida no timestep $t$ considerando o estado $s$\n",
        "- $\\pi_{\\theta}$: é a política (Actor) parametrizada por $\\theta$\n",
        "- $V^{\\pi}_{\\theta}$: é a função de valor (Critic) também parametrizada por $\\theta$\n",
        "- $G = G_{t}$: é o retorno esperado para um determinado par de ação e estado no timestep $t$\n",
        "\n",
        "Um termo negativo é adicionado à soma, já que a ideia é maximizar as probabilidades das ações gerando maiores recompensas ao minimizar a perda combinada.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y304O4OAxiAv"
      },
      "source": [
        "##### A vantagem\n",
        "\n",
        "O termo $G - V$ na nossa formulação $L_{actor}$ é chamado de [Vantagem](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), que indica o quão melhor é uma ação considerando um estado específico em comparação com uma ação aleatória selecionada de acordo com a política $\\pi$ para esse estado.\n",
        "\n",
        "Embora seja possível excluir uma linha de base, isso pode resultar em alta variância durante o treinamento. E o bom de escolher o crítico $V$ como linha de base é que ele é treinado para ficar o mais próximo possível de $G$, levando a menor variância .\n",
        "\n",
        "Além disso, sem o Critic, o algoritmo tenta aumentar as probabilidades de ações tomadas em um estado específico com base no retorno esperado, o que talvez não faça muita diferença se as probabilidades relativas entre as ações permanecerem as mesmas.\n",
        "\n",
        "Por exemplo, imagine que duas ações para um determinado estado geram o mesmo retorno esperado. Sem o Critic, o algoritmo tentaria aumentar a probabilidade dessas ações com base no objetivo $J$. Com o Critic, pode não haver Vantagem ($G - V = 0$). Portanto, nenhum benefício seria obtido ao aumentar as probabilidades das ações, e o algoritmo definiria os gradientes como zero.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hrPLrgGxlvb"
      },
      "source": [
        "#### A perda Critic\n",
        "\n",
        "O treinamento de $V$ o mais próximo possível de $G$ pode ser configurado como um problema de regressão com a seguinte função de perda:\n",
        "\n",
        "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
        "\n",
        "em que $L_{\\delta}$ é a [perda de Huber](https://en.wikipedia.org/wiki/Huber_loss), que é menos sensível a outliers nos dados do que a perda de erro quadrático.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EXwbEez6n9m"
      },
      "outputs": [],
      "source": [
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,  \n",
        "    values: tf.Tensor,  \n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSYkQOmRfV75"
      },
      "source": [
        "### 4. Defina o passo de treinamento para atualizar os parâmetros\n",
        "\n",
        "Todos os passos acimas são combinados em um passo de treinamento executado a cada episódio. Todos os passos que levam à função de perda são executados com o contexto `tf.GradientTape` para ativar a diferenciação automática.\n",
        "\n",
        "Este tutorial usa o otimizador Adam para aplicar os gradientes aos parâmetros do modelo.\n",
        "\n",
        "A soma das recompensas não descontadas, `episode_reward`, também é computada nesse passo. Esse valor será usado mais tarde para avaliar se os critérios de sucesso foram atendidos.\n",
        "\n",
        "O contexto `tf.function` é aplicado à função `train_step` para ser compilado em um grafo do TensorFlow chamável, o que pode levar a um treinamento 10x mais veloz.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoccrkF3IFCg"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor, \n",
        "    model: tf.keras.Model, \n",
        "    optimizer: tf.keras.optimizers.Optimizer, \n",
        "    gamma: float, \n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode) \n",
        "\n",
        "    # Calculate the expected returns\n",
        "    returns = get_expected_return(rewards, gamma)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
        "\n",
        "    # Calculate the loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFvZiDoAflGK"
      },
      "source": [
        "### 5. Execute o loop de treinamento\n",
        "\n",
        "O treinamento é executado ao realizar o passo de treinamento até alcançar os critérios de sucesso ou o número máximo de episódios.\n",
        "\n",
        "Um registro contínuo de recompensas dos episódios é mantido em uma fila. Depois de atingir 100 testes, a recompensa mais antiga é removida da ponta esquerda (cauda) da fila e a mais nova é adicionada à frente (direita). Uma soma contínua de recompensas também é mantida para a eficiência computacional.\n",
        "\n",
        "Dependendo do seu runtime, o treinamento pode acabar em menos de um minuto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbmBxnzLiUJx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "min_episodes_criterion = 100\n",
        "max_episodes = 10000\n",
        "max_steps_per_episode = 500\n",
        "\n",
        "# `CartPole-v1` is considered solved if average reward is >= 475 over 500 \n",
        "# consecutive trials\n",
        "reward_threshold = 475\n",
        "running_reward = 0\n",
        "\n",
        "# The discount factor for future rewards\n",
        "gamma = 0.99\n",
        "\n",
        "# Keep the last episodes reward\n",
        "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "t = tqdm.trange(max_episodes)\n",
        "for i in t:\n",
        "    initial_state, info = env.reset()\n",
        "    initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "    \n",
        "    episodes_reward.append(episode_reward)\n",
        "    running_reward = statistics.mean(episodes_reward)\n",
        "  \n",
        "\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "  \n",
        "    # Show the average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "  \n",
        "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru8BEwS1EmAv"
      },
      "source": [
        "## Visualização\n",
        "\n",
        "Após o treinamento, é bom visualizar o desempenho do modelo no ambiente. Você pode executar as células abaixo para gerar uma animação GIF da execução de um episódio do modelo. Observe que pacotes adicionais precisam ser instalados para o Gym renderizar as imagens do ambiente corretamente no Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbIMMkfmRHyC"
      },
      "outputs": [],
      "source": [
        "# Render an episode and save as a GIF file\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "\n",
        "render_env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "\n",
        "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
        "  state, info = env.reset()\n",
        "  state = tf.constant(state, dtype=tf.float32)\n",
        "  screen = env.render()\n",
        "  images = [Image.fromarray(screen)]\n",
        " \n",
        "  for i in range(1, max_steps + 1):\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    action_probs, _ = model(state)\n",
        "    action = np.argmax(np.squeeze(action_probs))\n",
        "\n",
        "    state, reward, done, truncated, info = env.step(action)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render()\n",
        "      images.append(Image.fromarray(screen))\n",
        "  \n",
        "    if done:\n",
        "      break\n",
        "  \n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(render_env, model, max_steps_per_episode)\n",
        "image_file = 'cartpole-v1.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLd720SejKmf"
      },
      "outputs": [],
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnq9Hzo1Po6X"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "Este tutorial demonstrou como implementar o método Actor-Critic usando o TensorFlow.\n",
        "\n",
        "Como próximo passo, você pode tentar treinar um modelo em um ambiente diferente no Gym.\n",
        "\n",
        "Para mais informações sobre os métodos Actor-Critic e o problema Cartpole-v0, consulte estes materiais:\n",
        "\n",
        "- [O método Actor-Critic](https://hal.inria.fr/hal-00840470/document)\n",
        "- [A palestra do Actor-Critic (CAL)](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=7&t=0s)\n",
        "- [Problema de controle de aprendizado Cart Pole [Barto, et al. 1983]](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n",
        "\n",
        "Para mais exemplos de aprendizado por reforço no TensorFlow, confira os seguintes recursos:\n",
        "\n",
        "- [Exemplos de código de aprendizado por reforço (keras.io)](https://keras.io/examples/rl/)\n",
        "- [Biblioteca TF-Agents de aprendizado por reforço](https://www.tensorflow.org/agents)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_jQ1tEQCxwRx"
      ],
      "name": "actor_critic.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
