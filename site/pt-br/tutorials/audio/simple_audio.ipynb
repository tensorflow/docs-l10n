{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fluF3_oOgkWF"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AJs7HHFmg1M9"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Reconhecimento de áudio simples: reconhecendo palavras-chave"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbqmZy0gbyE"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/simple_audio\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tutorials/audio/simple_audio.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfDNFlb66XF"
      },
      "source": [
        "Este tutorial demonstra como pré-processar arquivos de áudio no formato WAV e construir e treinar um modelo básico [de reconhecimento automático de fala](https://en.wikipedia.org/wiki/Speech_recognition) (ASR, na sigla em inglês) para reconhecer dez palavras diferentes. Você usará uma parte do [dataset Speech Commands](https://www.tensorflow.org/datasets/catalog/speech_commands) ([Warden, 2018](https://arxiv.org/abs/1804.03209)), que contém clipes de áudio curtos (de um segundo ou menos) de comandos em inglês, como \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\" e \"yes\".\n",
        "\n",
        "Os [sistemas](https://ai.googleblog.com/search/label/Speech%20Recognition) de reconhecimento de voz e áudio do mundo real são complexos. Mas, assim como a [classificação de imagens com o dataset MNIST](../quickstart/beginner.ipynb), este tutorial deve fornecer uma compreensão básica das técnicas envolvidas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9C3uLL8Izc"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Importe os módulos e dependências necessários. Você usará `tf.keras.utils.audio_dataset_from_directory` (introduzido no TensorFlow 2.10), que ajuda a gerar datasets de classificação de áudio a partir de diretórios de arquivos `.wav`. Você também precisará [do Seaborn](https://seaborn.pydata.org) para visualização neste tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhNW45sjDEDe"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q tensorflow tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## Importe o dataset mini Speech Commands\n",
        "\n",
        "Para economizar tempo com o carregamento de dados, você trabalhará com uma versão reduzida do dataset Speech Commands. O [dataset original](https://www.tensorflow.org/datasets/catalog/speech_commands) consiste em mais de 105.000 arquivos de áudio no [formato WAV (Waveform)](https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf) de pessoas falando 35 palavras diferentes. Esses dados foram coletados pelo Google e divulgados sob licença CC BY.\n",
        "\n",
        "Baixe e extraia o arquivo `mini_speech_commands.zip` que contém os datasets Speech Commands menores com `tf.keras.utils.get_file`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-rayb7-3Y0I"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = 'data/mini_speech_commands'\n",
        "\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgvFq3uYiS5G"
      },
      "source": [
        "Os clipes de áudio do dataset são armazenados em oito pastas correspondentes a cada comando de fala: `no`, `yes`, `down`, `go`, `left`, `up`, `right` e `stop`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70IBxSKxA1N9"
      },
      "outputs": [],
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
        "print('Commands:', commands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ7GJjDvHqtt"
      },
      "source": [
        "Divididos em diretórios dessa forma, você pode carregar facilmente os dados usando `keras.utils.audio_dataset_from_directory`.\n",
        "\n",
        "Os clipes de áudio têm 1 segundo ou menos a 16kHz. O `output_sequence_length=16000` preenche os curtos para exatamente 1 segundo (e cortaria os mais longos) para que possam ser facilmente agrupados em lote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFM4c3aMC8Qv"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=16000,\n",
        "    subset='both')\n",
        "\n",
        "label_names = np.array(train_ds.class_names)\n",
        "print()\n",
        "print(\"label names:\", label_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cestp83qFnU5"
      },
      "source": [
        "O dataset agora contém lotes de clipes de áudio e rótulos inteiros. Os clipes de áudio têm o formato `(batch, samples, channels)` (lote, amostras, canais). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yU6SQGIFb3H"
      },
      "outputs": [],
      "source": [
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppG9Dgq2Ex8R"
      },
      "source": [
        "Este dataset contém apenas áudio de canal único, então use a função `tf.squeeze` para eliminar o eixo extra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl-tnniUIBlM"
      },
      "outputs": [],
      "source": [
        "def squeeze(audio, labels):\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  return audio, labels\n",
        "\n",
        "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtsCSWZN5ILv"
      },
      "source": [
        "A função `utils.audio_dataset_from_directory` retorna apenas até duas divisões (splits). É uma boa ideia manter um dataset de testes separado do seu dataset de validação. Idealmente, você o manteria em um diretório separado, mas neste caso você pode usar `Dataset.shard` para dividir o dataset de validação em duas metades. Observe que a iteração sobre **qualquer** fragmento carregará **todos** os dados e manterá apenas sua fração. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5UEGsqM5Gss"
      },
      "outputs": [],
      "source": [
        "test_ds = val_ds.shard(num_shards=2, index=0)\n",
        "val_ds = val_ds.shard(num_shards=2, index=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIeoJcwJH5h9"
      },
      "outputs": [],
      "source": [
        "for example_audio, example_labels in train_ds.take(1):  \n",
        "  print(example_audio.shape)\n",
        "  print(example_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxGEwvuh2L7"
      },
      "source": [
        "Vamos desenhar algumas formas de onda de áudio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYtGq2zYNHuT"
      },
      "outputs": [],
      "source": [
        "label_names[[1,1,3,0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yuX6Nqzf6wT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows * cols\n",
        "for i in range(n):\n",
        "  plt.subplot(rows, cols, i+1)\n",
        "  audio_signal = example_audio[i]\n",
        "  plt.plot(audio_signal)\n",
        "  plt.title(label_names[example_labels[i]])\n",
        "  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  plt.ylim([-1.1, 1.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXPphxm0B4m"
      },
      "source": [
        "## Converta formas de onda em espectrogramas\n",
        "\n",
        "As formas de onda no dataset são representadas no domínio do tempo. A seguir, você transformará as formas de onda dos sinais no domínio do tempo em sinais no domínio da frequência e do tempo, calculando a [transformada de Fourier de tempo curto (STFT, da sigla em inglês)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) para converter as formas de onda em [espectrogramas](https://en.wikipedia.org/wiki/Spectrogram), que mostram mudanças de frequência ao longo do tempo e podem ser representadas como imagens 2D. Você alimentará as imagens do espectrograma em sua rede neural para treinar o modelo.\n",
        "\n",
        "Uma transformada de Fourier (`tf.signal.fft`) converte um sinal em seus componentes de frequência, mas perde todas as informações de tempo. Em comparação, o STFT (`tf.signal.stft`) divide o sinal em janelas de tempo e executa uma transformada de Fourier em cada janela, preservando algumas informações de tempo e retornando um tensor 2D no qual você pode executar convoluções padrão.\n",
        "\n",
        "Crie uma função utilitária para converter formas de onda em espectrogramas:\n",
        "\n",
        "- As formas de onda precisam ter o mesmo comprimento, para que, ao convertê-las em espectrogramas, os resultados tenham dimensões semelhantes. Isso pode ser feito simplesmente preenchendo com zero os clipes de áudio com menos de um segundo (usando `tf.zeros`).\n",
        "- Ao chamar `tf.signal.stft`, escolha os parâmetros `frame_length` e `frame_step` de forma que a \"imagem\" do espectrograma gerado seja quase quadrada. Para obter mais informações sobre a escolha dos parâmetros STFT, consulte [este vídeo do Coursera](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe) sobre processamento de sinal de áudio e STFT.\n",
        "- O STFT produz um array de números complexos representando magnitude e fase. No entanto, neste tutorial você usará apenas a magnitude, que pode ser derivada aplicando `tf.abs` na saída de `tf.signal.stft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4CK75DHz_OR"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "  # Convert the waveform to a spectrogram via a STFT.\n",
        "  spectrogram = tf.signal.stft(\n",
        "      waveform, frame_length=255, frame_step=128)\n",
        "  # Obtain the magnitude of the STFT.\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  # Add a `channels` dimension, so that the spectrogram can be used\n",
        "  # as image-like input data with convolution layers (which expect\n",
        "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdPiPYJphs2"
      },
      "source": [
        "Em seguida, comece a explorar os dados. Imprima as formas da forma de onda tensorizada de um exemplo e o espectrograma correspondente e reproduza o áudio original:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mu6Y7Yz3C-V"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "  label = label_names[example_labels[i]]\n",
        "  waveform = example_audio[i]\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "  print('Label:', label)\n",
        "  print('Waveform shape:', waveform.shape)\n",
        "  print('Spectrogram shape:', spectrogram.shape)\n",
        "  print('Audio playback')\n",
        "  display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnSuqyxJ1isF"
      },
      "source": [
        "Agora, defina uma função para exibir um espectrograma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e62jzb36-Jog"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "  # Convert the frequencies to log scale and transpose, so that the time is\n",
        "  # represented on the x-axis (columns).\n",
        "  # Add an epsilon to avoid taking a log of zero.\n",
        "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baa5c91e8603"
      },
      "source": [
        "Desenhe a forma de onda do exemplo ao longo do tempo e o espectrograma correspondente (frequências ao longo do tempo):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2_CikgY1tjv"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.suptitle(label.title())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYXjW07jCHA"
      },
      "source": [
        "Agora, crie datasets de espectrograma a partir dos datasets de áudio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAD0LpkgqtQo"
      },
      "outputs": [],
      "source": [
        "def make_spec_ds(ds):\n",
        "  return ds.map(\n",
        "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
        "      num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEVb_oK0oBLQ"
      },
      "outputs": [],
      "source": [
        "train_spectrogram_ds = make_spec_ds(train_ds)\n",
        "val_spectrogram_ds = make_spec_ds(val_ds)\n",
        "test_spectrogram_ds = make_spec_ds(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gQpAAgMnyDi"
      },
      "source": [
        "Examine os espectrogramas para diferentes exemplos do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaM2q5aGis-d"
      },
      "outputs": [],
      "source": [
        "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUbHfTuon4iF"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
        "\n",
        "for i in range(n):\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
        "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KdY8IF8rkt"
      },
      "source": [
        "## Construa e treine o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1uIh6F_TN9"
      },
      "source": [
        "Adicione operações `Dataset.cache` e `Dataset.prefetch` para reduzir a latência de leitura durante o treinamento do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdZ6M-F5_QzY"
      },
      "outputs": [],
      "source": [
        "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwHkKCQQb5oW"
      },
      "source": [
        "Para o modelo, você usará uma rede neural convolucional (CNN) simples, já que transformou os arquivos de áudio em imagens de espectrograma.\n",
        "\n",
        "Seu modelo `tf.keras.Sequential` usará as seguintes camadas de pré-processamento Keras:\n",
        "\n",
        "- `tf.keras.layers.Resizing`: para reduzir a resolução da entrada para permitir que o modelo treine mais rapidamente.\n",
        "- `tf.keras.layers.Normalization`: para normalizar cada pixel da imagem com base em sua média e desvio padrão.\n",
        "\n",
        "Para a camada `Normalization`, seu método `adapt` precisaria primeiro ser chamado nos dados de treinamento para calcular estatísticas agregadas (ou seja, a média e o desvio padrão)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALYz7PFCHblP"
      },
      "outputs": [],
      "source": [
        "input_shape = example_spectrograms.shape[1:]\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(label_names)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(32, 32),\n",
        "    # Normalize.\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de52e5afa2f3"
      },
      "source": [
        "Configure o modelo Keras com o otimizador Adam e a perda de entropia cruzada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFjj7-EmsTD-"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42b9e3a4705"
      },
      "source": [
        "Treine o modelo ao longo de 10 épocas para fins de demonstração:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttioPJVMcGtq"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_spectrogram_ds,\n",
        "    validation_data=val_spectrogram_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpCDeQ4mUfS"
      },
      "source": [
        "Vamos plotar as curvas de perda de treinamento e validação para verificar como seu modelo melhorou durante o treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzhipg3Gu2AY"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [CrossEntropy]')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.ylim([0, 100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy [%]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZTt3kO3mfm4"
      },
      "source": [
        "## Avalie o desempenho do modelo\n",
        "\n",
        "Execute o modelo sobre o dataset de testes e verifique o desempenho do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FapuRT_SsWGQ"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_spectrogram_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en9Znt1NOabH"
      },
      "source": [
        "### Exiba uma matriz de confusão\n",
        "\n",
        "Use uma [matriz de confusão](https://developers.google.com/machine-learning/glossary#confusion-matrix) para verificar o desempenho do modelo ao classificar cada um dos comandos no dataset de teste:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6vmWWQuuT1"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_spectrogram_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6F0il82u7lW"
      },
      "outputs": [],
      "source": [
        "y_pred = tf.argmax(y_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHSNoBYLvX81"
      },
      "outputs": [],
      "source": [
        "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvoSAOiXU3lL"
      },
      "outputs": [],
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx,\n",
        "            xticklabels=label_names,\n",
        "            yticklabels=label_names,\n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGi_mzPcLvl"
      },
      "source": [
        "## Execute inferência num arquivo de áudio\n",
        "\n",
        "Por fim, verifique o resultado da previsão do modelo usando um arquivo de áudio de entrada de alguém dizendo “no” (não). Qual é o desempenho do seu modelo?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxauKMdhofU"
      },
      "outputs": [],
      "source": [
        "x = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
        "x = tf.io.read_file(str(x))\n",
        "x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
        "x = tf.squeeze(x, axis=-1)\n",
        "waveform = x\n",
        "x = get_spectrogram(x)\n",
        "x = x[tf.newaxis,...]\n",
        "\n",
        "prediction = model(x)\n",
        "x_labels = ['no', 'yes', 'down', 'go', 'left', 'up', 'right', 'stop']\n",
        "plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
        "plt.title('No')\n",
        "plt.show()\n",
        "\n",
        "display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWICqdqQNaQ"
      },
      "source": [
        "Como a saída sugere, seu modelo deveria ter reconhecido o comando de áudio como \"no\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1icqlM3ISW0"
      },
      "source": [
        "## Exporte o modelo com pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7HX-MjgIbji"
      },
      "source": [
        "O modelo não é muito fácil de usar se você precisar aplicar essas etapas de pré-processamento antes de passar os dados ao modelo para inferência. Portanto, crie uma versão ponta a ponta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lIeXdWjIbDE"
      },
      "outputs": [],
      "source": [
        "class ExportModel(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "    # Accept either a string-filename or a batch of waveforms.\n",
        "    # YOu could add additional signatures for a single wave, or a ragged-batch. \n",
        "    self.__call__.get_concrete_function(\n",
        "        x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
        "    self.__call__.get_concrete_function(\n",
        "       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x):\n",
        "    # If they pass a string, load the file and decode it. \n",
        "    if x.dtype == tf.string:\n",
        "      x = tf.io.read_file(x)\n",
        "      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
        "      x = tf.squeeze(x, axis=-1)\n",
        "      x = x[tf.newaxis, :]\n",
        "    \n",
        "    x = get_spectrogram(x)  \n",
        "    result = self.model(x, training=False)\n",
        "    \n",
        "    class_ids = tf.argmax(result, axis=-1)\n",
        "    class_names = tf.gather(label_names, class_ids)\n",
        "    return {'predictions':result,\n",
        "            'class_ids': class_ids,\n",
        "            'class_names': class_names}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtZBmUiB9HGY"
      },
      "source": [
        "Teste a execução do modelo \"export\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1_8TYaCIRue"
      },
      "outputs": [],
      "source": [
        "export = ExportModel(model)\n",
        "export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J6Iuz829Cxo"
      },
      "source": [
        "Salve e recarregue o modelo, o modelo recarregado fornece uma saída idêntica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTAg4vsn3oEb"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(export, \"saved\")\n",
        "imported = tf.saved_model.load(\"saved\")\n",
        "imported(waveform[tf.newaxis, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3jF933m9z1J"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "Este tutorial demonstrou como realizar uma classificação de áudio simples/reconhecimento automático de fala usando uma rede neural convolucional com TensorFlow e Python. Para saber mais, considere os seguintes recursos:\n",
        "\n",
        "- O tutorial [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) (Classificação de som com YAMNet) mostra como usar o aprendizado por transferência para classificação de áudio.\n",
        "- Os notebooks do [desafio de reconhecimento de fala TensorFlow do Kaggle](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview).\n",
        "- O [codelab TensorFlow.js – Reconhecimento de áudio usando aprendizagem por transferência](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0) ensina como criar seu próprio aplicativo Web interativo para classificação de áudio.\n",
        "- [A tutorial on deep learning for music information retrieval](https://arxiv.org/abs/1709.04396) (Um tutorial sobre aprendizagem profunda para recuperação de informação musical) (Choi et al., 2017) em arXiv.\n",
        "- O TensorFlow também oferece suporte adicional para [preparação e aumento de dados de áudio](https://www.tensorflow.org/io/tutorials/audio) para ajudar em seus próprios projetos baseados em áudio.\n",
        "- Considere usar a biblioteca [librosa](https://librosa.org/) para análise de música e áudio."
      ]
    }
  ],
  "metadata": {
    "accelerator": "CPU",
    "colab": {
      "collapsed_sections": [],
      "name": "simple_audio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
