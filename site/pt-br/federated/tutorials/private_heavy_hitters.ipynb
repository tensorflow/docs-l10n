{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQxl99l0bZac"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YHz2D-oIqBWa"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXslvcRocA-0"
      },
      "source": [
        "# Heavy hitters privados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XBJJIqwcXKd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/private_heavy_hitters\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/federated/tutorials/private_heavy_hitters.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/federated/tutorials/private_heavy_hitters.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/federated/tutorials/private_heavy_hitters.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJqFp24bb2JN"
      },
      "source": [
        "**OBSERVAÇÃO**: foi verificado que este Colab funciona com a [versão mais recente lançada](https://github.com/tensorflow/federated#compatibility) do pacote pip `tensorflow_federated`. Talvez não seja possível atualizar este Colab para funcionar no `main`.\n",
        "\n",
        "Este tutorial mostra como usar a API `tff.analytics.heavy_hitters.iblt.build_iblt_computation` para construir uma computação analítica federada para descobrir as strings mais frequentes (heavy hitters privados) na população."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnUwFbCAKB2r"
      },
      "source": [
        "## Configuração do ambiente\n",
        "\n",
        "Execute o código abaixo para que o ambiente seja configurado corretamente. Se não for exibida uma saudação, consulte as instruções de [instalação](../install.md). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrGitA_KnRO0"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "# tensorflow_federated_nightly also bring in tf_nightly, which\n",
        "# can causes a duplicate tensorboard install, leading to errors.\n",
        "!pip install --quiet tensorflow-text-nightly\n",
        "!pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BKyHkMxKHfV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'Hello, World!'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "np.random.seed(0)\n",
        "tff.backends.test.set_sync_test_cpp_execution_context()\n",
        "\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhLs5GNQ-wWu"
      },
      "source": [
        "## Histórico – Heavy hitters privados em análise federada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgGacXm1mVE3"
      },
      "source": [
        "Considere o seguinte cenário: cada cliente tem uma lista de strings, e cada string é de um conjunto aberto, ou seja, pode ser arbitrário. O objetivo é descobrir as strings mais populares (**heavy hitters**) e suas contagens de forma privada em um ambiente federado. Este Colab demonstra uma solução para esse problema com as seguintes propriedades de privacidade:\n",
        "\n",
        "- Agregação segura – Computa as contagens agregadas de strings de tal forma que não deve ser possível para o servidor aprender um valor individual de qualquer cliente. Confira mais informações em `tff.federated_secure_sum`.\n",
        "- Privacidade diferencial (DP, na sigla em inglês) – Método amplamente usado para limitação e quantificação do vazamento de dados confidenciais em análises. Você pode aplicar a privacidade diferencial central em nível de usuário aos resultados de heavy hitter.\n",
        "\n",
        "A API de agregação segura `tff.federated_secure_sum` tem suporte a somas lineares de vetores de inteiros. Se as strings forem de um conjunto fechado de tamanho `n`, então é fácil codificar as strings de cada cliente em um vetor de tamanho `n` – seja o valor no índice `i` do vetor a contagem da `i`<sup>ésima</sup> string no conjunto fechado; então, é possível somar de forma segura os vetores de todos os clientes para obter as contagens de strings de toda a população. Entretanto, se as strings forem de um conjunto aberto, não é óbvio como codificá-los adequadamente para fazer a soma segura. Neste trabalho, você pode codificar as strings em [Invertible Bloom Lookup Tables (IBLT)](https://arxiv.org/abs/1101.2245), que são uma estrutura de dados probabilística que tem a capacidade de codificar itens em um domínio grande (ou aberto) de maneira eficiente. Os sketches de IBLT podem ser somados linearmente, então são compatíveis com a soma segura.\n",
        "\n",
        "Você pode usar `tff.analytics.heavy_hitters.iblt.build_iblt_computation` para criar uma computação do TFF que codifique as strings locais de cada cliente em uma estrutura de IBLT. Essas estruturas são somadas de forma segura por meio de um protocolo de computação seguro criptográfico multiparte em uma estrutura de IBLT agregada que o servidor pode decodificar. O servidor pode então retornar os principais heavy hitters. As próximas seções mostram como usar essa API para criar uma computação do TFF e executar simulações com o dataset Shakespeare. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFY_3z-x-3r6"
      },
      "source": [
        "## Carregue e pré-processe os dados de Shakespeare federados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O1CHhdDJcij"
      },
      "source": [
        "O dataset Shakespeare contém falas de personagens das peças de Shakespeare. Neste exemplo, um subconjunto de personagens (isto é, clientes) são selecionados. Um pré-processador converte as falas de cada personagem em uma lista de strings, e qualquer string que é apenas um sinal de pontuação ou símbolos é descartada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b65q5mp4r1n7"
      },
      "outputs": [],
      "source": [
        "# Load the simulation data.\n",
        "source, _ = tff.simulation.datasets.shakespeare.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReoTRs8ntJw7"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function to tokenize a line into words.\n",
        "def tokenize(ds):\n",
        "  \"\"\"Tokenizes a line into words with alphanum characters.\"\"\"\n",
        "  def extract_strings(example):\n",
        "    return tf.expand_dims(example['snippets'], 0)\n",
        "\n",
        "  def tokenize_line(line):\n",
        "    return tf.data.Dataset.from_tensor_slices(tokenizer.tokenize(line)[0])\n",
        "\n",
        "  def mask_all_symbolic_words(word):\n",
        "    return tf.math.logical_not(\n",
        "        tf_text.wordshape(word, tf_text.WordShape.IS_PUNCT_OR_SYMBOL))\n",
        "\n",
        "  tokenizer = tf_text.WhitespaceTokenizer()\n",
        "  ds = ds.map(extract_strings)\n",
        "  ds = ds.flat_map(tokenize_line)\n",
        "  ds = ds.map(tf_text.case_fold_utf8)\n",
        "  ds = ds.filter(mask_all_symbolic_words)\n",
        "  return ds\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return tokenize(source.create_tf_dataset_for_client(\n",
        "      source.client_ids[n])).batch(batch_size)\n",
        "\n",
        "# Pick a subset of client devices to participate in the computation.\n",
        "dataset = [client_data(n) for n in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGwJsssK9_e"
      },
      "source": [
        "## Simulações"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtCRYhI0nKcm"
      },
      "source": [
        "Para executar simulações a fim de descobrir as palavras mais populares (heavy hitters) no dataset Shakespeare, primeiro você precisa criar uma computação do TFF usando a API `tff.analytics.heavy_hitters.iblt.build_iblt_computation` com os seguintes parâmetros:\n",
        "\n",
        "- `capacity`: capacidade do sketch de IBLT. Este número deve ser aproximadamente o número total de strings únicas que podem aparecer em uma rodada de computação. O padrão é `1000`. Se esse número for pequeno demais, poderá haver falha na codificação devido à colisão de valores em hash. Se for grande demais, o consumo de memória poderá ser maior do que o necessário.\n",
        "- `string_max_bytes`: tamanho máximo de uma string na IBLT. O padrão é `10`. Precisa ser positivo. Strings maiores do que `string_max_bytes` serão truncadas.\n",
        "- `max_words_per_user`: número máximo de strings que cada cliente pode fornecer. Se não for igual a `None`, deve ser um inteiro positivo. O padrão é `None`, ou seja, todos os clientes fornecem todas as suas strings.\n",
        "- `max_heavy_hitters`: número máximo de itens a retornar. Se os resultados decodificados tiverem mais do que esse número de itens, a lista será ordenada de forma decrescente pelas contagens estimadas, e serão retornados os primeiros max_heavy_hitters itens. O padrão é `None`, ou seja, todos os heavy hitters são retornados no resultado.\n",
        "- `secure_sum_bitwidth`: comprimento de bits usado para a soma segura. O valor padrão é `None`, que desativa a soma segura. Se não for `None`, precisa estar no intervalo `[1,62]`. Confira `tff.federated_secure_sum`.\n",
        "- `multi_contribution`: define se cada cliente pode fornecer diversas contagens ou somente uma para cada palavra única. O padrão é `True`. Esse argumento pode melhorar a utilidade quando for necessário ter privacidade diferencial.\n",
        "- `batch_size`: número de elementos em cada lote do dataset. O padrão é `1`, ou seja, o dataset de entrada é processado por `tf.data.Dataset.batch(1)`. Deve ser um inteiro positivo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iyRWmV529qY"
      },
      "outputs": [],
      "source": [
        "max_words_per_user = 8\n",
        "iblt_computation = tff.analytics.heavy_hitters.iblt.build_iblt_computation(\n",
        "    capacity=100,\n",
        "    string_max_bytes=20,\n",
        "    max_words_per_user=max_words_per_user,\n",
        "    max_heavy_hitters=10,\n",
        "    secure_sum_bitwidth=32,\n",
        "    multi_contribution=False,\n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe8ZUIwH4C1y"
      },
      "source": [
        "Agora está tudo pronto para executar simulações com computação do TFF `iblt_computation` e o dataset de entrada pré-processado. A saída `iblt_computation` tem quatro atributos:\n",
        "\n",
        "- clients: número escalar de clientes que participaram da computação.\n",
        "- heavy_hitters: lista de heavy hitters agregados.\n",
        "- heavy_hitters_counts: lista das contagens de heavy hitters agregados.\n",
        "- num_not_decoded: número escalar de strings que não são decodificadas com êxito.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r8Y6GL-zhPv"
      },
      "outputs": [],
      "source": [
        "def run_simulation(one_round_computation: tff.Computation, dataset):\n",
        "  output = one_round_computation(dataset)\n",
        "  heavy_hitters = output.heavy_hitters\n",
        "  heavy_hitters_counts = output.heavy_hitters_counts\n",
        "  heavy_hitters = [word.decode('utf-8', 'ignore') for word in heavy_hitters]\n",
        "\n",
        "  results = {}\n",
        "  for index in range(len(heavy_hitters)):\n",
        "    results[heavy_hitters[index]] = heavy_hitters_counts[index]\n",
        "  return output.clients, dict(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w99wVdhW0OIR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of clients participated: 10\n",
            "Discovered heavy hitters and counts:\n",
            "{'to': 8, 'the': 8, 'and': 7, 'you': 4, 'i': 4, 'a': 3, 'he': 3, 'your': 3, 'is': 3, 'of': 2}\n"
          ]
        }
      ],
      "source": [
        "clients, result = run_simulation(iblt_computation, dataset)\n",
        "print(f'Number of clients participated: {clients}')\n",
        "print('Discovered heavy hitters and counts:')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4SdslRULCox"
      },
      "source": [
        "## Heavy hitters privados com privacidade diferencial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F4O2U7nGL1A"
      },
      "source": [
        "Para obter heavy hitters privados com privacidade diferencial central, um mecanismo de privacidade diferencial é aplicado nos histogramas de datasets abertos. A ideia é adicionar ruído às contagens de strings no histograma agregado e apenas manter as strings com contagens acima de um determinado limiar. O ruído e o limiar dependem do budget (epsilon, delta)-DP (confira os detalhes do algoritmo e da prova [neste documento](https://github.com/google/differential-privacy/blob/main/common_docs/Delta_For_Thresholding.pdf)). As contagens de ruído são arredondadas para inteiros como uma etapa de pré-processamento, o que não enfraquece a garantia de privacidade diferencial. Você descobrirá menos heavy hitters quando for necessário ter privacidade diferencial, pois a etapa de limiar filtra strings com baixas contagens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryZZgH8nJi9v"
      },
      "outputs": [],
      "source": [
        "iblt_computation = tff.analytics.heavy_hitters.iblt.build_iblt_computation(\n",
        "    capacity=100,\n",
        "    string_max_bytes=20,\n",
        "    max_words_per_user=max_words_per_user,\n",
        "    secure_sum_bitwidth=32,\n",
        "    multi_contribution=False,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "clients, result = run_simulation(iblt_computation, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxhBSUFs3Ku6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovered heavy hitters and counts with central DP:\n",
            "{'the': 8, 'you': 4, 'to': 7, 'tear': 3, 'and': 7, 'i': 3}\n"
          ]
        }
      ],
      "source": [
        "# DP parameters\n",
        "eps = 20\n",
        "delta = 0.01\n",
        "\n",
        "# Calculating scale for Laplace noise\n",
        "scale = max_words_per_user / eps\n",
        "\n",
        "# Calculating the threshold\n",
        "tau = 1 + (max_words_per_user / eps) * np.log(max_words_per_user / (2 * delta))\n",
        "\n",
        "result_with_dp = {}\n",
        "for word in result:\n",
        "  noised_count = result[word] + np.random.laplace(scale=scale)\n",
        "  if noised_count >= tau:\n",
        "    result_with_dp[word] = int(noised_count)\n",
        "print(f'Discovered heavy hitters and counts with central DP:')\n",
        "print(result_with_dp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "private_heavy_hitters.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
