{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beObUOFyuRjT"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6D70EeAZe-Q"
      },
      "source": [
        "# Drivers\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/4_drivers_tutorial\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/agents/tutorials/4_drivers_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/agents/tutorials/4_drivers_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/agents/tutorials/4_drivers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aPHF9kXFggA"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "Um padrão comum no aprendizado por reforço é executar uma política em um ambiente por um número específico de passos ou episódios. Isso ocorre, por exemplo, durante a coleta de dados, a avaliação ou a geração de um vídeo do agente.\n",
        "\n",
        "Enquanto é relativamente simples escrever isso em python, é muito mais complexo escrever e depurar no TensorFlow, porque envolve loops de `tf.while`, `tf.cond` e `tf.control_dependencies`. Por isso, abstraímos essa noção de um loop de execução em uma classe chamada `driver` e fornecemos implementações bem testadas tanto no Python quanto no TensorFlow.\n",
        "\n",
        "Além disso, os dados encontrados pelo driver em cada passo são salvos em uma tupla nomeada chamada Trajectory e transmitidos para um conjunto de observadores, como buffers de replay e métricas. Esses dados incluem a observação a partir do ambiente, a ação recomendada pela política, a recompensa obtida, o tipo do passo atual e do seguinte etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7PM1QfMZqkS"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w-Ykwl1bn4v"
      },
      "source": [
        "Se você ainda não instalou o tf-agents ou gym, execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnE2CgilrngG"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whYNP894FSkA"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.drivers import dynamic_episode_driver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V7DEcB8IeiQ"
      },
      "source": [
        "## Drivers do Python\n",
        "\n",
        "A classe `PyDriver` aceita um ambiente do python, uma política do python e uma lista de observadores para atualizar a cada passo. O método principal é `run()`, que realiza os passos do ambiente usando ações da política até que um dos seguintes critérios de término seja atendido: o número de passos atingir o `max_steps` ou o número de episódios alcançar o `max_episodes`.\n",
        "\n",
        "A implementação é mais ou menos assim:\n",
        "\n",
        "```python\n",
        "class PyDriver(object):\n",
        "\n",
        "  def __init__(self, env, policy, observers, max_steps=1, max_episodes=1):\n",
        "    self._env = env\n",
        "    self._policy = policy\n",
        "    self._observers = observers or []\n",
        "    self._max_steps = max_steps or np.inf\n",
        "    self._max_episodes = max_episodes or np.inf\n",
        "\n",
        "  def run(self, time_step, policy_state=()):\n",
        "    num_steps = 0\n",
        "    num_episodes = 0\n",
        "    while num_steps &lt; self._max_steps and num_episodes &lt; self._max_episodes:\n",
        "\n",
        "      # Compute an action using the policy for the given time_step\n",
        "      action_step = self._policy.action(time_step, policy_state)\n",
        "\n",
        "      # Apply the action to the environment and get the next step\n",
        "      next_time_step = self._env.step(action_step.action)\n",
        "\n",
        "      # Package information into a trajectory\n",
        "      traj = trajectory.Trajectory(\n",
        "         time_step.step_type,\n",
        "         time_step.observation,\n",
        "         action_step.action,\n",
        "         action_step.info,\n",
        "         next_time_step.step_type,\n",
        "         next_time_step.reward,\n",
        "         next_time_step.discount)\n",
        "\n",
        "      for observer in self._observers:\n",
        "        observer(traj)\n",
        "\n",
        "      # Update statistics to check termination\n",
        "      num_episodes += np.sum(traj.is_last())\n",
        "      num_steps += np.sum(~traj.is_boundary())\n",
        "\n",
        "      time_step = next_time_step\n",
        "      policy_state = action_step.state\n",
        "\n",
        "    return time_step, policy_state\n",
        "\n",
        "```\n",
        "\n",
        "Agora, como exemplo, vamos executar uma política aleatória no ambiente CartPole, salvar os resultados em um buffer de replay e computar algumas métricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj4_-77_5ExP"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "policy = random_py_policy.RandomPyPolicy(time_step_spec=env.time_step_spec(), \n",
        "                                         action_spec=env.action_spec())\n",
        "replay_buffer = []\n",
        "metric = py_metrics.AverageReturnMetric()\n",
        "observers = [replay_buffer.append, metric]\n",
        "driver = py_driver.PyDriver(\n",
        "    env, policy, observers, max_steps=20, max_episodes=1)\n",
        "\n",
        "initial_time_step = env.reset()\n",
        "final_time_step, _ = driver.run(initial_time_step)\n",
        "\n",
        "print('Replay Buffer:')\n",
        "for traj in replay_buffer:\n",
        "  print(traj)\n",
        "\n",
        "print('Average Return: ', metric.result())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Yrxg36Ik1x"
      },
      "source": [
        "## Drivers do TensorFlow\n",
        "\n",
        "Também temos drivers no TensorFlow que são funcionalmente semelhantes aos drivers do Python, mas usam ambientes, políticas, observadores etc. do TF. No momento, temos 2 drivers do TensorFlow: `DynamicStepDriver`, que é encerrado após um determinado número de passos (válidos) do ambiente, e `DynamicEpisodeDriver`, que é encerrado após um determinado número de episódios. Vamos conferir um exemplo do DynamicEpisode em ação.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC4ba3ObSceA"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "tf_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
        "                                            time_step_spec=tf_env.time_step_spec())\n",
        "\n",
        "\n",
        "num_episodes = tf_metrics.NumberOfEpisodes()\n",
        "env_steps = tf_metrics.EnvironmentSteps()\n",
        "observers = [num_episodes, env_steps]\n",
        "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "    tf_env, tf_policy, observers, num_episodes=2)\n",
        "\n",
        "# Initial driver.run will reset the environment and initialize the policy.\n",
        "final_time_step, policy_state = driver.run()\n",
        "\n",
        "print('final_time_step', final_time_step)\n",
        "print('Number of Steps: ', env_steps.result().numpy())\n",
        "print('Number of Episodes: ', num_episodes.result().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz5jhHnU0fX1"
      },
      "outputs": [],
      "source": [
        "# Continue running from previous state\n",
        "final_time_step, _ = driver.run(final_time_step, policy_state)\n",
        "\n",
        "print('final_time_step', final_time_step)\n",
        "print('Number of Steps: ', env_steps.result().numpy())\n",
        "print('Number of Episodes: ', num_episodes.result().numpy())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "4_drivers_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
