{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beObUOFyuRjT"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eutDVTs9aJEL"
      },
      "source": [
        "# Buffers de replay\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/agents/tutorials/5_replay_buffers_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/agents/tutorials/5_replay_buffers_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/agents/tutorials/5_replay_buffers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aPHF9kXFggA"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "Os algoritmos de aprendizado por reforço usam buffers de replay para armazenar trajetórias de experiência ao executar uma política em um ambiente. Durante o treinamento, os buffers de replay são consultados sobre um subconjunto das trajetórias (seja um subconjunto sequencial ou uma amostra) para dar \"replay\" na experiência do agente.\n",
        "\n",
        "Neste colab, exploramos dois tipos de buffers de replay: com suporte do python e do TensorFlow, que compartilham uma API. Nas seções a seguir, descrevemos a API, cada uma das implementações de buffer e como usá-las durante o treinamento da coleta de dados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uSlqYgvaG9b"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GztmUpWKZ7kq"
      },
      "source": [
        "Instale o tf-agents caso não tenha feito isso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnE2CgilrngG"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whYNP894FSkA"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents import specs\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQWclL9FpZl"
      },
      "source": [
        "## API Replay Buffer\n",
        "\n",
        "A classe Replay Buffer tem a definição e os métodos a seguir:\n",
        "\n",
        "```python\n",
        "class ReplayBuffer(tf.Module):\n",
        "  \"\"\"Abstract base class for TF-Agents replay buffer.\"\"\"\n",
        "\n",
        "  def __init__(self, data_spec, capacity):\n",
        "    \"\"\"Initializes the replay buffer.\n",
        "\n",
        "    Args:\n",
        "      data_spec: A spec or a list/tuple/nest of specs describing\n",
        "        a single item that can be stored in this buffer\n",
        "      capacity: number of elements that the replay buffer can hold.\n",
        "    \"\"\"\n",
        "\n",
        "  @property\n",
        "  def data_spec(self):\n",
        "    \"\"\"Returns the spec for items in the replay buffer.\"\"\"\n",
        "\n",
        "  @property\n",
        "  def capacity(self):\n",
        "    \"\"\"Returns the capacity of the replay buffer.\"\"\"\n",
        "\n",
        "  def add_batch(self, items):\n",
        "    \"\"\"Adds a batch of items to the replay buffer.\"\"\"\n",
        "\n",
        "  def get_next(self,\n",
        "               sample_batch_size=None,\n",
        "               num_steps=None,\n",
        "               time_stacked=True):\n",
        "    \"\"\"Returns an item or batch of items from the buffer.\"\"\"\n",
        "\n",
        "  def as_dataset(self,\n",
        "                 sample_batch_size=None,\n",
        "                 num_steps=None,\n",
        "                 num_parallel_calls=None):\n",
        "    \"\"\"Creates and returns a dataset that returns entries from the buffer.\"\"\"\n",
        "\n",
        "\n",
        "  def gather_all(self):\n",
        "    \"\"\"Returns all the items in buffer.\"\"\"\n",
        "    return self._gather_all()\n",
        "\n",
        "  def clear(self):\n",
        "    \"\"\"Resets the contents of replay buffer\"\"\"\n",
        "\n",
        "```\n",
        "\n",
        "Observe que, quando o objeto do buffer de replay é inicializado, ele exige a `data_spec` dos elementos que armazenará. Essa especificação corresponde à `TensorSpec` dos elementos da trajetória que serão adicionados ao buffer. Essa especificação é geralmente adquirida ao observar a `agent.collect_data_spec` de um agente, que define os formatos, tipos e estruturas esperados por um agente durante o treinamento (falaremos mais disso depois)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Yrxg36Ik1x"
      },
      "source": [
        "## TFUniformReplayBuffer\n",
        "\n",
        "`TFUniformReplayBuffer` é o buffer de replay geralmente usado no TF-Agents, portanto, será usado neste tutorial. No `TFUniformReplayBuffer`, o armazenamento do buffer de apoio é realizado pelas variáveis do TensorFlow, então faz parte do grafo de computação.\n",
        "\n",
        "O buffer armazena lotes de elementos e tem uma capacidade máxima de `max_length` elementos por segmento de lote. Portanto, a capacidade de buffer total é `batch_size` x `max_length` elementos. Os elementos armazenados no buffer precisam ter a mesma especificação de dados. Quando o buffer de replay é usado para a coleta de dados, a especificação é a de coleta de dados do agente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYk-bn2taXlw"
      },
      "source": [
        "### Crie o buffer:\n",
        "\n",
        "Para criar um `TFUniformReplayBuffer`, passamos o seguinte:\n",
        "\n",
        "1. a especificação dos elementos de dados que serão armazenados pelo buffer\n",
        "2. o `batch size` correspondente ao tamanho do lote do buffer\n",
        "3. o número `max_length` de elementos por segmento de lote\n",
        "\n",
        "Aqui está um exemplo de como criar um `TFUniformReplayBuffer` com especificações de dados de amostra, `batch_size` 32 e `max_length` 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj4_-77_5ExP"
      },
      "outputs": [],
      "source": [
        "data_spec =  (\n",
        "        tf.TensorSpec([3], tf.float32, 'action'),\n",
        "        (\n",
        "            tf.TensorSpec([5], tf.float32, 'lidar'),\n",
        "            tf.TensorSpec([3, 2], tf.float32, 'camera')\n",
        "        )\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "max_length = 1000\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec,\n",
        "    batch_size=batch_size,\n",
        "    max_length=max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB8rOw5ATDD2"
      },
      "source": [
        "### Escreva no buffer:\n",
        "\n",
        "Para adicionar elementos ao buffer de replay, usamos o método `add_batch(items)`, onde `items` é uma lista/tupla/ninho de tensores que representa o lote de itens a serem adicionados ao buffer. Cada elemento de `items` precisa ter uma dimensão exterior igual a `batch_size`, e as dimensões restantes precisam obedecer às especificações de dados do item (igual às especificações de dados passadas ao construtor de buffer de replay).\n",
        "\n",
        "Veja este exemplo de adição de um lote de itens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOvkp4vJhBOT"
      },
      "outputs": [],
      "source": [
        "action = tf.constant(1 * np.ones(\n",
        "    data_spec[0].shape.as_list(), dtype=np.float32))\n",
        "lidar = tf.constant(\n",
        "    2 * np.ones(data_spec[1][0].shape.as_list(), dtype=np.float32))\n",
        "camera = tf.constant(\n",
        "    3 * np.ones(data_spec[1][1].shape.as_list(), dtype=np.float32))\n",
        "  \n",
        "values = (action, (lidar, camera))\n",
        "values_batched = tf.nest.map_structure(lambda t: tf.stack([t] * batch_size),\n",
        "                                       values)\n",
        "  \n",
        "replay_buffer.add_batch(values_batched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smnVAxHghKly"
      },
      "source": [
        "### Leia o buffer:\n",
        "\n",
        "Há três maneiras de ler dados do `TFUniformReplayBuffer`:\n",
        "\n",
        "1. `get_next()` - retorna uma amostra do buffer. O tamanho do lote de amostra e o número de timesteps retornados podem ser especificados por argumentos para esse método.\n",
        "2. `as_dataset()` - retorna o buffer de replay como um `tf.data.Dataset`. É possível criar um iterador de datasets e iterar as amostras dos itens no buffer.\n",
        "3. `gather_all()` - retorna todos os itens no buffer como um Tensor de formato `[batch, time, data_spec]`\n",
        "\n",
        "Confira abaixo exemplos de como ler o buffer de replay usando cada um desses métodos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlQ1eGhohM3M"
      },
      "outputs": [],
      "source": [
        "# add more items to the buffer before reading\n",
        "for _ in range(5):\n",
        "  replay_buffer.add_batch(values_batched)\n",
        "\n",
        "# Get one sample from the replay buffer with batch size 10 and 1 timestep:\n",
        "\n",
        "sample = replay_buffer.get_next(sample_batch_size=10, num_steps=1)\n",
        "\n",
        "# Convert the replay buffer to a tf.data.Dataset and iterate through it\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=4,\n",
        "    num_steps=2)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "print(\"Iterator trajectories:\")\n",
        "trajectories = []\n",
        "for _ in range(3):\n",
        "  t, _ = next(iterator)\n",
        "  trajectories.append(t)\n",
        "  \n",
        "print(tf.nest.map_structure(lambda t: t.shape, trajectories))\n",
        "\n",
        "# Read all elements in the replay buffer:\n",
        "trajectories = replay_buffer.gather_all()\n",
        "\n",
        "print(\"Trajectories from gather all:\")\n",
        "print(tf.nest.map_structure(lambda t: t.shape, trajectories))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcS49HrNF34W"
      },
      "source": [
        "## PyUniformReplayBuffer\n",
        "\n",
        "O `PyUniformReplayBuffer` tem a mesma funcionalidade que `TFUniformReplayBuffer`, mas, em vez de variáveis tf, os dados são armazenados em arrays do numpy. Esse buffer pode ser usado para a coleta de dados fora do grafo. O armazenamento de apoio no numpy pode facilitar para alguns aplicativos manipularem dados (como a indexação para a atualização de prioridades) sem usar variáveis do TensorFlow. No entanto, essa implementação não terá o benefício das otimizações de grafo com o TensorFlow.\n",
        "\n",
        "Confira abaixo um exemplo de como instanciar um `PyUniformReplayBuffer` a partir das especificações de trajetória da política do agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4neLPpL25wI"
      },
      "outputs": [],
      "source": [
        "replay_buffer_capacity = 1000*32 # same capacity as the TFUniformReplayBuffer\n",
        "\n",
        "py_replay_buffer = py_uniform_replay_buffer.PyUniformReplayBuffer(\n",
        "    capacity=replay_buffer_capacity,\n",
        "    data_spec=tensor_spec.to_nest_array_spec(data_spec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V7DEcB8IeiQ"
      },
      "source": [
        "## Uso de buffers de replay durante o treinamento\n",
        "\n",
        "Agora que sabemos como criar, escrever itens e ler um buffer de replay, podemos usá-lo para armazenar trajetórias durante o treinamento dos nossos agentes.\n",
        "\n",
        "### Coleta de dados\n",
        "\n",
        "Primeiro, vamos ver como usar o buffer de replay durante a coleta de dados.\n",
        "\n",
        "No TF-Agents, usamos um `Driver` (confira o tutorial Driver para mais detalhes) para coletar a experiência em um ambiente. Para usar um `Driver`, especificamos um `Observer`, que é uma função para o `Driver` executar quando ele recebe uma trajetória.\n",
        "\n",
        "Portanto, para adicionar elementos de trajetória ao buffer de replay, adicionamos um observador que chama `add_batch(items)` para adicionar um lote de itens no buffer de replay.\n",
        "\n",
        "Veja abaixo um exemplo disso com o `TFUniformReplayBuffer`. Primeiro, criamos um ambiente, uma rede e um agente. Em seguida, criamos um `TFUniformReplayBuffer`. Observe que as especificações dos elementos de trajetória no buffer de replay são iguais às especificações de coleta de dados do agente. Depois, configuramos o método `add_batch` como o observer do driver que coletará os dados durante nosso treinamento:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCbTDO3Z5UCS"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    tf_env.time_step_spec().observation,\n",
        "    tf_env.action_spec(),\n",
        "    fc_layer_params=(100,))\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=tf.compat.v1.train.AdamOptimizer(0.001))\n",
        "\n",
        "replay_buffer_capacity = 1000\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    batch_size=tf_env.batch_size,\n",
        "    max_length=replay_buffer_capacity)\n",
        "\n",
        "# Add an observer that adds to the replay buffer:\n",
        "replay_observer = [replay_buffer.add_batch]\n",
        "\n",
        "collect_steps_per_iteration = 10\n",
        "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
        "  tf_env,\n",
        "  agent.collect_policy,\n",
        "  observers=replay_observer,\n",
        "  num_steps=collect_steps_per_iteration).run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huGCDbO4GAF1"
      },
      "source": [
        "### Leitura de dados para um passo de treinamento\n",
        "\n",
        "Após adicionar elementos de trajetória ao buffer de replay, podemos ler lotes de trajetórias a partir do buffer de replay para usar como dados de entrada para um passo de treinamento.\n",
        "\n",
        "Aqui está um exemplo de como treinar com as trajetórias do buffer de replay em um loop de treinamento: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg8SUyXXnSMr"
      },
      "outputs": [],
      "source": [
        "# Read the replay buffer as a Dataset,\n",
        "# read batches of 4 elements, each with 2 timesteps:\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=4,\n",
        "    num_steps=2)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "\n",
        "num_train_steps = 10\n",
        "\n",
        "for _ in range(num_train_steps):\n",
        "  trajectories, _ = next(iterator)\n",
        "  loss = agent.train(experience=trajectories)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "5_replay_buffers_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
