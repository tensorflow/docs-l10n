{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma19Ks2CTDbZ"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XljsiF6lYkuV"
      },
      "source": [
        "# Ambientes\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/2_environments_tutorial\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/agents/tutorials/2_environments_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/agents/tutorials/2_environments_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/agents/tutorials/2_environments_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h3B-YBHopJI"
      },
      "source": [
        "## Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9c6vCPGovOM"
      },
      "source": [
        "O objetivo do Aprendizado por Reforço (RL) é criar agentes que aprendem ao interagir com um ambiente. No cenário padrão de RL, o agente recebe uma observação a cada timestep e escolhe uma ação. A ação é aplicada ao ambiente, e o ambiente retorna uma recompensa e uma nova observação. O agente treina uma política para escolher ações que maximizam a soma de recompensas, também conhecida como retorno.\n",
        "\n",
        "No TF-Agents, os ambientes podem ser implementados no Python ou no TensorFlow. Os ambientes do Python são geralmente mais fácil de implementar, entender e depurar. Porém, os ambientes do TensorFlow são mais eficientes e permitem a paralelização natural. O fluxo de trabalho mais comum é implementar um ambiente no Python e usar um dos nossos wrappers para convertê-lo automaticamente para o TensorFlow.\n",
        "\n",
        "Vamos conferir os ambientes do Python primeiro. Os ambientes do TensorFlow seguem uma API bastante parecida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_16bQF0anmE"
      },
      "source": [
        "## Configuração\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qax00bg2a4Jj"
      },
      "source": [
        "Se você ainda não instalou o tf-agents ou gym, execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKU2iY_7at8Y"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents[reverb]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZAoFNwnRbKK"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-y4p9i9UURn"
      },
      "source": [
        "## Ambientes do Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPSwHONKMNv9"
      },
      "source": [
        "Os ambientes do Python têm um método `step(action) -> next_time_step` que aplica uma ação ao ambiente e retorna as seguintes informações sobre o próximo passo:\n",
        "\n",
        "1. `observation`: essa é a parte do estado do ambiente que o agente pode observar para escolher as ações dele no próximo passo.\n",
        "2. `reward`: o agente está aprendendo a maximizar a soma dessas recompensas em vários passos.\n",
        "3. `step_type`: as interações com o ambiente geralmente fazem parte de uma sequência/episódio, por exemplo, várias jogadas em um jogo de xadrez. step_type pode ser `FIRST`, `MID` ou `LAST`, para indicar se o timestep é o primeiro, intermediário ou último passo de uma sequência.\n",
        "4. `discount`: isso é um float que representa o peso atribuído à recompensa no próximo timestep em relação à recompensa no timestep atual.\n",
        "\n",
        "Elas são agrupadas em uma tupla chamada `TimeStep(step_type, reward, discount, observation)`.\n",
        "\n",
        "A interface que todos os ambientes do Python precisam implementar está em `environments/py_environment.PyEnvironment`. Os principais métodos são:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlD2Dd2vUTtg"
      },
      "outputs": [],
      "source": [
        "class PyEnvironment(object):\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Return initial_time_step.\"\"\"\n",
        "    self._current_time_step = self._reset()\n",
        "    return self._current_time_step\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Apply action and return new time_step.\"\"\"\n",
        "    if self._current_time_step is None:\n",
        "        return self.reset()\n",
        "    self._current_time_step = self._step(action)\n",
        "    return self._current_time_step\n",
        "\n",
        "  def current_time_step(self):\n",
        "    return self._current_time_step\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    \"\"\"Return time_step_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Return observation_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def action_spec(self):\n",
        "    \"\"\"Return action_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _reset(self):\n",
        "    \"\"\"Return initial_time_step.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _step(self, action):\n",
        "    \"\"\"Apply action and return new time_step.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfF8koryiGPR"
      },
      "source": [
        "Além do método `step()`, os ambientes também fornecem um método `reset()` que inicia uma nova sequência e oferece um `TimeStep` inicial. Não é necessário chamar o método `reset` explicitamente. Presumimos que os ambientes são redefinidos de maneira automática, seja quando chegam ao final de um episódio ou quando step() é chamado pela primeira vez.\n",
        "\n",
        "Observe que as subclasses não implementam `step()` ou `reset()` diretamente. Em vez disso, elas substituem os métodos `_step()` e `_reset()`. Os timesteps retornados por esses métodos serão armazenados em cache e expostos através de `current_time_step()`.\n",
        "\n",
        "Os métodos `observation_spec` e `action_spec` retornam um ninho de `(Bounded)ArraySpecs`, que descreve o nome, o formato, o tipo de dados e os intervalos das observações e das ações, respectivamente.\n",
        "\n",
        "No TF-Agents, nos referimos várias vezes aos ninhos, que são definidos como qualquer estrutura semelhante a uma árvore composta de listas, tuplas, tuplas nomeadas ou dicionários. Eles podem ser compostos arbitrariamente para manter a estrutura das observações e das ações. Isso é muito útil para ambientes mais complexos com várias observações e ações."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r63R-RbjcIRw"
      },
      "source": [
        "### Usando ambientes padrão\n",
        "\n",
        "O TF Agents tem wrappers integrados para vários ambientes padrão, como OpenAI Gym, DeepMind-control e Atari, para que sigam nossa interface `py_environment.PyEnvironment`. Esses ambientes com wrappers podem ser facilmente carregados usando nossas suítes de ambiente. Vamos carregar o ambiente CartPole no OpenAI Gym e conferir a ação e o time_step_spec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kBPE5T-nb2-"
      },
      "outputs": [],
      "source": [
        "environment = suite_gym.load('CartPole-v0')\n",
        "print('action_spec:', environment.action_spec())\n",
        "print('time_step_spec.observation:', environment.time_step_spec().observation)\n",
        "print('time_step_spec.step_type:', environment.time_step_spec().step_type)\n",
        "print('time_step_spec.discount:', environment.time_step_spec().discount)\n",
        "print('time_step_spec.reward:', environment.time_step_spec().reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWXOC863Apo_"
      },
      "source": [
        "Podemos ver que o ambiente espera ações do tipo `int64` em [0, 1] e retorna `TimeSteps` em que as observações são um vetor `float32` de comprimento 4 e o fator de desconto é um `float32` em [0.0, 1.0]. Agora, vamos tentar usar uma ação fixa `(1,)` para um episódio inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzIbOJ0-0y12"
      },
      "outputs": [],
      "source": [
        "action = np.array(1, dtype=np.int32)\n",
        "time_step = environment.reset()\n",
        "print(time_step)\n",
        "while not time_step.is_last():\n",
        "  time_step = environment.step(action)\n",
        "  print(time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAbBl4_PMtA"
      },
      "source": [
        "### Criando seu próprio ambiente do Python\n",
        "\n",
        "Para vários clientes, um caso de uso comum é aplicar um dos agentes padrão (veja agents/) no TF-Agents ao problema deles. Para isso, eles precisam enquadrar o problema como um ambiente. Confira como implementar um ambiente no Python.\n",
        "\n",
        "Digamos que queremos treinar um agente para jogar o seguinte jogo de cartas (inspirado no Blackjack):\n",
        "\n",
        "1. Para jogar, é usado um baralho infinito de cartas numeradas de 1...10.\n",
        "2. Durante cada vez, o agente pode fazer 2 coisas: receber uma nova carta aleatória ou interromper a rodada atual.\n",
        "3. O objetivo é que a soma das suas cartas chegue o mais próximo possível de 21 no final da rodada, sem passar disso.\n",
        "\n",
        "Um ambiente que representa o jogo pode parecer com isto:\n",
        "\n",
        "1. Ações: temos 2 ações. Ação 0: receber uma nova carta. Ação 1: terminar a rodada atual.\n",
        "2. Observações: a soma das cartas na rodada atual.\n",
        "3. Recompensa: o objetivo é chegar o mais perto possível de 21 sem ultrapassar disso. Portanto, podemos obter isso usando a seguinte recompensa no final da rodada: soma_das_cartas - 21 if soma_das_cartas &lt;= 21, else -21\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HD0cDykPL6I"
      },
      "outputs": [],
      "source": [
        "class CardGameEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    # Make sure episodes don't go on forever.\n",
        "    if action == 1:\n",
        "      self._episode_ended = True\n",
        "    elif action == 0:\n",
        "      new_card = np.random.randint(1, 11)\n",
        "      self._state += new_card\n",
        "    else:\n",
        "      raise ValueError('`action` should be 0 or 1.')\n",
        "\n",
        "    if self._episode_ended or self._state >= 21:\n",
        "      reward = self._state - 21 if self._state <= 21 else -21\n",
        "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
        "    else:\n",
        "      return ts.transition(\n",
        "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYEwyX7QsqeX"
      },
      "source": [
        "Vamos verificar se fizemos tudo certo definindo o ambiente acima. Ao criar seu próprio ambiente, você precisa conferir se as observações e os time_steps gerados seguem os formatos e tipos corretos conforme definido nas suas especificações. Eles são usados para gerar o grafo do TensorFlow e, portanto, podem criar problemas difíceis de depurar se estiverem errados.\n",
        "\n",
        "Para validar seu ambiente, usaremos uma política aleatória para gerar ações, e vamos iterar mais de 5 episódios para garantir que tudo esteja funcionando como esperado. Um erro será gerado se recebermos um time_step que não seguir as especificações do ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hhm-5R7spVx"
      },
      "outputs": [],
      "source": [
        "environment = CardGameEnv()\n",
        "utils.validate_py_environment(environment, episodes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_36eM7MvkNOg"
      },
      "source": [
        "Agora que sabemos que o ambiente está funcionando como esperado, vamos executar esse ambiente usando uma política fixa: peça 3 cartas e encerre a rodada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FILylafAkMEx"
      },
      "outputs": [],
      "source": [
        "get_new_card_action = np.array(0, dtype=np.int32)\n",
        "end_round_action = np.array(1, dtype=np.int32)\n",
        "\n",
        "environment = CardGameEnv()\n",
        "time_step = environment.reset()\n",
        "print(time_step)\n",
        "cumulative_reward = time_step.reward\n",
        "\n",
        "for _ in range(3):\n",
        "  time_step = environment.step(get_new_card_action)\n",
        "  print(time_step)\n",
        "  cumulative_reward += time_step.reward\n",
        "\n",
        "time_step = environment.step(end_round_action)\n",
        "print(time_step)\n",
        "cumulative_reward += time_step.reward\n",
        "print('Final Reward = ', cumulative_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vBLPN3ioyGx"
      },
      "source": [
        "### Wrappers de ambiente\n",
        "\n",
        "Um wrapper de ambiente aceita um ambiente do Python e retorna uma versão modificada do ambiente. Ambos os ambientes original e modificado são instâncias de `py_environment.PyEnvironment`, e vários wrappers podem ser ligados juntos em cadeia.\n",
        "\n",
        "Alguns wrappers comuns podem ser encontrados em `environments/wrappers.py`. Por exemplo:\n",
        "\n",
        "1. `ActionDiscretizeWrapper`: converte um espaço de ação contínuo para um espaço de ação discreto.\n",
        "2. `RunStats`: captura as estatísticas de execução do ambiente, como número de passos tomados, número de episódios concluídos etc.\n",
        "3. `TimeLimit`: termina o episódio após um número fixo de passos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8aIybRdnFfb"
      },
      "source": [
        "#### Exemplo 1: Action Discretize Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIaxJRUpvfyc"
      },
      "source": [
        "InvertedPendulum é um ambiente do PyBullet que aceita ações contínuas no intervalo `[-2, 2]`. Se quisermos treinar um agente de ação discreta como DQN nesse ambiente, precisamos discretizar (quantizar) o espaço da ação. Isso é exatamente o que `ActionDiscretizeWrapper` faz. Compare a `action_spec` antes e depois do wrapping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJxEoZ4HoyjR"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('Pendulum-v1')\n",
        "print('Action Spec:', env.action_spec())\n",
        "\n",
        "discrete_action_env = wrappers.ActionDiscretizeWrapper(env, num_actions=5)\n",
        "print('Discretized Action Spec:', discrete_action_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njFjW8bmwTWJ"
      },
      "source": [
        "O `discrete_action_env` envolvido é uma instância de `py_environment.PyEnvironment` e pode ser tratado como um ambiente Python normal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l5dwAhsP_F_"
      },
      "source": [
        "## Ambientes do TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZG39AjBkTjr"
      },
      "source": [
        "A interface para os ambientes do TF é definida em `environments/tf_environment.TFEnvironment` e se parece muito com os ambientes do Python. Os ambientes do TF diferem dos ambientes do Python de algumas maneiras:\n",
        "\n",
        "- Eles geram objetos de tensores em vez de arrays\n",
        "- Os ambientes do TF adicionam uma dimensão de lote aos tensores gerados em comparação com as especificações.\n",
        "\n",
        "A conversão dos ambientes do Python em TFEnvs permite que o TensorFlow paralelize as operações. Por exemplo, é possível definir uma `collect_experience_op` que coleta dados do ambiente e adiciona a um `replay_buffer`, e uma `train_op` que lê `replay_buffer` e treina o agente, e executá-las naturalmente em paralelo no TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKBDDZqKTxsL"
      },
      "outputs": [],
      "source": [
        "class TFEnvironment(object):\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    \"\"\"Describes the `TimeStep` tensors returned by `step()`.\"\"\"\n",
        "\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Defines the `TensorSpec` of observations provided by the environment.\"\"\"\n",
        "\n",
        "  def action_spec(self):\n",
        "    \"\"\"Describes the TensorSpecs of the action expected by `step(action)`.\"\"\"\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
        "    return self._reset()\n",
        "\n",
        "  def current_time_step(self):\n",
        "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
        "    return self._current_time_step()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\"\n",
        "    return self._step(action)\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _reset(self):\n",
        "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _current_time_step(self):\n",
        "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _step(self, action):\n",
        "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFkBIA92ThWf"
      },
      "source": [
        "O método `current_time_step()` retorna o time_step atual e inicializa o ambiente, se necessário.\n",
        "\n",
        "O método `reset()` força a redefinição do ambiente e retorna o current_step.\n",
        "\n",
        "Se a `action` não depender do `time_step` anterior, será necessária uma `tf.control_dependency` no modo `Graph`.\n",
        "\n",
        "Por enquanto, vamos ver como `TFEnvironments` são criados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6wS3AaLdVLT"
      },
      "source": [
        "### Criando seu próprio ambiente do TensorFlow\n",
        "\n",
        "Isso é mais complicado do que criar ambientes no Python, então não vamos abordar aqui neste colab. Um exemplo está disponível [aqui](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/tf_environment_test.py). O caso de uso mais comum é implementar seu ambiente no Python e adicionar nosso wrapper `TFPyEnvironment` a ele no TensorFlow (veja abaixo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Ny2lb-dU5R"
      },
      "source": [
        "### Envolvendo um ambiente do Python no TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv4-UcurZ8nb"
      },
      "source": [
        "Podemos envolver facilmente qualquer ambiente do Python em um ambiente do TensorFlow usando o wrapper `TFPyEnvironment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYerqyNfnVRL"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
        "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
        "print(\"Action Specs:\", tf_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3WFrnX9CNpC"
      },
      "source": [
        "Observe o novo tipo das especificações: `(Bounded)TensorSpec`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQPvC1ARYALj"
      },
      "source": [
        "### Exemplos de uso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov7EIrk8dKUU"
      },
      "source": [
        "#### Exemplo simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdvFqUqbdB7u"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "# reset() creates the initial time_step after resetting the environment.\n",
        "time_step = tf_env.reset()\n",
        "num_steps = 3\n",
        "transitions = []\n",
        "reward = 0\n",
        "for i in range(num_steps):\n",
        "  action = tf.constant([i % 2])\n",
        "  # applies the action and returns the new TimeStep.\n",
        "  next_time_step = tf_env.step(action)\n",
        "  transitions.append([time_step, action, next_time_step])\n",
        "  reward += next_time_step.reward\n",
        "  time_step = next_time_step\n",
        "\n",
        "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
        "print('\\n'.join(map(str, np_transitions)))\n",
        "print('Total reward:', reward.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWs48LNsdLnc"
      },
      "source": [
        "#### Episódios inteiros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t561kUXMk-KM"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "time_step = tf_env.reset()\n",
        "rewards = []\n",
        "steps = []\n",
        "num_episodes = 5\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
        "    time_step = tf_env.step(action)\n",
        "    episode_steps += 1\n",
        "    episode_reward += time_step.reward.numpy()\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps)\n",
        "  time_step = tf_env.reset()\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)\n",
        "\n",
        "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
        "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2_environments_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
