{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "**Copyright 2023 The TF-Agents Authors.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "# SAC minitaur com a API Actor-Learner\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/7_SAC_minitaur_tutorial\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/agents/tutorials/7_SAC_minitaur_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/agents/tutorials/7_SAC_minitaur_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/agents/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOUOQOrFs3zn"
      },
      "source": [
        "## Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "Este exemplo mostra como treinar um agente [Soft Actor Critic](https://arxiv.org/abs/1812.05905) no ambiente [Minitaur](https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/minitaur.py).\n",
        "\n",
        "Se você seguiu o [Colab do DQN](https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/agents/tutorials/1_dqn_tutorial.ipynb), isto parecerá bastante familiar. Algumas alterações importantes:\n",
        "\n",
        "- Mudança do agente de DQN para SAC.\n",
        "- Treinamento no Minitaur, que é um ambiente muito mais complexo do que o CartPole. O ambiente Minitaur busca treinar um robô quadrúpede para se mover para frente.\n",
        "- Uso da API Actor-Learner do TF-Agents para o aprendizado por reforço distribuído.\n",
        "\n",
        "A API é compatível tanto com a coleta de dados distribuída usando um buffer de replay de experiência e um container de variável (servidor de parâmetro) como o treinamento distribuído em vários dispositivos. A API foi criada para ser bastante simples e modular. Utilizamos o [Reverb](https://deepmind.com/research/open-source/Reverb) para o buffer de replay e o container de variável e a [API TF DistributionStrategy](https://www.tensorflow.org/guide/distributed_training) para o treinamento distribuído nas GPUs e TPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vUQms4DAY5A"
      },
      "source": [
        "Se você ainda não instalou as seguintes dependências, execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fskoLlB-AZ9j"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install matplotlib\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pybullet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNV5wyH3dyMl"
      },
      "source": [
        "Primeiro, vamos importar as diferentes ferramentas necessárias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import reverb\n",
        "import tempfile\n",
        "import PIL.Image\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.environments import suite_pybullet\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.train import actor\n",
        "from tf_agents.train import learner\n",
        "from tf_agents.train import triggers\n",
        "from tf_agents.train.utils import spec_utils\n",
        "from tf_agents.train.utils import strategy_utils\n",
        "from tf_agents.train.utils import train_utils\n",
        "\n",
        "tempdir = tempfile.gettempdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "outputs": [],
      "source": [
        "env_name = \"MinitaurBulletEnv-v0\" # @param {type:\"string\"}\n",
        "\n",
        "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
        "# 1e5 is just so this doesn't take too long (1 hr)\n",
        "num_iterations = 100000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256 # @param {type:\"integer\"}\n",
        "\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "actor_fc_layer_params = (256, 256)\n",
        "critic_joint_fc_layer_params = (256, 256)\n",
        "\n",
        "log_interval = 5000 # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
        "eval_interval = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "policy_save_interval = 5000 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Ambiente\n",
        "\n",
        "Os ambientes no RL representam a tarefa ou o problema que estamos tentando resolver. Os ambientes padrão podem ser facilmente criados no TF-Agents usando `suites`. Temos `suites` diferentes para carregar ambientes de origens como OpenAI Gym, Atari, DM Control, etc., considerando um nome de ambiente de string.\n",
        "\n",
        "Agora, vamos carregar o ambiente Minitaur a partir da suíte Pybullet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [],
      "source": [
        "env = suite_pybullet.load(env_name)\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY179d1xlmoM"
      },
      "source": [
        "Nesse ambiente, a meta para o agente é treinar uma política que controle o robô Minitaur e o faça avançar o mais rápido possível. Os episódios duram 1000 passos, e o retorno será a soma das recompensas durante o episódio.\n",
        "\n",
        "Vamos conferir as informações fornecidas pelo ambiente como uma `observation` que a política usará para gerar `actions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "outputs": [],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg5ysVTnctIm"
      },
      "source": [
        "A observação é bastante complexa. Recebemos 28 valores que representam os ângulos, as velocidades e os torques de todos os motores. Em troca, o ambiente espera 8 valores para as ações entre `[-1, 1]`. Esses são os ângulos de motor desejados.\n",
        "\n",
        "Geralmente, criamos dois ambientes: um para a coleta de dados durante o treinamento e outro para a avaliação. Os ambientes são escritos em python puro e usam arrays do numpy, que a API Actor Learner consome diretamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "outputs": [],
      "source": [
        "collect_env = suite_pybullet.load(env_name)\n",
        "eval_env = suite_pybullet.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da-z2yF66FR9"
      },
      "source": [
        "## Estratégia de distribuição\n",
        "\n",
        "Usamos a API DistributionStrategy para permitir a computação do passo de treinamento em vários dispositivos, como diversas GPUs ou TPUs que usam o paralelismo de dados. O passo de treinamento:\n",
        "\n",
        "- Recebe um lote de dados de treinamento\n",
        "- Divide em vários dispositivos\n",
        "- Computa o passo para frente\n",
        "- Agrega e computa a MÉDIA da perda\n",
        "- Computa o passo para trás e realiza uma atualização de variável gradiente\n",
        "\n",
        "Com as APIs DistributionStrategy e Learner do TF-Agents, é bastante fácil trocar entre a execução do passo de treinamento em GPUs (usando MirroredStrategy) para TPUs (usando TPUStrategy) sem mudar nada da lógica de treinamento abaixo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGREYZCaDB1h"
      },
      "source": [
        "### Ative a GPU\n",
        "\n",
        "Se você quiser tentar executar em uma GPU, primeiro precisará ativar as GPUs para o notebook.\n",
        "\n",
        "- Acesse Edit (Editar)→Notebook Settings (Configurações do notebook)\n",
        "- Selecione GPU no menu suspenso Hardware Accelerator (Acelerador de hardware)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZuvwDV66Mn1"
      },
      "source": [
        "### Escolha a estratégia\n",
        "\n",
        "Usamos `strategy_utils` para gerar uma estratégia. Nos bastidores, passar o parâmetro:\n",
        "\n",
        "- `use_gpu = False` retorna `tf.distribute.get_strategy()`, que usa a CPU\n",
        "- `use_gpu = True` retorna `tf.distribute.MirroredStrategy()`, que usa todas as GPUs visíveis ao TensorFlow em uma máquina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff5ZZRZI15ds"
      },
      "outputs": [],
      "source": [
        "use_gpu = True #@param {type:\"boolean\"}\n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMn5FTs5kHvt"
      },
      "source": [
        "Todas as variáveis e agentes precisam ser criados em `strategy.scope()`, como você verá abaixo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agente\n",
        "\n",
        "Para criar um agente SAC, primeiro precisamos criar as redes que ele treinará. O SAC é um agente actor-critic, então vamos precisar de duas redes.\n",
        "\n",
        "O critic dará estimativas de valores para `Q(s,a)`. Ou seja, ele receberá como entrada uma observação e uma ação e dará uma estimativa do desempenho dessa ação para o estado específico.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(collect_env))\n",
        "\n",
        "with strategy.scope():\n",
        "  critic_net = critic_network.CriticNetwork(\n",
        "        (observation_spec, action_spec),\n",
        "        observation_fc_layer_params=None,\n",
        "        action_fc_layer_params=None,\n",
        "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        last_kernel_initializer='glorot_uniform')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYy4AH4V7Ph4"
      },
      "source": [
        "Vamos usar esse critic para treinar uma rede `actor`, que permitirá gerar ações a partir de uma observação.\n",
        "\n",
        "A `ActorNetwork` preverá parâmetros para uma distribuição [MultivariateNormalDiag](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag) com a tangente hiperbólica achatada. Essa distribuição será usada como amostra e condicionada na observação atual sempre que for necessário gerar ações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB5Y3Oub4u7f"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=actor_fc_layer_params,\n",
        "      continuous_projection_net=(\n",
        "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Com essas redes disponíveis, agora podemos instanciar o agente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = sac_agent.SacAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        actor_network=actor_net,\n",
        "        critic_network=critic_net,\n",
        "        actor_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=actor_learning_rate),\n",
        "        critic_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=critic_learning_rate),\n",
        "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=alpha_learning_rate),\n",
        "        target_update_tau=target_update_tau,\n",
        "        target_update_period=target_update_period,\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\n",
        "        gamma=gamma,\n",
        "        reward_scale_factor=reward_scale_factor,\n",
        "        train_step_counter=train_step)\n",
        "\n",
        "  tf_agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Buffer de replay\n",
        "\n",
        "Para rastrear os dados coletados do ambiente, vamos usar o [Reverb](https://deepmind.com/research/open-source/Reverb), um sistema de replay eficiente, extensível e fácil de usar do Deepmind. Ele armazena dados de experiências coletados por Actors e consumidos pelo Learner durante o treinamento.\n",
        "\n",
        "Neste tutorial, isso é menos importante do que o `max_size`, mas, em uma configuração distribuída com treinamento e coleta assíncronos, você deve fazer testes com `rate_limiters.SampleToInsertRatio` usando um samples_per_insert entre 2 e 1000. Por exemplo:\n",
        "\n",
        "```\n",
        "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRNvAnkO7JK2"
      },
      "source": [
        "O buffer de replay é construído usando as especificações que descrevem os tensores a serem armazenados, que podem ser obtidas do agente usando `tf_agent.collect_data_spec`.\n",
        "\n",
        "Como o agente SAC precisa tanto da observação atual quanto da próxima para computar a perda, definimos `sequence_length=2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVLUxyUo7HQR"
      },
      "outputs": [],
      "source": [
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2,\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "Agora geramos um dataset do TensorFlow a partir do buffer de replay do Reverb. Vamos passar isso para o Learner criar uma amostra das experiências para o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba7bilizt_qW"
      },
      "outputs": [],
      "source": [
        "dataset = reverb_replay.as_dataset(\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
        "experience_dataset_fn = lambda: dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Políticas\n",
        "\n",
        "No TF-Agents, as políticas representam a noção padrão das políticas no RL: a partir de um `time_step`, é produzida uma ação ou distribuição sobre ações. O principal método é `policy_step = policy.step(time_step)`, em que `policy_step` é uma tupla nomeada `PolicyStep(action, state, info)`. A `policy_step.action` é a `action` que será aplicada ao ambiente, o `state` representa o estado das políticas stateful (RNN), e a `info` pode conter informações auxiliares, como probabilidades log de ações.\n",
        "\n",
        "Os agentes contêm duas políticas:\n",
        "\n",
        "- `agent.policy` — a principal política que é usada para avaliação e implantação.\n",
        "- `agent.collect_policy` — uma segunda política que é usada para coleta de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq7JE8IwFe0E"
      },
      "outputs": [],
      "source": [
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_eval_policy, use_tf_function=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_A4rZveEQzW"
      },
      "outputs": [],
      "source": [
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_collect_policy, use_tf_function=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azkJZ8oaF8uc"
      },
      "source": [
        "As políticas podem ser criadas independentemente dos agentes. Por exemplo, use `tf_agents.policies.random_tf_policy` para criar uma política que seleciona aleatoriamente uma ação para cada time_step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "outputs": [],
      "source": [
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  collect_env.time_step_spec(), collect_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1LMqw60Kuso"
      },
      "source": [
        "## Actors\n",
        "\n",
        "O actor gerencia as interações entre uma política e um ambiente.\n",
        "\n",
        "- Os componentes Actor contêm uma instância do ambiente (como `py_environment`) e uma cópia das variáveis da política.\n",
        "- Cada worker Actor executa uma sequência de passos de coleta de dados com os valores locais das variáveis da política.\n",
        "- As variáveis são atualizadas explicitamente usando a instância do cliente do container da variável no script de treinamento antes de chamar `actor.run()`.\n",
        "- A experiência observada é escrita no buffer de replay em cada passo de coleta de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjE59ct9fU7W"
      },
      "source": [
        "Conforme os Actors executam os passos de coleta de dados, eles passam trajetórias de (estado, ação, recompensa) ao observador, que as armazena em cache e escreve no sistema de replay Reverb.\n",
        "\n",
        "Estamos armazenando trajetórias para frames [(t0,t1) (t1,t2) (t2,t3), ...] porque `stride_length=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbyGmdiNfNDc"
      },
      "outputs": [],
      "source": [
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yaVVC22fOcF"
      },
      "source": [
        "Criamos um Actor com a política aleatória e coletamos experiências para semear o buffer de replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGq3SY0kKwsa"
      },
      "outputs": [],
      "source": [
        "initial_collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "initial_collect_actor.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pkg-0vZP_Ya"
      },
      "source": [
        "Instancie um Actor com a política de coleta para reunir mais experiências durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6ooXyk0FZ5j"
      },
      "outputs": [],
      "source": [
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR9CZ-jfPN2T"
      },
      "source": [
        "Crie um Actor que será usado para avaliar a política durante o treinamento. Passamos `actor.eval_metrics(num_eval_episodes)` para registrar as métricas mais tarde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHY2BT5lFhgL"
      },
      "outputs": [],
      "source": [
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=os.path.join(tempdir, 'eval'),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6eBGSYiOf83"
      },
      "source": [
        "## Learners\n",
        "\n",
        "O componente Learner contém o agente e realiza atualizações de passos de gradiente nas variáveis da política usando dados de experiências do buffer de replay. Após um ou mais passos de treinamento, o Learner pode enviar um novo conjunto de valores de variáveis para o container de variável."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi37YicSFTfF"
      },
      "outputs": [],
      "source": [
        "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
        "\n",
        "# Triggers to save the agent's policy checkpoints.\n",
        "learning_triggers = [\n",
        "    triggers.PolicySavedModelTrigger(\n",
        "        saved_model_dir,\n",
        "        tf_agent,\n",
        "        train_step,\n",
        "        interval=policy_save_interval),\n",
        "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
        "]\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  tempdir,\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  triggers=learning_triggers,\n",
        "  strategy=strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Métricas e avaliação\n",
        "\n",
        "Instanciamos o Actor de avaliação com a `actor.eval_metrics` acima, que gera as métricas mais usadas durante a avaliação da política:\n",
        "\n",
        "- Retorno médio. O retorno é a soma das recompensas obtidas ao executar uma política em um ambiente para um episódio e, geralmente, calculamos essa média em alguns episódios.\n",
        "- Duração média do episódio.\n",
        "\n",
        "Executamos o Actor para gerar essas métricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83iMSHUC71RG"
      },
      "outputs": [],
      "source": [
        "def get_eval_metrics():\n",
        "  eval_actor.run()\n",
        "  results = {}\n",
        "  for metric in eval_actor.metrics:\n",
        "    results[metric.name] = metric.result()\n",
        "  return results\n",
        "\n",
        "metrics = get_eval_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnOMvX_eZvOW"
      },
      "outputs": [],
      "source": [
        "def log_eval_metrics(step, metrics):\n",
        "  eval_results = (', ').join(\n",
        "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "  print('step = {0}: {1}'.format(step, eval_results))\n",
        "\n",
        "log_eval_metrics(0, metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWWURm_rXG-f"
      },
      "source": [
        "Confira o [módulo de métricas](https://github.com/tensorflow/agents/blob/master/tf_agents/metrics/tf_metrics.py) para outras implementações padrão de métricas diferentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Treinamento do agente\n",
        "\n",
        "O loop de treinamento envolve tanto coletar os dados do ambiente quanto otimizar as redes do agente. Ao longo do caminho, vamos avaliar ocasionalmente a política do agente para ver como está o desempenho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Training.\n",
        "  collect_actor.run()\n",
        "  loss_info = agent_learner.run(iterations=1)\n",
        "\n",
        "  # Evaluating.\n",
        "  step = agent_learner.train_step_numpy\n",
        "\n",
        "  if eval_interval and step % eval_interval == 0:\n",
        "    metrics = get_eval_metrics()\n",
        "    log_eval_metrics(step, metrics)\n",
        "    returns.append(metrics[\"AverageReturn\"])\n",
        "\n",
        "  if log_interval and step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualização\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Plots\n",
        "\n",
        "Podemos plotar o retorno médio em comparação com os passos dos globais para ver o desempenho do nosso agente. No `Minitaur`, a função de recompensa é baseada na distância que o minitaur caminha em 1000 passos e penaliza o gasto de energia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXKzyGt72HS8"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Vídeos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "É útil visualizar o desempenho de um agente ao renderizar o ambiente a cada passo. Antes de fazer isso, vamos criar uma função para incorporar vídeos neste colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "O código a seguir visualiza a política do agente por alguns episódios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSgaQN1nXT-h"
      },
      "outputs": [],
      "source": [
        "num_episodes = 3\n",
        "video_filename = 'sac_minitaur.mp4'\n",
        "with imageio.get_writer(video_filename, fps=60) as video:\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = eval_env.reset()\n",
        "    video.append_data(eval_env.render())\n",
        "    while not time_step.is_last():\n",
        "      action_step = eval_actor.policy.action(time_step)\n",
        "      time_step = eval_env.step(action_step.action)\n",
        "      video.append_data(eval_env.render())\n",
        "\n",
        "embed_mp4(video_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "7_SAC_minitaur_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
