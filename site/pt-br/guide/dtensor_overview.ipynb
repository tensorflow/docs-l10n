{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljvLya59ep5"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcQIa1uG86Wh"
      },
      "source": [
        "# Conceitos do DTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dWNQEum9AfY"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/dtensor_overview\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/dtensor_overview.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/dtensor_overview.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/dtensor_overview.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGZuakHVlVQf"
      },
      "source": [
        "## Visão geral\n",
        "\n",
        "Este Colab apresenta o DTensor, uma extensão do TensorFlow para fazer computação distribuída síncrona.\n",
        "\n",
        "O DTensor oferece um modelo global de programação que permite aos desenvolvedores criar aplicações que operem nos Tensores globalmente, gerenciando também a distribuição entre os dispositivos internamente. O DTensor distribui o programa e os tensores de acordo com as diretivas de fragmentação por meio de um procedimento chamado *expansão [Programa único, vários dados (SPMD, na sigla em inglês)](https://en.wikipedia.org/wiki/SPMD)*.\n",
        "\n",
        "Ao desacoplar a aplicação das diretivas de fragmentação, o DTensor permite a execução da mesma aplicação em um único dispositivo, vários dispositivos ou até mesmo vários clientes, sem deixar de preservar sua semântica global.\n",
        "\n",
        "Este guia apresenta conceitos do DTensor para fazer computação distribuída e como o DTensor se integra ao TensowFlow. Para ver uma demonstração do uso do DTensor em treinamento de modelos, confira o tutorial [Treinamento distribuído com o DTensor](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ZTDq7KngwA"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "O DTensor faz parte da versão 2.9.0 do TensorFlow e também está incluído nas compilações noturnas desde 09/04/2022."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKaPw8vwwZAC"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade --pre tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3pG29uZIWYO"
      },
      "source": [
        "Após a instalação, importe `tensorflow` e `tf.experimental.dtensor`. Depois, configure o TensorFlow para usar 6 CPUs virtuais.\n",
        "\n",
        "Embora este exemplo use vCPUs, o DTensor funciona da mesma forma em dispositivos com CPU, GPU ou TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q92lo0zjwej8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.experimental import dtensor\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "def configure_virtual_cpus(ncpu):\n",
        "  phy_devices = tf.config.list_physical_devices('CPU')\n",
        "  tf.config.set_logical_device_configuration(phy_devices[0], [\n",
        "        tf.config.LogicalDeviceConfiguration(),\n",
        "    ] * ncpu)\n",
        "\n",
        "configure_virtual_cpus(6)\n",
        "DEVICES = [f'CPU:{i}' for i in range(6)]\n",
        "\n",
        "tf.config.list_logical_devices('CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-lsrxUnlsCC"
      },
      "source": [
        "## Modelo do DTensor para tensores distribuídos\n",
        "\n",
        "O DTensor apresenta dois conceitos: `dtensor.Mesh` e `dtensor.Layout`, que são abstrações para modelar a fragmentação de tensores em dispositivos com relação topológica.\n",
        "\n",
        "- `Mesh` define a lista de dispositivos para computação.\n",
        "- `Layout` define como fragmentar a dimensão do Tensor em uma `Mesh` (malha)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjiHaH0ql9yo"
      },
      "source": [
        "### Mesh\n",
        "\n",
        "`Mesh` representa uma topologia cartesiana de um conjunto de dispositivos. Cada dimensão do grid cartesiano é chamada de **dimensão de malha** e referenciada com um nome. Os nomes da dimensão de malha dentro da mesma `Mesh` precisam ser únicos.\n",
        "\n",
        "Os nomes das dimensões de malha são referenciados por `Layout` para descrever o comportamento de fragmentação de um `tf.Tensor` em cada um de seus eixos. Confira mais detalhes adiante na seção sobre `Layout`.\n",
        "\n",
        "`Mesh` pode ser visto como uma matriz de dispositivos multidimensional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J6cOieEbaUw"
      },
      "source": [
        "Em uma `Mesh` unidimensional, todos os dispositivos formam uma lista em uma única dimensão de malha. O exemplo abaixo usa `dtensor.create_mesh` para criar uma malha de 6 dispositivos com CPU em uma dimensão de malha `'x'` com tamanho igual a 6 dispositivos:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_mesh_1d.png\" class=\"no-filter\" alt=\"A 1 dimensional mesh with 6 CPUs\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLH5fgdBmA58"
      },
      "outputs": [],
      "source": [
        "mesh_1d = dtensor.create_mesh([('x', 6)], devices=DEVICES)\n",
        "print(mesh_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSZwaUwnEgXB"
      },
      "source": [
        "Uma `Mesh` também pode ser multidimensional. No exemplo abaixo, 6 dispositivos com CPU formam uma malha `3x2`, em que a dimensão de malha `'x'` tem um tamanho igual a 3 dispositivos, e a dimensão de malha `'y'` tem um tamanho igual a 2 dispositivos:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_mesh_2d.png\" class=\"no-filter\" alt=\"A 2 dimensional mesh with 6 CPUs\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op6TmKUQE-sZ"
      },
      "outputs": [],
      "source": [
        "mesh_2d = dtensor.create_mesh([('x', 3), ('y', 2)], devices=DEVICES)\n",
        "print(mesh_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deAqdrDPFn2f"
      },
      "source": [
        "### Layout\n",
        "\n",
        "**`Layout`** especifica como um tensor é distribuído ou fragmentando em uma `Mesh`.\n",
        "\n",
        "Observação: para evitar confusões entre `Mesh` e `Layout`, o termo *dimensão* é sempre associado a `Mesh`, e o termo *eixo* é associado a `Tensor` e `Layout` neste guia.\n",
        "\n",
        "O posto de `Layout` sempre deve ser igual ao posto do `Tensor` no qual o `Layout` é aplicado. Para cada um dos eixos do `Tensor`, o `Layout` pode especificar uma dimensão de malha para fragmentar o tensor ou pode especificar o eixo como \"não fragmentado\". O tensor é replicado em toda dimensão de malha na qual não é fragmentado.\n",
        "\n",
        "O posto de um `Layout` e o número de dimensões de uma`Mesh` não precisam ser iguais. Os eixos `unsharded` (não fragmentados) de um `Layout` não precisam estar associados a uma dimensão de malha, e dimensões da malha `unsharded` não precisam estar associadas a um eixo do `layout`.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_components_diag.png\" class=\"no-filter\" alt=\"Diagram of dtensor components.\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px_bF1c-bQ7e"
      },
      "source": [
        "Vamos analisar alguns exemplos de `Layout` para as `Mesh`s criadas na seção anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqzCNlWAbm-c"
      },
      "source": [
        "Em uma malha unidimensional, como `[(\"x\", 6)]` (`mesh_1d` na seção anterior), `Layout([\"unsharded\", \"unsharded\"], mesh_1d)` é um layout de um tensor de posto 2 replicado em 6 dispositivos. <img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_replicated.png\" class=\"no-filter\" alt=\"A tensor replicated across a rank-1 mesh\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a3EnmZag6x1"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywRJwuLDt2Qq"
      },
      "source": [
        "Usando o mesmo tensor e a mesma mesh, o layout `Layout(['unsharded', 'x'])` deve fragmentar o segundo eixo do tensor em 6 dispositivos.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_rank1.png\" class=\"no-filter\" alt=\"A tensor sharded across a rank-1 mesh\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BgqL0jUvV5a"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgciDNmK76l9"
      },
      "source": [
        "Dada uma malha bidimensional 3x2, como `[(\"x\", 3), (\"y\", 2)]`, (`mesh_2d` da seção anterior), `Layout([\"y\", \"x\"], mesh_2d)` é um layout para um `Tensor` de posto 2 cujo primeiro eixo é fragmentado na dimensão de malha `\"y\"` e cujo segundo eixo é fragmentado na dimensão de malha `\"x\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyp_qOSyvieo"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_rank2.png\" class=\"no-filter\" alt=\"A tensorr with it's first axis sharded across mesh dimension 'y' and it's second axis sharded across mesh dimension 'x'\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8OrehEuhPbS"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout(['y', 'x'], mesh_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kyg0V3ehMNJ"
      },
      "source": [
        "Para a mesma `mesh_2d`, o layout `Layout([\"x\", dtensor.UNSHARDED], mesh_2d)` é um layout de um `Tensor` com posto 2 que é replicado em `\"y\"` e cujo primeiro eixo é fragmentado na dimensão de malha `x`.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_hybrid.png\" class=\"no-filter\" alt=\"A tensor replicated across mesh-dimension y, with it's first axis sharded across mesh dimension 'x'\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkWe6mVl7uRb"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout([\"x\", dtensor.UNSHARDED], mesh_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTalu6M-ISYb"
      },
      "source": [
        "### Aplicações com um cliente ou vários clientes\n",
        "\n",
        "O DTensor oferece suporte a aplicações com um cliente e com vários clientes. O kernel do Python do Colab é um exemplo de uma aplicação DTensor com um cliente, em que há um único processo do Python.\n",
        "\n",
        "Em uma aplicação DTensor com vários clientes, diversos processos do Python funcionam coletivamente como uma aplicação coesa. O grid cartesiano de uma `Mesh`em uma aplicação DTensor com vários clientes pode abarcar diversos dispositivos, independentemente se estiverem anexados localmente ao cliente atual ou anexados remotamente a outro cliente. O conjunto de todos os dispositivos usados por uma `Mesh` é chamado de *lista global de dispositivos*.\n",
        "\n",
        "A criação de uma `Mesh` em uma aplicação DTensor com vários clientes é uma operação coletiva, em que a *lista global de dispositivos* é idêntica para todos os clientes participantes, e a criação da `Mesh` serve como uma barreira global.\n",
        "\n",
        "Durante a criação da `Mesh`, cada cliente fornece sua *lista local de dispositivos* juntamente com a *lista global de dispositivos* esperada. O DTensor faz uma validação para verificar se as duas listas estão consistentes. Confira a documentação da API de `dtensor.create_mesh` e de `dtensor.create_distributed_mesh` para ver mais informações sobre a criação de uma malha com vários clientes e a *lista global de dispositivos*.\n",
        "\n",
        "O caso com um cliente pode ser visto como um caso especial de vários clientes, com apenas 1 cliente. Em uma aplicação com um cliente, a *lista global de dispositivos* é idêntica à *lista local de dispositivos*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_F7DWkXkB4w"
      },
      "source": [
        "## DTensor usado como tensor fragmentado\n",
        "\n",
        "Agora, vamos começar a programar com o `DTensor`. A função helper `dtensor_from_array` demonstra a criação de DTensors a partir de algo parecido com um `tf.Tensor`. A função realiza dois passos:\n",
        "\n",
        "- Replica o tensor em cada dispositivo da malha.\n",
        "- Fragmenta a cópia de acordo com o layout solicitado em seus argumentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6aws-b8dN9L"
      },
      "outputs": [],
      "source": [
        "def dtensor_from_array(arr, layout, shape=None, dtype=None):\n",
        "  \"\"\"Convert a DTensor from something that looks like an array or Tensor.\n",
        "\n",
        "  This function is convenient for quick doodling DTensors from a known,\n",
        "  unsharded data object in a single-client environment. This is not the\n",
        "  most efficient way of creating a DTensor, but it will do for this\n",
        "  tutorial.\n",
        "  \"\"\"\n",
        "  if shape is not None or dtype is not None:\n",
        "    arr = tf.constant(arr, shape=shape, dtype=dtype)\n",
        "\n",
        "  # replicate the input to the mesh\n",
        "  a = dtensor.copy_to_mesh(arr,\n",
        "          layout=dtensor.Layout.replicated(layout.mesh, rank=layout.rank))\n",
        "  # shard the copy to the desirable layout\n",
        "  return dtensor.relayout(a, layout=layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3o6IysrlGMu"
      },
      "source": [
        "### Anatomia de um DTensor\n",
        "\n",
        "O DTensor é um objeto `tf.Tensor`, mas ampliado com a anotação `Layout` que define seu comportamento de fragmentação. Um DTensor é composto por:\n",
        "\n",
        "- Metadados globais do tensor, incluindo o formato global e o tipo de dados (dtype) do tensor.\n",
        "- Um `Layout`, que define a `Mesh` à qual o `Tensor` pertence, além de como o `Tensor` é fragmentado na `Mesh`.\n",
        "- Uma lista de **tensores componentes**, um item por dispositivo local na `Mesh`.\n",
        "\n",
        "Usando `dtensor_from_array`, você pode criar seu primeiro DTensor, `my_first_dtensor`, e avaliar o conteúdo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQu_nScGUvYH"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
        "layout = dtensor.Layout([dtensor.UNSHARDED], mesh)\n",
        "\n",
        "my_first_dtensor = dtensor_from_array([0, 1], layout)\n",
        "\n",
        "# Examine the dtensor content\n",
        "print(my_first_dtensor)\n",
        "print(\"global shape:\", my_first_dtensor.shape)\n",
        "print(\"dtype:\", my_first_dtensor.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8LQy1nqmvFy"
      },
      "source": [
        "#### Layout e `fetch_layout`\n",
        "\n",
        "O layout de um DTensor não é um atributo comum de `tf.Tensor`. Em vez disso, o DTensor conta com uma função, `dtensor.fetch_layout`, para acessar o layout de um DTensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCSFyaAjmzGu"
      },
      "outputs": [],
      "source": [
        "print(dtensor.fetch_layout(my_first_dtensor))\n",
        "assert layout == dtensor.fetch_layout(my_first_dtensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7i3l2lmatm"
      },
      "source": [
        "#### Tensores componentes, `pack` e `unpack`\n",
        "\n",
        "O DTensor é composto por uma lista de **tensores componentes**. O tensor componente de um dispositivo na `Mesh` é o objeto `Tensor` que representa a parte do DTensor global armazenada neste dispositivo.\n",
        "\n",
        "Um DTensor pode ser dividido em tensores componentes usando `dtensor.unpack`. Você pode usar `dtensor.unpack` para avaliar os componentes do DTensor e confirmar se eles estão em todos os dispositivos da `Mesh`.\n",
        "\n",
        "Observe que as posições dos tensores componentes na visão global podem se sobrepor. Por exemplo, no caso de um layout totalmente replicado, todos os componentes são réplicas idênticas do tensor global."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGbjqVAOnXMk"
      },
      "outputs": [],
      "source": [
        "for component_tensor in dtensor.unpack(my_first_dtensor):\n",
        "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tqIQM52k788"
      },
      "source": [
        "Conforme mostrado, `my_first_dtensor` é um tensor de `[0, 1]` replicado em todos os 6 dispositivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6By3k-CGn3yv"
      },
      "source": [
        "A operação inversa de `dtensor.unpack` é `dtensor.pack`. Os tensores componentes podem ser empacotados de volta em um DTensor.\n",
        "\n",
        "Os componentes precisam ter o mesmo posto e tipo de dados (dtype), que serão o posto e dtype do DTensor retornado. Entretanto, não há um requisito estrito para o posicionamento de dispositivos dos tensores componentes como entrada de `dtensor.unpack`: a função copiará automaticamente os tensores componentes para os dispositivos correspondentes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lT-6qQwxOgf"
      },
      "outputs": [],
      "source": [
        "packed_dtensor = dtensor.pack(\n",
        "    [[0, 1], [0, 1], [0, 1],\n",
        "     [0, 1], [0, 1], [0, 1]],\n",
        "     layout=layout\n",
        ")\n",
        "print(packed_dtensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvS3autrpK2U"
      },
      "source": [
        "### Fragmentação de um DTensor em uma malha\n",
        "\n",
        "Até o momento, você trabalhou com o `my_first_dtensor`, um DTensor de posto 1 totalmente replicado em uma `Mesh` unidimensional.\n",
        "\n",
        "Agora, crie e avalie DTensors que sejam fragmentados em uma `Mesh` bidimensional. O próximo exemplo faz isso com uma `Mesh` 3x2 em 6 dispositivos com CPU, em que o tamanho da dimensão de malha `'x'` é 3 dispositivos, e o tamanho da dimensão de malha `'y'` é 2 dispositivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWb9Ae0VJ-Rc"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndSeQSFWKQk9"
      },
      "source": [
        "#### Tensor de posto 2 totalmente fragmentado em uma malha bidimensional\n",
        "\n",
        "Crie um DTensor 3x2 de posto 2, fragmentando seu primeiro eixo na dimensão de malha `'x'` e seu segundo eixo na dimensão de malha `'y'`.\n",
        "\n",
        "- Como o formato do tensor é igual à dimensão de malha em todos os eixos fragmentados, cada dispositivo recebe um único elemento do DTensor.\n",
        "- O posto do tensor componente é sempre o mesmo que o posto do formato global. O DTensor adota essa convenção como uma forma simplificada de preservar as informações para identificar a relação entre um tensor componente e o DTensor global."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax_ZHouJp1MX"
      },
      "outputs": [],
      "source": [
        "fully_sharded_dtensor = dtensor_from_array(\n",
        "    tf.reshape(tf.range(6), (3, 2)),\n",
        "    layout=dtensor.Layout([\"x\", \"y\"], mesh))\n",
        "\n",
        "for raw_component in dtensor.unpack(fully_sharded_dtensor):\n",
        "  print(\"Device:\", raw_component.device, \",\", raw_component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhsLC-NgrC2p"
      },
      "source": [
        "#### Tensor de posto 2 totalmente replicado em uma malha bidimensional\n",
        "\n",
        "Para fins comparativos, crie um DTensor 3x2 de posto 2 totalmente replicado na mesma malha bidimensional.\n",
        "\n",
        "- Como o DTensor é totalmente replicado, cada dispositivo recebe uma réplica completa do DTensor 3x2.\n",
        "- O posto dos tensores componentes é o mesmo que o posto do formato global. Esse fato é trivial, pois, neste caso, o formato dos tensores componentes é igual ao formato global, de toda forma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmyC6H6Ec90P"
      },
      "outputs": [],
      "source": [
        "fully_replicated_dtensor = dtensor_from_array(\n",
        "    tf.reshape(tf.range(6), (3, 2)),\n",
        "    layout=dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh))\n",
        "# Or, layout=tensor.Layout.fully_replicated(mesh, rank=2)\n",
        "\n",
        "for component_tensor in dtensor.unpack(fully_replicated_dtensor):\n",
        "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWoyv_oHMzk1"
      },
      "source": [
        "#### Tensor de posto 2 híbrido em uma malha bidimensional\n",
        "\n",
        "E quanto a um caso que não seja nem totalmente fragmentado nem totalmente replicado?\n",
        "\n",
        "O DTensor permite que um `Layout` seja híbrido, fragmentado em alguns eixos, mas replicado em outros.\n",
        "\n",
        "Por exemplo, você pode fragmentar o mesmo DTensor 3x2 de posto 2 da seguinte forma:\n",
        "\n",
        "- O primeiro eixo é fragmentado na dimensão de malha `'x'`.\n",
        "- O segundo eixo é replicado na dimensão de malha `'y'`.\n",
        "\n",
        "Para fazer esse esquema de fragmentação, você precisa apenas mudar a configuração de fragmentação do segundo eixo de `'y'` para `dtensor.UNSHARDED` a fim de indicar sua intenção de replicar no segundo eixo. O objeto layout ficará assim: `Layout(['x', dtensor.UNSHARDED], mesh)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DygnbkQ1Lu42"
      },
      "outputs": [],
      "source": [
        "hybrid_sharded_dtensor = dtensor_from_array(\n",
        "    tf.reshape(tf.range(6), (3, 2)),\n",
        "    layout=dtensor.Layout(['x', dtensor.UNSHARDED], mesh))\n",
        "\n",
        "for component_tensor in dtensor.unpack(hybrid_sharded_dtensor):\n",
        "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7FtZ9kQRZgE"
      },
      "source": [
        "Você pode avaliar os tensores componentes do DTensor criado e verificar se estão fragmentados de acordo com seu esquema. Pode ajudar ilustrar a situação com um gráfico:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_hybrid_mesh.png\" class=\"no-filter\" width=\"75%\" alt=\"A 3x2 hybrid mesh with 6 CPUs\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auAkA38XjL-q"
      },
      "source": [
        "#### Tensor.numpy() e DTensor fragmentado\n",
        "\n",
        "É importante saber que fazer uma chamada ao método `.numpy()` em um DTensor aciona um erro. O motivo para isso é proteger contra coleta indesejada de dados em diversos dispositivos computacionais para o dispositivo host com CPU que dá suporte ao retorno da array numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNdwmnL0jAXS"
      },
      "outputs": [],
      "source": [
        "print(fully_replicated_dtensor.numpy())\n",
        "\n",
        "try:\n",
        "  fully_sharded_dtensor.numpy()\n",
        "except tf.errors.UnimplementedError:\n",
        "  print(\"got an error as expected for fully_sharded_dtensor\")\n",
        "\n",
        "try:\n",
        "  hybrid_sharded_dtensor.numpy()\n",
        "except tf.errors.UnimplementedError:\n",
        "  print(\"got an error as expected for hybrid_sharded_dtensor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WcMkiagPF_6"
      },
      "source": [
        "## API do TensorFlow no DTensor\n",
        "\n",
        "O DTensor tem o objetivo de ser uma substituição rápida, sem necessidade de fazer alterações, para o tensor em seu programa. A API do Python do TensorFlow que consome `tf.Tensor`, com as funções da biblioteca de operações Ops, `tf.function`, `tf.GradientTape`, também funciona com o DTensor.\n",
        "\n",
        "Para conseguir isso, para cada [Grafo do TensorFlow](https://www.tensorflow.org/guide/intro_to_graphs), o DTensor gera e executa um grafo [SPMD](https://en.wikipedia.org/wiki/SPMD) equivalente em um procedimento chamado *expansão SPMD*. Veja alguns passos essenciais da expansão SPMD do DTensor:\n",
        "\n",
        "- Propagar o `Layout` de fragmentação do DTensor no grafo do TensorFlow.\n",
        "- Reescrever as Ops do TensorFlow no DTensor global com Ops do TensorFlow nos tensores componentes, inserindo Ops coletivas e de comunicação quando necessário.\n",
        "- Reduzir as Ops do TensorFlow neutras de backend para Ops do TensorFlow específicas de backend.\n",
        "\n",
        "O resultado final é que o **DTensor é uma substituição rápida, sem necessidade de fazer alterações, para o Tensor**.\n",
        "\n",
        "Observação: o DTensor ainda é uma API experimental e, portanto, você fará experimentos e desafiará os limites de seu modelo de programação.\n",
        "\n",
        "Existem duas maneiras de acionar a execução do DTensor:\n",
        "\n",
        "- O DTensor como operandos de uma função do Python, por exemplo, `tf.matmul(a, b)` passará pelo DTensor se `a`, `b` ou ambos forem DTensors.\n",
        "- Solicitando que o resultado de uma função do Python seja um DTensor, por exemplo, `dtensor.call_with_layout(tf.ones, layout, shape=(3, 2))` passará pelo DTensor, pois solicitamos que a saída de tf.ones seja fragmentada de acordo com um `layout`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urKzmqAoPssT"
      },
      "source": [
        "### DTensor como operandos\n",
        "\n",
        "Diversas funções da API do TensorFlow recebem `tf.Tensor` como operandos e retornam `tf.Tensor` como resultados. Para essas funções, você pode expressar a intenção de executar uma função pelo DTensor passando o DTensor como operandos. Esta seção usa `tf.matmul(a, b)` para exemplificar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LO8ZT7iWVga"
      },
      "source": [
        "#### Entrada e saída totalmente replicadas\n",
        "\n",
        "Neste caso, os DTensors são totalmente replicados. Em cada um dos dispositivos da `Mesh`,\n",
        "\n",
        "- o tensor componente do operando `a` é `[[1, 2, 3], [4, 5, 6]]` (2x3)\n",
        "- o tensor componente do operando `b` é `[[6, 5], [4, 3], [2, 1]]` (3x2)\n",
        "- a computação consiste de um único `MatMul` de `(2x3, 3x2) -> 2x2`,\n",
        "- o tensor componente do resultado `c` é `[[20, 14], [56,41]]` (2x2)\n",
        "\n",
        "O número total de operações mul de ponto flutuante é: `6 device * 4 result * 3 mul = 72` (6 dispositivos * 4 resultados * 3 muls = 72)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiZf2J9JNd2D"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
        "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)\n",
        "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=layout)\n",
        "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=layout)\n",
        "\n",
        "c = tf.matmul(a, b) # runs 6 identical matmuls in parallel on 6 devices\n",
        "\n",
        "# `c` is a DTensor replicated on all devices (same as `a` and `b`)\n",
        "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)\n",
        "print(\"components:\")\n",
        "for component_tensor in dtensor.unpack(c):\n",
        "  print(component_tensor.device, component_tensor.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXtR9qgKWgWV"
      },
      "source": [
        "#### Fragmentação de operandos no eixo contraído\n",
        "\n",
        "É possível reduzir a quantidade de computações por dispositivo fragmentando os operandos `a` e `b`. Um esquema de fragmentação muito usado para `tf.matmul` é fragmentar os operandos no eixo da contração, ou seja, fragmentar `a` no segundo eixo e `b` no primeiro eixo.\n",
        "\n",
        "O produto global de matrizes fragmentadas por esse esquema pode ser realizado de forma eficiente por matmuls locais executados simultaneamente, seguidos por uma redução coletiva para agregar os resultados locais. Essa também é a [forma canônica](https://github.com/open-mpi/ompi/blob/ee87ec391f48512d3718fc7c8b13596403a09056/docs/man-openmpi/man3/MPI_Reduce.3.rst?plain=1#L265) de implementar o produto escalar distribuído de matrizes.\n",
        "\n",
        "O número total de operações mul de ponto flutuante é `6 devices * 4 result * 1 = 24`, um fator de redução igual a 3 comparado ao caso totalmente replicado acima (72). O fator de 3 deve-se à fragmentação na dimensão de malha `x` com um tamanho igual a `3` dispositivos.\n",
        "\n",
        "A redução do número de operações executadas sequencialmente é o principal mecanismo que o paralelismo de modelo síncrono usa para acelerar o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyVAUvMePbms"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "a_layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh)\n",
        "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=a_layout)\n",
        "b_layout = dtensor.Layout(['x', dtensor.UNSHARDED], mesh)\n",
        "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=b_layout)\n",
        "\n",
        "c = tf.matmul(a, b)\n",
        "# `c` is a DTensor replicated on all devices (same as `a` and `b`)\n",
        "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhD8yYgJiCEh"
      },
      "source": [
        "#### Fragmentação adicional\n",
        "\n",
        "Você pode fazer uma fragmentação adicional nas entradas, e elas são transportadas de forma adequada para os resultados. Por exemplo, você pode aplicar uma fragmentação adicional do operando `a` em seu primeiro eixo para a dimensão de malha `'y'` . A fragmentação adicional poderá ser transportada para o primeiro eixo do resultado `c`.\n",
        "\n",
        "O número total de operações mul de ponto flutuante é `6 devices * 2 result * 1 = 12`, um fator de redução igual a 2 comparado ao caso acima (24). O fator de 2 deve-se à fragmentação na dimensão de malha `y` com um tamanho igual a `2` dispositivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PYqe0neiOpR"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "\n",
        "a_layout = dtensor.Layout(['y', 'x'], mesh)\n",
        "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=a_layout)\n",
        "b_layout = dtensor.Layout(['x', dtensor.UNSHARDED], mesh)\n",
        "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=b_layout)\n",
        "\n",
        "c = tf.matmul(a, b)\n",
        "# The sharding of `a` on the first axis is carried to `c'\n",
        "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)\n",
        "print(\"components:\")\n",
        "for component_tensor in dtensor.unpack(c):\n",
        "  print(component_tensor.device, component_tensor.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-1NazCVmLWZ"
      },
      "source": [
        "### DTensor como saída\n",
        "\n",
        "E quanto às funções do Python que não recebem operandos, mas retornam como resultado um Tensor que pode ser fragmentado? Veja alguns exemplos dessas funções:\n",
        "\n",
        "- `tf.ones`, `tf.zeros`, `tf.random.stateless_normal`\n",
        "\n",
        "Para essas funções do Python, o DTensor conta com `dtensor.call_with_layout` que faz uma execução adiantada (eager) do Python com o DTensor e garante que o Tensor retornado seja um DTensor com o `Layout` solicitado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0jo_8NPtJiO"
      },
      "outputs": [],
      "source": [
        "help(dtensor.call_with_layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-YdLvfytM7g"
      },
      "source": [
        "Geralmente, a função do Python executada de maneira adiantada (eager) contém somente uma única Op do TensorFlow não trivial.\n",
        "\n",
        "Para usar uma função do Python que gere diversas Ops do TensorFlow com `dtensor.call_with_layout`, a função deverá ser convertiva em uma `tf.function`. Fazer uma chamada a `tf.function` é uma única Op do TensorFlow. Quando a função `tf.function` é chamada, o DTensor pode fazer a propagação do layout ao analisar o grafo computacional de `tf.function`, antes que qualquer tensor intermediário seja materializado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLrksgFjqRLS"
      },
      "source": [
        "#### APIs que geram uma única Op do TensorFlow\n",
        "\n",
        "Se uma função gerar uma única Op do TensorFlow, você poderá aplicar `dtensor.call_with_layout` diretamente à função."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1CuKYSFtFeM"
      },
      "outputs": [],
      "source": [
        "help(tf.ones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m_EAwy-ozOh"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "ones = dtensor.call_with_layout(tf.ones, dtensor.Layout(['x', 'y'], mesh), shape=(6, 4))\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx-7Xo8Cpb8S"
      },
      "source": [
        "#### APIs que geram diversas Ops do TensorFlow\n",
        "\n",
        "Se a API gerar diversas Ops do TensorFlow, converta a função em uma única Op usando `tf.function`. Por exemplo, `tf.random.stateleess_normal`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8BQSTRFtCih"
      },
      "outputs": [],
      "source": [
        "help(tf.random.stateless_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvP81eYopSPm"
      },
      "outputs": [],
      "source": [
        "ones = dtensor.call_with_layout(\n",
        "    tf.function(tf.random.stateless_normal),\n",
        "    dtensor.Layout(['x', 'y'], mesh),\n",
        "    shape=(6, 4),\n",
        "    seed=(1, 1))\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKoojp9ZyWzW"
      },
      "source": [
        "É permitido encapsular uma função do Python que gera uma única Op do TensorFlow com `tf.function`. A única ressalva é arcar com o custo e a complexidade associados ao criar uma `tf.function` a partir de uma função do Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbAtKrSkpOaq"
      },
      "outputs": [],
      "source": [
        "ones = dtensor.call_with_layout(\n",
        "    tf.function(tf.ones),\n",
        "    dtensor.Layout(['x', 'y'], mesh),\n",
        "    shape=(6, 4))\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-m1816JP3CE"
      },
      "source": [
        "### De `tf.Variable` para `dtensor.DVariable`\n",
        "\n",
        "No Tensorflow, `tf.Variable` é o armazenador de um valor `Tensor` mutável. No DTensor, a semântica variável correspondente é fornecida por `dtensor.DVariable`.\n",
        "\n",
        "O motivo de um novo tipo `DVariable` ter sido lançado no DTensor é que as DVariables têm um requisito adicional: o layout não pode ter seu valor inicial alterado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awRPuR26P0Sc"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
        "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)\n",
        "\n",
        "v = dtensor.DVariable(\n",
        "    initial_value=dtensor.call_with_layout(\n",
        "        tf.function(tf.random.stateless_normal),\n",
        "        layout=layout,\n",
        "        shape=tf.TensorShape([64, 32]),\n",
        "        seed=[1, 1],\n",
        "        dtype=tf.float32))\n",
        "\n",
        "print(v.handle)\n",
        "assert layout == dtensor.fetch_layout(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb9jn473prC_"
      },
      "source": [
        "Exceto pelo requisito de não variar o `layout`, uma `DVariable` se comporta da mesma forma que uma `tf.Variable`. Por exemplo, você pode adicionar uma DVariable a um DTensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adxFw9wJpqQQ"
      },
      "outputs": [],
      "source": [
        "a = dtensor.call_with_layout(tf.ones, layout=layout, shape=(64, 32))\n",
        "b = v + a # add DVariable and DTensor\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxBdNHWSu-kV"
      },
      "source": [
        "Além disso, você pode atribuir um DTensor a uma DVariable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYwfiyw5P94U"
      },
      "outputs": [],
      "source": [
        "v.assign(a) # assign a DTensor to a DVariable\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fvSk_VUvGnj"
      },
      "source": [
        "Se você tentar alterar o layout de uma `DVariable` por meio da atribuição de um DTensor com um layout incompatível, será gerado um erro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pckUugYP_r-"
      },
      "outputs": [],
      "source": [
        "# variable's layout is immutable.\n",
        "another_mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "b = dtensor.call_with_layout(tf.ones,\n",
        "                     layout=dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], another_mesh),\n",
        "                     shape=(64, 32))\n",
        "try:\n",
        "  v.assign(b)\n",
        "except:\n",
        "  print(\"exception raised\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LadIcwRvR6f"
      },
      "source": [
        "## Quais são os próximos passos?\n",
        "\n",
        "Neste Colab, você aprendeu sobre o DTensor, uma extensão do TensorFlow para fazer computação distribuída. Para ver esses conceitos em um tutorial, confira [Treinamento distribuído com o DTensor](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "dtensor_overview.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
