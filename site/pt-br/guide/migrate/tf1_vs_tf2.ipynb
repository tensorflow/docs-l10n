{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D70XgUYdLwI6"
      },
      "source": [
        "# TensorFlow 1.x vs TensorFlow 2 – Comportamentos e APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/migrate/tf1_vs_tf2\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/migrate/tf1_vs_tf2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/migrate/tf1_vs_tf2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/migrate/tf1_vs_tf2.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akxmN3SQsEcb"
      },
      "source": [
        "Nos bastidores, o TensorFlow 2 segue um paradigma de programação fundamentalmente diferente do TF1.x.\n",
        "\n",
        "Este guia descreve as diferenças fundamentais entre TF1.x e TF2 em termos de comportamentos e APIs, e como tudo isso se relaciona com sua jornada de migração."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzy2mT87mwth"
      },
      "source": [
        "## Resumo de alto nível das principais mudanças\n",
        "\n",
        "Fundamentalmente, TF1.x e TF2 apresentam um conjunto diferente de comportamentos em tempo de execução quanto ao modo de execução (eager no TF2), variáveis, fluxo de controle, formatos de tensor e comparações de igualdade entre tensores. Para ser compatível com TF2, seu código precisa ser compatível com o conjunto completo de comportamentos do TF2. Durante a migração, você poderá ativar ou desativar a maioria desses comportamentos individualmente através das APIs `tf.compat.v1.enable_*` ou `tf.compat.v1.disable_*`. A única exceção é a remoção de coleções, que é um efeito colateral da ativação/desativação da execução eager.\n",
        "\n",
        "Em alto nível, o TensorFlow 2:\n",
        "\n",
        "- Remove [APIs redundantes](https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md).\n",
        "- Deixa as APIs mais consistentes - por exemplo, [RNNs unificadas](https://github.com/tensorflow/community/blob/master/rfcs/20180920-unify-rnn-interface.md) e [Otimizadores unificados](https://github.com/tensorflow/community/blob/master/rfcs/20181016-optimizer-unification.md).\n",
        "- Prefere [funções em vez de sessões](https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md) e integra-se melhor ao runtime do Python com [execução eager](https://www.tensorflow.org/guide/eager) ativada por padrão junto com `tf.function` que fornece dependências de controle automático para grafos e compilação.\n",
        "- Descontinua as [coleções](https://github.com/tensorflow/community/blob/master/rfcs/20180905-deprecate-collections.md) de grafos globais.\n",
        "- Altera a semântica de concorrência de variáveis ​​usando [`ResourceVariables` em vez de `ReferenceVariables`](https://github.com/tensorflow/community/blob/master/rfcs/20180817-variables-20.md).\n",
        "- Suporta [fluxo de controle](https://github.com/tensorflow/community/blob/master/rfcs/20180507-cond-v2.md) diferenciável e [baseado em funções](https://github.com/tensorflow/community/blob/master/rfcs/20180821-differentiable-functional-while.md) (fluxo de controle v2).\n",
        "- Simplifica a API TensorShape para conter valores `int` em vez de objetos `tf.compat.v1.Dimension`.\n",
        "- Atualiza a mecânica de igualdade de tensores. Em TF1.x, o operador `==` em tensores e variáveis ​​verificava a igualdade entre referências do objeto. No TF2 ele verifica a igualdade de valores. Além disso, não é maios possível fazer hash de tensores/variáveis, mas você pode fazer hash em referências de objetos para eles via `var.ref()` se precisar usá-los em conjuntos ou como chaves em um `dict`.\n",
        "\n",
        "As seções abaixo fornecem mais contexto sobre as diferenças entre TF1.x e TF2. Para saber mais sobre o processo de design por trás do TF2, leia as [RFCs](https://github.com/tensorflow/community/pulls?utf8=%E2%9C%93&q=is%3Apr) e os [documentos de design](https://github.com/tensorflow/community/tree/master/rfcs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlCiIgEE2OhY"
      },
      "source": [
        "## Limpeza de APIs\n",
        "\n",
        "Muitas APIs ou [desapareceram ou foram movidas](https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md) no TF2. Algumas das principais mudanças incluem a remoção de `tf.app`, `tf.flags` e `tf.logging` em favor do agora do código aberto [absl-py](https://github.com/abseil/abseil-py), realojando projetos que viviam em `tf.contrib` e limpando o namespace principal `tf.*` ao mover funções menos usadas para subpacotes como `tf.math`. Algumas APIs foram substituídas por equivalentes no TF2 - `tf.summary`, `tf.keras.metrics` e `tf.keras.optimizers`.\n",
        "\n",
        "### `tf.compat.v1`: endpoints de API legados e de compatibilidade\n",
        "\n",
        "Os símbolos no contexto dos namespaces `tf.compat` e `tf.compat.v1` não são considerados APIs do TF2. Esses namespaces expõem uma mistura de símbolos de compatibilidade, bem como endpoints de API herdados do TF 1.x. O objetivo é ajudar na migração do TF1.x para o TF2. No entanto, como nenhuma dessas APIs `compat.v1` são APIs TF2 idiomáticas, elas não devem ser usadas para escrever código TF2 novo.\n",
        "\n",
        "Símbolos `tf.compat.v1` individuais podem ser compatíveis com TF2 porque continuam a funcionar mesmo com comportamentos do TF2 habilitados (como `tf.compat.v1.losses.mean_squared_error`), enquanto outros são incompatíveis com TF2 (como `tf.compat.v1.metrics.accuracy`). Muitos símbolos `compat.v1` (embora não todos) contêm informações de migração dedicadas em sua documentação que explicam seu grau de compatibilidade com comportamentos do TF2, bem como como migrá-los para APIs do TF2.\n",
        "\n",
        "O [script de upgrade do TF2](https://www.tensorflow.org/guide/migrate/upgrade) pode mapear muitos símbolos da API `compat.v1` para APIs do TF2 equivalentes no caso em que eles sejam aliases ou tenham os mesmos argumentos, mas com uma ordem diferente. Você também pode usar o script de upgrade para renomear automaticamente APIs TF1.x.\n",
        "\n",
        "### Falsos cognatos\n",
        "\n",
        "Há um conjunto de símbolos que são falsos cognatos encontrados no namespace `tf` do TF2 (não em `compat.v1`) e que na verdade ignoram os comportamentos do TF2 nos bastidores e/ou não são totalmente compatíveis com o conjunto completo de comportamentos do TF2. Como tal, é provável que essas APIs se comportem mal com o código TF2, potencialmente de forma silenciosa.\n",
        "\n",
        "- `tf.estimator.*`: os estimadores criam e usam grafos e sessões nos bastidores. Como tal, estes não devem ser considerados compatíveis com TF2. Se o seu código executa estimadores, ele não está usando comportamentos do TF2.\n",
        "- `keras.Model.model_to_estimator(...)`: isto cria um estimador nos bastidores, que, como mencionado acima, não é compatível com TF2.\n",
        "- `tf.Graph().as_default()`: isto insere os comportamentos do grafo TF1.x e não segue os comportamentos `tf.function` padrão compatíveis com o TF2. O código que entra em grafos como este geralmente os executará via Sessions e não deve ser considerado compatível com TF2.\n",
        "- `tf.feature_column.*` As APIs da coluna de características geralmente dependem da criação de variáveis `tf.compat.v1.get_variable` no estilo TF1 e assumem que as variáveis ​​criadas serão acessadas via coleções globais. Como o TF2 não oferece suporte a coleções, as APIs podem não funcionar corretamente ao executá-las com os comportamentos do TF2 habilitados.\n",
        "\n",
        "### Outras alterações na API\n",
        "\n",
        "- O TF2 traz melhorias significativas nos algoritmos de posicionamento de dispositivos, o que torna desnecessário o uso de `tf.colocate_with`. Se removê-lo causar degradação no desempenho, por favor, [registre um bug](https://github.com/tensorflow/tensorflow/issues).\n",
        "\n",
        "- Substitua todo o uso de `tf.v1.ConfigProto` por funções equivalentes de `tf.config`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxEU79Rd83Yz"
      },
      "source": [
        "## Execução antecipada (eager)\n",
        "\n",
        "TF1.x exigia que os usuários construíssem manualmente uma [árvore de sintaxe abstrata](https://en.wikipedia.org/wiki/Abstract_syntax_tree) (o grafo), fazendo chamadas de API `tf.*` e, em seguida, compilassem manualmente a árvore de sintaxe abstrata, passando um conjunto de tensores de saída e tensores de entrada para uma chamada `session.run`. O TF2 é executado em modo eager (como o Python normalmente é) e isto faz com que grafos e as sessões pareçam detalhes de implementação.\n",
        "\n",
        "Um subproduto notável da execution eager é que `tf.control_dependencies` não é mais necessário, já que todas as linhas de código são executadas sequencialmente (dentro de uma `tf.function`, os códigos com efeitos colaterais são executados na ordem em que foram escritos)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH3YizX-9S7g"
      },
      "source": [
        "## O fim das variáveis globais\n",
        "\n",
        "O TensorFlow 1.X dependia muito de namespaces implicitamente globais. Quando você chamava `tf.Variable`, ela era colocada no grafo padrão e permanecia lá, mesmo se você perdesse a variável do Python que apontava para ela. Depois, você poderia recuperar essa `tf.Variable`, mas somente se soubesse o nome com que foi criada originalmente. Isto era difícil de fazer se você não tivesse controle sobre a criação da variável. Como resultado, houve uma proliferação de mecanismos para tentar ajudar os usuários a encontrar suas variáveis novamente e para os frameworks encontrarem variáveis criadas por usuários: escopos de variáveis, coleções globais, métodos auxiliares, como `tf.get_global_step` e `tf.global_variables_initializer`, otimizadores computando gradientes implicitamente sobre todas as variáveis treináveis e assim por diante. O TensorFlow 2.0 elimina todos esses mecanismos ([Variables 2.0 RFC](https://github.com/tensorflow/community/pull/11)), dando lugar ao mecanismo padrão: manter o controle de suas variáveis! Se você perder o controle de uma `tf.Variable`, ela é recolhida pelo coletor de lixo.\n",
        "\n",
        "O requisito de rastrear variáveis ​​cria algum trabalho extra, mas com ferramentas como os [shims de modelagem](./model_mapping.ipynb) e comportamentos como [coleções implícitas de variáveis ​​orientadas a objetos em `tf.Module` s e `tf.keras.layers.Layer`](https://www.tensorflow.org/guide/intro_to_modules), o trabalho é minimizado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXwBgAjJ98J2"
      },
      "source": [
        "## Funções, não sessões\n",
        "\n",
        "Uma chamada `session.run` é quase como uma chamada de função: você especifica as entradas e a função a ser chamada e recebe de volta um conjunto de saídas. No TF2, você pode decorar uma função Python usando `tf.function` para marcá-la para compilação JIT para que o TensorFlow a execute como um único grafo ([Functions 2.0 RFC](https://github.com/tensorflow/community/pull/20)). Este mecanismo permite que o TF2 obtenha todos os benefícios do modo grafo:\n",
        "\n",
        "- Desempenho: A função pode ser otimizada (remoção de nós, fusão de kernel, etc.)\n",
        "- Portabilidade: a função pode ser exportada/reimportada ([SavedModel 2.0 RFC](https://github.com/tensorflow/community/pull/34)), permitindo reutilizar e compartilhar funções modulares do TensorFlow.\n",
        "\n",
        "```python\n",
        "# TF1.x\n",
        "outputs = session.run(f(placeholder), feed_dict={placeholder: input})\n",
        "# TF2\n",
        "outputs = f(input)\n",
        "```\n",
        "\n",
        "Com o poder de intercalar livremente o código Python e TensorFlow, você pode aproveitar a expressividade do Python. No entanto, o TensorFlow portátil é executado em contextos sem um interpretador Python, como dispositivos móveis, C++ e JavaScript. Para ajudar a evitar reescrever seu código ao adicionar `tf.function`, use [AutoGraph](https://tensorflow.org/guide/function) para converter um subconjunto de construtos do Python em seus equivalentes do TensorFlow:\n",
        "\n",
        "- `for`/`while` -&gt; `tf.while_loop` (`break` e `continue` são suportados)\n",
        "- `if` -&gt; `tf.cond`\n",
        "- `for _ in dataset` -&gt; `dataset.reduce`\n",
        "\n",
        "O AutoGraph oferece suporte a aninhamentos arbitrários de fluxo de controle, o que torna possível implementar de maneira concisa e com bom desempenho muitos programas complexos de aprendizado de máquina, como modelos de sequência, aprendizado por reforço, loops de treinamento personalizados e muito mais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj3gaj4tpi7O"
      },
      "source": [
        "## Adaptação mudanças de comportamento do TF 2.x\n",
        "\n",
        "Sua migração para o TF2 só estará concluída depois que você migrar para o conjunto completo de comportamentos do TF2. O conjunto completo de comportamentos pode ser ativado ou desativado via `tf.compat.v1.enable_v2_behaviors` e `tf.compat.v1.disable_v2_behaviors`. As seções abaixo discutem detalhadamente cada mudança importante de comportamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M0zEtR9p0XD"
      },
      "source": [
        "### Usando `tf.function`\n",
        "\n",
        "As maiores mudanças em seus programas durante a migração provavelmente virão da mudança fundamental de paradigma do modelo de programação de grafos e sessões para execução eager e `tf.function`. Consulte os [guias de migração do TF2](https://tensorflow.org/guide/migrate) para saber mais sobre como migrar de APIs incompatíveis com execução eager e `tf.function` para APIs compatíveis.\n",
        "\n",
        "Observação: durante a migração, você poderá optar por ativar e desativar diretamente a execução eager com `tf.compat.v1.enable_eager_execution` e `tf.compat.v1.disable_eager_execution`, mas isto só pode ser feito uma vez durante a vida útil do seu programa.\n",
        "\n",
        "Abaixo estão alguns padrões de programação comuns não vinculados a nenhuma API que podem causar problemas ao trocar objetos `tf.Graph` por `tf.compat.v1.Session` para execução eager com objetos `tf.function`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgwEtwwN2PWy"
      },
      "source": [
        "#### Padrão 1: manipulação de objetos Python e variáveis ​​destinadas a serem criadas apenas uma vez e executadas múltiplas vezes\n",
        "\n",
        "<a id=\"pattern-1\"></a>\n",
        "\n",
        "Em programas TF1.x que dependem de grafos e sessões, a expectativa geralmente é que toda a lógica Python no seu programa seja executada apenas uma vez. No entanto, com execução eager e `tf.function` é justo esperar que sua lógica Python seja executada pelo menos uma vez, mas possivelmente mais vezes (várias vezes em modo eager ou várias vezes em diferentes rastreamentos de `tf.function`). Às vezes, `tf.function` irá rastrear duas vezes na mesma entrada, causando comportamentos inesperados (veja os Exemplos 1 e 2). Consulte o <a>guia</a> <code>tf.function</code> para obter mais detalhes.\n",
        "\n",
        "Observação: Esse padrão geralmente faz com que seu código se comporte mal silenciosamente ao executar de forma eager sem objetos `tf.function`, mas geralmente gera um `InaccessibleTensorError` ou um `ValueError` ao tentar empacotar o código problemático dentro de um `tf.function`. Para descobrir e depurar esse problema, é recomendável empacotar seu código com `tf.function` desde o início e usar [pdb](https://docs.python.org/3/library/pdb.html) ou depuração interativa para identificar a origem do `InaccessibleTensorError`.\n",
        "\n",
        "**Exemplo 1: criação de variáveis**\n",
        "\n",
        "Considere o exemplo abaixo, onde a função cria uma variável quando chamada:\n",
        "\n",
        "```python\n",
        "def f():\n",
        "  v = tf.Variable(1.0)\n",
        "  return v\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    res = f()\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    sess.run(res)\n",
        "```\n",
        "\n",
        "Não é permitido, porém, empacotar ingenuamente a função acima que contém a criação de variáveis ​​com `tf.function`, pois `tf.function` suporta apenas [criações de variáveis ​​​​singleton na primeira chamada](https://www.tensorflow.org/guide/function#creating_tfvariables). Para garantir isso, quando tf.function detectar a criação de variável na primeira chamada, ele tentará rastrear novamente e gerará um erro se acontecer a criação da variável no segundo rastreamento.\n",
        "\n",
        "```python\n",
        "@tf.function\n",
        "def f():\n",
        "  print(\"trace\") # This will print twice because the python body is run twice\n",
        "  v = tf.Variable(1.0)\n",
        "  return v\n",
        "\n",
        "try:\n",
        "  f()\n",
        "except ValueError as e:\n",
        "  print(e)\n",
        "```\n",
        "\n",
        "Uma solução alternativa é armazenar em cache e reutilizar a variável depois de ela ser criada na primeira chamada.\n",
        "\n",
        "```python\n",
        "class Model(tf.Module):\n",
        "  def __init__(self):\n",
        "    self.v = None\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self):\n",
        "    print(\"trace\") # This will print twice because the python body is run twice\n",
        "    if self.v is None:\n",
        "      self.v = tf.Variable(0)\n",
        "    return self.v\n",
        "\n",
        "m = Model()\n",
        "m()\n",
        "```\n",
        "\n",
        "**Exemplo 2: tensores fora do escopo devido ao rastreamento repetido de `tf.function`**\n",
        "\n",
        "Conforme demonstrado no Exemplo 1, `tf.function` irá rastrear novamente quando detectar a criação de variável na primeira chamada. Isto pode causar uma confusão extra, pois os dois rastreamentos criarão dois grafos. Quando o grafo criado no segundo rastreamento tenta acessar um Tensor do grafo gerado durante o primeiro rastreamento, o Tensorflow lançará um erro reclamando que o Tensor está fora do escopo. Para demonstrar o cenário, o código abaixo cria um dataset na primeira chamada `tf.function`. Isto iria executar como esperado.\n",
        "\n",
        "```python\n",
        "class Model(tf.Module):\n",
        "  def __init__(self):\n",
        "    self.dataset = None\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self):\n",
        "    print(\"trace\") # This will print once: only traced once\n",
        "    if self.dataset is None:\n",
        "      self.dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
        "    it = iter(self.dataset)\n",
        "    return next(it)\n",
        "\n",
        "m = Model()\n",
        "m()\n",
        "```\n",
        "\n",
        "No entanto, se também tentarmos criar uma variável na primeira chamada `tf.function`, o código produzirá um erro reclamando que o dataset está fora do escopo. Isto ocorre porque o dataset está no primeiro grafo, enquanto que o segundo grafo também tenta acessá-lo.\n",
        "\n",
        "```python\n",
        "class Model(tf.Module):\n",
        "  def __init__(self):\n",
        "    self.v = None\n",
        "    self.dataset = None\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self):\n",
        "    print(\"trace\") # This will print twice because the python body is run twice\n",
        "    if self.v is None:\n",
        "      self.v = tf.Variable(0)\n",
        "    if self.dataset is None:\n",
        "      self.dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
        "    it = iter(self.dataset)\n",
        "    return [self.v, next(it)]\n",
        "\n",
        "m = Model()\n",
        "try:\n",
        "  m()\n",
        "except TypeError as e:\n",
        "  print(e) # <tf.Tensor ...> is out of scope and cannot be used here.\n",
        "```\n",
        "\n",
        "A solução mais simples é garantir que a criação da variável e a criação do dataset estejam fora da chamada `tf.function`. Por exemplo:\n",
        "\n",
        "```python\n",
        "class Model(tf.Module):\n",
        "  def __init__(self):\n",
        "    self.v = None\n",
        "    self.dataset = None\n",
        "\n",
        "  def initialize(self):\n",
        "    if self.dataset is None:\n",
        "      self.dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
        "    if self.v is None:\n",
        "      self.v = tf.Variable(0)\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self):\n",
        "    it = iter(self.dataset)\n",
        "    return [self.v, next(it)]\n",
        "\n",
        "m = Model()\n",
        "m.initialize()\n",
        "m()\n",
        "```\n",
        "\n",
        "No entanto, às vezes não há como evitar a criação de variáveis ​​em `tf.function` (como variáveis ​​de slot em alguns [otimizadores TF keras](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#slots)). Ainda assim, podemos simplesmente mover a criação do dataset para fora da chamada `tf.function`. A razão pela qual podemos confiar nisso é porque `tf.function` receberá o dataset como uma entrada implícita e ambos os grafos poderão acessá-lo corretamente.\n",
        "\n",
        "```python\n",
        "class Model(tf.Module):\n",
        "  def __init__(self):\n",
        "    self.v = None\n",
        "    self.dataset = None\n",
        "\n",
        "  def initialize(self):\n",
        "    if self.dataset is None:\n",
        "      self.dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self):\n",
        "    if self.v is None:\n",
        "      self.v = tf.Variable(0)\n",
        "    it = iter(self.dataset)\n",
        "    return [self.v, next(it)]\n",
        "\n",
        "m = Model()\n",
        "m.initialize()\n",
        "m()\n",
        "```\n",
        "\n",
        "**Exemplo 3: recriações inesperadas de objetos Tensorflow devido ao uso de dict**\n",
        "\n",
        "`tf.function` tem suporte muito ruim para efeitos colaterais do python, como anexar a uma lista ou verificar/adicionar a um dicionário. Mais detalhes estão em [\"Melhor desempenho com tf.function\"](https://www.tensorflow.org/guide/function#executing_python_side_effects). No exemplo abaixo, o código usa dicionários para armazenar datasets e iteradores em cache. Para a mesma chave, cada chamada ao modelo retornará o mesmo iterador do dataset.\n",
        "\n",
        "```python\n",
        "class Model(tf.Module):\n",
        "  def __init__(self):\n",
        "    self.datasets = {}\n",
        "    self.iterators = {}\n",
        "\n",
        "  def __call__(self, key):\n",
        "    if key not in self.datasets:\n",
        "      self.datasets[key] = tf.compat.v1.data.Dataset.from_tensor_slices([1, 2, 3])\n",
        "      self.iterators[key] = self.datasets[key].make_initializable_iterator()\n",
        "    return self.iterators[key]\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    m = Model()\n",
        "    it = m('a')\n",
        "    sess.run(it.initializer)\n",
        "    for _ in range(3):\n",
        "      print(sess.run(it.get_next())) # prints 1, 2, 3\n",
        "```\n",
        "\n",
        "No entanto, o padrão acima não funcionará como esperado em `tf.function`. Durante o rastreamento, `tf.function` irá ignorar o efeito colateral do Python da adição aos dicionários. Em vez disso, ele apenas lembra a criação de um novo dataset e iterador. Como resultado, cada chamada ao modelo sempre retornará um novo iterador. Este problema é difícil de perceber, a menos que os resultados numéricos ou o desempenho sejam suficientemente significativos. Portanto, recomendamos que os usuários pensem cuidadosamente no código antes de empacotar `tf.function` ingenuamente em código python.\n",
        "\n",
        "```python\n",
        "class Model(tf.Module):\n",
        "  def __init__(self):\n",
        "    self.datasets = {}\n",
        "    self.iterators = {}\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, key):\n",
        "    if key not in self.datasets:\n",
        "      self.datasets[key] = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
        "      self.iterators[key] = iter(self.datasets[key])\n",
        "    return self.iterators[key]\n",
        "\n",
        "m = Model()\n",
        "for _ in range(3):\n",
        "  print(next(m('a'))) # prints 1, 1, 1\n",
        "```\n",
        "\n",
        "Podemos usar [`tf.init_scope`](https://www.tensorflow.org/api_docs/python/tf/init_scope) para elevar o dataset e a criação do iterador para fora do grafo, e assim alcançar o comportamento esperado:\n",
        "\n",
        "```python\n",
        "class Model(tf.Module):\n",
        "  def __init__(self):\n",
        "    self.datasets = {}\n",
        "    self.iterators = {}\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, key):\n",
        "    if key not in self.datasets:\n",
        "      # Lifts ops out of function-building graphs\n",
        "      with tf.init_scope():\n",
        "        self.datasets[key] = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
        "        self.iterators[key] = iter(self.datasets[key])\n",
        "    return self.iterators[key]\n",
        "\n",
        "m = Model()\n",
        "for _ in range(3):\n",
        "  print(next(m('a'))) # prints 1, 2, 3\n",
        "```\n",
        "\n",
        "A regra geral é evitar depender dos efeitos colaterais do Python em sua lógica e usá-los apenas para depurar seus rastreamentos.\n",
        "\n",
        "**Exemplo 4: manipulando uma lista Python global**\n",
        "\n",
        "O código TF1.x a seguir usa uma lista global de perdas que é usada apenas para manter a lista de perdas geradas pelo passo de treinamento atual. Observe que a lógica Python que anexa perdas à lista será chamada apenas uma vez, independentemente de quantos passos de treinamento forem executados para a sessão.\n",
        "\n",
        "```python\n",
        "all_losses = []\n",
        "\n",
        "class Model():\n",
        "  def __call__(...):\n",
        "    ...\n",
        "    all_losses.append(regularization_loss)\n",
        "    all_losses.append(label_loss_a)\n",
        "    all_losses.append(label_loss_b)\n",
        "    ...\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  ...\n",
        "  # initialize all objects\n",
        "  model = Model()\n",
        "  optimizer = ...\n",
        "  ...\n",
        "  # train step\n",
        "  model(...)\n",
        "  total_loss = tf.reduce_sum(all_losses)\n",
        "  optimizer.minimize(total_loss)\n",
        "  ...\n",
        "...\n",
        "sess = tf.compat.v1.Session(graph=g)\n",
        "sess.run(...)\n",
        "```\n",
        "\n",
        "No entanto, se esta lógica Python for ingenuamente mapeada para TF2 com execução eager, a lista global de perdas terá novos valores anexados a ela em cada passo de treinamento. Isto significa que o código do passo de treinamento, que anteriormente esperava que a lista contivesse apenas as perdas do passo de treinamento atual, agora vê na verdade a lista de perdas de todos os passos de treinamento executados até o momento. Esta é uma mudança de comportamento não intencional, e a lista precisará ser apagada no início de cada passo ou tornada local no passo de treinamento.\n",
        "\n",
        "```python\n",
        "all_losses = []\n",
        "\n",
        "class Model():\n",
        "  def __call__(...):\n",
        "    ...\n",
        "    all_losses.append(regularization_loss)\n",
        "    all_losses.append(label_loss_a)\n",
        "    all_losses.append(label_loss_b)\n",
        "    ...\n",
        "\n",
        "# initialize all objects\n",
        "model = Model()\n",
        "optimizer = ...\n",
        "\n",
        "def train_step(...)\n",
        "  ...\n",
        "  model(...)\n",
        "  total_loss = tf.reduce_sum(all_losses) # global list is never cleared,\n",
        "  # Accidentally accumulates sum loss across all training steps\n",
        "  optimizer.minimize(total_loss)\n",
        "  ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYnjPo-tmTI"
      },
      "source": [
        "#### Padrão 2: um tensor simbólico destinado a ser recomputado a cada passo no TF1.x é acidentalmente armazenado em cache com o valor inicial ao mudar para o modo eager.\n",
        "\n",
        "<a id=\"pattern-2\"></a>\n",
        "\n",
        "Este padrão geralmente faz com que seu código se comporte mal silenciosamente ao executar de forma eager fora de objetos tf.function, mas gera um `InaccessibleTensorError` se o cache do valor inicial ocorrer dentro de um `tf.function`. No entanto, esteja ciente de que, para evitar o [Padrão 1](#pattern-1) acima, você muitas vezes estruturará inadvertidamente seu código de forma que esse cache de valor inicial aconteça *fora* de qualquer `tf.function` que possa gerar um erro. Portanto, tome cuidado adicional se souber que seu programa pode ser suscetível a esse padrão.\n",
        "\n",
        "A solução geral para esse padrão é reestruturar o código ou usar callables do Python, se necessário, para garantir que o valor seja recalculado a cada vez, em vez de ser armazenado em cache acidentalmente.\n",
        "\n",
        "**Exemplo 1: Taxa de aprendizagem/hiperparâmetro/etc. cronogramas que dependem de um passo global**\n",
        "\n",
        "No trecho de código a seguir, a expectativa é que toda vez que a sessão for executada, o valor `global_step` mais recente seja lido e uma nova taxa de aprendizado seja computada.\n",
        "\n",
        "```python\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  ...\n",
        "  global_step = tf.Variable(0)\n",
        "  learning_rate = 1.0 / global_step\n",
        "  opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
        "  ...\n",
        "  global_step.assign_add(1)\n",
        "...\n",
        "sess = tf.compat.v1.Session(graph=g)\n",
        "sess.run(...)\n",
        "```\n",
        "\n",
        "No entanto, ao tentar mudar para eager, tenha cuidado para não acabar com a taxa de aprendizagem sendo computada apenas uma vez e depois reutilizada, em vez de seguir o cronograma pretendido:\n",
        "\n",
        "```python\n",
        "global_step = tf.Variable(0)\n",
        "learning_rate = 1.0 / global_step # Wrong! Only computed once!\n",
        "opt = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "def train_step(...):\n",
        "  ...\n",
        "  opt.apply_gradients(...)\n",
        "  global_step.assign_add(1)\n",
        "  ...\n",
        "```\n",
        "\n",
        "Como este exemplo específico é um padrão comum e os otimizadores devem ser inicializados apenas uma vez, e não em cada passo de treinamento, os otimizadores TF2 suportam cronogramas `tf.keras.optimizers.schedules.LearningRateSchedule` ou chamadas Python como argumentos para a taxa de aprendizagem e outros hiperparâmetros.\n",
        "\n",
        "**Exemplo 2: inicializações simbólicas de números aleatórios passados como atributos de objetos e reutilizados via ponteiros são acidentalmente armazenados em cache ao mudar para o modo eager**\n",
        "\n",
        "Considere o seguinte módulo `NoiseAdder`:\n",
        "\n",
        "```python\n",
        "class NoiseAdder(tf.Module):\n",
        "  def __init__(shape, mean):\n",
        "    self.noise_distribution = tf.random.normal(shape=shape, mean=mean)\n",
        "    self.trainable_scale = tf.Variable(1.0, trainable=True)\n",
        "  \n",
        "  def add_noise(input):\n",
        "    return (self.noise_distribution + input) * self.trainable_scale\n",
        "```\n",
        "\n",
        "Usá-lo da seguinte maneira em TF1.x calculará um novo tensor de ruído aleatório toda vez que a sessão for executada:\n",
        "\n",
        "```python\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  ...\n",
        "  # initialize all variable-containing objects\n",
        "  noise_adder = NoiseAdder(shape, mean)\n",
        "  ...\n",
        "  # computation pass\n",
        "  x_with_noise = noise_adder.add_noise(x)\n",
        "  ...\n",
        "...\n",
        "sess = tf.compat.v1.Session(graph=g)\n",
        "sess.run(...)\n",
        "```\n",
        "\n",
        "No entanto, no TF2, inicializar o `noise_adder` no início fará com que o `noise_distribution` seja computado apenas uma vez e fique congelado para todos os passos de treinamento:\n",
        "\n",
        "```python\n",
        "...\n",
        "# initialize all variable-containing objects\n",
        "noise_adder = NoiseAdder(shape, mean) # Freezes `self.noise_distribution`!\n",
        "...\n",
        "# computation pass\n",
        "x_with_noise = noise_adder.add_noise(x)\n",
        "...\n",
        "```\n",
        "\n",
        "Para corrigir isso, refatore `NoiseAdder` para chamar `tf.random.normal` toda vez que um novo tensor aleatório for necessário, em vez de se referir ao mesmo objeto tensor todas as vezes.\n",
        "\n",
        "```python\n",
        "class NoiseAdder(tf.Module):\n",
        "  def __init__(shape, mean):\n",
        "    self.noise_distribution = lambda: tf.random.normal(shape=shape, mean=mean)\n",
        "    self.trainable_scale = tf.Variable(1.0, trainable=True)\n",
        "  \n",
        "  def add_noise(input):\n",
        "    return (self.noise_distribution() + input) * self.trainable_scale\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2PXkSflCaCl"
      },
      "source": [
        "#### Padrão 3: o código TF1.x depende diretamente de tensores e os procura por nome\n",
        "\n",
        "<a id=\"pattern-3\"></a>\n",
        "\n",
        "É comum que os testes de código TF1.x dependam da verificação de quais tensores ou operações estão presentes em um grafo. Em alguns casos raros, o código de modelagem também dependerá dessas pesquisas por nome.\n",
        "\n",
        "Os nomes dos tensores não são gerados durante a execução eager fora de `tf.function`, portanto, todos os usos de `tf.Tensor.name` devem acontecer dentro de um `tf.function`. Tenha em mente que os nomes reais gerados provavelmente serão diferentes entre TF1.x e TF2, mesmo dentro do mesmo `tf.function`, e as garantias da API não garantem a estabilidade dos nomes gerados em diferentes versões do TF.\n",
        "\n",
        "Nota: nomes de variáveis ​​ainda são gerados mesmo fora de `tf.function`, mas também não é garantido que seus nomes correspondam entre TF1.x e TF2, exceto ao seguir as seções relevantes do [guia de mapeamento de modelos](./model_mapping.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NB3bycl5Lde"
      },
      "source": [
        "#### Padrão 4: a sessão TF1.x executa seletivamente apenas parte do grafo gerado\n",
        "\n",
        "<a id=\"pattern-4\"></a>\n",
        "\n",
        "No TF1.x, você pode construir um grafo e, em seguida, optar por executar seletivamente apenas um subconjunto dele com uma sessão, escolhendo um conjunto de entradas e saídas que não exigem a execução de cada operação no grafo.\n",
        "\n",
        "Por exemplo, você pode ter um gerador e um discriminador dentro de um único grafo e usar chamadas `tf.compat.v1.Session.run` separadas para alternar entre treinar apenas o discriminador ou apenas treinar o gerador.\n",
        "\n",
        "No TF2, devido às dependências de controle automático em `tf.function` e à execução eager, não há pruning seletivo dos rastreamentos `tf.function`. Um grafo completo contendo todas as atualizações de variáveis ​​seria executado mesmo se, por exemplo, apenas a saída do discriminador ou do gerador fosse gerada como saída do `tf.function`.\n",
        "\n",
        "Portanto, você precisaria ou usar múltiplos `tf.function` contendo diferentes partes do programa ou um argumento condicional para o `tf.function` que você poderia usar para decidir executar apenas as coisas que você realmente quer que sejam executadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnNaUmROp5fV"
      },
      "source": [
        "### Remoção de coleções\n",
        "\n",
        "Quando a execução eager está ativada, as APIs `compat.v1` relacionadas à coleção de grafos (incluindo aquelas que leem ou gravam em coleções ocultas, como `tf.compat.v1.trainable_variables`) não estarão mais disponíveis. Algumas podem gerar `ValueError`, enquanto outras podem retornar silenciosamente listas vazias.\n",
        "\n",
        "O uso mais comum de coleções no TF1.x é manter inicializadores, o passo global, pesos, perdas de regularização, perdas de saída do modelo e atualizações de variáveis ​​que precisam ser executadas, como nas camadas `BatchNormalization`.\n",
        "\n",
        "Veja como lidar com cada um desses usos comuns:\n",
        "\n",
        "1. Inicializadores - ignore. A inicialização manual de variáveis ​​não é necessária com a execução eager ativada.\n",
        "2. Passo global – consulte a documentação de `tf.compat.v1.train.get_or_create_global_step` para obter instruções de migração.\n",
        "3. Pesos - mapeie seus modelos para objetos `tf.Module`/ `tf.keras.layers.Layer`/ `tf.keras.Model` seguindo as orientações no [guia de mapeamento de modelos](./model_mapping.ipynb) e, em seguida, use seus respectivos mecanismos de rastreamento de pesos, como `tf.module.trainable_variables`.\n",
        "4. Perdas de regularização – mapeie seus modelos para objetos `tf.Module`/ `tf.keras.layers.Layer`/ `tf.keras.Model` seguindo as orientações no [guia de mapeamento de modelos](./model_mapping.ipynb) e, em seguida, use `tf.keras.losses`. Alternativamente, você também pode rastrear manualmente suas perdas de regularização.\n",
        "5. Perdas de saída do modelo - use mecanismos de gerenciamento de perdas `tf.keras.Model` ou rastreie separadamente suas perdas sem usar coleções.\n",
        "6. Atualizações de peso – ignore esta coleção. Execução eager e `tf.function` (com dependências de autógrafo e controle automático) significam que todas as atualizações de variáveis ​​serão executadas automaticamente. Portanto, você não terá que executar explicitamente todas as atualizações de peso no final, mas observe que isto significa que as atualizações de peso podem acontecer num momento diferente do que aconteciam no seu código TF1.x, dependendo de como você estava usando as dependências de controle.\n",
        "7. Resumos – Veja o [guia da API sobre migração de resumos](https://www.tensorflow.org/tensorboard/migrate).\n",
        "\n",
        "O uso de coleções mais complexas (como o uso de coleções personalizadas) pode exigir que você refatore seu código para manter seus próprios armazenamentos globais ou para que ele não dependa de nenhum armazenamento global."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J_ckZstp8y1"
      },
      "source": [
        "### `ResourceVariables` em vez de `ReferenceVariables`\n",
        "\n",
        "`ResourceVariables` têm garantias de consistência de leitura e gravação mais fortes do que `ReferenceVariables`. Isso leva a uma semântica mais previsível e mais fácil de entender se você observará ou não o resultado de uma gravação anterior ao usar suas variáveis. É extremamente improvável que essa alteração faça com que o código existente gere erros ou falhe silenciosamente.\n",
        "\n",
        "No entanto, é ***possível, embora improvável,*** que essas garantias de consistência mais fortes aumentem o uso de memória do seu programa específico. Registre um [issue](https://github.com/tensorflow/tensorflow/issues) se você achar que esse é o caso. Além disso, se você tiver testes de unidade que dependam de comparações exatas de strings com os nomes dos operadores em um grafo correspondente às leituras de variáveis, esteja ciente de que a ativação de variáveis ​​de recursos poderá alterar ligeiramente os nomes desses operadores.\n",
        "\n",
        "Para isolar o impacto dessa mudança de comportamento em seu código, se a execução eager estiver ativada, você pode usar `tf.compat.v1.disable_resource_variables()` e `tf.compat.v1.enable_resource_variables()` para desativar ou ativar globalmente essa mudança de comportamento. As `ResourceVariables` sempre serão usadas ​​se a execução eager estiver ativada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTU-4P1vux0e"
      },
      "source": [
        "### Controle de fluxo v2\n",
        "\n",
        "No TF1.x, as ops de controle de fluxo são ops como `tf.cond` e `tf.while_loop`, operações de baixo nível inline, como `Switch`, `Merge`, etc. O TF2 fornece operações de controle de fluxo com funcionalidade aprimorada que são implementadas com rastreamentos `tf.function` separados para cada ramo e que suportam diferenciação de ordem superior.\n",
        "\n",
        "Para isolar o impacto dessa mudança de comportamento em seu código, se a execução eager estiver desativada, você pode usar `tf.compat.v1.disable_control_flow_v2()` e `tf.compat.v1.enable_control_flow_v2()` para desativar ou ativar globalmente essa mudança de comportamento. No entanto, você só poderá desativar o fluxo de controle v2 se a execução eager também estiver desativada. Se estiver ativada, o fluxo de controle v2 sempre será utilizado.\n",
        "\n",
        "Esta mudança de comportamento pode alterar drasticamente a estrutura dos programas TF gerados que usam fluxo de controle, pois eles conterão vários rastreamento de função aninhados em vez de um grafo plano. Portanto, qualquer código que seja altamente dependente da semântica exata dos rastreamentos produzidos pode exigir algumas modificações, que incluem:\n",
        "\n",
        "- Código que depende de nomes de operadores e tensores\n",
        "- Código que possui referências a tensores criados dentro de um ramo de fluxo de controle do TensorFlow chamadas de fora desse ramo. É provável que isto produza um `InaccessibleTensorError`\n",
        "\n",
        "É esperado que essa mudança de comportamento tenha uma mudança de desempenho neutro ou positivo, mas se você encontrar um problema em que o fluxo de controle v2 tem um desempenho pior do que o fluxo de controle TF1.x, registre um [issue](https://github.com/tensorflow/tensorflow/issues) descrevendo as etapas de reprodução do problema. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7VwgVCGqE9S"
      },
      "source": [
        "## Mudanças de comportamento na API TensorShape\n",
        "\n",
        "A classe `TensorShape` foi simplificada para conter valores `int`, em vez de objetos `tf.compat.v1.Dimension`. Portanto, não há necessidade de chamar `.value` para obter um `int`.\n",
        "\n",
        "Objetos `tf.compat.v1.Dimension` individuais ainda podem ser acessados ​​em `tf.TensorShape.dims`.\n",
        "\n",
        "Para isolar o impacto dessa mudança de comportamento em seu código, você pode usar `tf.compat.v1.disable_v2_tensorshape()` e `tf.compat.v1.enable_v2_tensorshape()` para desativar ou ativar globalmente essa mudança de comportamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x36cWcmM8Eu1"
      },
      "source": [
        "A seguir demonstramos as diferenças entre TF1.x e TF2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF4un9UpVTRA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbpD-kHOZR4A"
      },
      "outputs": [],
      "source": [
        "# Create a shape and choose an index\n",
        "i = 0\n",
        "shape = tf.TensorShape([16, None, 256])\n",
        "shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFck03neNy0"
      },
      "source": [
        "Se você tinha isto no TF1.x:\n",
        "\n",
        "```python\n",
        "value = shape[i].value\n",
        "```\n",
        "\n",
        "Então faça isto no TF2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuR73QGEeNdH"
      },
      "outputs": [],
      "source": [
        "value = shape[i]\n",
        "value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPWPNKRiZmkd"
      },
      "source": [
        "Se você tinha isto no TF1.x:\n",
        "\n",
        "```python\n",
        "for dim in shape:\n",
        "    value = dim.value\n",
        "    print(value)\n",
        "```\n",
        "\n",
        "Então faça isto no TF2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s0vuuprJfc"
      },
      "outputs": [],
      "source": [
        "for value in shape:\n",
        "  print(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpRgngu3Zw-A"
      },
      "source": [
        "Se você tinha isto no TF1.x (ou usou qualquer outro método de dimensão):\n",
        "\n",
        "```python\n",
        "dim = shape[i]\n",
        "dim.assert_is_compatible_with(other_dim)\n",
        "```\n",
        "\n",
        "Então faça isto no TF2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpViGEcUZDGX"
      },
      "outputs": [],
      "source": [
        "other_dim = 16\n",
        "Dimension = tf.compat.v1.Dimension\n",
        "\n",
        "if shape.rank is None:\n",
        "  dim = Dimension(None)\n",
        "else:\n",
        "  dim = shape.dims[i]\n",
        "dim.is_compatible_with(other_dim) # or any other dimension method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaiGe36dOdZ_"
      },
      "outputs": [],
      "source": [
        "shape = tf.TensorShape(None)\n",
        "\n",
        "if shape:\n",
        "  dim = shape.dims[i]\n",
        "  dim.is_compatible_with(other_dim) # or any other dimension method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kLLY0I3PI-l"
      },
      "source": [
        "O valor booleano de um `tf.TensorShape` é `True` se a classificação for conhecida, `False` caso contrário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ow1ndKpOnJd"
      },
      "outputs": [],
      "source": [
        "print(bool(tf.TensorShape([])))      # Scalar\n",
        "print(bool(tf.TensorShape([0])))     # 0-length vector\n",
        "print(bool(tf.TensorShape([1])))     # 1-length vector\n",
        "print(bool(tf.TensorShape([None])))  # Unknown-length vector\n",
        "print(bool(tf.TensorShape([1, 10, 100])))       # 3D tensor\n",
        "print(bool(tf.TensorShape([None, None, None]))) # 3D tensor with no known dimensions\n",
        "print()\n",
        "print(bool(tf.TensorShape(None)))  # A tensor with unknown rank."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvfEd-uSsWqN"
      },
      "source": [
        "### Erros potenciais devido a alterações no TensorShape\n",
        "\n",
        "É improvável que as mudanças de comportamento do TensorShape quebrem silenciosamente seu código. No entanto, você poderá perceber que códigos relacionados a formatos começam a gerar `AttributeError`, pois valores`int` e `None` não têm os mesmos atributos que objetos `tf.compat.v1.Dimension`. Abaixo estão alguns exemplos de `AttributeError`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r18f8JAGsQi6"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # Create a shape and choose an index\n",
        "  shape = tf.TensorShape([16, None, 256])\n",
        "  value = shape[0].value\n",
        "except AttributeError as e:\n",
        "  # 'int' object has no attribute 'value'\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9flHru1uIdT"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # Create a shape and choose an index\n",
        "  shape = tf.TensorShape([16, None, 256])\n",
        "  dim = shape[1]\n",
        "  other_dim = shape[2]\n",
        "  dim.assert_is_compatible_with(other_dim)\n",
        "except AttributeError as e:\n",
        "  # 'NoneType' object has no attribute 'assert_is_compatible_with'\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og7H_TwJqIOF"
      },
      "source": [
        "## Igualdade de tensores por valor\n",
        "\n",
        "Os operadores binários `==` e `!=` em variáveis ​​e tensores foram alterados para comparar por valor no TF2 em vez de comparar por referência de objeto como em TF1.x. Além disso, não é mais possível fazer hash em tensores e variáveis ​​​​ou utilizá-los ​​em conjuntos ou chaves de dicts, porque pode não ser possível fazer hash deles por valor. Em vez disso, eles expõem um método `.ref()` que você pode usar para obter uma referência com hash ao tensor ou variável.\n",
        "\n",
        "Para isolar o impacto dessa mudança de comportamento, você pode usar `tf.compat.v1.disable_tensor_equality()` e `tf.compat.v1.enable_tensor_equality()` para desativar ou ativar globalmente essa mudança de comportamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGN4oL3lz0ki"
      },
      "source": [
        "Por exemplo, no TF1.x, duas variáveis ​​com o mesmo valor retornarão falso quando você usar o operador `==`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkGPGpEZ5DI-"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.disable_tensor_equality()\n",
        "x = tf.Variable(0.0)\n",
        "y = tf.Variable(0.0)\n",
        "\n",
        "x == y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqbewjIFz_oz"
      },
      "source": [
        "Enquanto que no TF2 com verificações de igualdade de tensor ativadas, `x == y` retornará `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5P_Rwy-zxVE"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.enable_tensor_equality()\n",
        "x = tf.Variable(0.0)\n",
        "y = tf.Variable(0.0)\n",
        "\n",
        "x == y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqdUPLhHypfs"
      },
      "source": [
        "Então, no TF2, se você precisar comparar por referência de objeto, certifique-se de usar `is` e `is not`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEjXVxlu4uxo"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.enable_tensor_equality()\n",
        "x = tf.Variable(0.0)\n",
        "y = tf.Variable(0.0)\n",
        "\n",
        "x is y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2ai1BGN01VI"
      },
      "source": [
        "### Hash em tensores e variáveis\n",
        "\n",
        "Com os comportamentos do TF1.x, você costumava adicionar variáveis ​​​​e tensores diretamente a estruturas de dados que exigem hashing, como chaves para objetos `set` e `dict`.\n",
        "\n",
        "```python\n",
        "x = tf.Variable(0.0)\n",
        "set([x, tf.constant(2.0)])\n",
        "```\n",
        "\n",
        "No entanto, no TF2 com igualdade de tensores ativada, não é possível fazer hash de tensores e variáveis ​​devido à mudança da semântica do operador `==` e `!=` para verificações de igualdade de valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TR1KfJu462w"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.enable_tensor_equality()\n",
        "x = tf.Variable(0.0)\n",
        "\n",
        "try:\n",
        "  set([x, tf.constant(2.0)])\n",
        "except TypeError as e:\n",
        "  # TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQY7NvNAa7be"
      },
      "source": [
        "Portanto, no TF2, se você precisar usar tensores ou objetos variáveis ​​​​como chaves ou conteúdo de um `set`, poderá usar `tensor.ref()` para obter uma referência com hash que poderá ser usada como chave:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-1kVPs01ZuU"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.enable_tensor_equality()\n",
        "x = tf.Variable(0.0)\n",
        "\n",
        "tensor_set = set([x.ref(), tf.constant(2.0).ref()])\n",
        "assert x.ref() in tensor_set\n",
        "\n",
        "tensor_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqqRqfOYbaOX"
      },
      "source": [
        "Se necessário, você também pode obter o tensor ou variável da referência usando `reference.deref()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwRZMYV06M7q"
      },
      "outputs": [],
      "source": [
        "referenced_var = x.ref().deref()\n",
        "assert referenced_var is x\n",
        "referenced_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XSFQbJaReVC"
      },
      "source": [
        "## Recursos e leitura adicional\n",
        "\n",
        "- Veja a seção [Migrando para TF2](https://tensorflow.org/guide/migrate) para ler mais sobre como migrar do TF1.x para TF2.\n",
        "- Leia o [guia de mapeamento de modelos](./model_mapping.ipynb) para saber mais sobre como mapear seus modelos TF1.x para trabalhar diretamente no TF2. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tf1_vs_tf2.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
