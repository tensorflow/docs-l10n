{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rmpybwysXGV"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m8y3rGtQsYP2"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrXv0rU9sIma"
      },
      "source": [
        "# Loops de treinamento básicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S0BwJ_8sLu7"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/basic_training_loops\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/basic_training_loops.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/basic_training_loops.ipynb\"> <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"> Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/basic_training_loops.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2o3TTG4TFpt"
      },
      "source": [
        "Nos guias anteriores, você aprendeu sobre [tensores](./tensor.ipynb), [variáveis](./variable.ipynb), [fitas de gradiente](autodiff.ipynb) e [módulos](./intro_to_modules.ipynb). Neste guia, você usará tudo isso para treinar modelos.\n",
        "\n",
        "O TensorFlow também inclui a [API tf.Keras](https://www.tensorflow.org/guide/keras/overview), uma API de redes neurais de alto nível que fornece abstrações úteis para reduzir o uso de código boilerplate. No entanto, neste guia, você usará classes básicas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LXMVuV0VhDr"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiolgWMPgpwI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKD__8kFCKNt"
      },
      "source": [
        "## Solucionando problemas de aprendizado de máquina\n",
        "\n",
        "A resolução de um problema de aprendizado de máquina geralmente consiste nas seguintes etapas:\n",
        "\n",
        "- Obtenção de dados de treinamento.\n",
        "- Definição do modelo\n",
        "- Definição de uma função de perda\n",
        "- Percorrer os dados de treinamento, calculando a perda a partir do valor ideal\n",
        "- Calcular gradientes para essa perda e usar um *otimizador* para ajustar as variáveis ​​para que os dados caibam.\n",
        "- Avaliação dos resultados.\n",
        "\n",
        "Para fins de ilustração, neste guia você desenvolverá um modelo linear simples, $f(x) = x * W + b$, que possui duas variáveis: $W$ (pesos) e $b$ (bias).\n",
        "\n",
        "Este é o mais básico dos problemas de aprendizado de máquina: dados $x$ e $y$, tente encontrar a inclinação e o deslocamento de uma linha por meio da [regressão linear simples](https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qutT_fkl_CBc"
      },
      "source": [
        "## Dados\n",
        "\n",
        "O aprendizado supervisionado usa *entradas* (geralmente denotadas como *x*) e *saídas* (denotadas como *y*, geralmente chamadas *de rótulos*). O objetivo é aprender com entradas e saídas emparelhadas para que você possa prever o valor de uma saída a partir de uma entrada.\n",
        "\n",
        "Cada entrada de dados, no TensorFlow, quase sempre é representada por um tensor e geralmente é um vetor. No treinamento supervisionado, a saída (ou valor que você gostaria de prever) também é um tensor.\n",
        "\n",
        "Aqui estão alguns dados sintetizados pela adição de ruído gaussiano (normal) a pontos ao longo de uma linha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzivK2ATByOz"
      },
      "outputs": [],
      "source": [
        "# The actual line\n",
        "TRUE_W = 3.0\n",
        "TRUE_B = 2.0\n",
        "\n",
        "NUM_EXAMPLES = 201\n",
        "\n",
        "# A vector of random x values\n",
        "x = tf.linspace(-2,2, NUM_EXAMPLES)\n",
        "x = tf.cast(x, tf.float32)\n",
        "\n",
        "def f(x):\n",
        "  return x * TRUE_W + TRUE_B\n",
        "\n",
        "# Generate some noise\n",
        "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "\n",
        "# Calculate y\n",
        "y = f(x) + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlFd_HVBFGIF"
      },
      "outputs": [],
      "source": [
        "# Plot all the data\n",
        "plt.plot(x, y, '.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH95XUzhL99d"
      },
      "source": [
        "Os tensores geralmente são reunidos em *lotes* ou grupos de entradas e saídas empilhadas. O lote pode conferir alguns benefícios ao treinamento e funciona bem com aceleradores e computação vetorizada. Dado o tamanho desse dataset, você pode tratar todo o dataset como um único lote."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFzH64Jn9PIm"
      },
      "source": [
        "## Definição do modelo\n",
        "\n",
        "Use `tf.Variable` para representar todos os pesos em um modelo. Um `tf.Variable` armazena um valor e o fornece na forma de tensor conforme necessário. Consulte o [guia de variáveis](./variable.ipynb) ​​para mais detalhes.\n",
        "\n",
        "Use `tf.Module` para encapsular as variáveis ​​e a computação. Você pode usar qualquer objeto Python, mas dessa forma ele pode ser salvo com facilidade.\n",
        "\n",
        "Aqui, você define como variáveis tanto *w* como *b*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WRu7Pze7wk8"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # Initialize the weights to `5.0` and the bias to `0.0`\n",
        "    # In practice, these should be randomly initialized\n",
        "    self.w = tf.Variable(5.0)\n",
        "    self.b = tf.Variable(0.0)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.w * x + self.b\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "# List the variables tf.modules's built-in variable aggregation.\n",
        "print(\"Variables:\", model.variables)\n",
        "\n",
        "# Verify the model works\n",
        "assert model(3.0).numpy() == 15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdpN_3ssG9D5"
      },
      "source": [
        "As variáveis ​​iniciais são definidas aqui de forma fixa, mas o Keras vem com diversos [inicializadores](https://www.tensorflow.org/api_docs/python/tf/keras/initializers) que você pode usar, com ou sem o restante do Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa6j_yXa-j79"
      },
      "source": [
        "### Definição de uma função de perda\n",
        "\n",
        "Uma função de perda mede o quanto a saída de um modelo para uma determinada entrada corresponde à saída-alvo. O objetivo é minimizar essa diferença durante o treinamento. Defina a perda L2 padrão, também conhecida como erro \"quadrado médio\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0ysUFGY924U"
      },
      "outputs": [],
      "source": [
        "# This computes a single loss value for an entire batch\n",
        "def loss(target_y, predicted_y):\n",
        "  return tf.reduce_mean(tf.square(target_y - predicted_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-50nq-wPBsAW"
      },
      "source": [
        "Antes de treinar o modelo, você pode prever o valor da perda plotando as previsões do modelo em vermelho e os dados de treinamento em azul:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eb83LtrB4nt"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, '.', label=\"Data\")\n",
        "plt.plot(x, f(x), label=\"Ground truth\")\n",
        "plt.plot(x, model(x), label=\"Predictions\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Current loss: %1.6f\" % loss(y, model(x)).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSDP-yeq_4jE"
      },
      "source": [
        "### Definição de um loop de treinamento\n",
        "\n",
        "O loop de treinamento consiste em fazer repetidamente três tarefas em ordem:\n",
        "\n",
        "- Enviar um lote de entradas através do modelo para gerar saídas\n",
        "- Calcular a perda comparando as saídas com a saída (ou rótulo)\n",
        "- Usar fitas de gradiente para encontrar os gradientes\n",
        "- Otimizar as variáveis ​​com esses gradientes\n",
        "\n",
        "Para este exemplo, você pode treinar o modelo usando o [método do gradiente descendente](https://en.wikipedia.org/wiki/Gradient_descent).\n",
        "\n",
        "Existem muitas variantes do esquema do método do gradiente descendente que são capturadas em `tf.keras.optimizers`. Mas no espírito de construir a partir dos princípios básicos, aqui você mesmo vai implementar a matemática básica com a ajuda de `tf.GradientTape` para diferenciação automática e `tf.assign_sub` para decrementar um valor (que combina `tf.assign` e `tf.sub`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBIACgdnA55X"
      },
      "outputs": [],
      "source": [
        "# Given a callable model, inputs, outputs, and a learning rate...\n",
        "def train(model, x, y, learning_rate):\n",
        "\n",
        "  with tf.GradientTape() as t:\n",
        "    # Trainable variables are automatically tracked by GradientTape\n",
        "    current_loss = loss(y, model(x))\n",
        "\n",
        "  # Use GradientTape to calculate the gradients with respect to W and b\n",
        "  dw, db = t.gradient(current_loss, [model.w, model.b])\n",
        "\n",
        "  # Subtract the gradient scaled by the learning rate\n",
        "  model.w.assign_sub(learning_rate * dw)\n",
        "  model.b.assign_sub(learning_rate * db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwWPaJryD2aN"
      },
      "source": [
        "Para dar uma olhada no treinamento, você pode enviar o mesmo lote de *x* e *y* através do loop de treinamento e ver como `W` e `b` evoluem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdfkR223D9dW"
      },
      "outputs": [],
      "source": [
        "model = MyModel()\n",
        "\n",
        "# Collect the history of W-values and b-values to plot later\n",
        "weights = []\n",
        "biases = []\n",
        "epochs = range(10)\n",
        "\n",
        "# Define a training loop\n",
        "def report(model, loss):\n",
        "  return f\"W = {model.w.numpy():1.2f}, b = {model.b.numpy():1.2f}, loss={loss:2.5f}\"\n",
        "\n",
        "\n",
        "def training_loop(model, x, y):\n",
        "\n",
        "  for epoch in epochs:\n",
        "    # Update the model with the single giant batch\n",
        "    train(model, x, y, learning_rate=0.1)\n",
        "\n",
        "    # Track this before I update\n",
        "    weights.append(model.w.numpy())\n",
        "    biases.append(model.b.numpy())\n",
        "    current_loss = loss(y, model(x))\n",
        "\n",
        "    print(f\"Epoch {epoch:2d}:\")\n",
        "    print(\"    \", report(model, current_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dKKLU4KkQEq"
      },
      "source": [
        "Faça o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRuNUghs1lHY"
      },
      "outputs": [],
      "source": [
        "current_loss = loss(y, model(x))\n",
        "\n",
        "print(f\"Starting:\")\n",
        "print(\"    \", report(model, current_loss))\n",
        "\n",
        "training_loop(model, x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPJgimg8kSA4"
      },
      "source": [
        "Plote a evolução dos pesos ao longo do tempo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND1fQw8sbTNr"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, weights, label='Weights', color=colors[0])\n",
        "plt.plot(epochs, [TRUE_W] * len(epochs), '--',\n",
        "         label = \"True weight\", color=colors[0])\n",
        "\n",
        "plt.plot(epochs, biases, label='bias', color=colors[1])\n",
        "plt.plot(epochs, [TRUE_B] * len(epochs), \"--\",\n",
        "         label=\"True bias\", color=colors[1])\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhlwj1ojkcUP"
      },
      "source": [
        "Visualize o desempenho do modelo treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpTEjWWex568"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, '.', label=\"Data\")\n",
        "plt.plot(x, f(x), label=\"Ground truth\")\n",
        "plt.plot(x, model(x), label=\"Predictions\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Current loss: %1.6f\" % loss(model(x), y).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DODMMmfLIiOC"
      },
      "source": [
        "## A mesma solução, mas com Keras\n",
        "\n",
        "É útil comparar o código acima com o equivalente em Keras.\n",
        "\n",
        "A definição do modelo parece exatamente igual se você usar uma subclasse de `tf.keras.Model`. Lembre-se que os modelos Keras herdam, em última instância, de module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z86hCI0x1YX3"
      },
      "outputs": [],
      "source": [
        "class MyModelKeras(tf.keras.Model):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # Initialize the weights to `5.0` and the bias to `0.0`\n",
        "    # In practice, these should be randomly initialized\n",
        "    self.w = tf.Variable(5.0)\n",
        "    self.b = tf.Variable(0.0)\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.w * x + self.b\n",
        "\n",
        "keras_model = MyModelKeras()\n",
        "\n",
        "# Reuse the training loop with a Keras model\n",
        "training_loop(keras_model, x, y)\n",
        "\n",
        "# You can also save a checkpoint using Keras's built-in support\n",
        "keras_model.save_weights(\"my_checkpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kw5P4jt2Az8"
      },
      "source": [
        "Em vez de escrever novos loops de treinamento sempre que criar um modelo, você pode usar os recursos integrados do Keras como um atalho. Isso pode ser útil quando você não quiser escrever ou depurar loops de treinamento no Python.\n",
        "\n",
        "Se fizer isso, você precisará usar `model.compile()` para definir os parâmetros e `model.fit()` para treinar. Pode ter menos código para usar as implementações Keras de perda de L2 e método do gradiente descendente, novamente como um atalho. As perdas e otimizadores do Keras também podem ser usados ​​fora dessas funções de conveniência, e o exemplo anterior poderia tê-los usado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nbLLfPE2pEl"
      },
      "outputs": [],
      "source": [
        "keras_model = MyModelKeras()\n",
        "\n",
        "# compile sets the training parameters\n",
        "keras_model.compile(\n",
        "    # By default, fit() uses tf.function().  You can\n",
        "    # turn that off for debugging, but it is on now.\n",
        "    run_eagerly=False,\n",
        "\n",
        "    # Using a built-in optimizer, configuring as an object\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "\n",
        "    # Keras comes with built-in MSE error\n",
        "    # However, you could use the loss function\n",
        "    # defined above\n",
        "    loss=tf.keras.losses.mean_squared_error,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrlHODiZccu2"
      },
      "source": [
        "O Keras `fit` espera dados em lote ou um dataset completo como um array NumPy. As matrizes NumPy são divididas em lotes e padronizadas para um tamanho de lote de 32.\n",
        "\n",
        "Nesse caso, para corresponder ao comportamento do loop escrito à mão, você deve passar `x` como um único lote de tamanho 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfAYqtu136PO"
      },
      "outputs": [],
      "source": [
        "print(x.shape[0])\n",
        "keras_model.fit(x, y, epochs=10, batch_size=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zKZIO9P5s1G"
      },
      "source": [
        "Observe que Keras imprime a perda após o treinamento, não antes, então a primeira perda parece menor, mas, caso contrário, isso mostra essencialmente o mesmo desempenho de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPnIVuaSJwWz"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "Neste guia, você viu como usar as classes principais de tensores, variáveis, módulos e fitas de gradiente para construir e treinar um modelo e, além disso, como essas ideias são implementadas no Keras.\n",
        "\n",
        "Este é, no entanto, um problema extremamente simples. Para uma introdução mais prática, consulte [Passo a passo de treinamento personalizado](../tutorials/customization/custom_training_walkthrough.ipynb).\n",
        "\n",
        "Para saber mais sobre o uso de loops de treinamento integrados no Keras, consulte [este guia](https://www.tensorflow.org/guide/keras/train_and_evaluate). Para saber mais sobre loops de treinamento e Keras, consulte [este guia](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch). Para escrever loops de treinamento distribuídos personalizados, consulte [este guia](distributed_training.ipynb#using_tfdistributestrategy_with_basic_training_loops_loops)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5rmpybwysXGV",
        "iKD__8kFCKNt"
      ],
      "name": "basic_training_loops.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
