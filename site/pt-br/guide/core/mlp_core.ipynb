{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhGuhbZ6M5tl"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AwOEIRJC6Une"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Perceptrons multicamadas para reconhecimento de dígitos com APIs Core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBIlTPscrIT9"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/core/mlp_core\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/core/mlp_core.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/core/mlp_core.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/core/mlp_core.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjAxxRpBzVYg"
      },
      "source": [
        "Este notebook usa as [APIs de baixo nível do TensorFlow Core](https://www.tensorflow.org/guide/core) para criar um fluxo de trabalho de aprendizado de máquina completo para classificação de dígitos manuscritos com [perceptrons multicamadas](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy) e o [dataset MNIST](http://yann.lecun.com/exdb/mnist). Veja a [Visão geral das APIs Core](https://www.tensorflow.org/guide/core) para saber mais sobre o TensorFlow Core e seus casos de uso pretendidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHVMVIFHSzl1"
      },
      "source": [
        "## Visão geral do perceptron multicamadas (MLP)\n",
        "\n",
        "O Perceptron Multicamadas (Multilayer Perceptron - MLP) é um tipo de rede neural feedforward usada para abordar problemas de [classificação multiclasse](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/video-lecture) . Antes de construir um MLP, é de fundamental importância entender os conceitos de perceptrons, camadas e funções de ativação.\n",
        "\n",
        "Perceptrons multicamadas são compostos de unidades funcionais chamadas de perceptrons. A equação de um perceptron é a seguinte:\n",
        "\n",
        "$$Z = \\vec{w}⋅\\mathrm{X} + b$$\n",
        "\n",
        "onde\n",
        "\n",
        "- $Z$: saída do perceptron\n",
        "- $\\mathrm{X}$: matriz de características\n",
        "- $\\vec{w}$: vetor de peso\n",
        "- $b$: bias\n",
        "\n",
        "Quando esses perceptrons são empilhados, eles formam estruturas chamadas de camadas densas, que podem ser conectadas para construir uma rede neural. A equação de uma camada densa é semelhante à de um perceptron, mas usa uma matriz de peso e um vetor de bias:\n",
        "\n",
        "$$Z = \\mathrm{W}⋅\\mathrm{X} + \\vec{b}$$\n",
        "\n",
        "onde\n",
        "\n",
        "- $Z$: saída da camada densa\n",
        "- $\\mathrm{X}$: matriz de características\n",
        "- $\\mathrm{W}$: matriz de pesos\n",
        "- $\\vec{b}$: vetor de bias\n",
        "\n",
        "Numa MLP, múltiplas camadas densas são conectadas de forma tal que as saídas de uma camada sejam totalmente conectadas às entradas da camada seguinte. Adicionar funções de ativação não lineares às saídas de camadas densas pode ajudar o classificador MLP a aprender limites de decisão complexos e fazer boas generalizações para dados não vistos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nchsZfwEVtVs"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Importe TensorFlow, [pandas](https://pandas.pydata.org) , [Matplotlib](https://matplotlib.org) e [seaborn](https://seaborn.pydata.org) para começar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSfgqmwBagw_"
      },
      "outputs": [],
      "source": [
        "# Use seaborn for countplot.\n",
        "!pip install -q seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import tempfile\n",
        "import os\n",
        "# Preset Matplotlib figure sizes.\n",
        "matplotlib.rcParams['figure.figsize'] = [9, 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xQKvCJ85kCQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "print(tf.__version__)\n",
        "# Set random seed for reproducible results \n",
        "tf.random.set_seed(22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_72b0LCNbjx"
      },
      "source": [
        "## Carregando os dados\n",
        "\n",
        "Este tutorial usa o [dataset MNIST](http://yann.lecun.com/exdb/mnist) e demonstra como construir um modelo MLP que pode classificar dígitos manuscritos. O dataset está disponível em [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/mnist).\n",
        "\n",
        "Divida o conjunto de dados MNIST em conjuntos de treinamento, validação e teste. O conjunto de validação pode ser usado para avaliar a generalização do modelo durante o treinamento para que o dataset de teste possa servir como um estimador final imparcial para o desempenho do modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uiuh0B098_3p"
      },
      "outputs": [],
      "source": [
        "train_data, val_data, test_data = tfds.load(\"mnist\", \n",
        "                                            split=['train[10000:]', 'train[0:10000]', 'test'],\n",
        "                                            batch_size=128, as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9uN3Lf6ANtn"
      },
      "source": [
        "O dataset MNIST consiste em dígitos manuscritos e seus true labels (rótulos verdadeiros) correspondentes. Veja alguns exemplos abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V8hSqJ7AMjQ"
      },
      "outputs": [],
      "source": [
        "x_viz, y_viz = tfds.load(\"mnist\", split=['train[:1500]'], batch_size=-1, as_supervised=True)[0]\n",
        "x_viz = tf.squeeze(x_viz, axis=3)\n",
        "\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,1+i)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x_viz[i], cmap='gray')\n",
        "    plt.title(f\"True Label: {y_viz[i]}\")\n",
        "    plt.subplots_adjust(hspace=.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRald9dSE4qS"
      },
      "source": [
        "Veja também a distribuição de dígitos nos dados de treinamento para verificar se cada classe está bem representada no dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj3K4XgQE7qR"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=y_viz.numpy());\n",
        "plt.xlabel('Digits')\n",
        "plt.title(\"MNIST Digit Distribution\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Wt4bDx_BRV"
      },
      "source": [
        "## Pré-processamento dos dados\n",
        "\n",
        "Primeiro, remodele as matrizes de características para que sejam bidimensionais achatando as imagens. Em seguida, redimensione os dados para que os valores de pixel de [0,255] caibam no intervalo de [0,1]. Essa etapa garante que os pixels de entrada tenham distribuições semelhantes e ajuda na convergência do treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSyCm2V2_AvI"
      },
      "outputs": [],
      "source": [
        "def preprocess(x, y):\n",
        "  # Reshaping the data\n",
        "  x = tf.reshape(x, shape=[-1, 784])\n",
        "  # Rescaling the data\n",
        "  x = x/255\n",
        "  return x, y\n",
        "\n",
        "train_data, val_data = train_data.map(preprocess), val_data.map(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3CrycBXA2s"
      },
      "source": [
        "## Construção do MLP\n",
        "\n",
        "Comece visualizando as funções de ativação [ReLU](https://developers.google.com/machine-learning/glossary#ReLU) e [Softmax](https://developers.google.com/machine-learning/glossary#softmax) . Ambas as funções estão disponíveis em `tf.nn.relu` e `tf.nn.softmax` respectivamente. O ReLU é uma função de ativação não linear que produz na saída a entrada se for positiva e 0 caso contrário:\n",
        "\n",
        "$$\\text{ReLU}(X) = max(0, X)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYunzt3UyT9G"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-2, 2, 201)\n",
        "x = tf.cast(x, tf.float32)\n",
        "plt.plot(x, tf.nn.relu(x));\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('ReLU(x)')\n",
        "plt.title('ReLU activation function');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuGrM9jMwsRM"
      },
      "source": [
        "A função de ativação softmax é uma função exponencial normalizada que converte $m$ números reais numa distribuição de probabilidade com $m$ resultados/classes. Isto é útil para prever probabilidades de classe a partir da saída de uma rede neural:\n",
        "\n",
        "$$\\text{Softmax}(X) = \\frac{e^{X}}{\\sum_{i=1}^{m}e^{X_i}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVM8pvhWwuwI"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-4, 4, 201)\n",
        "x = tf.cast(x, tf.float32)\n",
        "plt.plot(x, tf.nn.softmax(x, axis=0));\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Softmax(x)')\n",
        "plt.title('Softmax activation function');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHW6Yvg2yS6H"
      },
      "source": [
        "### A camada densa\n",
        "\n",
        "Crie uma classe para a camada densa. Por definição, as saídas de uma camada estão totalmente conectadas às entradas da camada seguinte numa MLP. Portanto, a dimensão de entrada para uma camada densa pode ser inferida com base na dimensão de saída de sua camada anterior e não precisa ser especificada antecipadamente durante a inicialização. Os pesos também devem ser inicializados corretamente para evitar que as saídas de ativação se tornem muito grandes ou pequenas. Um dos métodos de inicialização de pesos mais populares é o esquema Xavier, onde cada elemento da matriz de pesos é amostrado da seguinte maneira:\n",
        "\n",
        "$$W_{ij} \\sim \\text{Uniform}(-\\frac{\\sqrt{6}}{\\sqrt{n + m}},\\frac{\\sqrt{6}}{\\sqrt{n + m}})$$\n",
        "\n",
        "O vetor de bias pode ser inicializado com zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re1SSFyBdMrS"
      },
      "outputs": [],
      "source": [
        "def xavier_init(shape):\n",
        "  # Computes the xavier initialization values for a weight matrix\n",
        "  in_dim, out_dim = shape\n",
        "  xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(in_dim + out_dim, tf.float32))\n",
        "  weight_vals = tf.random.uniform(shape=(in_dim, out_dim), \n",
        "                                  minval=-xavier_lim, maxval=xavier_lim, seed=22)\n",
        "  return weight_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otDFX4u6e6ml"
      },
      "source": [
        "O método de inicialização Xavier também pode ser implementado com `tf.keras.initializers.GlorotUniform` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM0yJos25FG5"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(tf.Module):\n",
        "\n",
        "  def __init__(self, out_dim, weight_init=xavier_init, activation=tf.identity):\n",
        "    # Initialize the dimensions and activation functions\n",
        "    self.out_dim = out_dim\n",
        "    self.weight_init = weight_init\n",
        "    self.activation = activation\n",
        "    self.built = False\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if not self.built:\n",
        "      # Infer the input dimension based on first call\n",
        "      self.in_dim = x.shape[1]\n",
        "      # Initialize the weights and biases\n",
        "      self.w = tf.Variable(self.weight_init(shape=(self.in_dim, self.out_dim)))\n",
        "      self.b = tf.Variable(tf.zeros(shape=(self.out_dim,)))\n",
        "      self.built = True\n",
        "    # Compute the forward pass\n",
        "    z = tf.add(tf.matmul(x, self.w), self.b)\n",
        "    return self.activation(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-7MzpjgyHg6"
      },
      "source": [
        "Em seguida, construa uma classe para o modelo MLP que execute as camadas sequencialmente. Lembre-se de que as variáveis ​​do modelo só estão disponíveis após a primeira sequência de chamadas de camada densa devido à inferência de dimensão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XisRWiCyHAb"
      },
      "outputs": [],
      "source": [
        "class MLP(tf.Module):\n",
        "\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "   \n",
        "  @tf.function\n",
        "  def __call__(self, x, preds=False): \n",
        "    # Execute the model's layers sequentially\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luXKup-43nd7"
      },
      "source": [
        "Inicialize um modelo MLP com a seguinte arquitetura:\n",
        "\n",
        "- Passo para a frente: ReLU (784 x 700) x ReLU (700 x 500) x Softmax (500 x 10)\n",
        "\n",
        "A função de ativação softmax não precisa ser aplicada pelo MLP. É computada separadamente nas funções de perda e previsão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmlACuki3oPi"
      },
      "outputs": [],
      "source": [
        "hidden_layer_1_size = 700\n",
        "hidden_layer_2_size = 500\n",
        "output_size = 10\n",
        "\n",
        "mlp_model = MLP([\n",
        "    DenseLayer(out_dim=hidden_layer_1_size, activation=tf.nn.relu),\n",
        "    DenseLayer(out_dim=hidden_layer_2_size, activation=tf.nn.relu),\n",
        "    DenseLayer(out_dim=output_size)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyBATDoRmDkg"
      },
      "source": [
        "### Definição da função de perda\n",
        "\n",
        "A função de perda de entropia cruzada é uma ótima escolha para problemas de classificação multiclasse, pois mede a verossimilhança logarítmica negativa dos dados de acordo com as previsões de probabilidade do modelo. Quanto maior a probabilidade atribuída à classe verdadeira, menor a perda. A equação para a perda de entropia cruzada é a seguinte:\n",
        "\n",
        "$$L = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{i=j}^{n} {y_j}^{[i]}⋅\\log(\\hat{{y_j}}^{[i]})$$\n",
        "\n",
        "onde\n",
        "\n",
        "- $\\underset{n\\times m}{\\hat{y}}$: uma matriz de distribuições de classes previstas\n",
        "- $\\underset{n\\times m}{y}$: uma matriz codificada (com hot encoding) de classes verdadeiras\n",
        "\n",
        "A função `tf.nn.sparse_softmax_cross_entropy_with_logits` pode ser usada para calcular a perda de entropia cruzada. Esta função não requer que a última camada do modelo aplique a função de ativação softmax nem requer que os rótulos de classe sejam codificados com hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rskOYA7FVCwg"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(y_pred, y):\n",
        "  # Compute cross entropy loss with a sparse operation\n",
        "  sparse_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
        "  return tf.reduce_mean(sparse_ce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvWxED1km8jh"
      },
      "source": [
        "Escreva uma função básica de exatidão que calcule a proporção de classificações corretas durante o treinamento. Para gerar previsões de classe a partir das saídas softmax, retorne o índice que corresponde à maior probabilidade de classe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPJMWx2UgiBm"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred, y):\n",
        "  # Compute accuracy after extracting class predictions\n",
        "  class_preds = tf.argmax(tf.nn.softmax(y_pred), axis=1)\n",
        "  is_equal = tf.equal(y, class_preds)\n",
        "  return tf.reduce_mean(tf.cast(is_equal, tf.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSiNRhTOnKZr"
      },
      "source": [
        "### Treinamento do modelo\n",
        "\n",
        "O uso de um otimizador pode resultar numa convergência significativamente mais rápida em comparação com o método do gradiente descendente padrão. O otimizador Adam é implementado abaixo. Consulte o guia [Otimizadores](https://www.tensorflow.org/guide/core/optimizers_core) para saber mais sobre como criar otimizadores personalizados com o TensorFlow Core."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGIBDk3cAv6a"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "\n",
        "    def __init__(self, learning_rate=1e-3, beta_1=0.9, beta_2=0.999, ep=1e-7):\n",
        "      # Initialize optimizer parameters and variable slots\n",
        "      self.beta_1 = beta_1\n",
        "      self.beta_2 = beta_2\n",
        "      self.learning_rate = learning_rate\n",
        "      self.ep = ep\n",
        "      self.t = 1.\n",
        "      self.v_dvar, self.s_dvar = [], []\n",
        "      self.built = False\n",
        "      \n",
        "    def apply_gradients(self, grads, vars):\n",
        "      # Initialize variables on the first call\n",
        "      if not self.built:\n",
        "        for var in vars:\n",
        "          v = tf.Variable(tf.zeros(shape=var.shape))\n",
        "          s = tf.Variable(tf.zeros(shape=var.shape))\n",
        "          self.v_dvar.append(v)\n",
        "          self.s_dvar.append(s)\n",
        "        self.built = True\n",
        "      # Update the model variables given their gradients\n",
        "      for i, (d_var, var) in enumerate(zip(grads, vars)):\n",
        "        self.v_dvar[i].assign(self.beta_1*self.v_dvar[i] + (1-self.beta_1)*d_var)\n",
        "        self.s_dvar[i].assign(self.beta_2*self.s_dvar[i] + (1-self.beta_2)*tf.square(d_var))\n",
        "        v_dvar_bc = self.v_dvar[i]/(1-(self.beta_1**self.t))\n",
        "        s_dvar_bc = self.s_dvar[i]/(1-(self.beta_2**self.t))\n",
        "        var.assign_sub(self.learning_rate*(v_dvar_bc/(tf.sqrt(s_dvar_bc) + self.ep)))\n",
        "      self.t += 1.\n",
        "      return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osEK3rqpYfKd"
      },
      "source": [
        "Agora, escreva um loop de treinamento personalizado que atualize os parâmetros MLP com o método do gradiente descendente em minilotes. O uso de minilotes para treinamento garante uma eficiência de memória e convergência mais rápida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJLeY2ao1aw6"
      },
      "outputs": [],
      "source": [
        "def train_step(x_batch, y_batch, loss, acc, model, optimizer):\n",
        "  # Update the model state given a batch of data\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(x_batch)\n",
        "    batch_loss = loss(y_pred, y_batch)\n",
        "  batch_acc = acc(y_pred, y_batch)\n",
        "  grads = tape.gradient(batch_loss, model.variables)\n",
        "  optimizer.apply_gradients(grads, model.variables)\n",
        "  return batch_loss, batch_acc\n",
        "\n",
        "def val_step(x_batch, y_batch, loss, acc, model):\n",
        "  # Evaluate the model on given a batch of validation data\n",
        "  y_pred = model(x_batch)\n",
        "  batch_loss = loss(y_pred, y_batch)\n",
        "  batch_acc = acc(y_pred, y_batch)\n",
        "  return batch_loss, batch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC85kuZgmh3q"
      },
      "outputs": [],
      "source": [
        "def train_model(mlp, train_data, val_data, loss, acc, optimizer, epochs):\n",
        "  # Initialize data structures\n",
        "  train_losses, train_accs = [], []\n",
        "  val_losses, val_accs = [], []\n",
        "\n",
        "  # Format training loop and begin training\n",
        "  for epoch in range(epochs):\n",
        "    batch_losses_train, batch_accs_train = [], []\n",
        "    batch_losses_val, batch_accs_val = [], []\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for x_batch, y_batch in train_data:\n",
        "      # Compute gradients and update the model's parameters\n",
        "      batch_loss, batch_acc = train_step(x_batch, y_batch, loss, acc, mlp, optimizer)\n",
        "      # Keep track of batch-level training performance\n",
        "      batch_losses_train.append(batch_loss)\n",
        "      batch_accs_train.append(batch_acc)\n",
        "\n",
        "    # Iterate over the validation data\n",
        "    for x_batch, y_batch in val_data:\n",
        "      batch_loss, batch_acc = val_step(x_batch, y_batch, loss, acc, mlp)\n",
        "      batch_losses_val.append(batch_loss)\n",
        "      batch_accs_val.append(batch_acc)\n",
        "\n",
        "    # Keep track of epoch-level model performance\n",
        "    train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n",
        "    val_loss, val_acc = tf.reduce_mean(batch_losses_val), tf.reduce_mean(batch_accs_val)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    print(f\"Training loss: {train_loss:.3f}, Training accuracy: {train_acc:.3f}\")\n",
        "    print(f\"Validation loss: {val_loss:.3f}, Validation accuracy: {val_acc:.3f}\")\n",
        "  return train_losses, train_accs, val_losses, val_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvbfXlN5lwwB"
      },
      "source": [
        "Treine o modelo MLP por 10 épocas com tamanho de lote de 128. Aceleradores de hardware como GPUs ou TPUs também podem ajudar a acelerar o tempo de treinamento. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPlT8QfxptYl"
      },
      "outputs": [],
      "source": [
        "train_losses, train_accs, val_losses, val_accs = train_model(mlp_model, train_data, val_data, \n",
        "                                                             loss=cross_entropy_loss, acc=accuracy,\n",
        "                                                             optimizer=Adam(), epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_RVmt43G12R"
      },
      "source": [
        "### Avaliação de desempenho\n",
        "\n",
        "Comece escrevendo uma função de plotagem para visualizar a perda e a precisão do modelo durante o treinamento. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXTCYVtNDjAM"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(train_metric, val_metric, metric_type):\n",
        "  # Visualize metrics vs training Epochs\n",
        "  plt.figure()\n",
        "  plt.plot(range(len(train_metric)), train_metric, label = f\"Training {metric_type}\")\n",
        "  plt.plot(range(len(val_metric)), val_metric, label = f\"Validation {metric_type}\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric_type)\n",
        "  plt.legend()\n",
        "  plt.title(f\"{metric_type} vs Training epochs\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC-qIvZbHo0G"
      },
      "outputs": [],
      "source": [
        "plot_metrics(train_losses, val_losses, \"cross entropy loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-w2xk2PIDve"
      },
      "outputs": [],
      "source": [
        "plot_metrics(train_accs, val_accs, \"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbrJJaFrD_XR"
      },
      "source": [
        "## Salvando e carregando o modelo\n",
        "\n",
        "Comece criando um módulo de exportação que receba dados brutos e execute as seguintes operações:\n",
        "\n",
        "- Pré-processamento de dados\n",
        "- Previsão de probabilidade\n",
        "- Previsão de classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sszfWuJJZoo"
      },
      "outputs": [],
      "source": [
        "class ExportModule(tf.Module):\n",
        "  def __init__(self, model, preprocess, class_pred):\n",
        "    # Initialize pre and postprocessing functions\n",
        "    self.model = model\n",
        "    self.preprocess = preprocess\n",
        "    self.class_pred = class_pred\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None, None, None], dtype=tf.uint8)]) \n",
        "  def __call__(self, x):\n",
        "    # Run the ExportModule for new data points\n",
        "    x = self.preprocess(x)\n",
        "    y = self.model(x)\n",
        "    y = self.class_pred(y)\n",
        "    return y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8x6gjTDVi5d"
      },
      "outputs": [],
      "source": [
        "def preprocess_test(x):\n",
        "  # The export module takes in unprocessed and unlabeled data\n",
        "  x = tf.reshape(x, shape=[-1, 784])\n",
        "  x = x/255\n",
        "  return x\n",
        "\n",
        "def class_pred_test(y):\n",
        "  # Generate class predictions from MLP output\n",
        "  return tf.argmax(tf.nn.softmax(y), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu9H5STrJzdo"
      },
      "source": [
        "Este módulo de exportação agora pode ser salvo com a função `tf.saved_model.save`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN9pPBQTKTe3"
      },
      "outputs": [],
      "source": [
        "mlp_model_export = ExportModule(model=mlp_model,\n",
        "                                preprocess=preprocess_test,\n",
        "                                class_pred=class_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idS7rQKbKwRS"
      },
      "outputs": [],
      "source": [
        "models = tempfile.mkdtemp()\n",
        "save_path = os.path.join(models, 'mlp_model_export')\n",
        "tf.saved_model.save(mlp_model_export, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zZxO8iqBGZ-"
      },
      "source": [
        "Carregue o modelo salvo com `tf.saved_model.load` e examine seu desempenho nos dados de teste não vistos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5cwBTUqxldW"
      },
      "outputs": [],
      "source": [
        "mlp_loaded = tf.saved_model.load(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmv0u6j_b5OC"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(y_pred, y):\n",
        "  # Generic accuracy function\n",
        "  is_equal = tf.equal(y_pred, y)\n",
        "  return tf.reduce_mean(tf.cast(is_equal, tf.float32))\n",
        "\n",
        "x_test, y_test = tfds.load(\"mnist\", split=['test'], batch_size=-1, as_supervised=True)[0]\n",
        "test_classes = mlp_loaded(x_test)\n",
        "test_acc = accuracy_score(test_classes, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5t9vgv_ciQ_"
      },
      "source": [
        "O modelo faz um ótimo trabalho de classificação de dígitos manuscritos no dataset de treinamento e também generaliza bem para dados não vistos. Agora, examine a precisão de classe do modelo para garantir um bom desempenho para cada dígito. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD8YiC1Vfeyp"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy breakdown by digit:\")\n",
        "print(\"---------------------------\")\n",
        "label_accs = {}\n",
        "for label in range(10):\n",
        "  label_ind = (y_test == label)\n",
        "  # extract predictions for specific true label\n",
        "  pred_label = test_classes[label_ind]\n",
        "  labels = y_test[label_ind]\n",
        "  # compute class-wise accuracy\n",
        "  label_accs[accuracy_score(pred_label, labels).numpy()] = label\n",
        "for key in sorted(label_accs):\n",
        "  print(f\"Digit {label_accs[key]}: {key:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcykuJFhdGb0"
      },
      "source": [
        "Parece que o modelo luta com alguns dígitos um pouco mais do que outros, o que é bastante comum em muitos problemas de classificação multiclasse. Como exercício final, desenhe uma matriz de confusão das previsões do modelo e seus true labels correspondentes para reunir mais insights em nível de classe. Sklearn e seaborn têm funções para gerar e visualizar matrizes de confusão. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqCaqPwwh1tN"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as sk_metrics\n",
        "\n",
        "def show_confusion_matrix(test_labels, test_classes):\n",
        "  # Compute confusion matrix and normalize\n",
        "  plt.figure(figsize=(10,10))\n",
        "  confusion = sk_metrics.confusion_matrix(test_labels.numpy(), \n",
        "                                          test_classes.numpy())\n",
        "  confusion_normalized = confusion / confusion.sum(axis=1, keepdims=True)\n",
        "  axis_labels = range(10)\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.4f', square=True)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "show_confusion_matrix(y_test, test_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT-WA7GVda6d"
      },
      "source": [
        "Os insights em nível de classe podem ajudar a identificar motivos para erros de classificação e melhorar o desempenho do modelo em futuros ciclos de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFLfEH4ManbW"
      },
      "source": [
        "## Conclusão\n",
        "\n",
        "Este notebook apresentou algumas técnicas para lidar com um problema de classificação multiclasse usando uma [MLP](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax). Aqui estão mais algumas dicas que podem ser úteis:\n",
        "\n",
        "- As [APIs do TensorFlow Core](https://www.tensorflow.org/guide/core) podem ser usadas para criar workflows de aprendizado de máquina com altos níveis de configurabilidade\n",
        "- Os esquemas de inicialização podem ajudar a evitar que os parâmetros do modelo desapareçam ou explodam durante o treinamento.\n",
        "- O overfitting é outro problema comum para redes neurais, embora não tenha sido um problema neste tutorial. Consulte o tutorial [Overfit e underfit](overfit_and_underfit.ipynb) para mais informações sobre esse tema.\n",
        "\n",
        "Para obter mais exemplos de uso das APIs Core do TensorFlow, confira o [guia](https://www.tensorflow.org/guide/core). Se você quiser saber mais sobre como carregar e preparar dados, veja os tutoriais sobre [carregamento de dados de imagem](https://www.tensorflow.org/tutorials/load_data/images) ou [carregamento de dados CSV](https://www.tensorflow.org/tutorials/load_data/csv)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FhGuhbZ6M5tl"
      ],
      "name": "mlp_core.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
