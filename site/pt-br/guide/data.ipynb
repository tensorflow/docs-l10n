{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxv6goXm7oGF"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "llMNufAK7nfK"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Byow2J6LaPl"
      },
      "source": [
        "# tf.data: criando pipelines de entrada no TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGXS3UWBBNoc"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/data\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/data.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/data.ipynb\"> <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"> Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/data.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qo3HgDjbDcI"
      },
      "source": [
        "A API `tf.data` permite a criação de pipelines de entrada complexos a partir de peças simples e reutilizáveis. Por exemplo, o pipeline para um modelo de imagem pode agregar dados de arquivos num sistema de arquivos distribuído, aplicar perturbações aleatórias a cada imagem e mesclar imagens selecionadas aleatoriamente em um lote para treinamento. O pipeline para um modelo de texto pode envolver a extração de símbolos de dados de texto brutos, convertendo-os em identificadores de embeddings numa tabela de pesquisa e agrupar sequências de comprimentos diferentes em um lote. A API `tf.data` torna possível lidar com grandes quantidades de dados, ler diferentes formatos de dados e realizar transformações complexas.\n",
        "\n",
        "A API `tf.data` introduz uma abstração `tf.data.Dataset` que representa uma sequência de elementos, em que cada elemento consiste em um ou mais componentes. Por exemplo, num pipeline de imagem, um elemento pode ser um único exemplo de treinamento, com um par de componentes tensores representando a imagem e seu rótulo.\n",
        "\n",
        "Existem duas maneiras distintas de criar um dataset:\n",
        "\n",
        "- Uma **fonte** de dados constrói um `Dataset` a partir de dados armazenados na memória ou em um ou mais arquivos.\n",
        "\n",
        "- Uma **transformação** de dados constrói um dataset a partir de um ou mais objetos `tf.data.Dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJIEjEIBdf-h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y0JtWBNR9E5"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l4a0ALxdaWF"
      },
      "source": [
        "## Mecanismo básico\n",
        "\n",
        "<a id=\"basic-mechanics\"></a>\n",
        "\n",
        "Para criar um pipeline de entrada, você deve começar com uma *fonte* de dados. Por exemplo, para construir um `Dataset` a partir de dados na memória, você pode usar `tf.data.Dataset.from_tensors()` ou `tf.data.Dataset.from_tensor_slices()`. Alternativamente, se seus dados de entrada estiverem armazenados em arquivo no formato TFRecord recomendado, você poderá usar `tf.data.TFRecordDataset()`.\n",
        "\n",
        "Assim que você tiver um objeto `Dataset`, você pode *transformá-lo* num novo `Dataset` encadeando chamadas de método no objeto `tf.data.Dataset`. Por exemplo, você pode aplicar transformações por elemento, como `Dataset.map`, e transformações multi-elemento, como `Dataset.batch`. Consulte a documentação de `tf.data.Dataset` para uma lista completa de transformações.\n",
        "\n",
        "O objeto `Dataset` é um iterável (iterable) em Python. Isso possibilita o consumo dos seus elementos usando um loop for:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F-FDnjB6t6J"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwJsRJ-FbDcJ"
      },
      "outputs": [],
      "source": [
        "for elem in dataset:\n",
        "  print(elem.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0yy80MobDcM"
      },
      "source": [
        "Ou criando explicitamente um iterador Python com `iter` e consumindo seus elementos usando `next`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03w9oxFfbDcM"
      },
      "outputs": [],
      "source": [
        "it = iter(dataset)\n",
        "\n",
        "print(next(it).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4CgCL8qbDcO"
      },
      "source": [
        "Alternativamente, os elementos do dataset podem ser consumidos usando a transformação `reduce`, que reduz todos os elementos para produzir um único resultado. O exemplo a seguir ilustra como usar a transformação `reduce` para calcular a soma de um conjunto de dados de números inteiros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2bHAeNxbDcO"
      },
      "outputs": [],
      "source": [
        "print(dataset.reduce(0, lambda state, value: state + value).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Fzwt2nbDcR"
      },
      "source": [
        "<!-- TODO(jsimsa): Talk about `tf.function` support. -->\n",
        "\n",
        "<a id=\"dataset_structure\"></a>\n",
        "\n",
        "### Estrutura do dataset\n",
        "\n",
        "Um dataset produz uma sequência de *elementos* , onde cada elemento é a mesma estrutura (aninhada) de *componentes*. Os componentes individuais da estrutura podem ser de qualquer tipo representável por `tf.TypeSpec`, incluindo `tf.Tensor`, `tf.sparse.SparseTensor`, `tf.RaggedTensor`, `tf.TensorArray` ou `tf.data.Dataset`.\n",
        "\n",
        "Os construtos do Python que podem ser usados para expressar a estrutura (aninhada) dos elementos incluem `tuple`, `dict`, `NamedTuple` e `OrderedDict`. No entanto, `list` não é considerado um construto válido para expressar a estrutura dos elementos do dataset. Isto ocorre devido aos primeiros usuários de `tf.data` se preocuparem com a possibilidade das entradas `list` (por exemplo, quando passadas para `tf.data.Dataset.from_tensors`) serem automaticamente empacotadas como tensores e as saídas `list` (por exemplo, valores de retorno de funções definidas pelo usuário) serem coagidas para uma `tuple`. Como consequência disso, se você quiser que uma entrada `list` seja tratada como uma estrutura, você precisa convertê-la numa `tuple` e se quiser que uma saída de `list` seja tratado como um único componente, então você precisará empacotá-la explicitamente usando `tf.stack`.\n",
        "\n",
        "A propriedade `Dataset.element_spec` permite inspecionar o tipo de cada componente de um elemento. A propriedade retorna uma *estrutura aninhada* de objetos `tf.TypeSpec`, correspondendo à estrutura do elemento, que pode ser um único componente, uma tupla de componentes ou uma tupla aninhada de componentes. Por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg0m1beIhXGn"
      },
      "outputs": [],
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
        "\n",
        "dataset1.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwyemaghhXaG"
      },
      "outputs": [],
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "   (tf.random.uniform([4]),\n",
        "    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
        "\n",
        "dataset2.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CL7aB0ahXn_"
      },
      "outputs": [],
      "source": [
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "\n",
        "dataset3.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5bz7R1xhX1f"
      },
      "outputs": [],
      "source": [
        "# Dataset containing a sparse tensor.\n",
        "dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))\n",
        "\n",
        "dataset4.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVOPHur_hYQv"
      },
      "outputs": [],
      "source": [
        "# Use value_type to see the type of value represented by the element spec\n",
        "dataset4.element_spec.value_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5xNsFFvhUnr"
      },
      "source": [
        "As transformações `Dataset` suportam datasets de qualquer estrutura. Ao usar as transformações `Dataset.map` e `Dataset.filter`, que aplicam uma função a cada elemento, a estrutura do elemento determina os argumentos da função:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2myAr3Pxd-zF"
      },
      "outputs": [],
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
        "    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))\n",
        "\n",
        "dataset1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woPXMP14gUTg"
      },
      "outputs": [],
      "source": [
        "for z in dataset1:\n",
        "  print(z.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53PA4x6XgLar"
      },
      "outputs": [],
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "   (tf.random.uniform([4]),\n",
        "    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
        "\n",
        "dataset2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ju4sNSebDcR"
      },
      "outputs": [],
      "source": [
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "\n",
        "dataset3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgxsfAS2g6gk"
      },
      "outputs": [],
      "source": [
        "for a, (b,c) in dataset3:\n",
        "  print('shapes: {a.shape}, {b.shape}, {c.shape}'.format(a=a, b=b, c=c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1s2K0g-bDcT"
      },
      "source": [
        "## Lendo dados de entrada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3JG2f0h2683"
      },
      "source": [
        "### Consumindo arrays NumPy\n",
        "\n",
        "Consulte o tutorial [Carregando arrays NumPy](../tutorials/load_data/numpy.ipynb) para mais exemplos.\n",
        "\n",
        "Se todos os seus dados de entrada couberem na memória, a maneira mais simples de criar um `Dataset` a partir deles é convertê-los em objetos `tf.Tensor` e usar `Dataset.from_tensor_slices`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmaE6PjjhQ47"
      },
      "outputs": [],
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6cNiuDBbDcU"
      },
      "outputs": [],
      "source": [
        "images, labels = train\n",
        "images = images/255\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkwrDHN5bDcW"
      },
      "source": [
        "Observação: o snippet de código acima incorporará os arrays `features` e `labels` no seu grafo do TensorFlow como operações `tf.constant()`. Isto funciona bem para um dataset pequeno, mas desperdiça memória --- porque o conteúdo do array será copiado múltiplas vezes --- e poderá atingir o limite de 2 GB para o buffer de protocolo `tf.GraphDef`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO4ua2gEmIhR"
      },
      "source": [
        "### Consumindo geradores Python\n",
        "\n",
        "Outra fonte de dados comum que pode ser facilmente ingerida como `tf.data.Dataset` é o gerador python.\n",
        "\n",
        "Cuidado: Embora esta seja uma abordagem conveniente, ela é limitada quanto à portabilidade e escalabilidade. Ela deve ser executado no mesmo processo python que criou o gerador, e ainda está sujeita ao Python [GIL](https://en.wikipedia.org/wiki/Global_interpreter_lock) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9njpME-jmDza"
      },
      "outputs": [],
      "source": [
        "def count(stop):\n",
        "  i = 0\n",
        "  while i<stop:\n",
        "    yield i\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwqLrjnTpD8Y"
      },
      "outputs": [],
      "source": [
        "for n in count(5):\n",
        "  print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_BB_PhxnVVx"
      },
      "source": [
        "O construtor `Dataset.from_generator` converte o gerador python em um `tf.data.Dataset` totalmente funcional.\n",
        "\n",
        "O construtor recebe um callable como entrada, não um iterator. Isto permite reiniciar o gerador quando ele chega ao fim. Ele recebe um argumento `args` opcional, que é passado como os argumentos do callable.\n",
        "\n",
        "O argumento `output_types` é necessário porque `tf.data` constrói um `tf.Graph` internamente e as bordas do grafo requerem um `tf.dtype`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFga_OTwm0Je"
      },
      "outputs": [],
      "source": [
        "ds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes = (), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fel1SUuBnDUE"
      },
      "outputs": [],
      "source": [
        "for count_batch in ds_counter.repeat().batch(10).take(10):\n",
        "  print(count_batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxy9hDMTq1zD"
      },
      "source": [
        "O argumento `output_shapes` não é *obrigatório*, mas é altamente recomendado, pois muitas operações do TensorFlow não oferecem suporte a tensores com posto desconhecido. Se o comprimento de um eixo específico for desconhecido ou variável, defina-o como `None` em `output_shapes`.\n",
        "\n",
        "Também é importante observar que `output_shapes` e `output_types` seguem as mesmas regras de aninhamento de outros métodos de um dataset.\n",
        "\n",
        "Eis um exemplo de gerador que demonstra ambos os aspectos: ele retorna tuplas de arrays, onde o segundo array é um vetor com comprimento desconhecido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "allFX1g8rGKe"
      },
      "outputs": [],
      "source": [
        "def gen_series():\n",
        "  i = 0\n",
        "  while True:\n",
        "    size = np.random.randint(0, 10)\n",
        "    yield i, np.random.normal(size=(size,))\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ku26Yb9rcJX"
      },
      "outputs": [],
      "source": [
        "for i, series in gen_series():\n",
        "  print(i, \":\", str(series))\n",
        "  if i > 5:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmkynGilx0qf"
      },
      "source": [
        "A primeira saída é um `int32` e a segunda é um `float32`.\n",
        "\n",
        "O primeiro item é um escalar, formato `()`, e o segundo é um vetor de comprimento desconhecido, formato `(None,)` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDTfhEzhsliM"
      },
      "outputs": [],
      "source": [
        "ds_series = tf.data.Dataset.from_generator(\n",
        "    gen_series, \n",
        "    output_types=(tf.int32, tf.float32), \n",
        "    output_shapes=((), (None,)))\n",
        "\n",
        "ds_series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWxvSyQiyN0o"
      },
      "source": [
        "Agora ele pode ser usado como um `tf.data.Dataset` comum. Observe que ao construir um lote com um dataset de formato variável, você precisa usar `Dataset.padded_batch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7jEpj3As1lO"
      },
      "outputs": [],
      "source": [
        "ds_series_batch = ds_series.shuffle(20).padded_batch(10)\n",
        "\n",
        "ids, sequence_batch = next(iter(ds_series_batch))\n",
        "print(ids.numpy())\n",
        "print()\n",
        "print(sequence_batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hcqOccJ1CxG"
      },
      "source": [
        "Para um exemplo mais realista, experimente empacotar `preprocessing.image.ImageDataGenerator` como um `tf.data.Dataset`.\n",
        "\n",
        "Primeiro baixe os dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-_JCFRQ1CXM"
      },
      "outputs": [],
      "source": [
        "flowers = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIjPhvQ87jUT"
      },
      "source": [
        "Crie o `image.ImageDataGenerator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPCZeBQE5DfH"
      },
      "outputs": [],
      "source": [
        "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my4PxqfH26p6"
      },
      "outputs": [],
      "source": [
        "images, labels = next(img_gen.flow_from_directory(flowers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd96nH1w3eKH"
      },
      "outputs": [],
      "source": [
        "print(images.dtype, images.shape)\n",
        "print(labels.dtype, labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvRwvt5E2rTH"
      },
      "outputs": [],
      "source": [
        "ds = tf.data.Dataset.from_generator(\n",
        "    lambda: img_gen.flow_from_directory(flowers), \n",
        "    output_types=(tf.float32, tf.float32), \n",
        "    output_shapes=([32,256,256,3], [32,5])\n",
        ")\n",
        "\n",
        "ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcaULBCXj_2_"
      },
      "outputs": [],
      "source": [
        "for images, labels in ds.take(1):\n",
        "  print('images.shape: ', images.shape)\n",
        "  print('labels.shape: ', labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma4XoYzih2f4"
      },
      "source": [
        "### Consumindo dados TFRecord\n",
        "\n",
        "Consulte o tutorial [Carregando TFRecords](../tutorials/load_data/tfrecord.ipynb) para um exemplo completo.\n",
        "\n",
        "A API `tf.data` oferece suporte a uma variedade de formatos de arquivo para que você possa processar grandes datasets que não cabem na memória. Por exemplo, o formato de arquivo TFRecord é um formato binário simples orientado a registros que muitos aplicativos TensorFlow usam para dados de treinamento. A classe `tf.data.TFRecordDataset` permite transmitir o conteúdo de um ou mais arquivos TFRecord como parte de um pipeline de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiatWUloRJc4"
      },
      "source": [
        "Eis um exemplo usando o arquivo de teste da French Street Name Signs (FSNS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZo_4fzdbDcW"
      },
      "outputs": [],
      "source": [
        "# Creates a dataset that reads all of the examples from two files.\n",
        "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seD5bOH3RhBP"
      },
      "source": [
        "O argumento `filenames` (nomes de arquivos) passado para o inicializador `TFRecordDataset` pode ser uma string, uma lista de strings ou um `tf.Tensor` de strings. Portanto, se você tiver dois conjuntos de arquivos para fins de treinamento e validação, poderá criar um método de fábrica que produza o dataset, tomando filenames como argumento de entrada:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2WV5d7DRUA-"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62NC3vz9U8ww"
      },
      "source": [
        "Muitos projetos do TensorFlow usam registros `tf.train.Example` serializados em seus arquivos TFRecord. Eles precisam ser decodificados antes de serem inspecionados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tk29nlMl5P3"
      },
      "outputs": [],
      "source": [
        "raw_example = next(iter(dataset))\n",
        "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "parsed.features.feature['image/text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJAUib10bDcb"
      },
      "source": [
        "### Consumindo dados de texto\n",
        "\n",
        "Consulte o tutorial [Carregando texto](../tutorials/load_data/text.ipynb) para um exemplo completo.\n",
        "\n",
        "Muitos datasets são distribuídos como um ou mais arquivos de texto. O `tf.data.TextLineDataset` fornece uma maneira fácil de extrair linhas de um ou mais arquivos de texto. Dado um ou mais nomes de arquivos, um `TextLineDataset` produzirá um elemento com valor de string por linha desses arquivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQMoFu2TbDcc"
      },
      "outputs": [],
      "source": [
        "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "file_paths = [\n",
        "    tf.keras.utils.get_file(file_name, directory_url + file_name)\n",
        "    for file_name in file_names\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il4cOjiVwj95"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.TextLineDataset(file_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MevIbDiwy4MC"
      },
      "source": [
        "Aqui estão as primeiras linhas do primeiro arquivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpEHKyvHxu8A"
      },
      "outputs": [],
      "source": [
        "for line in dataset.take(5):\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJyVw8ro7fey"
      },
      "source": [
        "Para alternar linhas entre arquivos use `Dataset.interleave`. Isso facilita o processo de embaralhar os arquivos. Aqui estão a primeira, segunda e terceira linhas de cada tradução:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UCveWOt7fDE"
      },
      "outputs": [],
      "source": [
        "files_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
        "lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)\n",
        "\n",
        "for i, line in enumerate(lines_ds.take(9)):\n",
        "  if i % 3 == 0:\n",
        "    print()\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F_pOIDubDce"
      },
      "source": [
        "Por padrão, um `TextLineDataset` produz *cada* linha de cada arquivo, o que pode não ser desejável, por exemplo, se o arquivo começar com uma linha de cabeçalho ou contiver comentários. Essas linhas podem ser removidas usando as transformações `Dataset.skip()` ou `Dataset.filter`. Aqui, você pula a primeira linha e depois filtra para encontrar apenas linhas com sobreviventes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6b20Gua2jPO"
      },
      "outputs": [],
      "source": [
        "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic_lines = tf.data.TextLineDataset(titanic_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M1pauNT68B2"
      },
      "outputs": [],
      "source": [
        "for line in titanic_lines.take(10):\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEIP95cibDcf"
      },
      "outputs": [],
      "source": [
        "def survived(line):\n",
        "  return tf.not_equal(tf.strings.substr(line, 0, 1), \"0\")\n",
        "\n",
        "survivors = titanic_lines.skip(1).filter(survived)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odQ4618h1XqD"
      },
      "outputs": [],
      "source": [
        "for line in survivors.take(10):\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5z5B11UjDTd"
      },
      "source": [
        "### Consumindo dados CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChDHNi3qbDch"
      },
      "source": [
        "Consulte os tutoriais [Carregando arquivos CSV](../tutorials/load_data/csv.ipynb) e [Carregando DataFrames Pandas](../tutorials/load_data/pandas_dataframe.ipynb) para mais exemplos.\n",
        "\n",
        "O formato de arquivo CSV é um formato popular para armazenar dados tabulares em texto simples.\n",
        "\n",
        "Por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj28j5u49Bjm"
      },
      "outputs": [],
      "source": [
        "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghvtmW40LM0B"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(titanic_file)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9uBqt5oGsR-"
      },
      "source": [
        "Se seus dados couberem na memória o mesmo método `Dataset.from_tensor_slices` funciona em dicionários, permitindo que esses dados sejam facilmente importados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmAMCiPJA0qO"
      },
      "outputs": [],
      "source": [
        "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
        "\n",
        "for feature_batch in titanic_slices.take(1):\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"  {!r:20s}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47yippqaHFk6"
      },
      "source": [
        "Uma abordagem mais escalável é carregar do disco conforme necessário.\n",
        "\n",
        "O módulo `tf.data` fornece métodos para extrair registros de um ou mais arquivos CSV que estejam em conformidade com a [RFC 4180](https://tools.ietf.org/html/rfc4180).\n",
        "\n",
        "A função `tf.data.experimental.make_csv_dataset` é a interface de alto nível para leitura de conjuntos de arquivos CSV. Ela suporta inferência de tipo de coluna e muitos outros recursos, como lote e embaralhamento, para simplificar o uso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHUDrM_s_brq"
      },
      "outputs": [],
      "source": [
        "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
        "    titanic_file, batch_size=4,\n",
        "    label_name=\"survived\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsZfhz79_Wlg"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in titanic_batches.take(1):\n",
        "  print(\"'survived': {}\".format(label_batch))\n",
        "  print(\"features:\")\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"  {!r:20s}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_5N7CdNGYAa"
      },
      "source": [
        "Você pode usar o argumento `select_columns` se precisar apenas de um subconjunto das colunas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9KNHyDwF2Sc"
      },
      "outputs": [],
      "source": [
        "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
        "    titanic_file, batch_size=4,\n",
        "    label_name=\"survived\", select_columns=['class', 'fare', 'survived'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C2uosFnGIT8"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in titanic_batches.take(1):\n",
        "  print(\"'survived': {}\".format(label_batch))\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"  {!r:20s}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSVgJJ1HJD6M"
      },
      "source": [
        "Há também uma classe `experimental.CsvDataset` de nível inferior que oferece um controle mais refinado. Mas ela não suporta inferência de tipo de coluna. Em vez disso, você precisa especificar o tipo de cada coluna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP1Y_NXA8bYl"
      },
      "outputs": [],
      "source": [
        "titanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string]\n",
        "dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True)\n",
        "\n",
        "for line in dataset.take(10):\n",
        "  print([item.numpy() for item in line])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZSuLVsTbDcj"
      },
      "source": [
        "Se algumas colunas estiverem vazias, esta interface de baixo nível permite fornecer valores padrão em vez de tipos de colunas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qry-g90FMo2I"
      },
      "outputs": [],
      "source": [
        "%%writefile missing.csv\n",
        "1,2,3,4\n",
        ",2,3,4\n",
        "1,,3,4\n",
        "1,2,,4\n",
        "1,2,3,\n",
        ",,,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5_hbiE9bDck"
      },
      "outputs": [],
      "source": [
        "# Creates a dataset that reads all of the records from two CSV files, each with\n",
        "# four float columns which may have missing values.\n",
        "\n",
        "record_defaults = [999,999,999,999]\n",
        "dataset = tf.data.experimental.CsvDataset(\"missing.csv\", record_defaults)\n",
        "dataset = dataset.map(lambda *items: tf.stack(items))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__jc7iD9M9FC"
      },
      "outputs": [],
      "source": [
        "for line in dataset:\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_4g0cIvbDcl"
      },
      "source": [
        "Por padrão, um `CsvDataset` produz *cada* coluna de *cada* linha do arquivo, o que pode não ser desejável, por exemplo, se o arquivo começar com uma linha de cabeçalho que deve ser ignorada ou se algumas colunas não forem obrigatórias na entrada. Essas linhas e campos podem ser removidos com os argumentos `header` e `select_cols` respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2IF_K0obDcm"
      },
      "outputs": [],
      "source": [
        "# Creates a dataset that reads all of the records from two CSV files with\n",
        "# headers, extracting float data from columns 2 and 4.\n",
        "record_defaults = [999, 999] # Only provide defaults for the selected columns\n",
        "dataset = tf.data.experimental.CsvDataset(\"missing.csv\", record_defaults, select_cols=[1, 3])\n",
        "dataset = dataset.map(lambda *items: tf.stack(items))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5aLprDeRNb0"
      },
      "outputs": [],
      "source": [
        "for line in dataset:\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJfhb03koVN"
      },
      "source": [
        "### Consumindo conjuntos de arquivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAO7SZDSk57_"
      },
      "source": [
        "Existem muitos datasets distribuídos como um conjunto de arquivos, onde cada arquivo é um exemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dZwN3CS-jV2"
      },
      "outputs": [],
      "source": [
        "flowers_root = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)\n",
        "flowers_root = pathlib.Path(flowers_root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4099UU8n-jHP"
      },
      "source": [
        "Observação: essas imagens são licenciadas de acordo com a CC-BY. Veja LICENSE.txt para mais detalhes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCyTYpmDs_jE"
      },
      "source": [
        "O diretório raiz contém um diretório para cada classe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2iCXsHu6jJH"
      },
      "outputs": [],
      "source": [
        "for item in flowers_root.glob(\"*\"):\n",
        "  print(item.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylj9fgkamgWZ"
      },
      "source": [
        "Os arquivos em cada diretório de classe são exemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAkQp5uxoINu"
      },
      "outputs": [],
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
        "\n",
        "for f in list_ds.take(5):\n",
        "  print(f.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91CPfUUJ_8SZ"
      },
      "source": [
        "Leia os dados usando a função `tf.io.read_file` e extraia o rótulo do caminho, retornando pares `(image, label)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xhBRgvNqRRe"
      },
      "outputs": [],
      "source": [
        "def process_path(file_path):\n",
        "  label = tf.strings.split(file_path, os.sep)[-2]\n",
        "  return tf.io.read_file(file_path), label\n",
        "\n",
        "labeled_ds = list_ds.map(process_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxrl0lGdnpRz"
      },
      "outputs": [],
      "source": [
        "for image_raw, label_text in labeled_ds.take(1):\n",
        "  print(repr(image_raw.numpy()[:100]))\n",
        "  print()\n",
        "  print(label_text.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEh46Ee0oSH5"
      },
      "source": [
        "<!--\n",
        "TODO(mrry): Add this section.\n",
        "\n",
        "### Handling text data with unusual sizes\n",
        "-->\n",
        "\n",
        "## Lotes de elementos de um dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR-2xY-8oSH4"
      },
      "source": [
        "### Lotes simples\n",
        "\n",
        "A forma mais simples de criar um lote é empilhando `n` elementos consecutivos de um dataset num único elemento. A transformação `Dataset.batch()` faz exatamente isso, com as mesmas restrições do operador `tf.stack()`, aplicadas a cada componente dos elementos: ou seja, para cada componente *i*, todos os elementos devem ter um tensor do mesmo formato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB7KeceLoSH0"
      },
      "outputs": [],
      "source": [
        "inc_dataset = tf.data.Dataset.range(100)\n",
        "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
        "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
        "batched_dataset = dataset.batch(4)\n",
        "\n",
        "for batch in batched_dataset.take(4):\n",
        "  print([arr.numpy() for arr in batch])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlV1tpFdoSH0"
      },
      "source": [
        "Embora `tf.data` tente propagar informações de formato, as configurações padrão de `Dataset.batch` resultam num tamanho de lote desconhecido porque o último lote pode não estar cheio. Observe os `None` no formato:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN7hn7OBoSHx"
      },
      "outputs": [],
      "source": [
        "batched_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It1fPA3NoSHw"
      },
      "source": [
        "Use o argumento `drop_remainder` para ignorar o último lote e obter a propagação completa do formato:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BycWC7WCoSHt"
      },
      "outputs": [],
      "source": [
        "batched_dataset = dataset.batch(7, drop_remainder=True)\n",
        "batched_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj9nRxFZoSHs"
      },
      "source": [
        "### Lotes de tensores com preenchimento\n",
        "\n",
        "A receita acima funciona para tensores que têm todos o mesmo tamanho. No entanto, muitos modelos (incluindo modelos de sequência) trabalham com dados de entrada que podem ter tamanhos variados (por exemplo, sequências de comprimentos diferentes). Para lidar com esse caso, a transformação `Dataset.padded_batch` permite agrupar em lote tensores de diferentes formatos especificando uma ou mais dimensões nas quais eles podem ser preenchidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kycwO0JooSHn"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.range(100)\n",
        "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
        "dataset = dataset.padded_batch(4, padded_shapes=(None,))\n",
        "\n",
        "for batch in dataset.take(2):\n",
        "  print(batch.numpy())\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl3yhth1oSHm"
      },
      "source": [
        "A transformação `Dataset.padded_batch` permite definir preenchimentos diferentes para cada dimensão de cada componente e pode ter comprimento variável (representado por `None` no exemplo acima) ou comprimento constante. Também é possível substituir o valor do preenchimento, cujo padrão é 0.\n",
        "\n",
        "<!--\n",
        "TODO(mrry): Add this section.\n",
        "\n",
        "### Dense ragged -> tf.SparseTensor\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8zbAxMwoSHl"
      },
      "source": [
        "## Workflows de treinamento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnlhzF_AoSHk"
      },
      "source": [
        "### Processando múltiplas épocas\n",
        "\n",
        "A API `tf.data` oferece duas formas principais de processar múltiplas épocas dos mesmos dados.\n",
        "\n",
        "A maneira mais simples de iterar sobre um dataset em múltiplas épocas é usar a transformação `Dataset.repeat()`. Primeiro, crie um conjunto de dados do Titanic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tODHZzRoSHg"
      },
      "outputs": [],
      "source": [
        "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic_lines = tf.data.TextLineDataset(titanic_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMO6mlXxoSHc"
      },
      "outputs": [],
      "source": [
        "def plot_batch_sizes(ds):\n",
        "  batch_sizes = [batch.shape[0] for batch in ds]\n",
        "  plt.bar(range(len(batch_sizes)), batch_sizes)\n",
        "  plt.xlabel('Batch number')\n",
        "  plt.ylabel('Batch size')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfVzmqL7oSHa"
      },
      "source": [
        "Aplicando a transformação `Dataset.repeat()` sem argumentos repetirá a entrada indefinidamente.\n",
        "\n",
        "A transformação `Dataset.repeat` concatena seus argumentos sem sinalizar o fim de uma época e o início da época seguinte. Por causa disso, um `Dataset.batch` aplicado após `Dataset.repeat` produzirá lotes que ultrapassam os limites da época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ0G1cztoSHX"
      },
      "outputs": [],
      "source": [
        "titanic_batches = titanic_lines.repeat(3).batch(128)\n",
        "plot_batch_sizes(titanic_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moH-4gBEoSHW"
      },
      "source": [
        "Se você precisar de uma separação clara de épocas, coloque `Dataset.batch` antes do repeat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmbmdK1qoSHS"
      },
      "outputs": [],
      "source": [
        "titanic_batches = titanic_lines.batch(128).repeat(3)\n",
        "\n",
        "plot_batch_sizes(titanic_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlEM5f9loSHR"
      },
      "source": [
        "Se você quiser realizar uma computação personalizada (por exemplo, para coletar estatísticas) ao final de cada época, é mais simples reiniciar a iteração do dataset em cada época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyekyeY7oSHO"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "dataset = titanic_lines.batch(128)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch in dataset:\n",
        "    print(batch.shape)\n",
        "  print(\"End of epoch: \", epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bci79WCoSHN"
      },
      "source": [
        "### Embaralhando dados de entrada aleatoriamente\n",
        "\n",
        "A transformação `Dataset.shuffle()` mantém um buffer de tamanho fixo e escolhe o próximo elemento de maneira uniforme e aleatória a partir desse buffer.\n",
        "\n",
        "Observação: embora um buffer_size grande seja embaralhado de maneira mais completa, ele pode consumir muita memória e demorar um tempo considerável para ser preenchido. Considere usar `Dataset.interleave` entre arquivos se isso se tornar um problema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YvXr-qeoSHL"
      },
      "source": [
        "Adicione um índice ao dataset para ver o efeito:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io4iJH1toSHI"
      },
      "outputs": [],
      "source": [
        "lines = tf.data.TextLineDataset(titanic_file)\n",
        "counter = tf.data.experimental.Counter()\n",
        "\n",
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "dataset = dataset.shuffle(buffer_size=100)\n",
        "dataset = dataset.batch(20)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6tNYRcsoSHH"
      },
      "source": [
        "Já que o `buffer_size` é 100 e o tamanho do lote é 20, o primeiro lote não contém elementos com índice superior a 120."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayM3FFFAoSHC"
      },
      "outputs": [],
      "source": [
        "n,line_batch = next(iter(dataset))\n",
        "print(n.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLrfIjTHoSHB"
      },
      "source": [
        "Tal como acontece com `Dataset.batch`, a ordem relativa a `Dataset.repeat` é importante.\n",
        "\n",
        "`Dataset.shuffle` não sinaliza o fim de uma época até que o buffer aleatório esteja vazio. Portanto, um shuffle colocado antes de um repeat mostrará todos os elementos de uma época antes de passar para a seguinte:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX3pe7zZoSG6"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "shuffled = dataset.shuffle(buffer_size=100).batch(10).repeat(2)\n",
        "\n",
        "print(\"Here are the item ID's near the epoch boundary:\\n\")\n",
        "for n, line_batch in shuffled.skip(60).take(5):\n",
        "  print(n.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9hlE-lGoSGz"
      },
      "outputs": [],
      "source": [
        "shuffle_repeat = [n.numpy().mean() for n, line_batch in shuffled]\n",
        "plt.plot(shuffle_repeat, label=\"shuffle().repeat()\")\n",
        "plt.ylabel(\"Mean item ID\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UucIgCxWoSGx"
      },
      "source": [
        "Mas um repeat antes de um shuffle mistura os limites da época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhxb5YGZoSGm"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "shuffled = dataset.repeat(2).shuffle(buffer_size=100).batch(10)\n",
        "\n",
        "print(\"Here are the item ID's near the epoch boundary:\\n\")\n",
        "for n, line_batch in shuffled.skip(55).take(15):\n",
        "  print(n.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAM4cbpZoSGL"
      },
      "outputs": [],
      "source": [
        "repeat_shuffle = [n.numpy().mean() for n, line_batch in shuffled]\n",
        "\n",
        "plt.plot(shuffle_repeat, label=\"shuffle().repeat()\")\n",
        "plt.plot(repeat_shuffle, label=\"repeat().shuffle()\")\n",
        "plt.ylabel(\"Mean item ID\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ianlfbrxbDco"
      },
      "source": [
        "## Pré-processamento de dados\n",
        "\n",
        "A transformação `Dataset.map(f)` produz um novo dataset aplicando uma determinada função `f` a cada elemento do dataset de entrada. É baseado na função [`map()`](https://en.wikipedia.org/wiki/Map_(higher-order_function)) que é frequentemente aplicada a listas (e outras estruturas) em linguagens de programação funcionais. A função `f` pega os objetos `tf.Tensor` que representam um único elemento na entrada e retorna os objetos `tf.Tensor` que representarão um único elemento no novo dataset. Sua implementação usa operações padrão do TensorFlow para transformar um elemento em outro.\n",
        "\n",
        "Esta seção cobre exemplos comuns de como usar `Dataset.map()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXw1IZVdbDcq"
      },
      "source": [
        "### Decodificando e redimensionando dados de imagem\n",
        "\n",
        "<!-- TODO(markdaoust): link to image augmentation when it exists -->\n",
        "\n",
        "Ao treinar uma rede neural com dados de imagens do mundo real, muitas vezes é necessário converter imagens de tamanhos diferentes em um tamanho comum, para que possam ser agrupadas em lotes de tamanho fixo.\n",
        "\n",
        "Reconstrua o dataset de nomes de arquivos de flores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMGlj8V-u-NH"
      },
      "outputs": [],
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyhZLB8N5jBm"
      },
      "source": [
        "Escreva uma função que manipule os elementos do dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZObC0debDcr"
      },
      "outputs": [],
      "source": [
        "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
        "# to a fixed shape.\n",
        "def parse_image(filename):\n",
        "  parts = tf.strings.split(filename, os.sep)\n",
        "  label = parts[-2]\n",
        "\n",
        "  image = tf.io.read_file(filename)\n",
        "  image = tf.io.decode_jpeg(image)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  image = tf.image.resize(image, [128, 128])\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0dVJlCA5qHA"
      },
      "source": [
        "Teste para ver se funciona."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8xuN_HBzGup"
      },
      "outputs": [],
      "source": [
        "file_path = next(iter(list_ds))\n",
        "image, label = parse_image(file_path)\n",
        "\n",
        "def show(image, label):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(label.numpy().decode('utf-8'))\n",
        "  plt.axis('off')\n",
        "\n",
        "show(image, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3P8N-S55vDu"
      },
      "source": [
        "Faça o mapeamento com o dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzO8LI_H5Sk_"
      },
      "outputs": [],
      "source": [
        "images_ds = list_ds.map(parse_image)\n",
        "\n",
        "for image, label in images_ds.take(2):\n",
        "  show(image, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ff7IqB9bDcs"
      },
      "source": [
        "### Aplicando lógica Python arbitrária\n",
        "\n",
        "Por questões de desempenho, use operações do TensorFlow para pré-processar seus dados sempre que possível. No entanto, às vezes é útil chamar bibliotecas Python externas ao processar seus dados de entrada. Você pode usar a operação `tf.py_function` em uma transformação `Dataset.map`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u7CeA67DU8"
      },
      "source": [
        "Por exemplo, se você quiser aplicar uma rotação aleatória, o módulo `tf.image` possui apenas `tf.image.rot90`, o que não é muito útil para aumento de imagem.\n",
        "\n",
        "Observação: O `tensorflow_addons` tem uma `rotate` compatível com TensorFlow em `tensorflow_addons.image.rotate`.\n",
        "\n",
        "Para demonstrar a `tf.py_function`, tente usar a função `scipy.ndimage.rotate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBUmbERt7Czz"
      },
      "outputs": [],
      "source": [
        "import scipy.ndimage as ndimage\n",
        "\n",
        "def random_rotate_image(image):\n",
        "  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wEyL7bS9S6t"
      },
      "outputs": [],
      "source": [
        "image, label = next(iter(images_ds))\n",
        "image = random_rotate_image(image)\n",
        "show(image, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxVx7z-ABNyq"
      },
      "source": [
        "Para usar esta função com `Dataset.map` as mesmas ressalvas se aplicam a `Dataset.from_generator`, você precisa descrever os formatos e tipos de retorno ao aplicar a função:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn2nIu92BMp0"
      },
      "outputs": [],
      "source": [
        "def tf_random_rotate_image(image, label):\n",
        "  im_shape = image.shape\n",
        "  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])\n",
        "  image.set_shape(im_shape)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWPqKbTnbDct"
      },
      "outputs": [],
      "source": [
        "rot_ds = images_ds.map(tf_random_rotate_image)\n",
        "\n",
        "for image, label in rot_ds.take(2):\n",
        "  show(image, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykx59-cMBwOT"
      },
      "source": [
        "### Processando mensagens de buffer de protocolo `tf.Example`\n",
        "\n",
        "Muitos pipelines de entrada extraem mensagens de buffer de protocolo `tf.train.Example` de um formato TFRecord. Cada registro `tf.train.Example` contém uma ou mais \"características\", e o pipeline de entrada normalmente converte essas características em tensores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wnE134b32KY"
      },
      "outputs": [],
      "source": [
        "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\n",
        "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGypdgYOlXZz"
      },
      "source": [
        "Você pode trabalhar com protos `tf.train.Example` fora de um `tf.data.Dataset` para entender os dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4znsVNqnF73C"
      },
      "outputs": [],
      "source": [
        "raw_example = next(iter(dataset))\n",
        "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "feature = parsed.features.feature\n",
        "raw_img = feature['image/encoded'].bytes_list.value[0]\n",
        "img = tf.image.decode_png(raw_img)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "_ = plt.title(feature[\"image/text\"].bytes_list.value[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwzqp8IGC_vQ"
      },
      "outputs": [],
      "source": [
        "raw_example = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2X1dQNfC8Lu"
      },
      "outputs": [],
      "source": [
        "def tf_parse(eg):\n",
        "  example = tf.io.parse_example(\n",
        "      eg[tf.newaxis], {\n",
        "          'image/encoded': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
        "          'image/text': tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "      })\n",
        "  return example['image/encoded'][0], example['image/text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGJhKDp_61A_"
      },
      "outputs": [],
      "source": [
        "img, txt = tf_parse(raw_example)\n",
        "print(txt.numpy())\n",
        "print(repr(img.numpy()[:20]), \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vFIUFzD5qIC"
      },
      "outputs": [],
      "source": [
        "decoded = dataset.map(tf_parse)\n",
        "decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRYNYkEej7Ix"
      },
      "outputs": [],
      "source": [
        "image_batch, text_batch = next(iter(decoded.batch(10)))\n",
        "image_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1n0UBeczit"
      },
      "source": [
        "<a id=\"time_series_windowing\"></a>\n",
        "\n",
        "### Janelas de série temporal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0JMgvXEz9y1"
      },
      "source": [
        "Para um exemplo de série temporal de ponta a ponta, consulte: [Previsão em séries temporais](../../tutorials/structured_data/time_series.ipynb) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzBABBkAkkVJ"
      },
      "source": [
        "Os dados de séries temporais geralmente são organizados com o eixo temporal intacto.\n",
        "\n",
        "Use um `Dataset.range` simples para demonstrar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTQgo49skjuY"
      },
      "outputs": [],
      "source": [
        "range_ds = tf.data.Dataset.range(100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6GLGhxgpazJ"
      },
      "source": [
        "Normalmente, os modelos baseados neste tipo de dados desejarão um intervalo de tempo contíguo.\n",
        "\n",
        "A abordagem mais simples seria agrupar os dados em lote:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETqB7QvTCNty"
      },
      "source": [
        "#### Usando `batch`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSs9XqwQpvIN"
      },
      "outputs": [],
      "source": [
        "batches = range_ds.batch(10, drop_remainder=True)\n",
        "\n",
        "for batch in batches.take(5):\n",
        "  print(batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgb2qikEtk5W"
      },
      "source": [
        "Ou, para fazer previsões densas um passo no futuro, você pode mudar as características e os rótulos um passo em relação um ao outro:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47XfwPhetkIN"
      },
      "outputs": [],
      "source": [
        "def dense_1_step(batch):\n",
        "  # Shift features and labels one step relative to each other.\n",
        "  return batch[:-1], batch[1:]\n",
        "\n",
        "predict_dense_1_step = batches.map(dense_1_step)\n",
        "\n",
        "for features, label in predict_dense_1_step.take(3):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjsXuINKqsS_"
      },
      "source": [
        "Para prever uma janela inteira em vez de um deslocamento fixo, você pode dividir os lotes em duas partes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMmkQB1Gqo6x"
      },
      "outputs": [],
      "source": [
        "batches = range_ds.batch(15, drop_remainder=True)\n",
        "\n",
        "def label_next_5_steps(batch):\n",
        "  return (batch[:-5],   # Inputs: All except the last 5 steps\n",
        "          batch[-5:])   # Labels: The last 5 steps\n",
        "\n",
        "predict_5_steps = batches.map(label_next_5_steps)\n",
        "\n",
        "for features, label in predict_5_steps.take(3):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a611Qr3jlhl"
      },
      "source": [
        "Para permitir alguma sobreposição entre os recursos de um lote e os rótulos de outro, use `Dataset.zip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11dF3wyFjk2J"
      },
      "outputs": [],
      "source": [
        "feature_length = 10\n",
        "label_length = 3\n",
        "\n",
        "features = range_ds.batch(feature_length, drop_remainder=True)\n",
        "labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:label_length])\n",
        "\n",
        "predicted_steps = tf.data.Dataset.zip((features, labels))\n",
        "\n",
        "for features, label in predicted_steps.take(5):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adew3o2mCURC"
      },
      "source": [
        "#### Usando `window`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF6pEdlduq8E"
      },
      "source": [
        "Embora o uso `Dataset.batch` funcione, há situações em que você poderá precisar de um controle mais preciso. O método `Dataset.window` garante controle total, mas requer alguns cuidados: ele retorna um `Dataset` de `Datasets`. Vá para a seção [Estrutura do dataset](#dataset_structure) para mais detalhes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEI2W_EBw2OX"
      },
      "outputs": [],
      "source": [
        "window_size = 5\n",
        "\n",
        "windows = range_ds.window(window_size, shift=1)\n",
        "for sub_ds in windows.take(5):\n",
        "  print(sub_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r82hWdk4x-46"
      },
      "source": [
        "O método `Dataset.flat_map` pode pegar um dataset de datasets e achatá-lo num único dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB8AI03mnF8u"
      },
      "outputs": [],
      "source": [
        " for x in windows.flat_map(lambda x: x).take(30):\n",
        "   print(x.numpy(), end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgLIwq9Anc34"
      },
      "source": [
        "Em praticamente todas as situações, você vai querer primeiro fazer o `Dataset.batch` do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j_y84rmyVQa"
      },
      "outputs": [],
      "source": [
        "def sub_to_batch(sub):\n",
        "  return sub.batch(window_size, drop_remainder=True)\n",
        "\n",
        "for example in windows.flat_map(sub_to_batch).take(5):\n",
        "  print(example.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVugrmND3Grp"
      },
      "source": [
        "Agora, você pode ver que o argumento `shift` controla o quanto cada janela se move.\n",
        "\n",
        "Juntando tudo isso, você pode escrever esta função:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdFRv_0D4FqW"
      },
      "outputs": [],
      "source": [
        "def make_window_dataset(ds, window_size=5, shift=1, stride=1):\n",
        "  windows = ds.window(window_size, shift=shift, stride=stride)\n",
        "\n",
        "  def sub_to_batch(sub):\n",
        "    return sub.batch(window_size, drop_remainder=True)\n",
        "\n",
        "  windows = windows.flat_map(sub_to_batch)\n",
        "  return windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iVxcVfEdf5b"
      },
      "outputs": [],
      "source": [
        "ds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3)\n",
        "\n",
        "for example in ds.take(10):\n",
        "  print(example.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMGMTPQ4w8pr"
      },
      "source": [
        "Depois disso fica fácil extrair rótulos, como antes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0fPfZkZw6j_"
      },
      "outputs": [],
      "source": [
        "dense_labels_ds = ds.map(dense_1_step)\n",
        "\n",
        "for inputs,labels in dense_labels_ds.take(3):\n",
        "  print(inputs.numpy(), \"=>\", labels.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyi_-ft0kvy4"
      },
      "source": [
        "### Reamostragem\n",
        "\n",
        "Ao trabalhar com um dataset que é pouco balanceado quanto a classes, você poderá querer fazer uma reamostragem do dataset. `tf.data` fornece dois métodos para fazer isso. O dataset credit card fraud é um ótimo exemplo desse tipo de problema.\n",
        "\n",
        "Observação: Vá para [Classificação de dados não balanceados](../tutorials/structured_data/imbalanced_data.ipynb) para um tutorial completo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2e8dxVUlFHO"
      },
      "outputs": [],
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip',\n",
        "    fname='creditcard.zip',\n",
        "    extract=True)\n",
        "\n",
        "csv_path = zip_path.replace('.zip', '.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhkkM4Wx75S_"
      },
      "outputs": [],
      "source": [
        "creditcard_ds = tf.data.experimental.make_csv_dataset(\n",
        "    csv_path, batch_size=1024, label_name=\"Class\",\n",
        "    # Set the column types: 30 floats and an int.\n",
        "    column_defaults=[float()]*30+[int()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8O47EmHlxYX"
      },
      "source": [
        "Agora, verifique a distribuição das classes. Ela tem um grande desvio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8-Ss69XlzXD"
      },
      "outputs": [],
      "source": [
        "def count(counts, batch):\n",
        "  features, labels = batch\n",
        "  class_1 = labels == 1\n",
        "  class_1 = tf.cast(class_1, tf.int32)\n",
        "\n",
        "  class_0 = labels == 0\n",
        "  class_0 = tf.cast(class_0, tf.int32)\n",
        "\n",
        "  counts['class_0'] += tf.reduce_sum(class_0)\n",
        "  counts['class_1'] += tf.reduce_sum(class_1)\n",
        "\n",
        "  return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1a3t_B4l_f6"
      },
      "outputs": [],
      "source": [
        "counts = creditcard_ds.take(10).reduce(\n",
        "    initial_state={'class_0': 0, 'class_1': 0},\n",
        "    reduce_func = count)\n",
        "\n",
        "counts = np.array([counts['class_0'].numpy(),\n",
        "                   counts['class_1'].numpy()]).astype(np.float32)\n",
        "\n",
        "fractions = counts/counts.sum()\n",
        "print(fractions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1b8lFhSnDdv"
      },
      "source": [
        "Uma abordagem comum para treinar com um dataset desbalanceado é balanceá-lo. `tf.data` inclui alguns métodos que permitem realizar este workflow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8jQWsgMnjQG"
      },
      "source": [
        "#### Reamostragem de datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov14SRrQyQE3"
      },
      "source": [
        "Uma abordagem para fazer a reamostragem de um conjunto de dados é usar `sample_from_datasets`. Isto é mais útil quando você tem um `tf.data.Dataset` separado para cada classe.\n",
        "\n",
        "Aqui, basta usar o filtro para gerá-los a partir dos dados do dataset credit card fraud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YKfCPa-nioA"
      },
      "outputs": [],
      "source": [
        "negative_ds = (\n",
        "  creditcard_ds\n",
        "    .unbatch()\n",
        "    .filter(lambda features, label: label==0)\n",
        "    .repeat())\n",
        "positive_ds = (\n",
        "  creditcard_ds\n",
        "    .unbatch()\n",
        "    .filter(lambda features, label: label==1)\n",
        "    .repeat())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FNd3sQjzl9-"
      },
      "outputs": [],
      "source": [
        "for features, label in positive_ds.batch(10).take(1):\n",
        "  print(label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxLAr-7p0ATX"
      },
      "source": [
        "Para usar `tf.data.Dataset.sample_from_datasets`, passe os datasets e o peso de cada um:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjdPVIFCngOb"
      },
      "outputs": [],
      "source": [
        "balanced_ds = tf.data.Dataset.sample_from_datasets(\n",
        "    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K4ObOms082B"
      },
      "source": [
        "Agora o dataset produz exemplos de cada classe com probabilidade de 50/50:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myvkw21Rz-fH"
      },
      "outputs": [],
      "source": [
        "for features, labels in balanced_ds.take(10):\n",
        "  print(labels.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUTE3eb9nckY"
      },
      "source": [
        "#### Reamostragem de rejeição"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ9ezkK6irMD"
      },
      "source": [
        "Um problema com a abordagem `Dataset.sample_from_datasets` acima é que ela precisa de um `tf.data.Dataset` separado por classe. Você poderia usar `Dataset.filter` para criar esses dois datasets, mas isso resulta no carregamento de todos os dados duas vezes.\n",
        "\n",
        "O método `tf.data.Dataset.rejection_resample` pode ser aplicado a um dataset para rebalanceá-lo, carregando-o apenas uma vez. Os elementos serão eliminados ou repetidos para alcançar o balanceamento.\n",
        "\n",
        "O método `rejection_resample` recebe um argumento `class_func`. Este `class_func` é aplicado a cada elemento do dataset e é usado para determinar a qual classe um exemplo pertence para fins de balanceamento.\n",
        "\n",
        "O objetivo aqui é balancear a distribuição dos rótulos, e os elementos de `creditcard_ds` já são pares `(features, label)`. Então `class_func` só precisa retornar esses rótulos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC_Cuzw8lhI5"
      },
      "outputs": [],
      "source": [
        "def class_func(features, label):\n",
        "  return label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxJrOZVToGuE"
      },
      "source": [
        "O método de reamostragem lida com exemplos individuais, portanto, neste caso, você deve fazer `unbatch` do lote do dataset antes de aplicar esse método.\n",
        "\n",
        "O método precisa de uma distribuição-alvo e, opcionalmente, de uma estimativa de distribuição inicial como entradas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY6VIhr3oGHG"
      },
      "outputs": [],
      "source": [
        "resample_ds = (\n",
        "    creditcard_ds\n",
        "    .unbatch()\n",
        "    .rejection_resample(class_func, target_dist=[0.5,0.5],\n",
        "                        initial_dist=fractions)\n",
        "    .batch(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-HnC1s8idqV"
      },
      "source": [
        "O método `rejection_resample` retorna pares `(class, example)` onde a `class` é a saída de `class_func`. Neste caso, o `example` já era um par `(feature, label)`, então use `map` para descartar a cópia extra dos rótulos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpfCGU6BiaZq"
      },
      "outputs": [],
      "source": [
        "balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3d2jyEhx9kD"
      },
      "source": [
        "Agora o dataset produz exemplos de cada classe com probabilidade de 50/50:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGLYChBQwkDV"
      },
      "outputs": [],
      "source": [
        "for features, labels in balanced_ds.take(10):\n",
        "  print(labels.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYFKQx3bUBeU"
      },
      "source": [
        "## Checkpoints do iterador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOGg1UFhUE4z"
      },
      "source": [
        "O Tensorflow oferece suporte à [tomada de checkpoints](./checkpoint.ipynb) para que, quando o processo de treinamento for reiniciado, ele possa restaurar o checkpoint mais recente para recuperar a maior parte de seu progresso. Além de fazer checkpointing das variáveis ​​do modelo, você também pode fazer checkpointing do progresso do iterador do dataset. Isso pode ser útil se você tiver um dataset grande e não quiser iniciá-lo do zero a cada reinicialização. Observe, entretanto, que os checkpoints do iterador podem ser grandes, pois transformações como `Dataset.shuffle` e `Dataset.prefetch` requerem elementos de buffer dentro do iterador.\n",
        "\n",
        "Para incluir seu iterador num checkpoint, passe o iterador para o construtor `tf.train.Checkpoint`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fsm9wvKUsNC"
      },
      "outputs": [],
      "source": [
        "range_ds = tf.data.Dataset.range(20)\n",
        "\n",
        "iterator = iter(range_ds)\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)\n",
        "manager = tf.train.CheckpointManager(ckpt, '/tmp/my_ckpt', max_to_keep=3)\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])\n",
        "\n",
        "save_path = manager.save()\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])\n",
        "\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxWglTwX9Fex"
      },
      "source": [
        "Observação: Não é possível aplicar um checkpoint em um iterador que dependa de um estado externo, como `tf.py_function`. Tentar fazer isso gerará uma exceção reclamando do estado externo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLRdedPpbDdD"
      },
      "source": [
        "## Usando `tf.data` com `tf.keras`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTQe8daMcgFz"
      },
      "source": [
        "A API `tf.keras` simplifica muitos aspectos da criação e execução de modelos de aprendizado de máquina. Suas APIs `Model.fit`, `Model.evaluate` e `Model.predict` suportam datasets como entradas. Aqui está a configuração rápida de um dataset e modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bfjqm0hOfES"
      },
      "outputs": [],
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "images, labels = train\n",
        "images = images/255.0\n",
        "labels = labels.astype(np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDhF3rGnbDdD"
      },
      "outputs": [],
      "source": [
        "fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdogg8CfHs-G"
      },
      "source": [
        "Passar um dataset de pares `(feature, label)` é tudo o que você precisa para `Model.fit` e `Model.evaluate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cu4kPzOHnlt"
      },
      "outputs": [],
      "source": [
        "model.fit(fmnist_train_ds, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzpAQfJMJF41"
      },
      "source": [
        "Se você passar um dataset infinito, por exemplo, ao chamar `Dataset.repeat`, você só precisa passar também o argumento `steps_per_epoch`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp1BpzlyJinb"
      },
      "outputs": [],
      "source": [
        "model.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTLsw_nqJpTw"
      },
      "source": [
        "Para a avaliação você pode passar o número de passos de avaliação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnlRHlaL-XUI"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(fmnist_train_ds)\n",
        "print(\"Loss :\", loss)\n",
        "print(\"Accuracy :\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8UBU3CJKEA4"
      },
      "source": [
        "Para datasets longos, defina o número de passos a serem avaliados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVgamf9HKDon"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)\n",
        "print(\"Loss :\", loss)\n",
        "print(\"Accuracy :\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZYhJ_YSIl6w"
      },
      "source": [
        "Os rótulos não são necessários ao chamar `Model.predict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "343lXJ-pIqWD"
      },
      "outputs": [],
      "source": [
        "predict_ds = tf.data.Dataset.from_tensor_slices(images).batch(32)\n",
        "result = model.predict(predict_ds, steps = 10)\n",
        "print(result.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfzZORwLI202"
      },
      "source": [
        "Mas os rótulos serão ignorados se você passar um dataset que os contenha:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgQJTPrT-2WF"
      },
      "outputs": [],
      "source": [
        "result = model.predict(fmnist_train_ds, steps = 10)\n",
        "print(result.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
