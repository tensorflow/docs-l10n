{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5620ee4049e"
      },
      "source": [
        "# Personalize o que acontece em Model.fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a56ffedf331"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Veja em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/customizing_what_happens_in_fit.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/keras-team/keras-io/blob/master/guides/customizing_what_happens_in_fit.py\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte em GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/keras/customizing_what_happens_in_fit.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ebb4e65ef9b"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "Quando você está fazendo aprendizado supervisionado, pode usar `fit()` e tudo funciona sem problemas.\n",
        "\n",
        "Quando você precisa escrever seu próprio loop de treinamento do zero, pode usar o `GradientTape` e assumir o controle de cada pequeno detalhe.\n",
        "\n",
        "Mas e se você precisar de um algoritmo de treinamento personalizado, mas ainda quiser se beneficiar dos recursos convenientes de `fit()`, como callbacks, suporte de distribuição integrado ou fusão de etapas?\n",
        "\n",
        "Um princípio central do Keras é **a divulgação progressiva da complexidade**. Você sempre deve ser capaz de entrar em workflows de nível inferior de forma gradual. Você não deve despencar de um penhasco se a funcionalidade de alto nível não corresponder exatamente ao seu caso de uso. Você deve ser capaz de obter mais controle sobre os pequenos detalhes, mantendo uma quantidade proporcional de conveniência de alto nível.\n",
        "\n",
        "Quando precisar personalizar o que `fit()` faz, você deve **sobrepor a função de etapa de treinamento da classe `Model`**. Esta é a função chamada por `fit()` para cada lote de dados. Você poderá chamar `fit()` como de costume - e ela executará seu próprio algoritmo de aprendizado.\n",
        "\n",
        "Observe que esse padrão não impede que você construa modelos com a API Funcional. Você pode fazer isso se estiver criando modelos `Sequential`, modelos da Functional API ou modelos derivados em subclasses.\n",
        "\n",
        "Vamos ver como isso funciona."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2849e371b9b6"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Requer TensorFlow 2.2 ou posterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dadb6688663"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9022333acaa7"
      },
      "source": [
        "## Um primeiro exemplo simples\n",
        "\n",
        "Vamos começar com um exemplo simples:\n",
        "\n",
        "- Criamos uma nova classe que seja uma subclasse de `keras.Model` .\n",
        "- Simplesmente sobrescrevemos o método `train_step(self, data)`.\n",
        "- Retornamos um dicionário mapeando nomes de métrica (incluindo a perda) aos seus valores atuais.\n",
        "\n",
        "O argumento de entrada `data` é o que é passado ​​como dados de treinamento:\n",
        "\n",
        "- Se você passar arrays Numpy, chamando `fit(x, y, ...)`, então `data` será a tupla `(x, y)`\n",
        "- Se você passar um `tf.data.Dataset` , chamando `fit(dataset, ...)` , então `data` será o que será gerado pelo `dataset` em cada lote.\n",
        "\n",
        "No corpo do método `train_step`, implementamos uma atualização de treinamento comum, semelhante à que você já conhece. É importante observar que **calculamos a perda através de `self.compiled_loss`**, que empacota a(s) função(ões) de perda que foram passadas para `compile()`.\n",
        "\n",
        "Da mesma forma, chamamos `self.compiled_metrics.update_state(y, y_pred)` para atualizar o estado das métricas que foram passadas em `compile()` e consultamos os resultados de `self.metrics` no final para recuperar seus valores atuais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "060c8bf4150d"
      },
      "outputs": [],
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9d2cc7a7014"
      },
      "source": [
        "Vamos experimentar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e6bd7b554f6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Construct and compile an instance of CustomModel\n",
        "inputs = keras.Input(shape=(32,))\n",
        "outputs = keras.layers.Dense(1)(inputs)\n",
        "model = CustomModel(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Just use `fit` as usual\n",
        "x = np.random.random((1000, 32))\n",
        "y = np.random.random((1000, 1))\n",
        "model.fit(x, y, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a882cb6467d6"
      },
      "source": [
        "## Baixando o nível\n",
        "\n",
        "Naturalmente, você poderia simplesmente ignorar a passagem de uma função de perda em `compile()` e, em vez disso, fazer tudo *manualmente* em `train_step`. O mesmo vale para as métricas.\n",
        "\n",
        "Eis aqui um exemplo de nível mais baixo, que usa apenas `compile()` para configurar o otimizador:\n",
        "\n",
        "- Começamos criando instâncias `Metric` para rastrear nossa perda e uma pontuação MAE.\n",
        "- Implementamos um `train_step()` personalizado que atualiza o estado dessas métricas (chamando `update_state()` nelas) e, em seguida, consulta-as (via `result()`) para retornar seu valor médio atual, que será exibido pela barra de progresso e passado para qualquer callback.\n",
        "- Observe que precisaríamos chamar `reset_states()` em nossas métricas entre cada época! Caso contrário, chamar `result()` retornaria uma média desde o início do treinamento, enquanto normalmente trabalhamos com médias por época. Felizmente, o framework pode fazer isto por nós: basta listar qualquer métrica que você queira redefinir na propriedade `metrics` do modelo. O modelo chamará `reset_states()` em qualquer objeto listado aqui no início de cada época `fit()` ou no início de uma chamada `evaluate()` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2308abf5fe7d"
      },
      "outputs": [],
      "source": [
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
        "\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            # Compute our own loss\n",
        "            loss = keras.losses.mean_squared_error(y, y_pred)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss)\n",
        "        mae_metric.update_state(y, y_pred)\n",
        "        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker, mae_metric]\n",
        "\n",
        "\n",
        "# Construct an instance of CustomModel\n",
        "inputs = keras.Input(shape=(32,))\n",
        "outputs = keras.layers.Dense(1)(inputs)\n",
        "model = CustomModel(inputs, outputs)\n",
        "\n",
        "# We don't passs a loss or metrics here.\n",
        "model.compile(optimizer=\"adam\")\n",
        "\n",
        "# Just use `fit` as usual -- you can use callbacks, etc.\n",
        "x = np.random.random((1000, 32))\n",
        "y = np.random.random((1000, 1))\n",
        "model.fit(x, y, epochs=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f451e382c6a8"
      },
      "source": [
        "## Dando suporte a `sample_weight` e `class_weight`\n",
        "\n",
        "Você pode ter percebido que nosso primeiro exemplo básico não fez nenhuma menção a amostras ponderadas. Se você quiser oferecer suporte aos argumentos `fit()` `sample_weight` e `class_weight` , basta fazer o seguinte:\n",
        "\n",
        "- Desempacote `sample_weight` do argumento `data`\n",
        "- Passe-o para `compiled_loss` &amp; `compiled_metrics` (claro, você também pode simplesmente aplicá-lo manualmente se não depender de `compile()` para perdas e métricas)\n",
        "- É isso. Essa é a lista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "522d7281f948"
      },
      "outputs": [],
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        if len(data) == 3:\n",
        "            x, y, sample_weight = data\n",
        "        else:\n",
        "            sample_weight = None\n",
        "            x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            # Compute the loss value.\n",
        "            # The loss function is configured in `compile()`.\n",
        "            loss = self.compiled_loss(\n",
        "                y,\n",
        "                y_pred,\n",
        "                sample_weight=sample_weight,\n",
        "                regularization_losses=self.losses,\n",
        "            )\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics.\n",
        "        # Metrics are configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "# Construct and compile an instance of CustomModel\n",
        "inputs = keras.Input(shape=(32,))\n",
        "outputs = keras.layers.Dense(1)(inputs)\n",
        "model = CustomModel(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# You can now use sample_weight argument\n",
        "x = np.random.random((1000, 32))\n",
        "y = np.random.random((1000, 1))\n",
        "sw = np.random.random((1000, 1))\n",
        "model.fit(x, y, sample_weight=sw, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03000c5590db"
      },
      "source": [
        "## Fornecendo sua própria etapa de avaliação\n",
        "\n",
        "E se você quiser fazer o mesmo para chamadas a `model.evaluate()`? Nesse caso você substituiria `test_step` exatamente da mesma maneira. Seria algo assim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "999edb22c50e"
      },
      "outputs": [],
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self(x, training=False)\n",
        "        # Updates the metrics tracking the loss\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "# Construct an instance of CustomModel\n",
        "inputs = keras.Input(shape=(32,))\n",
        "outputs = keras.layers.Dense(1)(inputs)\n",
        "model = CustomModel(inputs, outputs)\n",
        "model.compile(loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Evaluate with our custom test_step\n",
        "x = np.random.random((1000, 32))\n",
        "y = np.random.random((1000, 1))\n",
        "model.evaluate(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e6a662e6588"
      },
      "source": [
        "## Conclusão: um exemplo GAN completo\n",
        "\n",
        "Vamos seguir os passos de um exemplo completo que aproveite tudo o que você acabou de aprender.\n",
        "\n",
        "Vamos considerar:\n",
        "\n",
        "- Uma rede geradora destinada a gerar imagens 28x28x1.\n",
        "- Uma rede discriminadora destinada a classificar imagens 28x28x1 em duas classes (\"false\" e \"real\").\n",
        "- Um otimizador para cada.\n",
        "- Uma função de perda para treinar o discriminador.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6748db01dc7c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create the discriminator\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "\n",
        "# Create the generator\n",
        "latent_dim = 128\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
        "        layers.Dense(7 * 7 * 128),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((7, 7, 128)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "801e8dd0c92a"
      },
      "source": [
        "Eis aqui uma classe GAN completa, substituindo `compile()` para usar sua própria assinatura e implementando todo o algoritmo GAN em apenas 17 linhas em `train_step` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc3fb4111393"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "095c499a6149"
      },
      "source": [
        "Vamos fazer um test-drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46832f2077ac"
      },
      "outputs": [],
      "source": [
        "# Prepare the dataset. We use both the training & test MNIST digits.\n",
        "batch_size = 64\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "# To limit the execution time, we only train on 100 batches. You can train on\n",
        "# the entire dataset. You will need about 20 epochs to get nice results.\n",
        "gan.fit(dataset.take(100), epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ed211016c96"
      },
      "source": [
        "As ideias por trás do aprendizado profundo são simples, então por que sua implementação deveria ser dolorosa?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "customizing_what_happens_in_fit.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
