{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81c428fc2d3"
      },
      "source": [
        "# Aprendizado por transferência e ajuste fino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5a59f0aefd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/transfer_learning\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/keras/transfer_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/keras/transfer_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/keras/transfer_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4ac441b1fc"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a7e9b92f963"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00d4c41cfe2f"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "**O aprendizado por transferência** consiste em pegar características aprendidas em um problema e aproveitá-las em um novo problema semelhante. Por exemplo, características de um modelo que aprendeu a identificar guaxinins podem ser úteis para iniciar um modelo destinado a identificar tanukis.\n",
        "\n",
        "O aprendizado por transferência geralmente é feito para tarefas em que seu dataset tem poucos dados para treinar um modelo completo do zero.\n",
        "\n",
        "A encarnação mais comum do aprendizado por transferência no contexto do aprendizado profundo é o seguinte fluxo de trabalho:\n",
        "\n",
        "1. Pegue camadas de um modelo previamente treinado.\n",
        "2. Congele-as, para evitar a destruição de qualquer informação que elas contenham durante as próximas rodadas de treinamento.\n",
        "3. Adicione algumas camadas novas e treináveis ​​em cima das camadas congeladas. Elas aprenderão a transformar as características antigas em previsões num novo dataset.\n",
        "4. Treine as novas camadas no seu dataset.\n",
        "\n",
        "Uma última etapa opcional é **o ajuste fino**, que consiste em descongelar todo o modelo obtido acima (ou parte dele) e treiná-lo novamente nos novos dados com uma taxa de aprendizado muito baixa. Isto pode potencialmente alcançar melhorias significativas, adaptando de forma incremental as características pré-treinadas aos novos dados.\n",
        "\n",
        "Primeiro, examinaremos detalhadamente a API `trainable` do Keras, que é a base da maioria dos fluxos de trabalho de aprendizado de transferência e ajuste fino.\n",
        "\n",
        "Em seguida, demonstraremos o fluxo de trabalho típico pegando um modelo pré-treinado no conjunto de dados ImageNet e treinando-o novamente no dataset de classificação Kaggle \"cães vs gatos\".\n",
        "\n",
        "Isto é uma adaptação de [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) e da postagem do blog de 2016 [\"construindo modelos poderosos de classificação de imagens usando muito poucos dados\"](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbf8630c325b"
      },
      "source": [
        "## Congelando camadas: entendendo o atributo `trainable`\n",
        "\n",
        "Camadas e modelos têm três atributos de peso:\n",
        "\n",
        "- `weights` é a lista de todas as variáveis ​​de pesos da camada.\n",
        "- `trainable_weights` é a lista daqueles que devem ser atualizados (via método do gradiente descendente) para minimizar a perda durante o treinamento.\n",
        "- `non_trainable_weights` é a lista daqueles que não devem ser treinados. Normalmente, eles são atualizados pelo modelo durante o passo para a frente.\n",
        "\n",
        "**Exemplo: a camada `Dense` tem 2 pesos treináveis ​​(kernel e bias)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "407deab1855e"
      },
      "outputs": [],
      "source": [
        "layer = keras.layers.Dense(3)\n",
        "layer.build((None, 4))  # Create the weights\n",
        "\n",
        "print(\"weights:\", len(layer.weights))\n",
        "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
        "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79fcb9cc960d"
      },
      "source": [
        "Em geral, todos os pesos são pesos treináveis. A única camada integrada que possui pesos não treináveis ​​é a camada `BatchNormalization`. Ela usa pesos não treináveis ​​para acompanhar a média e a variância de suas entradas durante o treinamento. Para saber como usar pesos não treináveis ​​em suas próprias camadas personalizadas, consulte o[guia para escrever novas camadas do zero](https://keras.io/guides/making_new_layers_and_models_via_subclassing/).\n",
        "\n",
        "**Exemplo: a camada `BatchNormalization` tem 2 pesos treináveis ​​e 2 pesos não treináveis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbc87a09bc3c"
      },
      "outputs": [],
      "source": [
        "layer = keras.layers.BatchNormalization()\n",
        "layer.build((None, 4))  # Create the weights\n",
        "\n",
        "print(\"weights:\", len(layer.weights))\n",
        "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
        "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cddcdbf2bd5b"
      },
      "source": [
        "Camadas e modelos também apresentam um atributo booleano `trainable`. Seu valor pode ser alterado. Definir `layer.trainable` como `False` move todos os pesos da camada de treinável para não treinável. Isto se chama \"congelar\" a camada: o estado de uma camada congelada não será atualizado durante o treinamento (ao treinar com `fit()` ou ao treinar com qualquer loop personalizado que dependa de `trainable_weights` para aplicar atualizações de gradiente).\n",
        "\n",
        "**Exemplo: definindo `trainable` como `False`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51bbc5d12742"
      },
      "outputs": [],
      "source": [
        "layer = keras.layers.Dense(3)\n",
        "layer.build((None, 4))  # Create the weights\n",
        "layer.trainable = False  # Freeze the layer\n",
        "\n",
        "print(\"weights:\", len(layer.weights))\n",
        "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
        "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32904f9a58db"
      },
      "source": [
        "Quando um peso treinável se torna não treinável, seu valor não é mais atualizado durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c26c27a8291"
      },
      "outputs": [],
      "source": [
        "# Make a model with 2 layers\n",
        "layer1 = keras.layers.Dense(3, activation=\"relu\")\n",
        "layer2 = keras.layers.Dense(3, activation=\"sigmoid\")\n",
        "model = keras.Sequential([keras.Input(shape=(3,)), layer1, layer2])\n",
        "\n",
        "# Freeze the first layer\n",
        "layer1.trainable = False\n",
        "\n",
        "# Keep a copy of the weights of layer1 for later reference\n",
        "initial_layer1_weights_values = layer1.get_weights()\n",
        "\n",
        "# Train the model\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
        "\n",
        "# Check that the weights of layer1 have not changed during training\n",
        "final_layer1_weights_values = layer1.get_weights()\n",
        "np.testing.assert_allclose(\n",
        "    initial_layer1_weights_values[0], final_layer1_weights_values[0]\n",
        ")\n",
        "np.testing.assert_allclose(\n",
        "    initial_layer1_weights_values[1], final_layer1_weights_values[1]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "412d7d659aa1"
      },
      "source": [
        "Não confunda o atributo `layer.trainable` com o argumento `training` em `layer.__call__()` (que controla se a camada deve executar seu passo para a frente no modo de inferência ou no modo de treinamento). Para obter mais informações, consulte as [perguntas frequentes do Keras](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6ccd3c7ab1a"
      },
      "source": [
        "## Configuração recursiva do atributo `trainable`\n",
        "\n",
        "Se você definir `trainable = False` em um modelo ou em qualquer camada que tenha subcamadas, todas as camadas filhas também se tornarão não treináveis.\n",
        "\n",
        "**Exemplo:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4235d0c69821"
      },
      "outputs": [],
      "source": [
        "inner_model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(3,)),\n",
        "        keras.layers.Dense(3, activation=\"relu\"),\n",
        "        keras.layers.Dense(3, activation=\"relu\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [keras.Input(shape=(3,)), inner_model, keras.layers.Dense(3, activation=\"sigmoid\"),]\n",
        ")\n",
        "\n",
        "model.trainable = False  # Freeze the outer model\n",
        "\n",
        "assert inner_model.trainable == False  # All layers in `model` are now frozen\n",
        "assert inner_model.layers[0].trainable == False  # `trainable` is propagated recursively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61535ba76727"
      },
      "source": [
        "## O típico fluxo de trabalho de aprendizado por transferência\n",
        "\n",
        "Isso nos leva a como um típico fluxo de trabalho de aprendizado por transferência pode ser implementado em Keras:\n",
        "\n",
        "1. Instancie um modelo base e carregue pesos pré-treinados nele.\n",
        "2. Congele todas as camadas no modelo base definindo `trainable = False`.\n",
        "3. Crie um novo modelo sobre a saída de uma (ou várias) camadas do modelo base.\n",
        "4. Treine seu novo modelo em seu novo dataset.\n",
        "\n",
        "Observe que um fluxo de trabalho alternativo e mais leve também poderia ser:\n",
        "\n",
        "1. Instancie um modelo base e carregue pesos pré-treinados nele.\n",
        "2. Execute seu novo dataset através dele e registre a saída de uma (ou várias) camadas do modelo base. Isto é chamado de **extração de características** (feature extraction).\n",
        "3. Use essa saída como dados de entrada para um novo modelo menor.\n",
        "\n",
        "Uma vantagem importante desse segundo fluxo de trabalho é que você só executa o modelo base uma vez em seus dados, em vez de uma vez por época de treinamento. Então é muito mais rápido e barato.\n",
        "\n",
        "Um problema com esse segundo fluxo de trabalho, porém, é que ele não permite que você modifique dinamicamente os dados de entrada de seu novo modelo durante o treinamento, o que é necessário ao fazer aumento de dados, por exemplo. O aprendizado por transferência geralmente é usado para tarefas quando seu novo dataset tem poucos dados para treinar um modelo em escala real do zero e, nesses cenários, o aumento de dados é muito importante. Portanto, a seguir, vamos nos concentrar no primeiro fluxo de trabalho.\n",
        "\n",
        "É assim que fica o primeiro workflow no Keras:\n",
        "\n",
        "Primeiro, instancie um modelo base com pesos pré-treinados.\n",
        "\n",
        "```python\n",
        "base_model = keras.applications.Xception(\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False)  # Do not include the ImageNet classifier at the top.\n",
        "```\n",
        "\n",
        "Em seguida, congele o modelo base.\n",
        "\n",
        "```python\n",
        "base_model.trainable = False\n",
        "```\n",
        "\n",
        "Crie um novo modelo em cima dele.\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "# We make sure that the base_model is running in inference mode here,\n",
        "# by passing `training=False`. This is important for fine-tuning, as you will\n",
        "# learn in a few paragraphs.\n",
        "x = base_model(inputs, training=False)\n",
        "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "# A Dense classifier with a single unit (binary classification)\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "```\n",
        "\n",
        "Treine o modelo com novos dados.\n",
        "\n",
        "```python\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])\n",
        "model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "736c99aea690"
      },
      "source": [
        "## Ajustes finos\n",
        "\n",
        "Depois que seu modelo convergir para os novos dados, você pode tentar descongelar todo ou parte do modelo básico e treinar novamente todo o modelo de ponta a ponta com uma taxa de aprendizado muito baixa.\n",
        "\n",
        "Esta é uma última etapa opcional que pode fornecer melhorias incrementais. Também pode levar a um overfitting rápido - lembre-se disso.\n",
        "\n",
        "É fundamental realizar esta etapa somente *depois* que o modelo com camadas congeladas tiver sido treinado para convergência. Se você misturar camadas treináveis ​​inicializadas aleatoriamente com camadas treináveis ​​que contêm características pré-treinadas, as camadas inicializadas aleatoriamente causarão atualizações de gradiente muito grandes durante o treinamento, o que destruirá suas características pré-treinadas.\n",
        "\n",
        "Também é fundamental usar uma taxa de aprendizado muito baixa neste estágio, porque você está treinando um modelo muito maior do que na primeira rodada de treinamento, em um dataset que geralmente é muito pequeno. Como resultado, existe o risco de alcançar o overfitting muito rapidamente se você aplicar grandes atualizações de peso. Aqui, você só quer readaptar os pesos pré-treinados de forma incremental.\n",
        "\n",
        "Veja como implementar o ajuste fino de todo o modelo básico:\n",
        "\n",
        "```python\n",
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# It's important to recompile your model after you make any changes\n",
        "# to the `trainable` attribute of any inner layer, so that your changes\n",
        "# are take into account\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
        "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "# Train end-to-end. Be careful to stop before you overfit!\n",
        "model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n",
        "```\n",
        "\n",
        "**Observação importante sobre `compile()` e `trainable`**\n",
        "\n",
        "Chamar `compile()` em um modelo destina-se a \"congelar\" o comportamento desse modelo. Isso implica que os valores de atributo `trainable` ​​no momento em que o modelo é compilado devem ser preservados durante todo o tempo de vida desse modelo, até que `compile` seja chamada novamente. Portanto, se você alterar qualquer valor `trainable`, não esqueça de chamar `compile()` novamente em seu modelo para que suas alterações sejam levadas em consideração.\n",
        "\n",
        "**Observações importantes sobre a camada `BatchNormalization`**\n",
        "\n",
        "Muitos modelos de imagem contêm camadas `BatchNormalization`. Essa camada é um caso especial em todas as contagens imagináveis. Aqui estão algumas coisas para se manter em mente.\n",
        "\n",
        "- `BatchNormalization` contém 2 pesos não treináveis ​​que são atualizados durante o treinamento. Essas são as variáveis ​​que acompanham a média e a variância das entradas.\n",
        "- Quando você define `bn_layer.trainable = False`, a camada `BatchNormalization` será executada no modo de inferência e não atualizará suas estatísticas de média e variação. Este não é o caso de outras camadas em geral, pois [treinabilidade de peso e modos de inferência/treinamento são dois conceitos ortogonais](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute). Mas os dois estão empatados no caso da camada `BatchNormalization`.\n",
        "- Ao descongelar um modelo que contém camadas `BatchNormalization` para fazer o ajuste fino, você deve manter as camadas `BatchNormalization` no modo de inferência passando `training=False` ao chamar o modelo base. Caso contrário, as atualizações aplicadas aos pesos não treináveis ​​destruirão repentinamente o que o modelo aprendeu.\n",
        "\n",
        "Você verá esse padrão em ação no exemplo completo no final deste guia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bce9ffc4e290"
      },
      "source": [
        "## Aprendizado por transferência e ajuste fino com um loop de treinamento personalizado\n",
        "\n",
        "Se, em vez de `fit()`, você estiver usando seu próprio loop de treinamento de baixo nível, o fluxo de trabalho permanecerá essencialmente o mesmo. Você deve ter o cuidado de levar em conta apenas a lista `model.trainable_weights` ao aplicar atualizações de gradiente:\n",
        "\n",
        "```python\n",
        "# Create base model\n",
        "base_model = keras.applications.Xception(\n",
        "    weights='imagenet',\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False)\n",
        "# Freeze base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top.\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = keras.optimizers.Adam()\n",
        "\n",
        "# Iterate over the batches of a dataset.\n",
        "for inputs, targets in new_dataset:\n",
        "    # Open a GradientTape.\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass.\n",
        "        predictions = model(inputs)\n",
        "        # Compute the loss value for this batch.\n",
        "        loss_value = loss_fn(targets, predictions)\n",
        "\n",
        "    # Get gradients of loss wrt the *trainable* weights.\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "    # Update the weights of the model.\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e63ba34ce1c"
      },
      "source": [
        "Da mesma forma para o ajuste fino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852447087ba9"
      },
      "source": [
        "## Um exemplo completo: ajuste fino de um modelo de classificação de imagens num dataset \"cães vs. gatos\"\n",
        "\n",
        "Para solidificar esses conceitos, vejamos de um exemplo concreto de aprendizado por transferência e ajuste-fino do início ao fim. Vamos carregar o modelo Xception, pré-treinado no ImageNet, e usá-lo no dataset de classificação Kaggle \"cats vs. dogs\" (cães vs. gatos)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba75835e0de6"
      },
      "source": [
        "### Obtendo os dados\n",
        "\n",
        "Primeiro, vamos baixar o dataset \"cats vs. dogs\" usando TFDS. Se você tiver seu próprio dataset, provavelmente vai querer usar o utilitário `tf.keras.preprocessing.image_dataset_from_directory` para gerar objetos de dataset rotulados similares a partir de um conjunto de imagens em disco arquivadas em pastas de classe específicas.\n",
        "\n",
        "O aprendizado por transferência é mais útil ao trabalhar com datasets muito pequenos. Para manter nosso dataset pequeno, usaremos 40% dos dados de treinamento originais (25.000 imagens) para treinamento, 10% para validação e 10% para teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a99f56934f7"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "train_ds, validation_ds, test_ds = tfds.load(\n",
        "    \"cats_vs_dogs\",\n",
        "    # Reserve 10% for validation and 10% for test\n",
        "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
        "    as_supervised=True,  # Include labels\n",
        ")\n",
        "\n",
        "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
        "print(\n",
        "    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\n",
        ")\n",
        "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9db548603642"
      },
      "source": [
        "Estas são as primeiras 9 imagens do dataset de treinamento. Como você pode ver, elas têm tamanhos diferentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00c8cbd1de88"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, (image, label) in enumerate(train_ds.take(9)):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(int(label))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "168c4a10c072"
      },
      "source": [
        "Também podemos ver que o rótulo 1 é \"dog\" (cão) e o rótulo 0 é \"cat\" (gato)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f749203cd740"
      },
      "source": [
        "### Padronizando os dados\n",
        "\n",
        "Nossas imagens brutas têm uma variedade de tamanhos. Além disso, cada pixel consiste de 3 valores inteiros entre 0 e 255 (valores do nível RGB). Este não é um bom formato para alimentar uma rede neural. Precisamos fazer 2 coisas:\n",
        "\n",
        "- Padronize para um tamanho de imagem fixo. Escolhemos 150x150.\n",
        "- Normalize os valores de pixel entre -1 e 1. Faremos isso usando uma camada `Normalization` como parte do próprio modelo.\n",
        "\n",
        "Em geral, é uma boa prática desenvolver modelos que usam dados brutos como entrada, em vez de modelos que usam dados já pré-processados. A razão é que, se seu modelo espera dados pré-processados, sempre que você exportar seu modelo para usá-lo em outro lugar (em um navegador web, em um aplicativo móvel), você precisará reimplementar exatamente o mesmo pipeline de pré-processamento. Isto rapidamente aumenta a complexidade. Portanto, devemos fazer o mínimo possível de pré-processamento antes de usar o modelo.\n",
        "\n",
        "Aqui, faremos o redimensionamento da imagem no pipeline de dados (porque uma rede neural profunda só pode processar lotes contíguos de dados) e faremos o dimensionamento do valor de entrada como parte do modelo, ao criá-lo.\n",
        "\n",
        "Vamos redimensionar as imagens para 150x150:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3678f38e087"
      },
      "outputs": [],
      "source": [
        "size = (150, 150)\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "708bf9792a35"
      },
      "source": [
        "Além disso, vamos agrupar os dados e usar cache e pré-busca para otimizar a velocidade de carregamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ef9e6092e3"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b60f852c462f"
      },
      "source": [
        "### Usando aumento de dados aleatórios\n",
        "\n",
        "Quando você não tem um grande dataset de imagens, é uma boa prática introduzir artificialmente a diversidade de amostras aplicando transformações aleatórias, mas realistas, às imagens de treinamento, como invertendo a imagem horizontalmente de forma aleatória ou fazer pequenas rotações aleatórias. Isto ajuda a expor o modelo a diferentes aspectos dos dados de treinamento enquanto desacelera o overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6a5f072ae3b"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fa8ddeda36e"
      },
      "source": [
        "Vamos visualizar como fica a primeira imagem do primeiro lote após várias transformações aleatórias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9077f9fd022e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for images, labels in train_ds.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = images[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(\n",
        "            tf.expand_dims(first_image, 0), training=True\n",
        "        )\n",
        "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
        "        plt.title(int(labels[0]))\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6743c69b0952"
      },
      "source": [
        "## Construção de um modelo\n",
        "\n",
        "Agora vamos construir um modelo que segue o gabarito que explicamos anteriormente.\n",
        "\n",
        "Observe que:\n",
        "\n",
        "- Adicionamos uma camada `Rescaling` para redimensionar os valores de entrada (inicialmente no intervalo `[0, 255]` ) para o intervalo `[-1, 1]`.\n",
        "- Adicionamos uma camada `Dropout` antes da camada de classificação, para regularização.\n",
        "- Passamos `training=False` ao chamar o modelo base, para que ele seja executado no modo de inferência, de forma que as estatísticas batchnorm não sejam atualizadas mesmo depois de descongelarmos o modelo base para ajuste fino."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35d00d5e5722"
      },
      "outputs": [],
      "source": [
        "base_model = keras.applications.Xception(\n",
        "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False,\n",
        ")  # Do not include the ImageNet classifier at the top.\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = data_augmentation(inputs)  # Apply random data augmentation\n",
        "\n",
        "# Pre-trained Xception weights requires that input be scaled\n",
        "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
        "# outputs: `(inputs * scale) + offset`\n",
        "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
        "x = scale_layer(x)\n",
        "\n",
        "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
        "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
        "# base_model is running in inference mode here.\n",
        "x = base_model(x, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e8237de81e8"
      },
      "source": [
        "## Treinamento da camada superior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9137b8daedad"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa51d4562fa7"
      },
      "source": [
        "## Faça uma rodada de ajustes finos em todo o modelo\n",
        "\n",
        "Por fim, vamos descongelar o modelo base e treinar todo o modelo de do início ao fim com uma baixa taxa de aprendizado.\n",
        "\n",
        "É importante ressaltar que, embora o modelo base se torne treinável, ele ainda está sendo executado no modo de inferência, pois passamos `training=False` ao chamá-lo quando construímos o modelo. Isto significa que as camadas internas de normalização de lote não atualizarão suas estatísticas de lote. Se o fizessem, destruiriam as representações aprendidas pelo modelo até agora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cc299505b72"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
        "# since we passed `training=False` when calling it. This means that\n",
        "# the batchnorm layers will not update their batch statistics.\n",
        "# This prevents the batchnorm layers from undoing all the training\n",
        "# we've done so far.\n",
        "base_model.trainable = True\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afa73d989302"
      },
      "source": [
        "Após 10 épocas, o ajuste fino nos dá uma boa melhoria aqui."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "transfer_learning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
