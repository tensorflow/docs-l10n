{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daf323e33b84"
      },
      "source": [
        "# Escrevendo um loop de treinamento do zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2440f6e0c5ef"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/writing_a_training_loop_from_scratch.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/keras-team/keras-io/blob/master/guides/writing_a_training_loop_from_scratch.py\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/keras/writing_a_training_loop_from_scratch.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4ac441b1fc"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae2407ad926f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f5a253901f8"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "O Keras fornece loops padrão de treinamento e avaliação, `fit()` e `evaluate()`. Seu uso é abordado no guia [Treinamento e avaliação com métodos integrados](https://www.tensorflow.org/guide/keras/train_and_evaluate/).\n",
        "\n",
        "Se você deseja personalizar o algoritmo de aprendizado de seu modelo enquanto ainda aproveita a conveniência de `fit()` (por exemplo, para treinar uma GAN usando `fit()`), você pode criar uma subclasse da classe `Model` e implementar seu próprio método `train_step()`, que é chamado repetidamente durante `fit()`. Isto é abordado no guia <a href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/\" data-md-type=\"link\">Personalizando o que acontece em `fit()`</a>.\n",
        "\n",
        "Agora, se você deseja um controle de nível muito baixo sobre o treinamento e avaliação, deve escrever seus próprios loops de treinamento e avaliação do zero. Este guia trata desse assunto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4f47351a3ec"
      },
      "source": [
        "## Usando o `GradientTape`: um primeiro exemplo completo\n",
        "\n",
        "Chamar um modelo dentro de um escopo `GradientTape` permite recuperar os gradientes dos pesos treináveis ​​da camada em relação a um valor de perda. Usando uma instância do otimizador, você pode usar esses gradientes para atualizar essas variáveis ​​(que podem ser recuperadas usando `model.trainable_weights`).\n",
        "\n",
        "Vamos considerar um modelo MNIST simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaa775ce7dab"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8b02a5759cf"
      },
      "source": [
        "Vamos treiná-lo usando gradiente de minilote com um loop de treinamento personalizado.\n",
        "\n",
        "Primeiro, vamos precisar de um otimizador, uma função de perda e um dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2c6257b8d02"
      },
      "outputs": [],
      "source": [
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the training dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_test, (-1, 784))\n",
        "\n",
        "# Reserve 10,000 samples for validation.\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "\n",
        "# Prepare the training dataset.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c30285b1a2e"
      },
      "source": [
        "Aqui está o nosso loop de treinamento:\n",
        "\n",
        "- Abrimos um loop `for` que itera sobre épocas\n",
        "- Para cada época, abrimos um loop `for` que itera sobre o dataset, em lotes\n",
        "- Para cada lote, abrimos um escopo `GradientTape()`\n",
        "- Dentro deste escopo, chamamos o modelo (forward pass) e calculamos a perda\n",
        "- Fora do escopo, recuperamos os gradientes dos pesos do modelo em relação à perda\n",
        "- Finalmente, usamos o otimizador para atualizar os pesos do modelo com base nos gradientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bf4c10ceb50"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d600076b7be0"
      },
      "source": [
        "## Tratamento de métricas em baixo nível\n",
        "\n",
        "Vamos adicionar monitoramento de métricas a esse loop básico.\n",
        "\n",
        "Você pode reutilizar prontamente as métricas integradas (ou personalizadas que você escreveu) em tais loops de treinamento escritos do zero. Aqui está o fluxo:\n",
        "\n",
        "- Instancie a métrica no início do loop\n",
        "- Chame `metric.update_state()` após cada lote\n",
        "- Chame `metric.result()` quando precisar exibir o valor atual da métrica\n",
        "- Chame `metric.reset_states()` quando precisar limpar o estado da métrica (normalmente no final de uma época)\n",
        "\n",
        "Vamos usar esse conhecimento para calcular `SparseCategoricalAccuracy` em dados de validação no final de cada época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2602509b16c7"
      },
      "outputs": [],
      "source": [
        "# Get model\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9111a5cc87dc"
      },
      "source": [
        "Eis aqui nosso loop de treinamento e avaliação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "654e2311dbff"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update training metric.\n",
        "        train_acc_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_logits = model(x_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c9a16c21790"
      },
      "source": [
        "## Acelerando seu passo de treinamento com `tf.function`\n",
        "\n",
        "O runtime padrão no TensorFlow 2 é [execução antecipada](https://www.tensorflow.org/guide/eager) (eager execution). Como tal, nosso loop de treinamento acima é executado imediatamente de forma \"eager\".\n",
        "\n",
        "Isso é muito bom para a depuração, mas a compilação de grafos tem uma vantagem de desempenho definitiva. Descrever sua computação como um grafo estático permite que a estrutura aplique otimizações de desempenho global. Isto é impossível quando o framework é limitado a executar avidamente uma operação depois da outra, sem saber o que vem a seguir.\n",
        "\n",
        "Você pode compilar num grafo estático qualquer função que receba tensores como entrada. Basta adicionar um decorador `@tf.function` nele, como mostrado a seguir:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdacc2d48ade"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab61b0bf3126"
      },
      "source": [
        "Vamos fazer o mesmo com o passo de avaliação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da4828fd8ef7"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d552377968f1"
      },
      "source": [
        "Agora, vamos executar novamente nosso loop de treinamento com este passo de treinamento compilado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69d73c94e44"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8977d77a8095"
      },
      "source": [
        "Muito mais rápido, não foi?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5b5a54d339a"
      },
      "source": [
        "## Manipulação de perdas rastreadas pelo modelo em baixo nível\n",
        "\n",
        "Camadas e modelos rastreiam recursivamente quaisquer perdas criadas durante o forward pass por camadas que chamam `self.add_loss(value)`. A lista resultante de valores de perda escalar está disponível através da propriedade `model.losses` no final do forward pass.\n",
        "\n",
        "Se você quiser usar esses componentes de perda, você deve somá-los e adicioná-los à perda principal em seu passo de treinamento.\n",
        "\n",
        "Considere a camada a seguir, que gera uma perda de regularização da atividade:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ec7c4b16596"
      },
      "outputs": [],
      "source": [
        "class ActivityRegularizationLayer(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b12260b8bf2"
      },
      "source": [
        "Vamos construir um modelo bem simples que a utilize:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57afe49e6b93"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "# Insert activity regularization as a layer\n",
        "x = ActivityRegularizationLayer()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aadb58115c13"
      },
      "source": [
        "O nosso passo de treinamento deve agora ficar assim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf674776a0d2"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "        # Add any extra losses created during the forward pass.\n",
        "        loss_value += sum(model.losses)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0af04732fe78"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Agora você sabe tudo o que há para saber sobre como usar loops de treinamento integrados e escrever seus próprios loops do zero.\n",
        "\n",
        "Para concluir, eis um exemplo completo e simples que junta tudo o que você aprendeu neste guia: um DCGAN treinado em dígitos MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fb325331a1e"
      },
      "source": [
        "## Um exemplo completo: um loop de treinamento GAN do zero\n",
        "\n",
        "Você talvez esteja familiarizado com Redes Adversárias Generativas (Generative Adversarial Networks), ou GANs. As GANs podem gerar novas imagens que parecem quase reais, aprendendo a distribuição latente de um dataset de treinamento de imagens (o \"espaço latente\" das imagens).\n",
        "\n",
        "Uma GAN é composta de duas partes: um modelo \"gerador\" que mapeia pontos no espaço latente para pontos no espaço da imagem, um modelo \"discriminador\", um classificador que pode diferenciar entre imagens reais (do dataset de treinamento) e imagens falsas (a saída da rede do gerador).\n",
        "\n",
        "Um loop de treinamento GAN tem a seguinte estrutura:\n",
        "\n",
        "1. Treine o discriminador.\n",
        "\n",
        "- Obtenha uma amostra de um lote de pontos aleatórios no espaço latente.\n",
        "- Transforme os pontos em imagens falsas por meio do modelo \"gerador\".\n",
        "- Obtenha um lote de imagens reais e combine-as com as imagens geradas.\n",
        "- Treine o modelo \"discriminador\" para classificar imagens geradas versus imagens reais.\n",
        "\n",
        "1. Treine o gerador.\n",
        "\n",
        "- Obtenha uma amostra de pontos aleatórios no espaço latente.\n",
        "- Transforme os pontos em imagens falsas através da rede \"gerador\".\n",
        "- Obtenha um lote de imagens reais e combine-as com as imagens geradas.\n",
        "- Treine o modelo \"gerador\" para \"enganar\" o discriminador e classificar as imagens falsas como reais.\n",
        "\n",
        "Para uma visão geral muito mais detalhada de como as GANs funcionam, consulte [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) .\n",
        "\n",
        "Vamos implementar este loop de treinamento. Primeiro, crie o discriminador destinado a classificar dígitos falsos vs. reais:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fabf9cef3400"
      },
      "outputs": [],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73396eb6daf9"
      },
      "source": [
        "Em seguida criaremos uma rede geradora, que transforma vetores latentes em saídas de formato `(28, 28, 1)` (representando dígitos MNIST):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "821d203bfb3e"
      },
      "outputs": [],
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
        "        layers.Dense(7 * 7 * 128),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((7, 7, 128)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d6d54a78a0"
      },
      "source": [
        "Aqui está a parte chave: o loop de treinamento. Como você pode ver, é bastante simples. A função de passo de treinamento requer apenas 17 linhas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a11c875142e"
      },
      "outputs": [],
      "source": [
        "# Instantiate one optimizer for the discriminator and another for the generator.\n",
        "d_optimizer = keras.optimizers.Adam(learning_rate=0.0003)\n",
        "g_optimizer = keras.optimizers.Adam(learning_rate=0.0004)\n",
        "\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_images):\n",
        "    # Sample random points in the latent space\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "    # Decode them to fake images\n",
        "    generated_images = generator(random_latent_vectors)\n",
        "    # Combine them with real images\n",
        "    combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "    # Assemble labels discriminating real from fake images\n",
        "    labels = tf.concat(\n",
        "        [tf.ones((batch_size, 1)), tf.zeros((real_images.shape[0], 1))], axis=0\n",
        "    )\n",
        "    # Add random noise to the labels - important trick!\n",
        "    labels += 0.05 * tf.random.uniform(labels.shape)\n",
        "\n",
        "    # Train the discriminator\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = discriminator(combined_images)\n",
        "        d_loss = loss_fn(labels, predictions)\n",
        "    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
        "    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
        "\n",
        "    # Sample random points in the latent space\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "    # Assemble labels that say \"all real images\"\n",
        "    misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the generator (note that we should *not* update the weights\n",
        "    # of the discriminator)!\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = discriminator(generator(random_latent_vectors))\n",
        "        g_loss = loss_fn(misleading_labels, predictions)\n",
        "    grads = tape.gradient(g_loss, generator.trainable_weights)\n",
        "    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
        "    return d_loss, g_loss, generated_images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa6bd6292488"
      },
      "source": [
        "Vamos treinar nossa GAN, chamando repetidamente `train_step` em lotes de imagens.\n",
        "\n",
        "Já que nosso discriminador e gerador são convnets, você vai querer executar esse código numa GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6a4e3d42262"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Prepare the dataset. We use both the training & test MNIST digits.\n",
        "batch_size = 64\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "epochs = 1  # In practice you need at least 20 epochs to generate nice digits.\n",
        "save_dir = \"./\"\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart epoch\", epoch)\n",
        "\n",
        "    for step, real_images in enumerate(dataset):\n",
        "        # Train the discriminator & generator on one batch of real images.\n",
        "        d_loss, g_loss, generated_images = train_step(real_images)\n",
        "\n",
        "        # Logging.\n",
        "        if step % 200 == 0:\n",
        "            # Print metrics\n",
        "            print(\"discriminator loss at step %d: %.2f\" % (step, d_loss))\n",
        "            print(\"adversarial loss at step %d: %.2f\" % (step, g_loss))\n",
        "\n",
        "            # Save one generated image\n",
        "            img = tf.keras.preprocessing.image.array_to_img(\n",
        "                generated_images[0] * 255.0, scale=False\n",
        "            )\n",
        "            img.save(os.path.join(save_dir, \"generated_img\" + str(step) + \".png\"))\n",
        "\n",
        "        # To limit execution time we stop after 10 steps.\n",
        "        # Remove the lines below to actually train the model!\n",
        "        if step > 10:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a92959ac630b"
      },
      "source": [
        "É isso! Você vai obter dígitos MNIST falsos bem bonitos depois de cerca de 30 segundos de treinamento na GPU Colab."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "writing_a_training_loop_from_scratch.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
