{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e083398b477"
      },
      "source": [
        "# Trabalhando com camadas de pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64010bd23c2e"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/preprocessing_layers\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Veja em TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/keras/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/keras/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/keras/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1d403f04693"
      },
      "source": [
        "## Pré-processamento com o Keras\n",
        "\n",
        "A API de camadas de pré-processamento do Keras permite que desenvolvedores criem pipelines de processamento de entrada nativos do Keras. Esses pipelines de processamento de entrada podem ser usados ​​como código de pré-processamento independente em outros workflows (que não usam Keras), combinados diretamente com modelos Keras e exportados como parte de um Keras SavedModel.\n",
        "\n",
        "Com as camadas de pré-processamento do Keras, você pode construir e exportar modelos que são realmente modelos de ponta a ponta: modelos que aceitam imagens brutas ou dados estruturados brutos como entrada; modelos que lidam com normalização de recursos ou indexação de valores de recursos por conta própria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313360fa9024"
      },
      "source": [
        "## Pré-processamentos disponíveis\n",
        "\n",
        "### Pré-processamento de texto\n",
        "\n",
        "- `tf.keras.layers.TextVectorization`: transforma strings brutas em uma representação codificada que pode ser lida por uma camada `Embedding` ou camada `Dense`.\n",
        "\n",
        "### Pré-processamento de recursos numéricos\n",
        "\n",
        "- `tf.keras.layers.Normalization`: executa uma normalização dos recursos de entrada.\n",
        "- `tf.keras.layers.Discretization`: transforma recursos numéricos contínuos em recursos categóricos inteiros.\n",
        "\n",
        "### Pré-processamento de recursos categóricos\n",
        "\n",
        "- `tf.keras.layers.CategoryEncoding`: transforma recursos categóricos inteiros em representações one-hot, multi-hot ou contagens densas.\n",
        "- `tf.keras.layers.Hashing`: realiza hashing de recursos categóricos, também conhecido como \"truque de hash\".\n",
        "- `tf.keras.layers.StringLookup`: transforma valores categóricos de string em uma representação codificada que pode ser lida por uma camada `Embedding` ou camada `Dense`.\n",
        "- `tf.keras.layers.IntegerLookup`: transforma valores categóricos inteiros em uma representação codificada que pode ser lida por uma camada `Embedding` ou camada `Dense`.\n",
        "\n",
        "### Pré-processamento de imagens\n",
        "\n",
        "Estas camadas servem para padronizar as entradas de um modelo de imagem.\n",
        "\n",
        "- `tf.keras.layers.Resizing`: redimensiona um lote de imagens para um tamanho-alvo.\n",
        "- `tf.keras.layers.Rescaling`: redimensiona e desloca os valores de um lote de imagem (por exemplo, das entradas no intervalo `[0, 255]` para entradas no intervalo `[0, 1]`.\n",
        "- `tf.keras.layers.CenterCrop`: retorna um corte central de um lote de imagens.\n",
        "\n",
        "### Ampliação de dados de imagem\n",
        "\n",
        "Essas camadas aplicam transformações de ampliação aleatória a um lote de imagens. Elas só são ativas durante o treinamento.\n",
        "\n",
        "- `tf.keras.layers.RandomCrop`\n",
        "- `tf.keras.layers.RandomFlip`\n",
        "- `tf.keras.layers.RandomTranslation`\n",
        "- `tf.keras.layers.RandomRotation`\n",
        "- `tf.keras.layers.RandomZoom`\n",
        "- `tf.keras.layers.RandomHeight`\n",
        "- `tf.keras.layers.RandomWidth`\n",
        "- `tf.keras.layers.RandomContrast`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c923e41fb1b4"
      },
      "source": [
        "## O método `adapt()`\n",
        "\n",
        "Algumas camadas de pré-processamento têm um estado interno que pode ser computado com base numa amostra dos dados de treinamento. A lista de camadas de pré-processamento stateful é:\n",
        "\n",
        "- `TextVectorization`: mantém um mapeamento entre tokens de string e índices inteiros\n",
        "- `StringLookup` e `IntegerLookup`: mantêm um mapeamento entre valores de entrada e índices inteiros.\n",
        "- `Normalization`: mantém a média e o desvio padrão dos recursos.\n",
        "- `Discretization`: mantém informações sobre limites de buckets de valores.\n",
        "\n",
        "Crucialmente, essas camadas **não são treináveis**. Seu estado não é definido durante o treinamento; ele precisa ser definido **antes do treinamento**, ou inicializados a partir de uma constante pré-computada ou \"adaptando-os\" aos dados.\n",
        "\n",
        "Você define o estado de uma camada de pré-processamento expondo-a a dados de treinamento através do método `adapt()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cac6bd80812"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data = np.array([[0.1, 0.2, 0.3], [0.8, 0.9, 1.0], [1.5, 1.6, 1.7],])\n",
        "layer = layers.Normalization()\n",
        "layer.adapt(data)\n",
        "normalized_data = layer(data)\n",
        "\n",
        "print(\"Features mean: %.2f\" % (normalized_data.numpy().mean()))\n",
        "print(\"Features std: %.2f\" % (normalized_data.numpy().std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d43b8246b8a3"
      },
      "source": [
        "O método `adapt()` usa um array Numpy ou um objeto `tf.data.Dataset`. No caso de `StringLookup` e `TextVectorization`, você também pode passar uma lista de strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48d95713348a"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    \"ξεῖν᾽, ἦ τοι μὲν ὄνειροι ἀμήχανοι ἀκριτόμυθοι\",\n",
        "    \"γίγνοντ᾽, οὐδέ τι πάντα τελείεται ἀνθρώποισι.\",\n",
        "    \"δοιαὶ γάρ τε πύλαι ἀμενηνῶν εἰσὶν ὀνείρων:\",\n",
        "    \"αἱ μὲν γὰρ κεράεσσι τετεύχαται, αἱ δ᾽ ἐλέφαντι:\",\n",
        "    \"τῶν οἳ μέν κ᾽ ἔλθωσι διὰ πριστοῦ ἐλέφαντος,\",\n",
        "    \"οἵ ῥ᾽ ἐλεφαίρονται, ἔπε᾽ ἀκράαντα φέροντες:\",\n",
        "    \"οἱ δὲ διὰ ξεστῶν κεράων ἔλθωσι θύραζε,\",\n",
        "    \"οἵ ῥ᾽ ἔτυμα κραίνουσι, βροτῶν ὅτε κέν τις ἴδηται.\",\n",
        "]\n",
        "layer = layers.TextVectorization()\n",
        "layer.adapt(data)\n",
        "vectorized_text = layer(data)\n",
        "print(vectorized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7619914dfb40"
      },
      "source": [
        "Além disso, as camadas adaptáveis ​​sempre apresentam uma alternativa para definir diretamente o estado via argumentos do construtor ou atribuição de peso. Se os valores de estado pretendidos forem conhecidos no momento da construção da camada ou forem calculados fora da chamada `adapt()`, eles poderão ser definidos sem depender da computação interna da camada. Por exemplo, se já existirem arquivos de vocabulário externos para as camadas `TextVectorization`, `StringLookup` ou `IntegerLookup`, eles podem ser carregados diretamente nas tabelas de pesquisa passando um caminho para o arquivo de vocabulário nos argumentos do construtor da camada.\n",
        "\n",
        "Aqui está um exemplo onde instanciamos uma camada `StringLookup` com vocabulário pré-computado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9df56efc7f3b"
      },
      "outputs": [],
      "source": [
        "vocab = [\"a\", \"b\", \"c\", \"d\"]\n",
        "data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n",
        "layer = layers.StringLookup(vocabulary=vocab)\n",
        "vectorized_data = layer(data)\n",
        "print(vectorized_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49cbfe135b00"
      },
      "source": [
        "## Pré-processamento de dados antes do modelo ou dentro do modelo\n",
        "\n",
        "Há duas maneiras de usar camadas de pré-processamento:\n",
        "\n",
        "**Opção 1:** Torná-las parte do modelo, assim:\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = preprocessing_layer(inputs)\n",
        "outputs = rest_of_the_model(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "```\n",
        "\n",
        "Com esta opção, o pré-processamento acontecerá no dispositivo, de forma sincronizada com o restante da execução do modelo, o que significa que ele se beneficiará da aceleração da GPU. Se você estiver treinando em GPU, esta é a melhor opção para a camada `Normalization` e para todas as camadas de pré-processamento de imagem e ampliação de dados.\n",
        "\n",
        "**Opção 2:** aplicá-las ao seu `tf.data.Dataset`, para obter um dataset que gere lotes de dados pré-processados, como este:\n",
        "\n",
        "```python\n",
        "dataset = dataset.map(lambda x, y: (preprocessing_layer(x), y))\n",
        "```\n",
        "\n",
        "Com esta opção, seu pré-processamento acontecerá na CPU, de forma assíncrona, e será armazenado em buffer antes de entrar no modelo. Além disso, se você chamar `dataset.prefetch(tf.data.AUTOTUNE)` em seu dataset, o pré-processamento acontecerá de forma eficiente em paralelo com o treinamento:\n",
        "\n",
        "```python\n",
        "dataset = dataset.map(lambda x, y: (preprocessing_layer(x), y))\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "model.fit(dataset, ...)\n",
        "```\n",
        "\n",
        "Esta é a melhor alternativa para `TextVectorization` e todas as camadas de pré-processamento de dados estruturados. Também pode ser uma boa opção se você estiver treinando na CPU e usa camadas de pré-processamento de imagem.\n",
        "\n",
        "**Ao executar em TPU, você sempre deve colocar as camadas de pré-processamento no pipeline `tf.data`** (com exceção de `Normalization` e `Rescaling`, que funcionam bem em TPU e são frequentemente usadas ​​porque a primeira camada é um modelo de imagem)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32f6d2a104b7"
      },
      "source": [
        "## Vantagens de fazer o pré-processamento dentro do modelo na hora da inferência\n",
        "\n",
        "Mesmo que você escolha a opção 2, poderá desejar exportar posteriormente um modelo de ponta a ponta, de inferência apenas, que incluirá as camadas de pré-processamento. A principal vantagem de se fazer isso é que **torna seu modelo portátil** e **ajuda a reduzir o [desvio de treinamento/serviço](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)**.\n",
        "\n",
        "Quando todo o pré-processamento de dados faz parte do modelo, outras pessoas podem carregar e usar seu modelo sem precisar saber como cada recurso deve ser codificado e normalizado. Seu modelo de inferência será capaz de processar imagens brutas ou dados estruturados brutos e não exigirá que os usuários do modelo estejam cientes dos detalhes de, por exemplo, o esquema de tokenização usado para texto, o esquema de indexação usado para recursos categóricos, se os valores de pixel da imagem são normalizados para `[-1, +1]` ou para `[0, 1]`, etc. Isto é muito poderoso se você estiver exportando seu modelo para outro ambiente de execução, como TensorFlow.js: você não precisará reimplementar seu pipeline de pré-processamento em JavaScript.\n",
        "\n",
        "Se você inicialmente colocar suas camadas de pré-processamento no seu pipeline `tf.data`, poderá exportar um modelo de inferência que empacote o pré-processamento. Basta instanciar um novo modelo que encadeia suas camadas de pré-processamento e seu modelo de treinamento:\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = preprocessing_layer(inputs)\n",
        "outputs = training_model(x)\n",
        "inference_model = keras.Model(inputs, outputs)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b41b381d48d4"
      },
      "source": [
        "## Receitas rápidas\n",
        "\n",
        "### Ampliação de dados de imagem\n",
        "\n",
        "Observe que as camadas de ampliação de dados de imagem são ativas apenas durante o treinamento (semelhante à camada `Dropout` )."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3793692e983"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load some data\n",
        "(x_train, y_train), _ = keras.datasets.cifar10.load_data()\n",
        "input_shape = x_train.shape[1:]\n",
        "classes = 10\n",
        "\n",
        "# Create a tf.data pipeline of augmented images (and their labels)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.batch(16).map(lambda x, y: (data_augmentation(x), y))\n",
        "\n",
        "\n",
        "# Create a model and train it on the augmented image data\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = layers.Rescaling(1.0 / 255)(inputs)  # Rescale inputs\n",
        "outputs = keras.applications.ResNet50(  # Add the rest of the model\n",
        "    weights=None, input_shape=input_shape, classes=classes\n",
        ")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "model.fit(train_dataset, steps_per_epoch=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51d369f0310f"
      },
      "source": [
        "Você pode ver uma configuração semelhante em ação no exemplo [classificação de imagem do zero](https://keras.io/examples/vision/image_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a79a1c48b2b7"
      },
      "source": [
        "### Normalizando recursos numéricos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cc2607a45c8"
      },
      "outputs": [],
      "source": [
        "# Load some data\n",
        "(x_train, y_train), _ = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.reshape((len(x_train), -1))\n",
        "input_shape = x_train.shape[1:]\n",
        "classes = 10\n",
        "\n",
        "# Create a Normalization layer and set its internal state using the training data\n",
        "normalizer = layers.Normalization()\n",
        "normalizer.adapt(x_train)\n",
        "\n",
        "# Create a model that include the normalization layer\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = normalizer(inputs)\n",
        "outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Train the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62685d477010"
      },
      "source": [
        "### Codificando recursos categóricos de string via one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae0d2b0405f1"
      },
      "outputs": [],
      "source": [
        "# Define some toy data\n",
        "data = tf.constant([[\"a\"], [\"b\"], [\"c\"], [\"b\"], [\"c\"], [\"a\"]])\n",
        "\n",
        "# Use StringLookup to build an index of the feature values and encode output.\n",
        "lookup = layers.StringLookup(output_mode=\"one_hot\")\n",
        "lookup.adapt(data)\n",
        "\n",
        "# Convert new test data (which includes unknown feature values)\n",
        "test_data = tf.constant([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [\"\"]])\n",
        "encoded_data = lookup(test_data)\n",
        "print(encoded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "686aeda532f5"
      },
      "source": [
        "Observe que, aqui, o índice 0 é reservado para valores fora do vocabulário (valores que não foram vistos durante `adapt()`).\n",
        "\n",
        "Você pode ver o `StringLookup` em ação no exemplo [classificação de dados estruturados do zero](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc8af3e290df"
      },
      "source": [
        "### Codificando recursos categóricos inteiros via one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75f3d6af4522"
      },
      "outputs": [],
      "source": [
        "# Define some toy data\n",
        "data = tf.constant([[10], [20], [20], [10], [30], [0]])\n",
        "\n",
        "# Use IntegerLookup to build an index of the feature values and encode output.\n",
        "lookup = layers.IntegerLookup(output_mode=\"one_hot\")\n",
        "lookup.adapt(data)\n",
        "\n",
        "# Convert new test data (which includes unknown feature values)\n",
        "test_data = tf.constant([[10], [10], [20], [50], [60], [0]])\n",
        "encoded_data = lookup(test_data)\n",
        "print(encoded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5a6be487be"
      },
      "source": [
        "Observe que o índice 0 é reservado para valores ausentes (que você deve especificar como o valor 0) e o índice 1 é reservado para valores fora do vocabulário (valores que não foram vistos durante `adapt()`). Isto pode ser configurado com os argumentos `mask_token` e `oov_token` do construtor `IntegerLookup`.\n",
        "\n",
        "Você pode ver o `IntegerLookup` em ação no exemplo [classificação de dados estruturados do zero](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fbfaa6ab3e2"
      },
      "source": [
        "### Aplicando o truque de hash a um recurso categórico inteiro\n",
        "\n",
        "Se você tiver um recurso categórico que pode assumir muitos valores diferentes (na ordem de 10e3 ou superior), onde cada valor aparece apenas algumas vezes nos dados, torna-se impraticável e ineficaz indexar e fazer one-hot encoding dos valores do recurso. Em vez disso, pode ser uma boa ideia aplicar o \"truque de hash\": fazer hash dos valores para um vetor de tamanho fixo. Isto mantém o tamanho do espaço de recurso gerenciável e remove a necessidade de indexação explícita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f6c1f84c43c"
      },
      "outputs": [],
      "source": [
        "# Sample data: 10,000 random integers with values between 0 and 100,000\n",
        "data = np.random.randint(0, 100000, size=(10000, 1))\n",
        "\n",
        "# Use the Hashing layer to hash the values to the range [0, 64]\n",
        "hasher = layers.Hashing(num_bins=64, salt=1337)\n",
        "\n",
        "# Use the CategoryEncoding layer to multi-hot encode the hashed values\n",
        "encoder = layers.CategoryEncoding(num_tokens=64, output_mode=\"multi_hot\")\n",
        "encoded_data = encoder(hasher(data))\n",
        "print(encoded_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df69b434d327"
      },
      "source": [
        "### Codificando texto como uma sequência de índices de token\n",
        "\n",
        "É assim que você deve pré-processar o texto a ser passado para uma camada `Embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "361b561bc88b"
      },
      "outputs": [],
      "source": [
        "# Define some text data to adapt the layer\n",
        "adapt_data = tf.constant(\n",
        "    [\n",
        "        \"The Brain is wider than the Sky\",\n",
        "        \"For put them side by side\",\n",
        "        \"The one the other will contain\",\n",
        "        \"With ease and You beside\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a TextVectorization layer\n",
        "text_vectorizer = layers.TextVectorization(output_mode=\"int\")\n",
        "# Index the vocabulary via `adapt()`\n",
        "text_vectorizer.adapt(adapt_data)\n",
        "\n",
        "# Try out the layer\n",
        "print(\n",
        "    \"Encoded text:\\n\", text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
        ")\n",
        "\n",
        "# Create a simple model\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(input_dim=text_vectorizer.vocabulary_size(), output_dim=16)(inputs)\n",
        "x = layers.GRU(8)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Create a labeled dataset (which includes unknown tokens)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0])\n",
        ")\n",
        "\n",
        "# Preprocess the string inputs, turning them into int sequences\n",
        "train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y))\n",
        "# Train the model on the int sequences\n",
        "print(\"\\nTraining model...\")\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
        "model.fit(train_dataset)\n",
        "\n",
        "# For inference, you can export a model that accepts strings as input\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "outputs = model(x)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Call the end-to-end model on test data (which includes unknown tokens)\n",
        "print(\"\\nCalling end-to-end model on test string...\")\n",
        "test_data = tf.constant([\"The one the other will absorb\"])\n",
        "test_output = end_to_end_model(test_data)\n",
        "print(\"Model output:\", test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e725dbcae3e4"
      },
      "source": [
        "Você pode ver a camada `TextVectorization` em ação, combinada com um modo `Embedding`, no exemplo [classificação de texto do zero](https://keras.io/examples/nlp/text_classification_from_scratch/).\n",
        "\n",
        "Observe que, ao treinar tal modelo, para melhor desempenho, você deve sempre usar a camada `TextVectorization` como parte do pipeline de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28c2f2ff61fb"
      },
      "source": [
        "### Codificando texto como uma matriz densa de ngrams com multi-hot encoding\n",
        "\n",
        "É assim que você deve pré-processar o texto a ser passado para uma camada `Dense`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bae1c223cd8"
      },
      "outputs": [],
      "source": [
        "# Define some text data to adapt the layer\n",
        "adapt_data = tf.constant(\n",
        "    [\n",
        "        \"The Brain is wider than the Sky\",\n",
        "        \"For put them side by side\",\n",
        "        \"The one the other will contain\",\n",
        "        \"With ease and You beside\",\n",
        "    ]\n",
        ")\n",
        "# Instantiate TextVectorization with \"multi_hot\" output_mode\n",
        "# and ngrams=2 (index all bigrams)\n",
        "text_vectorizer = layers.TextVectorization(output_mode=\"multi_hot\", ngrams=2)\n",
        "# Index the bigrams via `adapt()`\n",
        "text_vectorizer.adapt(adapt_data)\n",
        "\n",
        "# Try out the layer\n",
        "print(\n",
        "    \"Encoded text:\\n\", text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
        ")\n",
        "\n",
        "# Create a simple model\n",
        "inputs = keras.Input(shape=(text_vectorizer.vocabulary_size(),))\n",
        "outputs = layers.Dense(1)(inputs)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Create a labeled dataset (which includes unknown tokens)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0])\n",
        ")\n",
        "\n",
        "# Preprocess the string inputs, turning them into int sequences\n",
        "train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y))\n",
        "# Train the model on the int sequences\n",
        "print(\"\\nTraining model...\")\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
        "model.fit(train_dataset)\n",
        "\n",
        "# For inference, you can export a model that accepts strings as input\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "outputs = model(x)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Call the end-to-end model on test data (which includes unknown tokens)\n",
        "print(\"\\nCalling end-to-end model on test string...\")\n",
        "test_data = tf.constant([\"The one the other will absorb\"])\n",
        "test_output = end_to_end_model(test_data)\n",
        "print(\"Model output:\", test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "336a4d3426ed"
      },
      "source": [
        "### Codificando texto como uma matriz densa de ngrams com ponderação TF-IDF\n",
        "\n",
        "Esta é uma forma alternativa de pré-processar o texto antes de passá-lo para uma camada `Dense`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b6c0fec928e"
      },
      "outputs": [],
      "source": [
        "# Define some text data to adapt the layer\n",
        "adapt_data = tf.constant(\n",
        "    [\n",
        "        \"The Brain is wider than the Sky\",\n",
        "        \"For put them side by side\",\n",
        "        \"The one the other will contain\",\n",
        "        \"With ease and You beside\",\n",
        "    ]\n",
        ")\n",
        "# Instantiate TextVectorization with \"tf-idf\" output_mode\n",
        "# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\n",
        "text_vectorizer = layers.TextVectorization(output_mode=\"tf-idf\", ngrams=2)\n",
        "# Index the bigrams and learn the TF-IDF weights via `adapt()`\n",
        "\n",
        "with tf.device(\"CPU\"):\n",
        "    # A bug that prevents this from running on GPU for now.\n",
        "    text_vectorizer.adapt(adapt_data)\n",
        "\n",
        "# Try out the layer\n",
        "print(\n",
        "    \"Encoded text:\\n\", text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
        ")\n",
        "\n",
        "# Create a simple model\n",
        "inputs = keras.Input(shape=(text_vectorizer.vocabulary_size(),))\n",
        "outputs = layers.Dense(1)(inputs)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Create a labeled dataset (which includes unknown tokens)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0])\n",
        ")\n",
        "\n",
        "# Preprocess the string inputs, turning them into int sequences\n",
        "train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y))\n",
        "# Train the model on the int sequences\n",
        "print(\"\\nTraining model...\")\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
        "model.fit(train_dataset)\n",
        "\n",
        "# For inference, you can export a model that accepts strings as input\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "outputs = model(x)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Call the end-to-end model on test data (which includes unknown tokens)\n",
        "print(\"\\nCalling end-to-end model on test string...\")\n",
        "test_data = tf.constant([\"The one the other will absorb\"])\n",
        "test_output = end_to_end_model(test_data)\n",
        "print(\"Model output:\", test_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "143ce01c5558"
      },
      "source": [
        "## Detalhes importantes\n",
        "\n",
        "### Trabalhando com camadas de pesquisa com vocabulários muito grandes\n",
        "\n",
        "Você talvez se encontre trabalhando com um vocabulário muito grande numa `TextVectorization`, numa camada `StringLookup` ou numa camada `IntegerLookup`. Normalmente, um vocabulário maior que 500 MB já seria considerado \"muito grande\".\n",
        "\n",
        "Em tais casos, para melhor desempenho, evite usar `adapt()`. Em vez disso, pré-compute seu vocabulário com antecedência (você pode usar o Apache Beam ou o TF Transform para isso) e armazene-o em um arquivo. Em seguida, carregue o vocabulário na camada no momento da construção, passando o caminho do arquivo como o argumento `vocabulary`.\n",
        "\n",
        "### Usando camadas de pesquisa em um pod de TPU ou com `ParameterServerStrategy`.\n",
        "\n",
        "Há um bug importante que causa a degradação do desempenho ao usar uma camada `TextVectorization`, `StringLookup` ou `IntegerLookup` durante o treinamento em um pod de TPU ou em várias máquinas via `ParameterServerStrategy`. Isso está agendado para ser corrigido no TensorFlow 2.7."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocessing_layers.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
