{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# Use uma GPU\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/gpu\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/gpu.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/gpu.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/gpu.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoYIwe40vEPI"
      },
      "source": [
        "O código do TensorFlow e os modelos do `tf.keras` são executados de forma transparente em uma única GPU sem precisar de alterações no código.\n",
        "\n",
        "Observação: use `tf.config.list_physical_devices('GPU')` para confirmar que o TensorFlow está usando a GPU.\n",
        "\n",
        "A maneira mais simples de executar em várias GPUs, em uma ou várias máquinas, é usando as [estratégias de distribuição](distributed_training.ipynb).\n",
        "\n",
        "Este guia é para usuários que tentaram essas estratégias e descobriram que precisam controlar de forma granular como o TensorFlow usa a GPU. Para ver como fazer a depuração de problemas de desempenho para cenários com uma ou várias GPUs, confira o guia [Otimize o desempenho de GPUs no TensorFlow](gpu_performance_analysis.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Confirme se a versão de GPU do TensorFlow mais recente está instalada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZELutYNetv-v"
      },
      "source": [
        "## Visão geral\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "O TensorFlow tem suporte à execução de computações em diversos tipos de dispositivos, incluindo CPUs e GPUs, que são representadas por strings identificadores. Por exemplo:\n",
        "\n",
        "- `\"/device:CPU:0\"`: a CPU da sua máquina.\n",
        "- `\"/GPU:0\"`: notação abreviada para a primeira GPU da sua máquina visível no TensorFlow.\n",
        "- `\"/job:localhost/replica:0/task:0/device:GPU:1\"`: nome totalmente qualificado da segunda GPU da sua máquina visível no TensorFlow.\n",
        "\n",
        "Se uma operação do TensorFlow tiver implementações para CPUs e GPUs, por padrão o dispositivo com GPU será priorizado quando a operação for atribuída. Por exemplo, `tf.matmul` tem kernels para CPU e GPU e, em um sistema com dispositivos `CPU:0` e `GPU:0`, o dispositivo `GPU:0` será selecionado para executar `tf.matmul`, a menos que você solicite explicitamente que seja executado em outro dispositivo.\n",
        "\n",
        "Se uma operação do TensorFlow não tiver uma implementação para GPU correspondente, então a operaçãoo será feita no dispositivo com CPU. Por exemplo, como `tf.cast` tem apenas um kernel para CPU, em um sistema com dispositivos `CPU:0` e `GPU:0`, o dispositivo `CPU:0` será selecionado para executar `tf.cast`, mesmo que seja solicitado que execute no dispositivo `GPU:0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhNtHfuxCGVy"
      },
      "source": [
        "## Log do posicionamento dos dispositivos\n",
        "\n",
        "Para descobrir a quais dispositivos seus tensores e operações estão atribuídos, defina `tf.debugging.set_log_device_placement(True)` como a primeira declaração do seu programa. Ao ativar o log do posicionamento dos dispositivos, todas as alocações ou operações de tensores serão exibidas via print."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dbw0tpEirCd"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Create some tensors\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKhmFeraTdEI"
      },
      "source": [
        "O código acima exibirá via print que a operação `MatMul` foi executada na `GPU:0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U88FspwGjB7W"
      },
      "source": [
        "## Posicionamento manual dos dispositivos\n",
        "\n",
        "Se você quiser que uma operação específica seja executada em um dispositivo da sua escolha em vez do dispositivo selecionado automaticamente, pode usar `with tf.device` para criar um contexto de dispositivo, e todas as operações dentro desse contexto serão executadas no mesmo dispositivo designado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wqaQfEhjHit"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Place tensors on the CPU\n",
        "with tf.device('/CPU:0'):\n",
        "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "# Run on the GPU\n",
        "c = tf.matmul(a, b)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ixO89gRjJUu"
      },
      "source": [
        "Você verá que, agora, `a` e `b` estão atribuídos à `CPU:0`. Como um dispositivo não foi especificado explicitamente para a operação `MatMul`, o runtime do TensorFlow escolherá um baseado na operação e nos dispositivos disponíveis (`GPU:0` neste exemplo) e copiará automaticamente os tensores entre os dispositivos, se necessário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARrRhwqijPzN"
      },
      "source": [
        "## Como limitar o aumento de uso da memória das GPUs\n",
        "\n",
        "Por padrão, o TensorFlow mapeia praticamente toda a memória de GPU de todas as GPUs (sujeito aos [`CUDA_VISIBLE_DEVICES`](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars)) visíveis para o processo. Isso é feito para usar com mais eficiência os recursos de memória de GPU relativamente preciosos nos dispositivos ao reduzir a fragmentação de memória. Para limitar o TensorFlow a um conjunto de GPUs específico, use o método `tf.config.set_visible_devices`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPI--n_jhZhv"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3x4M55DhYk9"
      },
      "source": [
        "Em alguns casos, é desejável que o processo aloque somente uma parte da memória disponível ou aumente o uso de memória conforme o processo precisar. O TensorFlow conta com dois métodos para controlar esse comportamento.\n",
        "\n",
        "A primeira opção é ativar o aumento de memória chamando `tf.config.experimental.set_memory_growth`, que tenta alocar somente a quantidade de memória de GPU necessária para as alocações do runtime: ele começa alocando pouquíssima memória e, à medida que o programa é executado e mais memória de GPU se torna necessária, a região de memória de GPU é expandida para o processo do TensorFlow. A memória não é liberada, pois isso poderia levar à sua fragmentação. Para ativar o aumento de memória de uma GPU específica, use o código abaixo antes de alocar qualquer tensor ou executar qualquer operação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr3Kf1boFnCO"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1o8t51QFnmv"
      },
      "source": [
        "Outra forma de ativar essa opção é definindo a variável de ambiente `TF_FORCE_GPU_ALLOW_GROWTH` como `true`. Essa configuração é específica para a plataforma.\n",
        "\n",
        "O segundo método é configurar um dispositivo com GPUs virtuais por meio de `tf.config.set_logical_device_configuration` e definir um limite estrito para o total de memória de GPU que será alocada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qO2cS9QFn42"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
        "  try:\n",
        "    tf.config.set_logical_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Virtual devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsg1iLuHFoLW"
      },
      "source": [
        "Isso é útil se você quiser verdadeiramente vincular a quantidade de memória de GPU disponível ao processo do TensorFlow, o que é uma prática comum para desenvolvimento local quando a GPU é compartilhada com outras aplicações, como uma interface gráfica do usuário de uma estação de trabalho."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B27_-1gyjf-t"
      },
      "source": [
        "## Uso de uma única GPU em um sistema com várias GPUs\n",
        "\n",
        "Se você tiver mais de uma GPU em seu sistema, a GPU com o menor ID será selecionada por padrão. Se você quiser que a execução seja feita em uma GPU diferente, precisará especificar essa preferência explicitamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wep4iteljjG1"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "try:\n",
        "  # Specify an invalid GPU device\n",
        "  with tf.device('/device:GPU:2'):\n",
        "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "    c = tf.matmul(a, b)\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy-4cCO_jn4G"
      },
      "source": [
        "Se o dispositivo que você especificar não existir, será exibido um erro de dispositivo desconhecido: `RuntimeError`: `.../device:GPU:2 unknown device`.\n",
        "\n",
        "Se você quiser que o TensorFlow escolha automaticamente um dispositivo existente e compatível para executar as operações caso o dispositivo especificado não exista, pode chamar `tf.config.set_soft_device_placement(True)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sut_UHlkjvWd"
      },
      "outputs": [],
      "source": [
        "tf.config.set_soft_device_placement(True)\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Creates some tensors\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYTYPrQZj2d9"
      },
      "source": [
        "## Uso de várias GPUs\n",
        "\n",
        "Ao desenvolver para usar várias GPUs, o modelo poderá utilizar recursos adicionais. Ao desenvolver em um sistema com uma única GPU, você pode simular várias GPUs por meio de dispositivos virtuais, o que facilita o teste de ambientes com várias GPUs sem exigir recursos adicionais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EMGuGKbNkc6"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Create 2 virtual GPUs with 1GB memory each\n",
        "  try:\n",
        "    tf.config.set_logical_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
        "         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Virtual devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmNzO0FxNkol"
      },
      "source": [
        "Quando houver várias GPUs lógicas disponíveis para o runtime, você poderá utilizar as várias GPUs com `tf.distribute.Strategy` ou com posicionamento manual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDZmEGq4j6kG"
      },
      "source": [
        "#### Com `tf.distribute.Strategy`\n",
        "\n",
        "A prática recomendada de uso de várias GPUs é utilizar `tf.distribute.Strategy`. Veja um exemplo simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KgzY8V2AvRv"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.list_logical_devices('GPU')\n",
        "strategy = tf.distribute.MirroredStrategy(gpus)\n",
        "with strategy.scope():\n",
        "  inputs = tf.keras.layers.Input(shape=(1,))\n",
        "  predictions = tf.keras.layers.Dense(1)(inputs)\n",
        "  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy7nxlKsAxkK"
      },
      "source": [
        "Esse programa executará uma cópia do seu modelo em cada GPU, dividindo os dados de entrada entre elas. Isso também é conhecido como \"[paralelismo de dados](https://en.wikipedia.org/wiki/Data_parallelism)\".\n",
        "\n",
        "Confira mais informações sobre estratégias de distribuição [neste guia](./distributed_training.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8phxM5TVkAY_"
      },
      "source": [
        "#### Posicionamento manual\n",
        "\n",
        "`tf.distribute.Strategy` funciona por baixo dos panos por meio da replicação da computação entre os dispositivos. É possível implementar a replicação manualmente criando seu modelo em cada GPU. Por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqPo9ltUA_EY"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "gpus = tf.config.list_logical_devices('GPU')\n",
        "if gpus:\n",
        "  # Replicate your computation on multiple GPUs\n",
        "  c = []\n",
        "  for gpu in gpus:\n",
        "    with tf.device(gpu.name):\n",
        "      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "      c.append(tf.matmul(a, b))\n",
        "\n",
        "  with tf.device('/CPU:0'):\n",
        "    matmul_sum = tf.add_n(c)\n",
        "\n",
        "  print(matmul_sum)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "gpu.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
