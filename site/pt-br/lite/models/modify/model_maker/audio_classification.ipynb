{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XX46cTrh6iD"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sKrlWr6Kh-mF"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hST65kOHXpiL"
      },
      "source": [
        "# Aprendizado por transferência para áudio com o TensorFlow Lite Model Maker\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/audio_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/lite/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/lite/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/lite/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a> </td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelo do TF Hub</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB5k6xNKJ5Xe"
      },
      "source": [
        "Neste notebook do Colab, você verá como usar o [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker) (criador de modelos do TF Lite) para treinar um modelo personalizado de classificação de áudio.\n",
        "\n",
        "A biblioteca Model Maker usa aprendizado por transferência para simplificar o processo de treinar um modelo do TensorFlow Lite usando um dataset personalizado. Retreinar um modelo do TensorFlow Lite com seu próprio dataset personalizado reduz a quantidade necessária de dados de treinamento e tempo.\n",
        "\n",
        "Este notebook faz parte do [Codelab para personalizar um modelo de áudio e implantar no Android](https://codelabs.developers.google.com/codelabs/tflite-audio-classification-custom-model-android).\n",
        "\n",
        "Você usará um dataset personalizado de pássaros e exportará um modelo do TF Lite que pode ser usado em um celular, um modelo TensorFlow.JS que pode ser usado para inferência no navegador e também uma versão SavedModel que pode ser usada como serviço.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeZZ_cSsZfPx"
      },
      "source": [
        "## Instale as dependências\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbMc4vHjaYdQ"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install tflite-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2ck_Ghdcgt9"
      },
      "source": [
        "## Importe o TensorFlow, o Model Maker e outras bibliotecas\n",
        "\n",
        "Dentre as dependências necessárias, você usará o TensorFlow e o Model Maker. Além delas, as outras são para manipulação e reprodução de áudio, além de visualização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwUA9u4oWoCR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tflite_model_maker as mm\n",
        "from tflite_model_maker import audio_classifier\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import glob\n",
        "import random\n",
        "\n",
        "from IPython.display import Audio, Image\n",
        "from scipy.io import wavfile\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Model Maker Version: {mm.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIfm2TxKZAuA"
      },
      "source": [
        "## Dataset Birds\n",
        "\n",
        "O dataset Birds é uma coleção educativa do canto de cinco pássaros:\n",
        "\n",
        "- White-breasted Wood-Wren (Uirapuru-de-peito-branco)\n",
        "- House Sparrow (Pardal-doméstico)\n",
        "- Red Crossbill (Cruza-bico)\n",
        "- Chestnut-crowned Antpitta (Grallaria ruficapilla)\n",
        "- Azara's Spinetail (Synallaxis azarae)\n",
        "\n",
        "O áudio original vem de [Xeno-canto](https://www.xeno-canto.org/), um site dedicado ao compartilhamento de cantos de pássaros de todo o mundo.\n",
        "\n",
        "Vamos começar pelo download dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upNRfilkNSmr"
      },
      "outputs": [],
      "source": [
        "birds_dataset_folder = tf.keras.utils.get_file('birds_dataset.zip',\n",
        "                                                'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset.zip',\n",
        "                                                cache_dir='./',\n",
        "                                                cache_subdir='dataset',\n",
        "                                                extract=True)\n",
        "                                                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441bbzZ5d6oq"
      },
      "source": [
        "## Explore os dados\n",
        "\n",
        "Os áudios já estão divididos em pastas de teste e treinamento. Dentro de cada pasta, há uma pasta para cada pássaro, usando `bird_code` como nome.\n",
        "\n",
        "Todos os áudios são mono, com taxa de amostragem de 16 kHz.\n",
        "\n",
        "Confira mais informações sobre cada arquivo no arquivo `metadata.csv`, que contém todos os autores, licenças e informações adicionais. Para este tutorial, não é preciso ler esse arquivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayd7UqCfQQFU"
      },
      "outputs": [],
      "source": [
        "# @title [Run this] Util functions and data structures.\n",
        "\n",
        "data_dir = './dataset/small_birds_dataset'\n",
        "\n",
        "bird_code_to_name = {\n",
        "  'wbwwre1': 'White-breasted Wood-Wren',\n",
        "  'houspa': 'House Sparrow',\n",
        "  'redcro': 'Red Crossbill',  \n",
        "  'chcant2': 'Chestnut-crowned Antpitta',\n",
        "  'azaspi1': \"Azara's Spinetail\",   \n",
        "}\n",
        "\n",
        "birds_images = {\n",
        "  'wbwwre1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg/640px-Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg', # \tAlejandro Bayer Tamayo from Armenia, Colombia \n",
        "  'houspa': 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/House_Sparrow%2C_England_-_May_09.jpg/571px-House_Sparrow%2C_England_-_May_09.jpg', # \tDiliff\n",
        "  'redcro': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Red_Crossbills_%28Male%29.jpg/640px-Red_Crossbills_%28Male%29.jpg', #  Elaine R. Wilson, www.naturespicsonline.com\n",
        "  'chcant2': 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Chestnut-crowned_antpitta_%2846933264335%29.jpg/640px-Chestnut-crowned_antpitta_%2846933264335%29.jpg', # \tMike's Birds from Riverside, CA, US\n",
        "  'azaspi1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Synallaxis_azarae_76608368.jpg/640px-Synallaxis_azarae_76608368.jpg', # https://www.inaturalist.org/photos/76608368\n",
        "}\n",
        "\n",
        "test_files = os.path.abspath(os.path.join(data_dir, 'test/*/*.wav'))\n",
        "\n",
        "def get_random_audio_file():\n",
        "  test_list = glob.glob(test_files)\n",
        "  random_audio_path = random.choice(test_list)\n",
        "  return random_audio_path\n",
        "\n",
        "\n",
        "def show_bird_data(audio_path):\n",
        "  sample_rate, audio_data = wavfile.read(audio_path, 'rb')\n",
        "\n",
        "  bird_code = audio_path.split('/')[-2]\n",
        "  print(f'Bird name: {bird_code_to_name[bird_code]}')\n",
        "  print(f'Bird code: {bird_code}')\n",
        "  display(Image(birds_images[bird_code]))\n",
        "\n",
        "  plttitle = f'{bird_code_to_name[bird_code]} ({bird_code})'\n",
        "  plt.title(plttitle)\n",
        "  plt.plot(audio_data)\n",
        "  display(Audio(audio_data, rate=sample_rate))\n",
        "\n",
        "print('functions and data structures created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrv0uD7aXYl4"
      },
      "source": [
        "### Reproduza alguns áudios\n",
        "\n",
        "Para entender melhor os dados, vamos ouvir arquivos de áudio aleatórios do dataset de teste.\n",
        "\n",
        "Observação: posteriormente neste notebook, você executará a inferência nestes áudios para teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEeMZh-VQy97"
      },
      "outputs": [],
      "source": [
        "random_audio = get_random_audio_file()\n",
        "show_bird_data(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQj1Mf7YZELS"
      },
      "source": [
        "## Treine o modelo\n",
        "\n",
        "Ao usar o Model Maker para áudio, você precisa começar pela especificação do modelo, que será o modelo base do qual o novo modelo extrairá informações para aprender sobre as novas classes. Além disso, ele afeta como o dataset será transformado para respeitar os parâmetros de especificação dos modelos, como taxa de amostragem e número de canais.\n",
        "\n",
        "[YAMNet](https://tfhub.dev/google/yamnet/1) é um classificador de eventos de áudio treinado com o dataset AudioSet para prever eventos de áudios da ontologia AudioSet.\n",
        "\n",
        "É esperado que a entrada esteja em 16 kHz e tenha 1 canal.\n",
        "\n",
        "Você não precisa fazer novas amostragens, pois o Model Maker cuida dessa tarefa.\n",
        "\n",
        "- `frame_length` decide o tamanho de cada amostra de treinamento. Neste caso, EXPECTED_WAVEFORM_LENGTH * 3s\n",
        "\n",
        "- `frame_steps` indica a distância entre as amostras de treinamento. Neste caso, a amostra i começará EXPECTED_WAVEFORM_LENGTH * 6s após a amostra (i-1).\n",
        "\n",
        "O motivo de definir esses valores é poder contornar algumas limitações de datasets do mundo real.\n",
        "\n",
        "Por exemplo: no dataset Birds, os pássaros não cantam o tempo todo. Eles cantam, descansam, cantam novamente, e há ruídos nesses períodos. Usar um quadro longo ajuda a capturar o canto, mas, se for longo demais, reduz o número de amostras de treinamento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUcxtfHXY7XS"
      },
      "outputs": [],
      "source": [
        "spec = audio_classifier.YamNetSpec(\n",
        "    keep_yamnet_and_custom_heads=True,\n",
        "    frame_step=3 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,\n",
        "    frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF185yZ_M7zu"
      },
      "source": [
        "## Carregue os dados\n",
        "\n",
        "O Model Maker tem uma API para carregar os dados a partir de uma pasta e colocá-los no formato esperado pela especificação do modelo.\n",
        "\n",
        "Os datasets de treinamento e teste são baseados nas pastas. O dataset de validação será criado como 20% do de treinamento.\n",
        "\n",
        "Observação: é importante usar `cache=True` para deixar o treinamento posterior mais rápido, mas isso exigirá mais RAM para armazenar os dados. Para o dataset Birds, não é um problema, já que são apenas 300 MB, mas, se você usar seus próprios dados, tenha cuidado com isso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX0RqETqZgzo"
      },
      "outputs": [],
      "source": [
        "train_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, os.path.join(data_dir, 'train'), cache=True)\n",
        "train_data, validation_data = train_data.split(0.8)\n",
        "test_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, os.path.join(data_dir, 'test'), cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMghju-Rts2"
      },
      "source": [
        "## Treine o modelo\n",
        "\n",
        "O classificador de áudio audio_classifier conta com o método [`create`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/create), que cria um modelo e já começa a treiná-lo.\n",
        "\n",
        "Você pode personalizar diversos parâmetros. Confira mais informações na documentação.\n",
        "\n",
        "Neste primeiro teste, você usará as configurações padrão e fará o treinamento com 100 épocas.\n",
        "\n",
        "Observação: a primeira época leva mais tempo do que todas as outras porque é quando o cache é criado. Depois disso, cada época leva aproximadamente 1 segundo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r6Awvl4ZkIv"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "print('Training the model')\n",
        "model = audio_classifier.create(\n",
        "    train_data,\n",
        "    spec,\n",
        "    validation_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXMEHZkAxJTl"
      },
      "source": [
        "A exatidão está boa, mas é importante executar o passo de avaliação com os dados de teste para verificar se o modelo tem bons resultados com dados independentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDoQACMrZnOx"
      },
      "outputs": [],
      "source": [
        "print('Evaluating the model')\n",
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QRRAM39aOxS"
      },
      "source": [
        "## Sobre o modelo\n",
        "\n",
        "Ao treinar um classificador, é útil conferir a [matriz de confusão](https://en.wikipedia.org/wiki/Confusion_matrix), que fornece detalhes sobre o desempenho do classificador para os dados de teste.\n",
        "\n",
        "O Model Maker já cria a matriz de confusão para você."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqB3c0368iH3"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion, test_labels):\n",
        "  \"\"\"Compute confusion matrix and normalize.\"\"\"\n",
        "  confusion_normalized = confusion.astype(\"float\") / confusion.sum(axis=1)\n",
        "  axis_labels = test_labels\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.2f', square=True)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "confusion_matrix = model.confusion_matrix(test_data)\n",
        "show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gr1s7juBy7H"
      },
      "source": [
        "## Teste o modelo (opcional)\n",
        "\n",
        "Você pode testar o modelo com uma amostra de áudio do dataset de teste apenas para ver os resultados.\n",
        "\n",
        "Primeiro, obtenha o modelo de serviço."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmlmTl42Bq_u"
      },
      "outputs": [],
      "source": [
        "serving_model = model.create_serving_model()\n",
        "\n",
        "print(f'Model\\'s input shape and type: {serving_model.inputs}')\n",
        "print(f'Model\\'s output shape and type: {serving_model.outputs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQsZFO2mrYhx"
      },
      "source": [
        "Use o áudio aleatório que você carregou anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dv5ViK0reXc"
      },
      "outputs": [],
      "source": [
        "# if you want to try another file just uncoment the line below\n",
        "random_audio = get_random_audio_file()\n",
        "show_bird_data(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uixOfKSUj_9m"
      },
      "source": [
        "O modelo criado tem uma janela de entrada fixa.\n",
        "\n",
        "Para um determinado arquivo de áudio, você precisará dividi-lo em janelas de dados do tamanho esperado. Talvez a última janela precise ser preenchida com zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAvGKQL0lNty"
      },
      "outputs": [],
      "source": [
        "sample_rate, audio_data = wavfile.read(random_audio, 'rb')\n",
        "\n",
        "audio_data = np.array(audio_data) / tf.int16.max\n",
        "input_size = serving_model.input_shape[1]\n",
        "\n",
        "splitted_audio_data = tf.signal.frame(audio_data, input_size, input_size, pad_end=True, pad_value=0)\n",
        "\n",
        "print(f'Test audio path: {random_audio}')\n",
        "print(f'Original size of the audio data: {len(audio_data)}')\n",
        "print(f'Number of windows for inference: {len(splitted_audio_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLxKd0eFkMcR"
      },
      "source": [
        "Você fará um loop percorrendo todos os áudios divididos e aplicará o modelo em cada um deles.\n",
        "\n",
        "O modelo que você acabou de treinar tem duas saídas: a saída original do YAMNet e a que você treinou. Isso é importante, pois ambientes reais são mais complicados do que sons de pássaros. Você pode usar a saída do YAMNet para filtrar os áudios não relevantes. Por exemplo: para o dataset Birds, se o YAMNet não estiver classificando pássaros ou animais, isso pode mostrar que a saída do seu modelo pode ter uma classificação irrelevante.\n",
        "\n",
        "Veja abaixo as duas saídas para entender melhor a relação entre elas. A maioria dos erros que seu modelo comete são quando a previsão do YAMNet não estará relacionada à sua área (como pássaros)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-8fJLrxGwYT"
      },
      "outputs": [],
      "source": [
        "print(random_audio)\n",
        "\n",
        "results = []\n",
        "print('Result of the window ith:  your model class -> score,  (spec class -> score)')\n",
        "for i, data in enumerate(splitted_audio_data):\n",
        "  yamnet_output, inference = serving_model(data)\n",
        "  results.append(inference[0].numpy())\n",
        "  result_index = tf.argmax(inference[0])\n",
        "  spec_result_index = tf.argmax(yamnet_output[0])\n",
        "  t = spec._yamnet_labels()[spec_result_index]\n",
        "  result_str = f'Result of the window {i}: ' \\\n",
        "  f'\\t{test_data.index_to_label[result_index]} -> {inference[0][result_index].numpy():.3f}, ' \\\n",
        "  f'\\t({spec._yamnet_labels()[spec_result_index]} -> {yamnet_output[0][spec_result_index]:.3f})'\n",
        "  print(result_str)\n",
        "\n",
        "\n",
        "results_np = np.array(results)\n",
        "mean_results = results_np.mean(axis=0)\n",
        "result_index = mean_results.argmax()\n",
        "print(f'Mean result: {test_data.index_to_label[result_index]} -> {mean_results[result_index]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yASrikBgZ9ZO"
      },
      "source": [
        "## Exporte o modelo\n",
        "\n",
        "A última etapa é exportar o modelo que será usado em dispositivos embarcados ou no navegador.\n",
        "\n",
        "O método `export` exporta os dois formatos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw_ehPxAdQlz"
      },
      "outputs": [],
      "source": [
        "models_path = './birds_models'\n",
        "print(f'Exporing the TFLite model to {models_path}')\n",
        "\n",
        "model.export(models_path, tflite_filename='my_birds_model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjZRKmurA3y_"
      },
      "source": [
        "Além disso, você pode exportar a versão SavedModel para uso em serviço ou em um ambiente Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veBwppOsA-kn"
      },
      "outputs": [],
      "source": [
        "model.export(models_path, export_format=[mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xr0idac6xfi"
      },
      "source": [
        "## Próximos passos\n",
        "\n",
        "Você conseguiu.\n",
        "\n",
        "Agora, seu modelo pode ser implantado em dispositivos móveis usando a [API de tarefas AudioClassifier do TF Lite](https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier).\n",
        "\n",
        "Além disso, você pode fazer o mesmo processo com seus próprios dados e classes diferentes. Confira a documentação do [Model Maker para classificação de áudio](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier).\n",
        "\n",
        "Para saber mais, confira os aplicativos completos de referência para [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android/) e [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "audio_classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
