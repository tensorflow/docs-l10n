{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Resposta a perguntas BERT com o TensorFlow Lite Model Maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5Y7snSuG51"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/question_answer\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/lite/models/modify/model_maker/question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/lite/models/modify/model_maker/question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/lite/models/modify/model_maker/question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3q-gvm3cI8"
      },
      "source": [
        "A [biblioteca TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker) (criador de modelos do TF Lite) simplifica o processo de adaptar e converter um modelo do TensorFlow para dados de entrada específicos ao implantar esse modelo em aplicativos de aprendizado de máquina em dispositivos.\n",
        "\n",
        "Este notebook apresenta um exemplo completo que utiliza a biblioteca Model Maker para ilustrar a adaptação e conversão de um modelo de resposta a perguntas usado com frequência para tarefas de resposta a perguntas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxEHFTk755qw"
      },
      "source": [
        "# Introdução à tarefa de resposta a perguntas BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFbKTCF25-SG"
      },
      "source": [
        "A tarefa com suporte nesta biblioteca é uma tarefa de extrair a resposta a uma pergunta: dado um trecho e uma pergunta, a resposta está presente no trecho. A imagem abaixo mostra um exemplo de resposta a uma pergunta.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_squad_showcase.png\" width=\"500\"></p>\n",
        "\n",
        "<p align=\"center\">\n",
        "    <em>As respostas estão presentes no trecho (crédito da imagem: <a href=\"https://rajpurkar.github.io/mlx/qa-and-squad/\">blog SQuAD</a>)</em>\n",
        "</p>\n",
        "\n",
        "Quanto ao modelo de tarefa de resposta a perguntas, as entradas devem ser o par trecho/pergunta que já foram pré-processadas, e as saídas devem ser os logits de início e fim para cada token do trecho. O tamanho da entrada pode ser definido e ajustado de acordo com o tamanho do trecho e da pergunta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb7P4WQta8Ub"
      },
      "source": [
        "## Visão geral completa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7cIHjIfbDlG"
      },
      "source": [
        "O trecho de código abaixo demonstra como obter o modelo com algumas linhas de código. O processo geral inclui 5 etapas: (1) escolher um modelo, (2) carregar dados, (3) retreinar o modelo, (4) avaliar e (5) exportá-lo para o formato do TensorFlow Lite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQPdlxZBYuZG"
      },
      "source": [
        "```python\n",
        "# Chooses a model specification that represents the model.\n",
        "spec = model_spec.get('mobilebert_qa')\n",
        "\n",
        "# Gets the training data and validation data.\n",
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)\n",
        "\n",
        "# Fine-tunes the model.\n",
        "model = question_answer.create(train_data, model_spec=spec)\n",
        "\n",
        "# Gets the evaluation result.\n",
        "metric = model.evaluate(validation_data)\n",
        "\n",
        "# Exports the model to the TensorFlow Lite format with metadata in the export directory.\n",
        "model.export(export_dir)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exScAdvBbNEi"
      },
      "source": [
        "As próximas seções explicam o código com maiores detalhes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Pré-requisitos\n",
        "\n",
        "Para executar este exemplo, instale os pacotes exigidos, incluindo o pacote do Model Maker no [repositório do GitHub](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install -q tflite-model-maker-nightly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "Importe os pacotes necessários."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import question_answer\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.question_answer import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l65ctmtW7_FF"
      },
      "source": [
        "A \"Visão geral completa\" demonstra um exemplo completo simples. A próxima seção mostra mais detalhes do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ_B8fMDOhMR"
      },
      "source": [
        "## Escolha um model_spec que represente um modelo de resposta a perguntas\n",
        "\n",
        "Cada objeto `model_spec` representa um modelo específico de resposta a perguntas. Atualmente, o Model Maker tem suporte a modelos MobileBERT e BERT-Base.\n",
        "\n",
        "Modelo com suporte | Nome de model_spec | Descrição do modelo\n",
        "--- | --- | ---\n",
        "[MobileBERT](https://arxiv.org/pdf/2004.02984.pdf) | 'mobilebert_qa' | 4,3 vezes menor e 5,5 vezes mais rápido do que o BERT-Base, alcançando resultados competitivos, adequados para aplicativos em dispositivos.\n",
        "[MobileBERT-SQuAD](https://arxiv.org/pdf/2004.02984.pdf) | 'mobilebert_qa_squad' | Mesma arquitetura de modelo do MobileBERT, e o modelo inicial já é retreinado com [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/).\n",
        "[BERT-Base](https://arxiv.org/pdf/1810.04805.pdf) | 'bert_qa' | Modelo BERT padrão amplamente usado em tarefas de NLP.\n",
        "\n",
        "Neste tutorial, [MobileBERT-SQuAD](https://arxiv.org/pdf/2004.02984.pdf) é usado como exemplo. Como o modelo já foi retreinado com [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/), pode convergir mais rapidamente em tarefas de resposta a perguntas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEAWuZQ1PFiX"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('mobilebert_qa_squad')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEncJxtl-nQ"
      },
      "source": [
        "## Carregue dados de entrada específicos em um aplicativo de aprendizado de máquina em dispositivo e pré-processe os dados\n",
        "\n",
        "[TriviaQA](https://nlp.cs.washington.edu/triviaqa/) é um dataset de compreensão de leitura que contém mais de 650 mil tuplas pergunta-resposta-evidência. Neste tutorial, você usará um subconjunto desse dataset para aprender a usar a biblioteca Model Maker.\n",
        "\n",
        "Para carregar os dados, converta o dataset TriviaQA para o formato [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/) executando o [script conversor do Python](https://github.com/mandarjoshi90/triviaqa#miscellaneous) com `--sample_size=8000` e um conjunto de dados `web`. Modifique ligeiramente o código de conversão:\n",
        "\n",
        "- Ignore as amostras que não conseguiram encontrar uma resposta no documento do contexto.\n",
        "- Obtenha a resposta original no contexto sem diferenciar letras maiúsculas ou minúsculas.\n",
        "\n",
        "Baixe a versão arquivada do dataset já convertido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tOfUr2KlgpU"
      },
      "outputs": [],
      "source": [
        "train_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-web-train-8000.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json')\n",
        "validation_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-verified-web-dev.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfZk8GNr_1nc"
      },
      "source": [
        "Você também pode treinar o modelo MobileBERT com seu próprio dataset. Se você estiver executando este notebook no Colab, carregue os dados pela barra lateral esquerda.\n",
        "\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_question_answer.png\" width=\"800\" hspace=\"100\" alt=\"Upload File\">\n",
        "\n",
        "Se você preferir não carregar os dados na nuvem, pode executar a biblioteca offline de acordo com este [guia](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E051HBUM5owi"
      },
      "source": [
        "Use o método `DataLoader.from_squad` para carregar e pré-processar os dados no [formato SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) para um `model_spec` específico. Você pode usar o formato SQuAD2.0 ou SQuAD1.1. Ao definir o parâmetro `version_2_with_negative` como `True` (verdadeiro), o formato será SQuAD2.0. Caso contrário, será SQuAD1.1. Por padrão, `version_2_with_negative` é `False` (falso)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_fOlZsklmlL"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWuoensX4vDA"
      },
      "source": [
        "## Personalize o modelo do TensorFlow\n",
        "\n",
        "Crie um modelo personalizado de resposta a perguntas com base nos dados carregados. A função `create` consiste nas seguintes etapas:\n",
        "\n",
        "1. Cria o modelo de resposta a perguntas de acordo com `model_spec`.\n",
        "2. Treina o modelo de resposta a perguntas. As épocas padrão e o tamanho de lote padrão são definidos pelas duas variáveis `default_training_epochs` e `default_batch_size` no objeto `model_spec`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvYSUuJY3QxR"
      },
      "outputs": [],
      "source": [
        "model = question_answer.create(train_data, model_spec=spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JKI-pNc8idH"
      },
      "source": [
        "Confira a estrutura detalhada do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd7Hs8TF8n3H"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5FPk_tOxoZ"
      },
      "source": [
        "## Avalie o modelo personalizado\n",
        "\n",
        "Avalie o modelo e os dados de validação e obtenha um dicionário das métricas, incluindo a pontuação `f1` e `exact match`, etc. Observe que as métricas de SQuAD1.1 e SQuAD2.0 são diferentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8c2ZQ0J3Riy"
      },
      "outputs": [],
      "source": [
        "model.evaluate(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHoGAceO2xV"
      },
      "source": [
        "## Exporte para um modelo do TensorFlow Lite\n",
        "\n",
        "Converta o modelo treinado para o formato de modelos do TensorFlow Lite com [metadados](https://www.tensorflow.org/lite/models/convert/metadata) para poder usá-lo posteriormente em um aplicativo de aprendizado de máquina em dispositivos. O arquivo de vocabulário é incorporado aos metadados. O nome de arquivo padrão do TF Lite é `model.tflite`.\n",
        "\n",
        "Em diversos aplicativos de aprendizado de máquina em dispositivos, o tamanho do modelo é um fator importante. Portanto, recomendamos aplicar quantização no modelo para deixá-lo menor e possivelmente mais rápido. Para modelos BERT e MobileBERT, a técnica padrão de quantização pós-treinamento é a quantização de intervalo dinâmico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6wA9lK3TQB"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w12kvDdHJIGH"
      },
      "source": [
        "Você pode usar o arquivo de modelo do TensorFlow Lite no aplicativo de referência [bert_qa](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android) utilizando a [API BertQuestionAnswerer](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_question_answerer) na [biblioteca Task do TensorFlow Lite](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview), basta baixá-lo na barra lateral esquerda do Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFnJPvq3VGh3"
      },
      "source": [
        "Confira abaixo os formatos de exportação permitidos:\n",
        "\n",
        "- `ExportFormat.TFLITE`\n",
        "- `ExportFormat.VOCAB`\n",
        "- `ExportFormat.SAVED_MODEL`\n",
        "\n",
        "Por padrão, só é exportado o modelo do TensorFlow Lite com metadados. Você também pode exportar diferentes arquivos seletivamente. Por exemplo: exporte somente o arquivo de vocabulário da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro2hz4kXVImY"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', export_format=ExportFormat.VOCAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZKYthlVrTos"
      },
      "source": [
        "E você pode avaliar o modelo do TF Lite com o método `evaluate_tflite`. É esperado que essa etapa demore um tempo longo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ochbq95ZrVFX"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('model.tflite', validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWiA_zX8rxE"
      },
      "source": [
        "## Uso avançado\n",
        "\n",
        "A função `create` é uma parte essencial dessa biblioteca, em que o parâmetro `model_spec` define a especificação do modelo. Atualmente, há suporte à classe `BertQASpec`. Há dois modelos: MobileBERT e BERT-Base. A função `create` consiste nas seguintes etapas:\n",
        "\n",
        "1. Cria o modelo de resposta a perguntas de acordo com `model_spec`.\n",
        "2. Treina o modelo de resposta a perguntas.\n",
        "\n",
        "Nesta seção, descreveremos diversos tópicos avançados, incluindo como alterar o modelo, ajustar os hiperparâmetros de treinamento, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtiksguDfhl"
      },
      "source": [
        "### Ajuste o modelo\n",
        "\n",
        "Você pode ajustar a infraestrutura do modelo, como os parâmetros `seq_len` e `query_len`, na classe `BertQASpec`.\n",
        "\n",
        "Parâmetros do modelo ajustáveis:\n",
        "\n",
        "- `seq_len`: tamanho do trecho a ser alimentado no modelo.\n",
        "- `query_len`: tamanho da pergunta a ser alimentada no modelo.\n",
        "- `doc_stride`: stride ao usar a estratégia de janela deslizante para pegar partes do documento.\n",
        "- `initializer_range`: desvio padrão do truncated_normal_initializer para inicializar as matrizes de pesos.\n",
        "- `trainable`: booleano, indica se a camada pré-treinada é treinável.\n",
        "\n",
        "Parâmetros do pipeline de treinamento ajustáveis:\n",
        "\n",
        "- `model_dir`: local dos arquivos de checkpoint do modelo. Caso não seja definido, será usado um diretório temporário.\n",
        "- `dropout_rate`: taxa de dropout.\n",
        "- `learning_rate`: taxa de aprendizado inicial para Adam.\n",
        "- `predict_batch_size`: tamanho do lote para previsão.\n",
        "- `tpu`: endereço da TPU à qual se conectar. Usado somente ao utilizar TPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAOd5_bzH9AQ"
      },
      "source": [
        "Por exemplo: você pode treinar o modelo com um tamanho de sequência maior. Se você alterar o modelo, primeiro precisa construir um novo `model_spec`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9WBN0UTQoMN"
      },
      "outputs": [],
      "source": [
        "new_spec = model_spec.get('mobilebert_qa')\n",
        "new_spec.seq_len = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LSTdghTP0Cv"
      },
      "source": [
        "As outras etapas são as mesmas. Observação: você precisa executar novamente tanto `dataloader` quanto `create`, pois especificações diferentes de modelos podem ter etapas de pré-processamento diferentes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQuy7RSDir3"
      },
      "source": [
        "### Ajuste os hiperparâmetros de treinamento\n",
        "\n",
        "Você também pode ajustar os hiperparâmetros de treinamento, como `epochs` e `batch_size`, o que impacta o desempenho do modelo. Por exemplo:\n",
        "\n",
        "- `epochs`: mais épocas podem levar a um desempenho melhor, mas podem causar overfitting.\n",
        "- `batch_size`: número de amostras a serem usadas em um passo de treinamento.\n",
        "\n",
        "Por exemplo, você pode treinar com mais épocas e um tamanho de lote maior:\n",
        "\n",
        "```python\n",
        "model = question_answer.create(train_data, model_spec=spec, epochs=5, batch_size=64)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq6B9lKMfhS6"
      },
      "source": [
        "### Altere a arquitetura do modelo\n",
        "\n",
        "É possível alterar o modelo base usado para treinar os dados mudando `model_spec`. Por exemplo, para alterar para o modelo BERT-Base, execute:\n",
        "\n",
        "```python\n",
        "spec = model_spec.get('bert_qa')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2d7yycrgu6L"
      },
      "source": [
        "As outras etapas são as mesmas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFQrDMXzOVoB"
      },
      "source": [
        "### Personalize a quantização pós-treinamento em um modelo do TensorFlow Lite\n",
        "\n",
        "A [quantização pós-treinamento](https://www.tensorflow.org/lite/performance/post_training_quantization) é uma técnica de conversão que pode reduzir o tamanho do modelo e a latência de inferência, além de aumentar a velocidade de inferência da CPU e do acelerador de hardware com uma pequena redução da exatidão do modelo. A quantização é amplamente utilizada para otimizar o modelo.\n",
        "\n",
        "A biblioteca Model Maker aplica uma técnica padrão de quantização pós-treinamento ao exportar o modelo. Se você quiser personalizar a quantização pós-treinamento, o Model Maker oferece suporte a diversas opções usando [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig). Vejamos a quantização de float16 como exemplo. Primeiro, definimos a configuração de quantização.\n",
        "\n",
        "```python\n",
        "config = QuantizationConfig.for_float16()\n",
        "```\n",
        "\n",
        "Em seguida, exportamos o modelo do TensorFlow Lite com essa configuração.\n",
        "\n",
        "```python\n",
        "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPVopCeB6LV6"
      },
      "source": [
        "# Saiba mais\n",
        "\n",
        "Leia o exemplo de [pergunta e resposta BERT](https://www.tensorflow.org/lite/examples/bert_qa/overview) para aprender os detalhes técnicos. Confira mais informações em:\n",
        "\n",
        "- [Guia](https://www.tensorflow.org/lite/models/modify/model_maker) e [referência da API](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker) do TensorFlow Lite Model Maker.\n",
        "- Task Library: [BertQuestionAnswerer](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_question_answerer) para implantação.\n",
        "- Aplicativos de referência completos para [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android) e [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/ios)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "question_answer.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
