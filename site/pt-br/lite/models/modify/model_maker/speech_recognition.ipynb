{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XX46cTrh6iD"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sKrlWr6Kh-mF"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hST65kOHXpiL"
      },
      "source": [
        "# Retreine um modelo de reconhecimento de fala com o TensorFlow Lite Model Maker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nShlCXGkbRVA"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/speech_recognition\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/lite/models/modify/model_maker/speech_recognition.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/lite/models/modify/model_maker/speech_recognition.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/lite/models/modify/model_maker/speech_recognition.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB5k6xNKJ5Xe"
      },
      "source": [
        "Neste notebook do Colab, você aprenderá a usar o [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker) (criador de modelos do TF Lite) para treinar um modelo de reconhecimento de fala que consegue classificar palavras ou frases curtas faladas usando amostras de som de um segundo. A biblioteca Model Maker usa aprendizado por transferência para retreinar um modelo do TensorFlow com um novo dataset, reduzindo a quantidade de dados de amostra e o tempo necessários para o treinamento.\n",
        "\n",
        "Por padrão, este notebook retreina o modelo (BrowserFft, do [TFJS Speech Command Recognizer](https://github.com/tensorflow/tfjs-models/tree/master/speech-commands#speech-command-recognizer)) usando um subconjunto de palavras do [dataset de comandos de voz](https://www.tensorflow.org/datasets/catalog/speech_commands) (como \"up\" – cima, \"down\" – baixo, \"left\" – esquerda e \"right\" – direita). Em seguida, ele exporta um modelo do TF Lite que você pode executar em um dispositivo móvel ou sistema embarcado (como o Raspberry Pi). Além disso, exporta o modelo treinado como um SavedModel do TensorFlow.\n",
        "\n",
        "Este notebook também aceita um dataset personalizado de arquivos WAV, carregados no Colab em um arquivo ZIP. Quanto mais amostras você tiver para cada classe, maior será a exatidão, mas, como o processo de aprendizado por transferência usa embeddings de características do modelo pré-treinado, você ainda pode conseguir um modelo bastante exato com apenas algumas dezenas de amostras para cada uma das classes.\n",
        "\n",
        "**Observação:** o modelo que vamos treinar é otimizado para reconhecimento de fala com amostras de um segundo. Se você quiser fazer uma classificação de áudio mais genérica (como detectar diferentes tipos de música), sugerimos que confira [este Colab para retreinar um classificador de áudio](https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb).\n",
        "\n",
        "Se você quiser executar o notebook com o dataset de falas padrão, pode executar agora mesmo clicando em **Runtime &gt; Run all** (Runtime &gt; Executar tudo) na barra de ferramentas do Colab. Porém, se você quiser usar seu próprio dataset, prossiga para a seção [Prepare o dataset](#scrollTo=cBsSAeYLkc1Z) e siga as instruções.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeZZ_cSsZfPx"
      },
      "source": [
        "### Importe os pacotes necessários\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MelHQlE7FVue"
      },
      "source": [
        "Você precisará do TensorFlow, TF Lite Model Maker e alguns módulos para manipulação e reprodução de áudio, além de visualização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbMc4vHjaYdQ"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install tflite-model-maker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwUA9u4oWoCR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import tflite_model_maker as mm\n",
        "from tflite_model_maker import audio_classifier\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Model Maker Version: {mm.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBsSAeYLkc1Z"
      },
      "source": [
        "## Prepare o dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTTSXJxJq2Bz"
      },
      "source": [
        "Para treinar com o dataset de falas padrão, basta executar todo o código abaixo, sem fazer alterações.\n",
        "\n",
        "Porém, se você quiser treinar com seu próprio dataset de falas, siga estas etapas:\n",
        "\n",
        "**Observação:** o modelo que você vai retreinar espera que os dados de entrada sejam áudios de cerca de um segundo com 44,1 kHz. O Model Maker faz a reamostragem automática do dataset de treinamento, então você não precisa fazer a reamostragem do dataset se ele tiver uma taxa de amostragem diferente de 44,1 kHz. Mas saiba que as amostras de áudio com mais de um segundo serão divididas em diversos pedaços de um segundo, e o pedaço final será descartado se tiver duração menor do que um segundo.\n",
        "\n",
        "1. Confirme se cada amostra do seu dataset está no **formato de arquivo WAV com cerca de um segundo de duração**. Em seguida, crie um arquivo ZIP com todos os arquivos WAV organizados em subpastas separadas para cada classificação. Por exemplo: cada amostra de um comando de voz \"yes\" (sim) deve estar em uma subpasta chamada \"yes\". Mesmo se você tiver somente uma classe, as amostras precisam ser salvas em um subdiretório cujo nome seja o nome da classe (esse script pressupõe que seu dataset **não esteja dividido** em conjuntos de treinamento/validação/teste e faz a divisão para você).\n",
        "2. Clique na guia **Files** (Arquivos) no painel esquerdo e arraste seu arquivo ZIP para lá para carregá-lo.\n",
        "3. Use a seguinte opção do menu suspenso para definir **`use_custom_dataset`** como True (verdadeiro).\n",
        "4. Em seguida, vá à seção [Prepare um dataset de áudio personalizado](#scrollTo=EobYerLQkiF1) para especificar seu nome de arquivo ZIP e nome do diretório do dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AK9o98X7qyhU"
      },
      "outputs": [],
      "source": [
        "use_custom_dataset = False #@param [\"False\", \"True\"] {type:\"raw\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2sNXbYVHjjy"
      },
      "source": [
        "### Gere um dataset de ruídos de fundo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBVClNMwHtMD"
      },
      "source": [
        "Não importa se você esteja usando o dataset de falas padrão ou um personalizado, precisa ter um bom conjunto de ruídos de fundo para que o modelo consiga distinguir falas de outros ruídos (incluindo silêncio).\n",
        "\n",
        "Como as amostras de fundo abaixo são fornecidas em arquivos WAV com um minuto ou mais, precisamos dividi-las em amostras menores, de um segundo, para reservarmos alguns para o dataset de teste. Também vamos combinar algumas fontes de amostra diferentes para criar um conjunto abrangente de ruídos de fundo e silêncio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvJd9VfmHu29"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.get_file('speech_commands_v0.01.tar.gz',\n",
        "                        'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='dataset-speech',\n",
        "                        extract=True)\n",
        "tf.keras.utils.get_file('background_audio.zip',\n",
        "                        'https://storage.googleapis.com/download.tensorflow.org/models/tflite/sound_classification/background_audio.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='dataset-background',\n",
        "                        extract=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CAVFc3woB3_"
      },
      "source": [
        "**Observação:** embora exista uma versão mais nova disponível, estamos usando a v0.01 do dataset de comandos de voz porque é menor. A v0.01 inclui 30 comandos, enquanto a v0.02 adiciona mais cinco (\"backward\" – para trás, \"forward\" – para frente, \"follow\" – siga, \"learn\" – aprenda e \"visual\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgwWNifGL-3b"
      },
      "outputs": [],
      "source": [
        "# Create a list of all the background wav files\n",
        "files = glob.glob(os.path.join('./dataset-speech/_background_noise_', '*.wav'))\n",
        "files = files + glob.glob(os.path.join('./dataset-background', '*.wav'))\n",
        "\n",
        "background_dir = './background'\n",
        "os.makedirs(background_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all files and split each into several one-second wav files\n",
        "for file in files:\n",
        "  filename = os.path.basename(os.path.normpath(file))\n",
        "  print('Splitting', filename)\n",
        "  name = os.path.splitext(filename)[0]\n",
        "  rate = librosa.get_samplerate(file)\n",
        "  length = round(librosa.get_duration(filename=file))\n",
        "  for i in range(length - 1):\n",
        "    start = i * rate\n",
        "    stop = (i * rate) + rate\n",
        "    data, _ = sf.read(file, start=start, stop=stop)\n",
        "    sf.write(os.path.join(background_dir, name + str(i) + '.wav'), data, rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVlvVq-SkeeO"
      },
      "source": [
        "### Prepare o dataset de comandos de voz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_q22T5UHbJG"
      },
      "source": [
        "Já baixamos o dataset de comandos de voz, então agora só precisamos eliminar um determinado número de classes para nosso modelo.\n",
        "\n",
        "Esse dataset inclui mais de 30 classificações de comando de voz, e a maioria delas tem mais de 2.000 amostras. Porém, como estamos usando aprendizado por transferência, não precisamos de tantas amostras assim. Dessa forma, o código abaixo faz o seguinte:\n",
        "\n",
        "- Especifica quais classificações queremos usar e exclui as restantes.\n",
        "- Mantém somente 150 amostras de cada classe para treinamento (para comprovar que o aprendizado por transferência funciona bem mesmo com datasets menores e reduz o tempo de treinamento).\n",
        "- Cria um diretório separado para um dataset de teste para que possamos executar a inferência posteriormente com facilidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUSRpw2nOp8p"
      },
      "outputs": [],
      "source": [
        "if not use_custom_dataset:\n",
        "  commands = [ \"up\", \"down\", \"left\", \"right\", \"go\", \"stop\", \"on\", \"off\", \"background\"]\n",
        "  dataset_dir = './dataset-speech'\n",
        "  test_dir = './dataset-test'\n",
        "\n",
        "  # Move the processed background samples\n",
        "  shutil.move(background_dir, os.path.join(dataset_dir, 'background'))   \n",
        "\n",
        "  # Delete all directories that are not in our commands list\n",
        "  dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
        "  for dir in dirs:\n",
        "    name = os.path.basename(os.path.normpath(dir))\n",
        "    if name not in commands:\n",
        "      shutil.rmtree(dir)\n",
        "\n",
        "  # Count is per class\n",
        "  sample_count = 150\n",
        "  test_data_ratio = 0.2\n",
        "  test_count = round(sample_count * test_data_ratio)\n",
        "\n",
        "  # Loop through child directories (each class of wav files)\n",
        "  dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
        "  for dir in dirs:\n",
        "    files = glob.glob(os.path.join(dir, '*.wav'))\n",
        "    random.seed(42)\n",
        "    random.shuffle(files)\n",
        "    # Move test samples:\n",
        "    for file in files[sample_count:sample_count + test_count]:\n",
        "      class_dir = os.path.basename(os.path.normpath(dir))\n",
        "      os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)\n",
        "      os.rename(file, os.path.join(test_dir, class_dir, os.path.basename(file)))\n",
        "    # Delete remaining samples\n",
        "    for file in files[sample_count + test_count:]:\n",
        "      os.remove(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EobYerLQkiF1"
      },
      "source": [
        "### Prepare um dataset personalizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3xTvDP3knMd"
      },
      "source": [
        "Se você quiser treinar o modelo com seu próprio dataset de falas, precisa carregar suas amostras como arquivos WAV em um arquivo ZIP ([conforme descrito acima](#scrollTo=cBsSAeYLkc1Z)) e modificar as seguintes variáveis para especificar seu dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77PsQAKA4Arx"
      },
      "outputs": [],
      "source": [
        "if use_custom_dataset:\n",
        "  # Specify the ZIP file you uploaded:\n",
        "  !unzip YOUR-FILENAME.zip\n",
        "  # Specify the unzipped path to your custom dataset\n",
        "  # (this path contains all the subfolders with classification names):\n",
        "  dataset_dir = './YOUR-DIRNAME'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwp6EQqvttgf"
      },
      "source": [
        "Após alterar o nome do arquivo e o caminho acima, você já pode treinar o modelo com seu dataset personalizado. Na barra de ferramentas do Colab, selecione **Runtime &gt; Run all** (Runtime &gt; Executar tudo) para executar todo o notebook.\n",
        "\n",
        "O código abaixo integra nossas novas amostras de ruídos de fundo ao dataset e depois separa uma parte de todas as amostras para criar um dataset de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMQ6cpw_B9e_"
      },
      "outputs": [],
      "source": [
        "def move_background_dataset(dataset_dir):\n",
        "  dest_dir = os.path.join(dataset_dir, 'background')\n",
        "  if os.path.exists(dest_dir):\n",
        "    files = glob.glob(os.path.join(background_dir, '*.wav'))\n",
        "    for file in files:\n",
        "      shutil.move(file, dest_dir)\n",
        "  else:\n",
        "    shutil.move(background_dir, dest_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45iru8OdliG3"
      },
      "outputs": [],
      "source": [
        "if use_custom_dataset:\n",
        "  # Move background samples into custom dataset\n",
        "  move_background_dataset(dataset_dir)\n",
        "\n",
        "  # Now we separate some of the files that we'll use for testing:\n",
        "  test_dir = './dataset-test'\n",
        "  test_data_ratio = 0.2\n",
        "  dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
        "  for dir in dirs:\n",
        "    files = glob.glob(os.path.join(dir, '*.wav'))\n",
        "    test_count = round(len(files) * test_data_ratio)\n",
        "    random.seed(42)\n",
        "    random.shuffle(files)\n",
        "    # Move test samples:\n",
        "    for file in files[:test_count]:\n",
        "      class_dir = os.path.basename(os.path.normpath(dir))\n",
        "      os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)\n",
        "      os.rename(file, os.path.join(test_dir, class_dir, os.path.basename(file)))\n",
        "    print('Moved', test_count, 'images from', class_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPa1dfEoagz"
      },
      "source": [
        "### Reproduza uma amostra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jecBYREgMk6"
      },
      "source": [
        "Para confirmar se o dataset parece correto, vamos reproduzir uma amostra aleatória do dataset de teste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLC3ayJsoeNw"
      },
      "outputs": [],
      "source": [
        "def get_random_audio_file(samples_dir):\n",
        "  files = os.path.abspath(os.path.join(samples_dir, '*/*.wav'))\n",
        "  files_list = glob.glob(files)\n",
        "  random_audio_path = random.choice(files_list)\n",
        "  return random_audio_path\n",
        "\n",
        "def show_sample(audio_path):\n",
        "  audio_data, sample_rate = sf.read(audio_path)\n",
        "  class_name = os.path.basename(os.path.dirname(audio_path))\n",
        "  print(f'Class: {class_name}')\n",
        "  print(f'File: {audio_path}')\n",
        "  print(f'Sample rate: {sample_rate}')\n",
        "  print(f'Sample length: {len(audio_data)}')\n",
        "\n",
        "  plt.title(class_name)\n",
        "  plt.plot(audio_data)\n",
        "  display(Audio(audio_data, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "todbtEWFy0mj"
      },
      "outputs": [],
      "source": [
        "random_audio = get_random_audio_file(test_dir)\n",
        "show_sample(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-jRIWcQv7xt"
      },
      "source": [
        "## Defina o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQj1Mf7YZELS"
      },
      "source": [
        "Ao usar o Model Maker para retreinar qualquer modelo, você precisa começar pela definição de uma especificação do modelo. A especificação define o modelo base do qual o novo modelo extrairá os embeddings de características para começar a aprender novas classes. A especificação para este reconhecedor de fala é baseado no [modelo BrowserFft pré-treinado do TFJS](https://github.com/tensorflow/tfjs-models/tree/master/speech-commands#speech-command-recognizer).\n",
        "\n",
        "O modelo espera que a entrada seja uma amostra de áudio com 44,1 kHz e com menos de um segundo: o tamanho exato da amostra deve ser de 44.034 quadros.\n",
        "\n",
        "Você não precisa fazer reamostragem do dataset de treinamento, pois o Model Maker cuida dessa tarefa. Mas, quando você executar a inferência posteriormente, deve garantir que a entrada tenha o formato esperado.\n",
        "\n",
        "Você só precisa instanciar [`BrowserFftSpec`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUcxtfHXY7XS"
      },
      "outputs": [],
      "source": [
        "spec = audio_classifier.BrowserFftSpec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maTOoRvAwI9l"
      },
      "source": [
        "## Carregue seu dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UASCEHoVwQ1q"
      },
      "source": [
        "Agora você precisa carregar seu dataset de acordo com as especificações do modelo. O Model Maker inclui a API [`DataLoader`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/DataLoader), que carrega seu dataset a partir de uma pasta e garante que ele esteja no formato esperado segundo a especificação do modelo.\n",
        "\n",
        "Já reservamos alguns arquivos de teste quando os movemos para um diretório separado, o que facilita a execução da inferência posteriormente. Agora, vamos criar um `DataLoader` para cada divisão: o dataset de treinamento, validação e teste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAhAfHwiw2_F"
      },
      "source": [
        "#### Carregue o dataset de comandos de voz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX0RqETqZgzo"
      },
      "outputs": [],
      "source": [
        "if not use_custom_dataset:\n",
        "  train_data_ratio = 0.8\n",
        "  train_data = audio_classifier.DataLoader.from_folder(\n",
        "      spec, dataset_dir, cache=True)\n",
        "  train_data, validation_data = train_data.split(train_data_ratio)\n",
        "  test_data = audio_classifier.DataLoader.from_folder(\n",
        "      spec, test_dir, cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OWQ_O9_t-C-"
      },
      "source": [
        "#### Carregue um dataset personalizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPiwaJwMt7yo"
      },
      "source": [
        "**Observação:** é importante definir `cache=True` para acelerar o treinamento (principalmente quando o dataset for reamostrado), mas isso exige mais RAM para armazenar os dados. Se você estiver utilizando um dataset personalizado muito grande, é possível que o cache exceda a capacidade de RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "e86Ej-ZmuCzy"
      },
      "outputs": [],
      "source": [
        "if use_custom_dataset:\n",
        "  train_data_ratio = 0.8\n",
        "  train_data = audio_classifier.DataLoader.from_folder(\n",
        "      spec, dataset_dir, cache=True)\n",
        "  train_data, validation_data = train_data.split(train_data_ratio)\n",
        "  test_data = audio_classifier.DataLoader.from_folder(\n",
        "      spec, test_dir, cache=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh1P_zfzwbfE"
      },
      "source": [
        "## Treine o modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMghju-Rts2"
      },
      "source": [
        "Agora vamos usar a função [`create()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/create) do Model Maker para criar um modelo baseado na nossa especificação de modelo e dataset de treinamento. Em seguida, vamos começar o treinamento.\n",
        "\n",
        "Se você estiver utilizando um dataset personalizado, talvez queira alterar o tamanho do lote para o número de amostras em seu dataset de treinamento, conforme apropriado.\n",
        "\n",
        "**Observação:** a primeira época demora mais tempo porque é preciso criar o cache. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYaZvaOPgLUC"
      },
      "outputs": [],
      "source": [
        "# If your dataset has fewer than 100 samples per class,\n",
        "# you might want to try a smaller batch size\n",
        "batch_size = 25\n",
        "epochs = 25\n",
        "model = audio_classifier.create(train_data, spec, validation_data, batch_size, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtLuRA2xweZA"
      },
      "source": [
        "## Avalie o desempenho do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXMEHZkAxJTl"
      },
      "source": [
        "Mesmo que a exatidão e a perda pareçam boas na saída do treinamento acima, é importante executar o modelo usando os dados de teste que o modelo nunca viu. É isso que o método `evaluate()` faz:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_4MGpzhWVhr"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqvpAnqsVExO"
      },
      "source": [
        "### Confira a matriz de confusão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QRRAM39aOxS"
      },
      "source": [
        "Ao treinar um modelo de classificação como este, também vale a pena inspecionar a [matriz de confusão](https://en.wikipedia.org/wiki/Confusion_matrix), que fornece uma representação visual detalhada do desempenho do classificador para cada classificação nos dados de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqB3c0368iH3"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion, test_labels):\n",
        "  \"\"\"Compute confusion matrix and normalize.\"\"\"\n",
        "  confusion_normalized = confusion.astype(\"float\") / confusion.sum(axis=1)\n",
        "  sns.set(rc = {'figure.figsize':(6,6)})\n",
        "  sns.heatmap(\n",
        "      confusion_normalized, xticklabels=test_labels, yticklabels=test_labels,\n",
        "      cmap='Blues', annot=True, fmt='.2f', square=True, cbar=False)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "confusion_matrix = model.confusion_matrix(test_data)\n",
        "show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yASrikBgZ9ZO"
      },
      "source": [
        "## Exporte o modelo\n",
        "\n",
        "A última etapa é exportar seu modelo para o formato do TensorFlow Lite para execução em dispositivos embarcados/para dispositivos móveis e para o [formato SavedModel](https://www.tensorflow.org/guide/saved_model) para execução em outros lugares.\n",
        "\n",
        "Ao exportar um arquivo do `.tflite` pelo Model Maker, ele inclui [metadados do modelo](https://www.tensorflow.org/lite/inference_with_metadata/overview), que descrevem diversos detalhes que podem ajudar posteriormente durante a inferência. Ele inclui até mesmo uma cópia do arquivo de rótulos de classificação e, portanto, você não precisa de um arquivo `labels.txt` separado (na próxima seção, mostraremos como usar esses metadados para executar a inferência)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gEf59NfGWjq"
      },
      "outputs": [],
      "source": [
        "TFLITE_FILENAME = 'browserfft-speech.tflite'\n",
        "SAVE_PATH = './models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw_ehPxAdQlz"
      },
      "outputs": [],
      "source": [
        "print(f'Exporing the model to {SAVE_PATH}')\n",
        "model.export(SAVE_PATH, tflite_filename=TFLITE_FILENAME)\n",
        "model.export(SAVE_PATH, export_format=[mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIC1ddGq6xQX"
      },
      "source": [
        "## Execute a inferência com o modelo do TF Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xr0idac6xfi"
      },
      "source": [
        "Agora seu modelo do TF Lite pode ser implantado e executado usando qualquer uma das [bibliotecas de inferência](https://www.tensorflow.org/lite/guide/inference) disponíveis ou com a nova [API TFLite AudioClassifier Task](https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier). O código abaixo mostra como executar a inferência com um modelo `.tflite` no Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR5zV53YbCIQ"
      },
      "outputs": [],
      "source": [
        "# This library provides the TFLite metadata API\n",
        "! pip install -q tflite_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AC7PRyiayU5"
      },
      "outputs": [],
      "source": [
        "from tflite_support import metadata\n",
        "import json\n",
        "\n",
        "def get_labels(model):\n",
        "  \"\"\"Returns a list of labels, extracted from the model metadata.\"\"\"\n",
        "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
        "  labels_file = displayer.get_packed_associated_file_list()[0]\n",
        "  labels = displayer.get_associated_file_buffer(labels_file).decode()\n",
        "  return [line for line in labels.split('\\n')]\n",
        "\n",
        "def get_input_sample_rate(model):\n",
        "  \"\"\"Returns the model's expected sample rate, from the model metadata.\"\"\"\n",
        "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
        "  metadata_json = json.loads(displayer.get_metadata_json())\n",
        "  input_tensor_metadata = metadata_json['subgraph_metadata'][0][\n",
        "          'input_tensor_metadata'][0]\n",
        "  input_content_props = input_tensor_metadata['content']['content_properties']\n",
        "  return input_content_props['sample_rate']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC7TEvQ9o4mu"
      },
      "source": [
        "Para observar o desempenho do modelo com amostras reais, execute o bloco de código abaixo várias vezes. A cada vez, ele vai buscar uma nova amostra de teste e executar a inferência, e você pode ouvir a amostra de áudio abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loU6PleipSPf"
      },
      "outputs": [],
      "source": [
        "# Get a WAV file for inference and list of labels from the model\n",
        "tflite_file = os.path.join(SAVE_PATH, TFLITE_FILENAME)\n",
        "labels = get_labels(tflite_file)\n",
        "random_audio = get_random_audio_file(test_dir)\n",
        "\n",
        "# Ensure the audio sample fits the model input\n",
        "interpreter = tf.lite.Interpreter(tflite_file)\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_size = input_details[0]['shape'][1]\n",
        "sample_rate = get_input_sample_rate(tflite_file)\n",
        "audio_data, _ = librosa.load(random_audio, sr=sample_rate)\n",
        "if len(audio_data) < input_size:\n",
        "  audio_data.resize(input_size)\n",
        "audio_data = np.expand_dims(audio_data[:input_size], axis=0)\n",
        "\n",
        "# Run inference\n",
        "interpreter.allocate_tensors()\n",
        "interpreter.set_tensor(input_details[0]['index'], audio_data)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# Display prediction and ground truth\n",
        "top_index = np.argmax(output_data[0])\n",
        "label = labels[top_index]\n",
        "score = output_data[0][top_index]\n",
        "print('---prediction---')\n",
        "print(f'Class: {label}\\nScore: {score}')\n",
        "print('----truth----')\n",
        "show_sample(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtmfoJW6G2fd"
      },
      "source": [
        "## Baixe o modelo do TF Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zLDeiQ_z2Vj"
      },
      "source": [
        "Agora você pode implantar o modelo do TF Lite em seu dispositivo móvel ou embarcado. Você não precisa baixar o arquivo de rótulos, pois pode recuperar os rótulos usando os metadados do arquivo `.tflite`, conforme demonstrado no exemplo de inferência anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNuQoqtjG4zu"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download(tflite_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iERuGZz4z6rB"
      },
      "source": [
        "Confira os aplicativos de exemplo completos que realizam a inferência com modelos de áudio do TF Lite no [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android/) e no [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "speech_recognition.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
