{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Classificação de imagem com o TensorFlow Lite Model Maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDABAblytltI"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/image_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/lite/models/modify/model_maker/image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/lite/models/modify/model_maker/image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/lite/models/modify/model_maker/image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a> </td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelo do TF Hub</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86-Nh4pMHqY"
      },
      "source": [
        "A [biblioteca TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker) (criador de modelos do TF Lite) simplifica o processo de adaptar e converter um modelo de rede neural do TensorFlow para dados de entrada específicos ao implantar esse modelo em aplicativos de aprendizado de máquina em dispositivos.\n",
        "\n",
        "Este notebook apresenta um exemplo completo que utiliza a biblioteca Model Maker para ilustrar a adaptação e conversão de um modelo de classificação de imagens usado com frequência para classificar flores em um dispositivo móvel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Pré-requisitos\n",
        "\n",
        "Para executar este exemplo, primeiro precisamos instalar diversos pacotes exigidos, incluindo o pacote do Model Maker que está no [repositório](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) do GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cv3K3oaksJv"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install -q tflite-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx1HGRoFQ54j"
      },
      "source": [
        "Importe os pacotes necessários."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import image_classifier\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.config import QuantizationConfig\n",
        "from tflite_model_maker.image_classifier import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKRaYHABpob5"
      },
      "source": [
        "## Exemplo completo simples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiZZ5DHXotaW"
      },
      "source": [
        "### Obtenha o caminho dos dados\n",
        "\n",
        "Vamos obter alguns imagens para este exemplo completo simples. Algumas centenas de imagens são um bom ponto de partida para o Model Maker, enquanto mais dados poderão trazer uma exatidão maior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3jz5x0JoskPv"
      },
      "outputs": [],
      "source": [
        "image_path = tf.keras.utils.get_file(\n",
        "      'flower_photos.tgz',\n",
        "      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "      extract=True)\n",
        "image_path = os.path.join(os.path.dirname(image_path), 'flower_photos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a55MR6i6nuDm"
      },
      "source": [
        "Você pode substituir `image_path` pelas duas pastas de imagens. Para carregar os dados no Colab, use o botão de upload disponível na barra lateral esquerda, conforme mostrado na imagem abaixo – o botão Upload está destacado com um retângulo vermelho. Tente carregar um arquivo ZIP e descompactá-lo. O caminho do arquivo raiz é o caminho atual.\n",
        "\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_image_classification.png\" width=\"800\" hspace=\"100\" alt=\"Upload File\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNRNv_mloS89"
      },
      "source": [
        "Se você preferir não carregar suas imagens na nuvem, pode tentar executar a biblioteca localmente de acordo com este [guia](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) no GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-VDriAdsowu"
      },
      "source": [
        "### Execute o exemplo\n",
        "\n",
        "O exemplo tem apenas quatro linhas de código, conforme mostrado abaixo, em que cada uma representa uma etapa do processo geral.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ahtcO86tZBL"
      },
      "source": [
        "Etapa 1 – Carregue os dados específicos para um aplicativo de aprendizado de máquina em dispositivos. Divida-os em dados de treinamento e de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lANoNS_gtdH1"
      },
      "outputs": [],
      "source": [
        "data = DataLoader.from_folder(image_path)\n",
        "train_data, test_data = data.split(0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_9IWyIztuRF"
      },
      "source": [
        "Etapa 2 – Personalize o modelo do TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRXMZbrwtyRD"
      },
      "outputs": [],
      "source": [
        "model = image_classifier.create(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxU2fDr-t2Ya"
      },
      "source": [
        "Etapa 3 – Avalie o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQr02VxJt6Cs"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVZw9zU8t84y"
      },
      "source": [
        "Etapa 4 – Exporte para um modelo do TensorFlow Lite.\n",
        "\n",
        "Aqui, exportamos para um modelo do TensorFlow Lite com [metadados](https://www.tensorflow.org/lite/models/convert/metadata), que fornecem um padrão para descrição de modelos. O arquivo de rótulos é incorporado aos metadados. Para a tarefa de classificação de imagens, a técnica padrão de quantização pós-treinamento é a quantização completa em inteiros.\n",
        "\n",
        "Você pode baixá-lo pela barra lateral esquerda da mesma forma que fez para carregar os dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb-eIzfluCoa"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyju1qc_v-wy"
      },
      "source": [
        "Após essas quatro etapas simples, podemos usar o modelo do TensorFlow Lite em aplicativos em dispositivos, como no aplicativo de referência de [classificação de imagens](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1QG32ivs9lF"
      },
      "source": [
        "## Processo detalhado\n",
        "\n",
        "No momento, temos suporte a diversos métodos, como EfficientNet-Lite*, MobileNetV2, ResNet50, como modelos pré-treinados para classificação de imagens, o que é muito flexível, pois é possível adicionar novos modelos pré-treinados a esta biblioteca com apenas algumas linhas de código.\n",
        "\n",
        "Veja abaixo as etapas mais detalhadas deste exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEncJxtl-nQ"
      },
      "source": [
        "### Etapa 1 – Carregue dados de entrada específicos em um aplicativo de aprendizado de máquina em dispositivos\n",
        "\n",
        "O dataset Flower contém 3.670 imagens de 5 classes. Baixe a versão do arquivo do dataset e descompacte-a via tar.\n",
        "\n",
        "O dataset tem a seguinte estrutura de diretórios:\n",
        "\n",
        "<pre>\n",
        "&lt;b&gt;flower_photos&lt;/b&gt;\n",
        "|__ &lt;b&gt;daisy&lt;/b&gt;\n",
        "    |______ 100080576_f52e8ee070_n.jpg\n",
        "    |______ 14167534527_781ceb1b7a_n.jpg\n",
        "    |______ ...\n",
        "|__ &lt;b&gt;dandelion&lt;/b&gt;\n",
        "    |______ 10043234166_e6dd915111_n.jpg\n",
        "    |______ 1426682852_e62169221f_m.jpg\n",
        "    |______ ...\n",
        "|__ &lt;b&gt;roses&lt;/b&gt;\n",
        "    |______ 102501987_3cdb8e5394_n.jpg\n",
        "    |______ 14982802401_a3dfb22afb.jpg\n",
        "    |______ ...\n",
        "|__ &lt;b&gt;sunflowers&lt;/b&gt;\n",
        "    |______ 12471791574_bb1be83df4.jpg\n",
        "    |______ 15122112402_cafa41934f.jpg\n",
        "    |______ ...\n",
        "|__ &lt;b&gt;tulips&lt;/b&gt;\n",
        "    |______ 13976522214_ccec508fe7.jpg\n",
        "    |______ 14487943607_651e8062a1_m.jpg\n",
        "    |______ ...\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tOfUr2KlgpU"
      },
      "outputs": [],
      "source": [
        "image_path = tf.keras.utils.get_file(\n",
        "      'flower_photos.tgz',\n",
        "      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "      extract=True)\n",
        "image_path = os.path.join(os.path.dirname(image_path), 'flower_photos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E051HBUM5owi"
      },
      "source": [
        "Use a classe `DataLoader` para carregar dados.\n",
        "\n",
        "O método `from_folder()` pode carregar dados a partir da pasta. Ele pressupõe que os dados de imagem da mesma classe estejam no mesmo subdiretório e que o nome da subpasta seja o nome da classe. No momento, há suporte a imagens em JPEG e em PNG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_fOlZsklmlL"
      },
      "outputs": [],
      "source": [
        "data = DataLoader.from_folder(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u501eT4koURB"
      },
      "source": [
        "Divida o dataset em dados de treinamento (80%), de validação (10%, opcional) e de teste (10%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY4UU5SUobtJ"
      },
      "outputs": [],
      "source": [
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9_MYPie3EMO"
      },
      "source": [
        "Mostre 25 imagens de exemplo com seus respectivos rótulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih4Wx44I482b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i, (image, label) in enumerate(data.gen_dataset().unbatch().take(25)):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
        "  plt.xlabel(data.index_to_label[label.numpy()])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWuoensX4vDA"
      },
      "source": [
        "### Etapa 2 – Personalize o modelo do TensorFlow\n",
        "\n",
        "Crie um modelo de classificador de imagens personalizado com base nos dados carregados. O modelo padrão é EfficientNet-Lite0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvYSUuJY3QxR"
      },
      "outputs": [],
      "source": [
        "model = image_classifier.create(train_data, validation_data=validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JFOKWnH9x8_"
      },
      "source": [
        "Confira a estrutura detalhada do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNXAfjl192dC"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5FPk_tOxoZ"
      },
      "source": [
        "### Etapa 3 – Avalie o modelo\n",
        "\n",
        "Avalie o resultado do modelo; obtenha a perda e a exatidão do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8c2ZQ0J3Riy"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZCrYOWoCt05"
      },
      "source": [
        "Podemos plotar os resultados previstos em 100 imagens de teste. Os rótulos previstos com a cor vermelha são os resultados previstos incorretamente, enquanto os outros são os previstos corretamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9O9Kx7nDQWD"
      },
      "outputs": [],
      "source": [
        "# A helper function that returns 'red'/'black' depending on if its two input\n",
        "# parameter matches or not.\n",
        "def get_label_color(val1, val2):\n",
        "  if val1 == val2:\n",
        "    return 'black'\n",
        "  else:\n",
        "    return 'red'\n",
        "\n",
        "# Then plot 100 test images and their predicted labels.\n",
        "# If a prediction result is different from the label provided label in \"test\"\n",
        "# dataset, we will highlight it in red color.\n",
        "plt.figure(figsize=(20, 20))\n",
        "predicts = model.predict_top_k(test_data)\n",
        "for i, (image, label) in enumerate(test_data.gen_dataset().unbatch().take(100)):\n",
        "  ax = plt.subplot(10, 10, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
        "\n",
        "  predict_label = predicts[i][0][0]\n",
        "  color = get_label_color(predict_label,\n",
        "                          test_data.index_to_label[label.numpy()])\n",
        "  ax.xaxis.label.set_color(color)\n",
        "  plt.xlabel('Predicted: %s' % predict_label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3H0rkbLUZAG"
      },
      "source": [
        "Se a exatidão não atender aos requisitos do aplicativo, consulte a seção [Uso avançado](#scrollTo=zNDBP2qA54aK) para ver alternativas, como alterar para um modelo maior, ajustar os parâmetros de retreinamento, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHoGAceO2xV"
      },
      "source": [
        "### Etapa 4 – Exporte para um modelo do TensorFlow Lite\n",
        "\n",
        "Converta o modelo treinado para o formato de modelos do TensorFlow Lite com [metadados](https://www.tensorflow.org/lite/models/convert/metadata) para poder usá-lo posteriormente em um aplicativo de aprendizado de máquina em dispositivos. O arquivo de rótulos e o arquivo de vocabulário são incorporados aos metadados. O nome de arquivo padrão do TF Lite é `model.tflite`.\n",
        "\n",
        "Em diversos aplicativos de aprendizado de máquina em dispositivos, o tamanho do modelo é um fator importante. Portanto, recomendamos aplicar quantização no modelo para deixá-lo menor e possivelmente mais rápido. Para a tarefa de classificação de imagens, a técnica padrão de quantização pós-treinamento é a quantização completa em inteiros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6wA9lK3TQB"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROS2Ay2jMPCl"
      },
      "source": [
        "Confira mais detalhes sobre como integrar o modelo do TensorFlow Lite aos aplicativos para dispositivos móveis nos [exemplos do guia](https://www.tensorflow.org/lite/examples/image_classification/overview) de classificação de imagens.\n",
        "\n",
        "Este modelo pode ser integrado a um aplicativo para Android ou iOS usando a [API ImageClassifier](https://www.tensorflow.org/lite/inference_with_metadata/task_library/image_classifier) da [biblioteca Task do TensorFlow Lite](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "habFnvRxxQ4A"
      },
      "source": [
        "Confira abaixo os formatos de exportação permitidos:\n",
        "\n",
        "- `ExportFormat.TFLITE`\n",
        "- `ExportFormat.LABEL`\n",
        "- `ExportFormat.SAVED_MODEL`\n",
        "\n",
        "Por padrão, só é exportado o modelo do TensorFlow Lite com metadados. Você também pode exportar diferentes arquivos seletivamente. Por exemplo: exporte somente o arquivo de rótulos da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvxWsOTmKG4P"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', export_format=ExportFormat.LABEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4jQaxyT5_KV"
      },
      "source": [
        "E você pode avaliar o modelo do TF Lite com o método `evaluate_tflite`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1YoPX5wOK-u"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('model.tflite', test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNDBP2qA54aK"
      },
      "source": [
        "## Uso avançado\n",
        "\n",
        "A função `create` é uma parte essencial dessa biblioteca e usa aprendizado por transferência com um modelo pré-treinado similar ao do [tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning).\n",
        "\n",
        "A função `create` consiste nas seguintes etapas:\n",
        "\n",
        "1. Divide os dados em dados de treinamento, validação e teste de acordo com os parâmetros `validation_ratio` e `test_ratio`. O valor padrão de `validation_ratio` e `test_ratio` é `0.1` e `0.1`.\n",
        "2. Baixa um [vetor de características de imagens](https://www.tensorflow.org/hub/common_signatures/images#image_feature_vector) como modelo base no TensorFlow Hub. O modelo pré-treinado padrão é EfficientNet-Lite0.\n",
        "3. Adiciona um head de classificador com uma camada de dropout, com `dropout_rate` entre a camada do head e o modelo pré-treinado. O `dropout_rate` padrão é o valor `dropout_rate` padrão de [make_image_classifier_lib](https://github.com/tensorflow/hub/blob/master/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py#L55) do TensorFlow Hub.\n",
        "4. Faz o pré-processamento dos dados de entrada não tratados. No momento, as etapas de pré-processamento incluem a normalização do valor de cada pixel da imagem para a escala da entrada do modelo e o redimensionamento para o tamanho da entrada do modelo. EfficientNet-Lite0 tem escala de entrada igual a `[0, 1]` e tamanho da imagem de entrada igual a `[224, 224, 3]`.\n",
        "5. Alimenta o modelo do classificador com dados. Por padrão, os parâmetros de treinamento, como épocas de treinamento, tamanho do lote, taxa de aprendizado e momento, são os valores padrão de [make_image_classifier_lib](https://github.com/tensorflow/hub/blob/master/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py#L55) do TensorFlow Hub. Somente o head do classificador é treinado.\n",
        "\n",
        "Nesta seção, descreveremos diversos tópicos avançados, incluindo como alterar para um modelo de classificação de imagens diferente, como mudar os hiperparâmetros de treinamento, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc4Jk8TvBQfm"
      },
      "source": [
        "## Personalize a quantização pós-treinamento em um modelo do TensorFlow Lite\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD8BOYrHBiDt"
      },
      "source": [
        "A [quantização pós-treinamento](https://www.tensorflow.org/lite/performance/post_training_quantization) é uma técnica de conversão que pode reduzir o tamanho do modelo e a latência de inferência, além de aumentar a velocidade de inferência da CPU e do acelerador de hardware com uma pequena redução da exatidão do modelo. A quantização é amplamente utilizada para otimizar o modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyIo0d5TCzE2"
      },
      "source": [
        "A biblioteca Model Maker aplica uma técnica padrão de quantização pós-treinamento ao exportar o modelo. Se você quiser personalizar a quantização pós-treinamento, o Model Maker oferece suporte a diversas opções usando [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig). Vejamos a quantização de float16 como exemplo. Primeiro, definimos a configuração de quantização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8hL2mstCxQl"
      },
      "outputs": [],
      "source": [
        "config = QuantizationConfig.for_float16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1gzx_rmFMOA"
      },
      "source": [
        "Em seguida, exportamos o modelo do TensorFlow Lite com essa configuração."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTJzFQnJFMjr"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Safo0e40wKZW"
      },
      "source": [
        "No Colab, você pode baixar o modelo com nome `model_fp16.tflite` pela barra lateral esquerda da mesma forma que fez para carregar dados anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4kiTJtZ_sDm"
      },
      "source": [
        "## Altere o modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "794vgj6ud7Ep"
      },
      "source": [
        "### Altere para o modelo compatível com essa biblioteca.\n",
        "\n",
        "No momento, a biblioteca oferece suporte a modelos EfficientNet-Lite, MobileNetV2, ResNet50. [EfficientNet-Lite](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite) é uma família de modelos de classificação de imagens que atingem uma exatidão excepcional e são adequadas para dispositivos Edge. O modelo padrão é EfficientNet-Lite0.\n",
        "\n",
        "Para mudar o modelo para MobileNetV2, basta definir o parâmetro `model_spec` como a especificação do modelo MobileNetV2 no método `create`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JKsJ6-P6ae1"
      },
      "outputs": [],
      "source": [
        "model = image_classifier.create(train_data, model_spec=model_spec.get('mobilenet_v2'), validation_data=validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm_B1Wv08AxR"
      },
      "source": [
        "Avalie o modelo MobileNetV2 retreinado recentemente para ver a exatidão e a perda com os dados de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB2Go3HW8X7_"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAciGzVWtmWp"
      },
      "source": [
        "### Altere para o modelo do TensorFlow Hub\n",
        "\n",
        "Além disso, também podemos alterar para outros modelos novos que recebem uma imagem como entrada e geram como saída um vetor de características no formato do TensorFlow Hub.\n",
        "\n",
        "Usando o modelo [Inception V3](https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1) como exemplo, podemos definir `inception_v3_spec`, que é um objeto de [image_classifier.ModelSpec](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/image_classifier/ModelSpec) e contém a especificação do modelo Inception V3.\n",
        "\n",
        "Precisamos especificar o nome `name` e a URL `uri` do modelo do TensorFlow Hub. O valor padrão de `input_image_shape` é `[224, 224]`. Precisamos alterá-lo para `[299, 299]` para o modelo Inception V3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdiMF2WMfAR4"
      },
      "outputs": [],
      "source": [
        "inception_v3_spec = image_classifier.ModelSpec(\n",
        "    uri='https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1')\n",
        "inception_v3_spec.input_image_shape = [299, 299]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_GGIoXZCs5F"
      },
      "source": [
        "Em seguida, ao definir o parâmetro `model_spec` como `inception_v3_spec` no método `create`, podemos retreinar o modelo Inception V3.\n",
        "\n",
        "As etapas restantes são exatamente iguais, e poderemos ter um modelo InceptionV3 personalizado do TensorFlow Lite no final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZ5IRKdeex3"
      },
      "source": [
        "### Altere seu próprio modelo personalizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svTjlZhrCrcV"
      },
      "source": [
        "Se quisermos usar um modelo personalizado que não esteja no TensorFlow Hub, precisamos criar e exportar [ModelSpec](https://www.tensorflow.org/hub/api_docs/python/hub/ModuleSpec) no TensorFlow Hub.\n",
        "\n",
        "Começamos definindo o objeto `ModelSpec` conforme o processo acima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M9bn703AHt2"
      },
      "source": [
        "## Altere os hiperparâmetros de treinamento\n",
        "\n",
        "Também podemos alterar os hiperparâmetros de treinamento, como `epochs`, `dropout_rate` e `batch_size`, o que pode afetar a exatidão do modelo. Veja quais parâmetros do modelo podem ser ajustados:\n",
        "\n",
        "- `epochs`: mais épocas podem alcançar uma exatidão melhor até a convergência, mas treinar com épocas demais pode causar overfitting.\n",
        "- `dropout_rate`: taxa de dropout, evita overfitting. É igual a none (nenhuma) por padrão.\n",
        "- `batch_size`: número de amostras a serem usadas em um passo de treinamento. É igual a none (nenhum) por padrão.\n",
        "- `validation_data`: dados de validação. Se igual a none (nenhum), pula o processo de validação. É igual a none por padrão.\n",
        "- `train_whole_model`: se verdadeiro, o módulo do Hub é treinado junto com a camada de classificação superior. Caso contrário, treina somente a camada de classificação superior. É igual a none (nenhum) por padrão.\n",
        "- `learning_rate`: Taxa de aprendizado base. É igual a none (nenhuma) por padrão.\n",
        "- `momentum`: float do Python encaminhado ao otimizador. É usado somente quando `use_hub_library` é true (verdadeiro). É igual a none (nenhum) por padrão.\n",
        "- `shuffle`: booleano, indica se os dados devem ser misturados. É igual a false (falso) por padrão.\n",
        "- `use_augmentation`: booleano, indica o uso de ampliação de dados no pré-processamento. É igual a false (falso) por padrão.\n",
        "- `use_hub_library`: booleano, usa `make_image_classifier_lib` do TensorFlow Hub para retreinar o modelo. Este pipeline de treinamento pode alcançar um desempenho melhor para datasets complicados com muitas categorias. É igual a true (verdadeiro) por padrão.\n",
        "- `warmup_steps`: número de etapas de preparação para o cronograma de preparação da taxa de aprendizado. Se igual a none (nenhum), o warmup_steps padrão é usado, que é o total de passos de treinamento em duas épocas. É usado somente quando `use_hub_library` é false (falso). É igual a none (nenhum) por padrão.\n",
        "- `model_dir`: opcional, é o local dos arquivos de checkpoint do modelo. É usado somente quando `use_hub_library` é false (falso). É igual a none (nenhum) por padrão.\n",
        "\n",
        "Os parâmetros que são iguais a none (nenhum) por padrão, como `epochs`, obterão os parâmetros concretos padrão em [make_image_classifier_lib](https://github.com/tensorflow/hub/blob/02ab9b7d3455e99e97abecf43c5d598a5528e20c/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py#L54) da biblioteca do TensorFlow Hub ou em [train_image_classifier_lib](https://github.com/tensorflow/examples/blob/f0260433d133fd3cea4a920d1e53ecda07163aee/tensorflow_examples/lite/model_maker/core/task/train_image_classifier_lib.py#L61).\n",
        "\n",
        "Por exemplo: podemos treinar com mais épocas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3k7mhH54QcK"
      },
      "outputs": [],
      "source": [
        "model = image_classifier.create(train_data, validation_data=validation_data, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaYBQymQDsXU"
      },
      "source": [
        "Avalie o modelo retreinado recentemente com 10 épocas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VafIYpKWD4Sw"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhBU5NCy5Ji2"
      },
      "source": [
        "# Saiba mais\n",
        "\n",
        "Leia o exemplo de [classificação de imagens](https://www.tensorflow.org/lite/examples/image_classification/overview) para aprender os detalhes técnicos. Confira mais informações em:\n",
        "\n",
        "- [Guia](https://www.tensorflow.org/lite/models/modify/model_maker) e [referência da API](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker) do TensorFlow Lite Model Maker.\n",
        "- Task Library: [ImageClassifier](https://www.tensorflow.org/lite/inference_with_metadata/task_library/image_classifier) para implantação.\n",
        "- Aplicativos de referência completos para [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android), [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/ios) e [Raspberry PI](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/raspberry_pi).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "image_classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
