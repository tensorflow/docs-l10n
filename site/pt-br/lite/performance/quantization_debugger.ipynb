{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KimMZUVqcJ8_"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BRQ6HQ8zcV5v"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlWzg1D9_EhW"
      },
      "source": [
        "# Inspeção de erros de quantização com o depurador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLoHL19yb-a0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/performance/quantization_debugger\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/lite/performance/quantization_debugger.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/lite/performance/quantization_debugger.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/lite/performance/quantization_debugger.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelo do TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWO_yYDGcGWY"
      },
      "source": [
        "Embora a quantização de números inteiros forneça melhorias de tamanho do modelo e latência, o modelo quantizado nem sempre funcionará como esperado. Geralmente, o esperado é que a qualidade do modelo (por exemplo, exatidão, mAP, WER) seja um pouco inferior ao modelo float original. No entanto, há casos em que a qualidade do modelo pode ficar abaixo das suas expectativas ou gerar resultados totalmente errados.\n",
        "\n",
        "Quando ocorrer esse problema, é complicado e trabalhoso localizar a causa raiz do erro de quantização, mas corrigi-lo é ainda mais difícil. Para ajudar no processo de inspeção do modelo, o **depurador de quantização** pode ser usado para identificar as camadas problemáticas, e a **quantização seletiva** pode deixar essas camadas problemáticas em float para que a exatidão do modelo seja recuperada à custa de um benefício reduzido da quantização.\n",
        "\n",
        "Observação: essa API é experimental, e pode haver mudanças que causem quebras na API no decorrer das melhorias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kD29R1I_Mn6"
      },
      "source": [
        "## Depurador de quantização\n",
        "\n",
        "O depurador de quantização possibilita a análise de métricas de qualidade da quantização no modelo existente. Ele pode automatizar os processos para a execução do modelo com um dataset de depuração e coletar métricas de qualidade de quantização para cada tensor.\n",
        "\n",
        "Observação: no momento, o depurador de quantização e a quantização seletiva só funcionam para a quantização de números inteiros com ativações int8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "221Qon7G_PmZ"
      },
      "source": [
        "### Pré-requisitos\n",
        "\n",
        "Se você já tem um pipeline para quantizar um modelo, tem todos os elementos necessários para executar o depurador de quantização!\n",
        "\n",
        "- Modelo que será quantizado\n",
        "- Dataset representativo\n",
        "\n",
        "Além do modelo e dos dados, você precisará usar um framework de processamento de dados (por exemplo, pandas, Planilhas Google) para analisar os resultados exportados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEEzJWo_iZ_"
      },
      "source": [
        "### Configuração\n",
        "\n",
        "Esta seção prepara as bibliotecas, o modelo MobileNet v3 e o dataset de teste de 100 imagens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7epUDUP_6qo"
      },
      "outputs": [],
      "source": [
        "# Quantization debugger is available from TensorFlow 2.7.0\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tf-nightly\n",
        "!pip install tensorflow_datasets --upgrade  # imagenet_v2 needs latest checksum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLsgiUZe_hIa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "veWjO3u32vzz"
      },
      "outputs": [],
      "source": [
        "#@title Boilerplates and helpers\n",
        "MODEL_URI = 'https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5'\n",
        "\n",
        "\n",
        "def process_image(data):\n",
        "  data['image'] = tf.image.resize(data['image'], (224, 224)) / 255.0\n",
        "  return data\n",
        "\n",
        "\n",
        "# Representative dataset\n",
        "def representative_dataset(dataset):\n",
        "\n",
        "  def _data_gen():\n",
        "    for data in dataset.batch(1):\n",
        "      yield [data['image']]\n",
        "\n",
        "  return _data_gen\n",
        "\n",
        "\n",
        "def eval_tflite(tflite_model, dataset):\n",
        "  \"\"\"Evaluates tensorflow lite classification model with the given dataset.\"\"\"\n",
        "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_idx = interpreter.get_input_details()[0]['index']\n",
        "  output_idx = interpreter.get_output_details()[0]['index']\n",
        "\n",
        "  results = []\n",
        "\n",
        "  for data in representative_dataset(dataset)():\n",
        "    interpreter.set_tensor(input_idx, data[0])\n",
        "    interpreter.invoke()\n",
        "    results.append(interpreter.get_tensor(output_idx).flatten())\n",
        "\n",
        "  results = np.array(results)\n",
        "  gt_labels = np.array(list(dataset.map(lambda data: data['label'] + 1)))\n",
        "  accuracy = (\n",
        "      np.sum(np.argsort(results, axis=1)[:, -5:] == gt_labels.reshape(-1, 1)) /\n",
        "      gt_labels.size)\n",
        "  print(f'Top-5 accuracy (quantized): {accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input(shape=(224, 224, 3), batch_size=1),\n",
        "  hub.KerasLayer(MODEL_URI)\n",
        "])\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics='sparse_top_k_categorical_accuracy')\n",
        "model.build([1, 224, 224, 3])\n",
        "\n",
        "# Prepare dataset with 100 examples\n",
        "ds = tfds.load('imagenet_v2', split='test[:1%]')\n",
        "ds = ds.map(process_image)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.representative_dataset = representative_dataset(ds)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mX-R-xK4ADB"
      },
      "outputs": [],
      "source": [
        "test_ds = ds.map(lambda data: (data['image'], data['label'] + 1)).batch(16)\n",
        "loss, acc = model.evaluate(test_ds)\n",
        "print(f'Top-5 accuracy (float): {acc * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnp6yBnJSCoh"
      },
      "outputs": [],
      "source": [
        "eval_tflite(quantized_model, ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tblkk3cxxpuw"
      },
      "source": [
        "Podemos ver que o modelo original tem uma exatidão top-5 muito mais alta do que nosso pequeno dataset, enquanto o modelo quantizado tem uma perda significativa na exatidão."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBBcfCQw_Wqd"
      },
      "source": [
        "### Etapa 1. Prepare o depurador\n",
        "\n",
        "A maneira mais fácil de usar o depurador de quantização é fornecer o `tf.lite.TFLiteConverter` usado para quantizar o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOByihbD_NZZ"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset(ds)\n",
        "\n",
        "# my_debug_dataset should have the same format as my_representative_dataset\n",
        "debugger = tf.lite.experimental.QuantizationDebugger(\n",
        "    converter=converter, debug_dataset=representative_dataset(ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vR1IIrmQS9W"
      },
      "source": [
        "### Etapa 2. Execute o depurador e obtenha os resultados\n",
        "\n",
        "Ao chamar `QuantizationDebugger.run()`, o depurador registrará as diferenças entre os tensores float e quantizados para o mesmo local da op e os processará com as determinadas métricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsUM54g-_E52"
      },
      "outputs": [],
      "source": [
        "debugger.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQpX_SBUQXvr"
      },
      "source": [
        "As métricas processadas podem ser acessadas com `QuantizationDebugger.layer_statistics` ou despejadas em um arquivo de texto em formato CSV com `QuantizationDebugger.layer_statistics_dump()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-AGYUAbQUmx"
      },
      "outputs": [],
      "source": [
        "RESULTS_FILE = '/tmp/debugger_results.csv'\n",
        "with open(RESULTS_FILE, 'w') as f:\n",
        "  debugger.layer_statistics_dump(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQzEi6VnQaen"
      },
      "outputs": [],
      "source": [
        "!head /tmp/debugger_results.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4np7VqU-Qfke"
      },
      "source": [
        "Para cada linha no dump, o nome da op e o índice vêm primeiro, seguidos pelos parâmetros de quantização e métricas de erros (incluindo [métricas de erros definidas pelo usuário](#custom-metrics), se houver alguma). O arquivo CSV resultante pode ser usado para escolher as camadas problemáticas com grandes métricas de erros de quantização.\n",
        "\n",
        "Com o pandas ou outra biblioteca de processamento de dados, podemos inspecionar as métricas de erros detalhadas por camada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUcSqYFGQb-f"
      },
      "outputs": [],
      "source": [
        "layer_stats = pd.read_csv(RESULTS_FILE)\n",
        "layer_stats.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C_oHxWFOV6M"
      },
      "source": [
        "### Etapa 3. Analise os dados\n",
        "\n",
        "Há várias maneiras de analisar os resultados. Primeiro, vamos adicionar algumas métricas úteis derivadas das saídas do depurador. (`scale` significa o fator de escala de quantização para cada tensor.)\n",
        "\n",
        "- Range, ou intervalo, (`256 / scale`)\n",
        "- RMSE / scale, ou raiz do erro quadrático médio / escala, (`sqrt(mean_squared_error) / scale`)\n",
        "\n",
        "O `RMSE / scale` é próximo a `1 / sqrt(12)` (cerca de 0,289) quando a distribuição quantizada é semelhante à distribuição float original, indicando um bom modelo quantizado. Quanto maior for o valor, mais provável será que a camada não tenha sido bem quantizada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwviORyJN6e5"
      },
      "outputs": [],
      "source": [
        "layer_stats['range'] = 255.0 * layer_stats['scale']\n",
        "layer_stats['rmse/scale'] = layer_stats.apply(\n",
        "    lambda row: np.sqrt(row['mean_squared_error']) / row['scale'], axis=1)\n",
        "layer_stats[['op_name', 'range', 'rmse/scale']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAAv35CdPvc4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "ax1 = plt.subplot(121)\n",
        "ax1.bar(np.arange(len(layer_stats)), layer_stats['range'])\n",
        "ax1.set_ylabel('range')\n",
        "ax2 = plt.subplot(122)\n",
        "ax2.bar(np.arange(len(layer_stats)), layer_stats['rmse/scale'])\n",
        "ax2.set_ylabel('rmse/scale')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pqUQvRUWB3Q"
      },
      "source": [
        "Há várias camadas com grandes intervalos, e algumas camadas com valores altos de `RMSE/scale`. Vamos obter as camadas com métricas de erros altas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqFsUX4_Q-cE"
      },
      "outputs": [],
      "source": [
        "layer_stats[layer_stats['rmse/scale'] > 0.7][[\n",
        "    'op_name', 'range', 'rmse/scale', 'tensor_name'\n",
        "]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHeALFTGWl_e"
      },
      "source": [
        "Com essas camadas, você pode tentar a quantização seletiva para ver se a qualidade do modelo melhora ao deixar de quantizá-las."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvdkjsbwYC6e"
      },
      "outputs": [],
      "source": [
        "suspected_layers = list(\n",
        "    layer_stats[layer_stats['rmse/scale'] > 0.7]['tensor_name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6RQw9JobOTR"
      },
      "source": [
        "Além disso, pular a quantização nas primeiras camadas também ajuda a melhorar a qualidade do modelo quantizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikF2bp6NZcXN"
      },
      "outputs": [],
      "source": [
        "suspected_layers.extend(list(layer_stats[:5]['tensor_name']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DfT78w6W6Li"
      },
      "source": [
        "## Quantização seletiva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pubC-01cGEH"
      },
      "source": [
        "A quantização seletiva pula alguns nós, para que o cálculo possa ser feito no domínio de ponto flutuante original. Quando as camadas corretas são puladas, podemos esperar a recuperação de um pouco da qualidade do modelo à custa de maior latência e tamanho do modelo.\n",
        "\n",
        "No entanto, se você estiver planejando executar modelos quantizados em aceleradores somente números inteiros (por exemplo, DSP Hexagon ou EdgeTPU), a quantização seletiva causaria a fragmentação do modelo e resultaria em uma latência de inferência mais lenta, causada principalmente pelo custo da transferência de dados entre a CPU e esses aceleradores. Para evitar isso, considere realizar o [treinamento consciente de quantização](https://www.tensorflow.org/model_optimization/guide/quantization/training) para manter todas as camadas em números inteiros, preservando a exatidão do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQFBfR7YW-oh"
      },
      "source": [
        "O depurador de quantização aceita as opções `denylisted_nodes` e `denylisted_ops` para pular a quantização em camadas específicas ou todas as instâncias de determinadas ops. Usando as `suspected_layers` que preparamos na etapa anterior, podemos utilizar o depurador de quantização para obter um modelo quantizado de forma seletiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5KD0JAEbpsv"
      },
      "outputs": [],
      "source": [
        "debug_options = tf.lite.experimental.QuantizationDebugOptions(\n",
        "    denylisted_nodes=suspected_layers)\n",
        "debugger = tf.lite.experimental.QuantizationDebugger(\n",
        "    converter=converter,\n",
        "    debug_dataset=representative_dataset(ds),\n",
        "    debug_options=debug_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfj9gzv4b7h4"
      },
      "outputs": [],
      "source": [
        "selective_quantized_model = debugger.get_nondebug_quantized_model()\n",
        "eval_tflite(selective_quantized_model, ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RkfMYSHdtZy"
      },
      "source": [
        "A exatidão é ainda mais baixa em comparação com o modelo float original, mas o modelo totalmente quantizado tem melhorias notáveis ao pular a quantização para cerca de 10 das 111 camadas.\n",
        "\n",
        "Você também pode tentar não quantizar todas as operações na mesma classe. Por exemplo, para pular a quantização em todas as operações de média, você pode passar `MEAN` a `denylisted_ops`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruUoP7SgcLpO"
      },
      "outputs": [],
      "source": [
        "debug_options = tf.lite.experimental.QuantizationDebugOptions(\n",
        "    denylisted_ops=['MEAN'])\n",
        "debugger = tf.lite.experimental.QuantizationDebugger(\n",
        "    converter=converter,\n",
        "    debug_dataset=representative_dataset(ds),\n",
        "    debug_options=debug_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY6kb5g_cO4H"
      },
      "outputs": [],
      "source": [
        "selective_quantized_model = debugger.get_nondebug_quantized_model()\n",
        "eval_tflite(selective_quantized_model, ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa8488TeAyx-"
      },
      "source": [
        "Com essas técnicas, podemos melhorar a exatidão do modelo MobileNet V3. Em seguida, vamos explorar técnicas avançadas para melhorar ainda mais a exatidão."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD75cY9PUb2u"
      },
      "source": [
        "## Usos avançados\n",
        "\n",
        "Com os seguintes recursos, você pode personalizar ainda mais seu pipeline de depuração."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVj9yrQoUfGo"
      },
      "source": [
        "### Métricas personalizadas\n",
        "\n",
        "Por padrão, o depurador de quantização emite cinco métricas para cada diferença float-quant: tamanho do tensor, desvio padrão, erro médio, erro absoluto máximo e erro quadrático médio. Você pode adicionar mais métricas personalizadas ao passá-las às opções. Para cada métrica, o resultado deverá ser um único valor float, e a métrica resultante será uma média das métricas de todos os exemplos.\n",
        "\n",
        "- `layer_debug_metrics`: calcula uma métrica baseada na diferença de cada saída de operação em relação às saídas de operações float e quantizadas.\n",
        "- `layer_direct_compare_metrics`: em vez de só obter a diferença, calcula uma métrica baseada nos tensores float e quantizados brutos e seus parâmetros de quantização (escala, ponto zero).\n",
        "- `model_debug_metrics`: **só é usada quando `float_model_(path|content)` é passado** ao depurador. Além das métricas no nível da operação, a saída da camada final é comparada à saída de referência do modelo float original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqmRQSxoVVwu"
      },
      "outputs": [],
      "source": [
        "debug_options = tf.lite.experimental.QuantizationDebugOptions(\n",
        "    layer_debug_metrics={\n",
        "        'mean_abs_error': (lambda diff: np.mean(np.abs(diff)))\n",
        "    },\n",
        "    layer_direct_compare_metrics={\n",
        "        'correlation':\n",
        "            lambda f, q, s, zp: (np.corrcoef(f.flatten(),\n",
        "                                             (q.flatten() - zp) / s)[0, 1])\n",
        "    },\n",
        "    model_debug_metrics={\n",
        "        'argmax_accuracy': (lambda f, q: np.mean(np.argmax(f) == np.argmax(q)))\n",
        "    })\n",
        "\n",
        "debugger = tf.lite.experimental.QuantizationDebugger(\n",
        "    converter=converter,\n",
        "    debug_dataset=representative_dataset(ds),\n",
        "    debug_options=debug_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVQ4nEicXz2l"
      },
      "outputs": [],
      "source": [
        "debugger.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfKA90csX9UL"
      },
      "outputs": [],
      "source": [
        "CUSTOM_RESULTS_FILE = '/tmp/debugger_results.csv'\n",
        "with open(CUSTOM_RESULTS_FILE, 'w') as f:\n",
        "  debugger.layer_statistics_dump(f)\n",
        "\n",
        "custom_layer_stats = pd.read_csv(CUSTOM_RESULTS_FILE)\n",
        "custom_layer_stats[['op_name', 'mean_abs_error', 'correlation']].tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqq30oWsZF5b"
      },
      "source": [
        "O resultado de `model_debug_metrics` pode ser visto separadamente de `debugger.model_statistics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrXlmzEHYhQ5"
      },
      "outputs": [],
      "source": [
        "debugger.model_statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqJBLIsoUyIg"
      },
      "source": [
        "### Usando a API mlir_quantize (interna) para acessar recursos detalhados\n",
        "\n",
        "Observação: alguns recursos na seção a seguir, `TFLiteConverter._experimental_calibrate_only` e `converter.mlir_quantize`, são APIs internas experimentais e estão sujeitas a mudanças incompatíveis com versões anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJm66Cz-XpeF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.lite.python import convert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2krUVzpiUp3u"
      },
      "source": [
        "#### Modo de verificação do modelo inteiro\n",
        "\n",
        "O comportamento padrão para a geração do modelo de depuração é a verificação por camada. Nesse modo, a entrada para o par de operações float e quantizadas é da mesma fonte (operação quantizada anteriormente). Outro modo é a verificação do modelo inteiro, em que os modelos float e quantizados são separados. Esse modo seria útil para observar como o erro é propagado pelo modelo. Para ativar, defina `enable_whole_model_verify=True` para `convert.mlir_quantize` ao gerar o modelo de depuração manualmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zykINDlVLSg"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.representative_dataset = representative_dataset(ds)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter._experimental_calibrate_only = True\n",
        "calibrated_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqvXlEiFXfSu"
      },
      "outputs": [],
      "source": [
        "# Note that enable_numeric_verify and enable_whole_model_verify are set.\n",
        "quantized_model = convert.mlir_quantize(\n",
        "    calibrated_model,\n",
        "    enable_numeric_verify=True,\n",
        "    enable_whole_model_verify=True)\n",
        "debugger = tf.lite.experimental.QuantizationDebugger(\n",
        "    quant_debug_model_content=quantized_model,\n",
        "    debug_dataset=representative_dataset(ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ6TFsXQVHMe"
      },
      "source": [
        "#### Quantização seletiva de um modelo já calibrado\n",
        "\n",
        "Você pode chamar `convert.mlir_quantize` diretamente para obter o modelo quantizado seletivamente do modelo já calibrado. Isso é especialmente útil quando você quer calibrar o modelo uma vez e experimentar com várias combinações de denylist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCS-Fa9lbdc0"
      },
      "outputs": [],
      "source": [
        "selective_quantized_model = convert.mlir_quantize(\n",
        "    calibrated_model, denylisted_nodes=suspected_layers)\n",
        "eval_tflite(selective_quantized_model, ds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Eq_8T2oauIED"
      ],
      "name": "quantization_debugger.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
