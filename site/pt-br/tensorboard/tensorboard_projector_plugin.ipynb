{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFloNx163DCr"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iSdwTGPc3Hpj"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE2AKncl3QJZ"
      },
      "source": [
        "# Visualização de dados com o Projetor de embeddings no TensorBoard\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/tensorboard/tensorboard_projector_plugin.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/tensorboard/tensorboard_projector_plugin.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/tensorboard/tensorboard_projector_plugin.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4s3Sf2I3mJr"
      },
      "source": [
        "## Visão geral\n",
        "\n",
        "Usando o **Projetor de embeddings do TensorBoard**, você pode representar embeddings de muitas dimensões graficamente. Isso pode ser útil ao visualizar, examinar e entender suas camadas de embeddings.\n",
        "\n",
        "Neste tutorial, você aprenderá a visualizar esse tipo de camada treinada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-0rhuaW9f2-"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Para este tutorial, vamos usar o TensorBoard para visualizar uma camada de embeddings gerada para classificar dados de avaliações de filmes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjRkD3r3etuL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh22cCoM8t7e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorboard.plugins import projector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlp6ZASQB5go"
      },
      "source": [
        "## Dados do IMDB\n",
        "\n",
        "Vamos usar um dataset de 25 mil avaliações de filmes do IMDB, cada um com um rótulo de sentimento (positivo/negativo). Cada avaliação é pré-processada e codificada como uma sequência de índices de palavras (números inteiros). Para simplificar, as palavras são indexadas pela frequência geral no dataset, por exemplo, o inteiro \"3\" codifica a 3ª palavra que aparece com mais frequência em todas as avaliações. Isso permite filtrar rapidamente operações como: \"só considere as 10 mil palavras mais comuns, mas elimine as 20 palavras mais comuns\".\n",
        "\n",
        "Como convenção, \"0\" não representa nenhuma palavra específica, mas, em vez disso, é usado para codificar qualquer palavra desconhecida. Mais tarde no tutorial, vamos remover a linha de \"0\" na visualização.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0Yiw05gIgqS"
      },
      "outputs": [],
      "source": [
        "(train_data, test_data), info = tfds.load(\n",
        "    \"imdb_reviews/subwords8k\",\n",
        "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "encoder = info.features[\"text\"].encoder\n",
        "\n",
        "# Shuffle and pad the data.\n",
        "train_batches = train_data.shuffle(1000).padded_batch(\n",
        "    10, padded_shapes=((None,), ())\n",
        ")\n",
        "test_batches = test_data.shuffle(1000).padded_batch(\n",
        "    10, padded_shapes=((None,), ())\n",
        ")\n",
        "train_batch, train_labels = next(iter(train_batches))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpvPVCwO7bDj"
      },
      "source": [
        "# Camada de embeddings do Keras\n",
        "\n",
        "Uma [camada de embeddings do Keras](https://keras.io/layers/embeddings/) pode ser usada para treinar um embedding para cada palavra no seu vocabulário. Cada palavra (ou subpalavra, nesse caso) será associada a um vetor (ou embedding) de 16 dimensões que será treinado pelo modelo.\n",
        "\n",
        "Veja [este tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings?hl=en) para saber mais sobre embeddings de palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fgoq5haqw8Z5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2500/2500 [==============================] - 13s 5ms/step - loss: 0.5330 - accuracy: 0.6769 - val_loss: 0.4043 - val_accuracy: 0.7800\n"
          ]
        }
      ],
      "source": [
        "# Create an embedding layer.\n",
        "embedding_dim = 16\n",
        "embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)\n",
        "# Configure the embedding layer as part of a keras model.\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        embedding, # The embedding layer should be the first layer in a model.\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile model.\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Train model for one epoch.\n",
        "history = model.fit(\n",
        "    train_batches, epochs=1, validation_data=test_batches, validation_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9HmC29hdMnH"
      },
      "source": [
        "## Salve dados para o TensorBoard\n",
        "\n",
        "O TensorBoard lê tensores e metadados de logs dos seus projetos do TensorFlow. O caminho para o diretório de log é especificado com `log_dir` abaixo. Para este tutorial, vamos usar `/logs/imdb-example/`.\n",
        "\n",
        "Para carregar os dados no Tensorboard, precisamos salvar um checkpoint de treinamento para esse diretório, além dos metadados que permitem a visualização de uma camada específica de interesse no modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi8_SCYRdn9x"
      },
      "outputs": [],
      "source": [
        "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
        "log_dir='/logs/imdb-example/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Save Labels separately on a line-by-line manner.\n",
        "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
        "  for subwords in encoder.subwords:\n",
        "    f.write(\"{}\\n\".format(subwords))\n",
        "  # Fill in the rest of the labels with \"unknown\".\n",
        "  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
        "    f.write(\"unknown #{}\\n\".format(unknown))\n",
        "\n",
        "\n",
        "# Save the weights we want to analyze as a variable. Note that the first\n",
        "# value represents any unknown word, which is not in the metadata, here\n",
        "# we will remove this value.\n",
        "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
        "# Create a checkpoint from embedding, the filename and key are the\n",
        "# name of the tensor.\n",
        "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
        "\n",
        "# Set up config.\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtL_KzYMBIzP"
      },
      "outputs": [],
      "source": [
        "# Now run tensorboard against on log data we just saved.\n",
        "%tensorboard --logdir /logs/imdb-example/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtzW8mr_wmbD"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/embedding_projector.png?raw=1\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG4hcUzQQoWA"
      },
      "source": [
        "## Análise\n",
        "\n",
        "O Projetor do TensorBoard é uma ótima ferramenta para interpretar e visualizar embeddings. O painel de controle permite que os usuários pesquisem termos específicos e destaquem palavras adjacentes no espaço do embedding (poucas dimensões). Com esse exemplo, podemos ver que Wes **Anderson** e Alfred **Hitchcock** são termos bastante neutros, mas mencionados em contextos diferentes.\n",
        "\n",
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/embedding_projector_hitchcock.png?raw=1\"/> -->\n",
        "\n",
        "Nesse espaço, Hitchcock está mais perto de palavras como `nightmare` (pesadelo), provavelmente devido ao fato de que ele é conhecido como o \"Mestre do suspense\", enquanto Anderson está mais próximo da palavra `heart` (coração), que é consistente com o estilo implacavelmente detalhado e comovente.\n",
        "\n",
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/embedding_projector_anderson.png?raw=1\"/> -->"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tensorboard_projector_plugin.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
