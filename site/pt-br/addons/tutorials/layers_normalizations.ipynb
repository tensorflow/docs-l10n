{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFPyjGqMQ82Q"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aNZ7aEDyQIYU"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMOmzhPEQh7b"
      },
      "source": [
        "# Normalizações\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/layers_normalizations\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/addons/tutorials/layers_normalizations.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/addons/tutorials/layers_normalizations.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/addons/tutorials/layers_normalizations.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cthm5dovQMJl"
      },
      "source": [
        "## Visão geral\n",
        "\n",
        "Este notebook fornece uma breve introdução às [camadas de normalização](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py) do TensorFlow. As camadas compatíveis no momento são:\n",
        "\n",
        "- **Normalização de grupo** (TensorFlow Addons)\n",
        "- **Normalização de instância** (TensorFlow Addons)\n",
        "- **Normalização de camada** (TensorFlow Core)\n",
        "\n",
        "A ideia básica por trás dessas camadas é normalizar a saída de uma camada de ativação para melhorar a convergência durante o treinamento. Em comparação com a [normalização de lote](https://keras.io/layers/normalization/), essas normalizações não funcionam em lotes. Em vez disso, elas normalizam as ativações de uma única amostra, tornando-as adequadas para redes neurais recorrentes também.\n",
        "\n",
        "Geralmente, a normalização é realizada ao calcular a média e o desvio padrão de um subgrupo no seu tensor de entrada. Também é possível aplicar uma escala e um fator de deslocamento.\n",
        "\n",
        "$y_{i} = \\frac{\\gamma ( x_{i} - \\mu )}{\\sigma }+ \\beta$\n",
        "\n",
        "$ y$ : saída\n",
        "\n",
        "$x$ : entrada\n",
        "\n",
        "$\\gamma$ : fator de escala\n",
        "\n",
        "$\\mu$: média\n",
        "\n",
        "$\\sigma$: desvio padrão\n",
        "\n",
        "$\\beta$: fator de deslocamento\n",
        "\n",
        "A seguinte imagem demonstra a diferença entre essas técnicas. Cada subplot mostra um tensor de entrada, com N como o eixo do lote, C como o eixo do canal e (H, W) como os eixos espaciais (a altura e largura de uma foto, por exemplo). Os pixels em azul são normalizados pela mesma média e variância, computadas ao agregar os valores desses pixels.\n",
        "\n",
        "![](https://github.com/shaohua0116/Group-Normalization-Tensorflow/raw/master/figure/gn.png)\n",
        "\n",
        "Fonte: (https://arxiv.org/pdf/1803.08494.pdf)\n",
        "\n",
        "Os pesos gamma e beta são treináveis em todas as camadas de normalização para compensar a possível perda de capacidade representacional. Você pode ativar esses fatores ao definir a sinalização `center` ou `scale` como `True`. É claro que você pode usar `initializers`, `constraints` e `regularizer` para `beta` e `gamma`, ajustando esses valores durante o processo de treinamento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2XlcXf5WBHb"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlbneoEUKrD"
      },
      "source": [
        "### Instale o Tensorflow 2.0 e o Tensorflow-Addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZQGY_ALnirQ"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aGgPZG_WBHg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u82Gz_gOUPDZ"
      },
      "source": [
        "### Preparando o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wso9oidUZZQ"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTQH56j89POZ"
      },
      "source": [
        "## Tutorial de normalização de grupo\n",
        "\n",
        "### Introdução\n",
        "\n",
        "A normalização de grupo (GN) divide os canais das suas entradas em subgrupos menores e normaliza esses valores com base na média e na variância. Como a GN funciona em um único exemplo, essa técnica não depende do tamanho do lote.\n",
        "\n",
        "A GN teve uma pontuação experimentalmente mais próxima à normalização de lotes em tarefas de classificação de imagens. Pode ser vantajoso usar a GN em vez da normalização de lotes caso seu batch_size geral seja baixo, o que levaria ao mau desempenho da normalização de lotes\n",
        "\n",
        "###Exemplo Divisão de 10 canais após uma camada Conv2D em 5 subgrupos em uma configuração \"channels last\" padrão:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIGjLwYWAm0v"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # Reshape into \"channels last\" setup.\n",
        "  tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n",
        "  tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"),\n",
        "  # Groupnorm Layer\n",
        "  tfa.layers.GroupNormalization(groups=5, axis=3),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMwUfJUib3ka"
      },
      "source": [
        "## Tutorial de normalização de instância\n",
        "\n",
        "### Introdução\n",
        "\n",
        "A normalização de instância é o caso especial de normalização de grupo em que o tamanho do grupo é igual ao tamanho do canal (ou do eixo).\n",
        "\n",
        "Os resultados experimentais mostram que a normalização de instância tem um bom desempenho na transferência de estilo ao substituir a normalização de lote. Recentemente, a normalização de instância também foi usada como substituta para a normalização de lote em GANs.\n",
        "\n",
        "### Exemplo\n",
        "\n",
        "Aplicando InstanceNormalization após uma camada Conv2D e usando uma escala inicializada uniforme e um fator de deslocamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sLVv-C8f6Kf"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # Reshape into \"channels last\" setup.\n",
        "  tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n",
        "  tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"),\n",
        "  # LayerNorm Layer\n",
        "  tfa.layers.InstanceNormalization(axis=3, \n",
        "                                   center=True, \n",
        "                                   scale=True,\n",
        "                                   beta_initializer=\"random_uniform\",\n",
        "                                   gamma_initializer=\"random_uniform\"),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYdnEocRUCll"
      },
      "source": [
        "## Tutorial de normalização de camada\n",
        "\n",
        "### Introdução\n",
        "\n",
        "A normalização de camada é um caso especial de normalização de grupo em que o tamanho do grupo é 1. A média e o desvio padrão são calculados de todas as ativações de um único exemplo.\n",
        "\n",
        "Os resultados experimentais mostram que a normalização de camada é bastante adequada para redes neurais recorrentes, já que trabalha com o tamanho do lote de maneira independente.\n",
        "\n",
        "### Exemplo\n",
        "\n",
        "Aplicando Layernormalization após uma camada Conv2D Layer e usando uma escala e um fator de deslocamento. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh-Pp_e5UB54"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # Reshape into \"channels last\" setup.\n",
        "  tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n",
        "  tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"),\n",
        "  # LayerNorm Layer\n",
        "  tf.keras.layers.LayerNormalization(axis=3 , center=True , scale=True),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shvGfnB0WpQQ"
      },
      "source": [
        "## Bibliografia\n",
        "\n",
        "[Normalização de camada](https://arxiv.org/pdf/1607.06450.pdf)\n",
        "\n",
        "[Normalização de instância](https://arxiv.org/pdf/1607.08022.pdf)\n",
        "\n",
        "[Normalização de grupo](https://arxiv.org/pdf/1803.08494.pdf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "layers_normalizations.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
