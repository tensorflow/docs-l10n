{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAttKaKmT435"
      },
      "source": [
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/transform/census\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/transform/census.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/transform/census.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tfx/tutorials/transform/census.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tghWegsjhpkt"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rSGJWC5biBiG"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPt5BHTwy_0F"
      },
      "source": [
        "# TensorFlow Transform を使用したデータの前処理\n",
        "\n",
        "***TensorFlow Extended（TFX）の特徴量エンジニアリングコンポーネント***\n",
        "\n",
        "このコラボノートブックの例は、<a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/transform/get_started\">TensorFlow Transform</a>（`tf.Transform`）を使用して、データを前処理する方法のやや高度な例を提供します。モデルのトレーニングと本番環境での推論のサービングの両方に同じコードを使用します。\n",
        "\n",
        "TensorFlow Transform は、トレーニングデータセットのフルパスを必要とする機能の作成など、TensorFlow の入力データを前処理するためのライブラリです。たとえば、TensorFlow Transform を使用すると、次のことができます。\n",
        "\n",
        "- 平均と標準偏差を使用して入力値を正規化する\n",
        "- すべての入力値に対して語彙を生成することにより、文字列を整数に変換する\n",
        "- 観測されたデータ分布に基づいて、浮動小数点数をバケットに割り当てることにより、浮動小数点数を整数に変換する\n",
        "\n",
        "TensorFlow には、単一のサンプルまたはサンプルのバッチに対する操作のサポートが組み込まれています。`tf.Transform` は、これらの機能を拡張して、トレーニングデータセット全体のフルパスをサポートします。\n",
        "\n",
        "`tf.Transform` の出力は、トレーニングとサービングの両方に使用できる TensorFlow グラフとしてエクスポートされます。トレーニングとサービングの両方に同じグラフを使用すると、両方の段階で同じ変換が適用されるため、スキューを防ぐことができます。\n",
        "\n",
        "重要なポイント: `tf.Transform` とそれが Apache Beam でどのように機能するかを理解するには、Apache Beam についての知識が少し必要です。Apache Beam の基本的なコンセプトについては <a>Beam プログラミングガイド</a>を参照してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQUubddMvnP"
      },
      "source": [
        "##この例で何が行われているのか\n",
        "\n",
        "この例では、<a target=\"_blank\" href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult\">国勢調査データを含む広く使用されているデータセット</a>を処理し、分類を行うためのモデルをトレーニングします。また、`tf.Transform` を使用してデータを変換します。\n",
        "\n",
        "重要なポイント: モデラーおよび開発者の皆さんは、このデータがどのように使用されるか、モデルの予測が引き起こす可能性のある潜在的メリット・デメリットについて考えてください。このようなモデルは、社会的バイアスと格差を拡大する可能性があります。特徴量は解決しようとする問題に関連していますか、それともバイアスを導入しますか？詳細については、<a target=\"_blank\" href=\"https://developers.google.com/machine-learning/fairness-overview/\">機械学習における公平性</a>についてご一読ください。\n",
        "\n",
        "注意: <a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/model_analysis\">TensorFlow Model Analysis</a> は、モデルが社会的バイアスや格差をどのように強化するかを理解するなど、モデルがデータのさまざまなセグメントをどの程度適切に予測するかを理解するための強力なツールです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeonII4omTr1"
      },
      "source": [
        "### TensorFlow Transform のインストール\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ak6XDO5mT3m"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0mXLOJR_-dv"
      },
      "outputs": [],
      "source": [
        "# This cell is only necessary because packages were installed while python was\n",
        "# running. It avoids the need to restart the runtime when running in Colab.\n",
        "import pkg_resources\n",
        "import importlib\n",
        "\n",
        "importlib.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RptgLn2RYuK3"
      },
      "source": [
        "## インポートとグローバル\n",
        "\n",
        "まず、必要なものをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4QXVIM7iglN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TF: {}'.format(tf.__version__))\n",
        "\n",
        "import apache_beam as beam\n",
        "print('Beam: {}'.format(beam.__version__))\n",
        "\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "print('Transform: {}'.format(tft.__version__))\n",
        "\n",
        "from tfx_bsl.public import tfxio\n",
        "from tfx_bsl.coders.example_coder import RecordBatchToExamplesEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sutRmRNSGT5p"
      },
      "source": [
        "次に、データファイルをダウンロードします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKEYRl2g_vzl"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data\n",
        "!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.test\n",
        "\n",
        "train_path = './adult.data'\n",
        "test_path = './adult.test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxOxaaOYRfl7"
      },
      "source": [
        "### 列に名前を付ける\n",
        "\n",
        "データセットの列を参照するための便利なリストをいくつか作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bsr1nLHqyg_"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'native-country',\n",
        "]\n",
        "\n",
        "NUMERIC_FEATURE_KEYS = [\n",
        "    'age',\n",
        "    'capital-gain',\n",
        "    'capital-loss',\n",
        "    'hours-per-week',\n",
        "    'education-num'\n",
        "]\n",
        "\n",
        "ORDERED_CSV_COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n",
        "]\n",
        "\n",
        "LABEL_KEY = 'label'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R52dXlw0G0CN"
      },
      "source": [
        "以下は、データの簡易プレビューです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "312cQ5vwGjOu"
      },
      "outputs": [],
      "source": [
        "pandas_train = pd.read_csv(train_path, header=None, names=ORDERED_CSV_COLUMNS)\n",
        "\n",
        "pandas_train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzjzjR3351j0"
      },
      "outputs": [],
      "source": [
        "one_row = dict(pandas_train.loc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk2b8IPd4uPr"
      },
      "outputs": [],
      "source": [
        "COLUMN_DEFAULTS = [\n",
        "  '' if isinstance(v, str) else 0.0\n",
        "  for v in  dict(pandas_train.loc[1]).values()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LefAguV5ICMc"
      },
      "source": [
        "テストデータには、スキップする必要のあるヘッダー行が 1 行と、各行の末尾に \".\" があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RasgDIUKHCpV"
      },
      "outputs": [],
      "source": [
        "pandas_test = pd.read_csv(test_path, header=1, names=ORDERED_CSV_COLUMNS)\n",
        "\n",
        "pandas_test.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9aH5ZnDdD_z"
      },
      "outputs": [],
      "source": [
        "testing = os.getenv(\"WEB_TEST_BROWSER\", False)\n",
        "if testing:\n",
        "  pandas_train = pandas_train.loc[:1]\n",
        "  pandas_test = pandas_test.loc[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTn4at8rurk"
      },
      "source": [
        "###特徴量とスキーマを定義します。 入力の列の型に基づいてスキーマを定義します。 これはそれらを正しくインポートするのに役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oS2RfyCrzMr"
      },
      "outputs": [],
      "source": [
        "RAW_DATA_FEATURE_SPEC = dict(\n",
        "    [(name, tf.io.FixedLenFeature([], tf.string))\n",
        "     for name in CATEGORICAL_FEATURE_KEYS] +\n",
        "    [(name, tf.io.FixedLenFeature([], tf.float32))\n",
        "     for name in NUMERIC_FEATURE_KEYS] + \n",
        "    [(LABEL_KEY, tf.io.FixedLenFeature([], tf.string))]\n",
        ")\n",
        "\n",
        "SCHEMA = tft.DatasetMetadata.from_feature_spec(RAW_DATA_FEATURE_SPEC).schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j6M7ObpaLHi"
      },
      "source": [
        "### ［オプション］tf.train.Example proto のエンコードとデコード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgGO9-GkZ5Kv"
      },
      "source": [
        "このチュートリアルでは、いくつかの場所で、データセットの Example を `tf.train.Example` との間で変換する必要があります。\n",
        "\n",
        "以下の非表示の `encode_example` 関数は、データセットの特徴量のディクショナリを `tf.train.Example` に変換します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbhndy7uWqYp"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def encode_example(input_features):\n",
        "  input_features = dict(input_features)\n",
        "  output_features = {}\n",
        "  \n",
        "  for key in CATEGORICAL_FEATURE_KEYS:\n",
        "    value = input_features[key]\n",
        "    feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[value.strip().encode()]))\n",
        "    output_features[key] = feature \n",
        "\n",
        "  for key in NUMERIC_FEATURE_KEYS:\n",
        "    value = input_features[key]\n",
        "    feature = tf.train.Feature(\n",
        "        float_list=tf.train.FloatList(value=[value]))\n",
        "    output_features[key] = feature \n",
        "\n",
        "  label_value = input_features.get(LABEL_KEY, None)\n",
        "  if label_value is not None:\n",
        "    output_features[LABEL_KEY]  = tf.train.Feature(\n",
        "        bytes_list = tf.train.BytesList(value=[label_value.strip().encode()]))\n",
        "\n",
        "  example = tf.train.Example(\n",
        "      features = tf.train.Features(feature=output_features)\n",
        "  )\n",
        "  return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qx7fSVmmwIQ"
      },
      "source": [
        "次に、データセットの Example を `Example` proto に変換できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWd95yxJceXy"
      },
      "outputs": [],
      "source": [
        "tf_example = encode_example(pandas_train.loc[0])\n",
        "tf_example.features.feature['age']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EutF2aPXbAUd"
      },
      "outputs": [],
      "source": [
        "serialized_example_batch = tf.constant([\n",
        "  encode_example(pandas_train.loc[i]).SerializeToString()\n",
        "  for i in range(3)\n",
        "])\n",
        "\n",
        "serialized_example_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTqlJcI_m6az"
      },
      "source": [
        "また、シリアル化された Example proto のバッチをテンソルのディクショナリに変換することもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXlrur1vc4n_"
      },
      "outputs": [],
      "source": [
        "decoded_tensors = tf.io.parse_example(\n",
        "    serialized_example_batch,\n",
        "    features=RAW_DATA_FEATURE_SPEC\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUAcdCrEdDe3"
      },
      "source": [
        "一部のケースでは、ラベルが渡されないことがあるため、ラベルがオプションとなるようにエンコード関数を記述します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEt3nPr_o59f"
      },
      "outputs": [],
      "source": [
        "features_dict = dict(pandas_train.loc[0])\n",
        "features_dict.pop(LABEL_KEY)\n",
        "\n",
        "LABEL_KEY in features_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0yqvsHtpDdX"
      },
      "source": [
        "`Example` proto を作成する際には、単にラベルキーが含まれません。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N5FMXO7dRzM"
      },
      "outputs": [],
      "source": [
        "no_label_example = encode_example(features_dict)\n",
        "\n",
        "LABEL_KEY in no_label_example.features.feature.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdXy9lo4t45d"
      },
      "source": [
        "###ハイパーパラメータの設定と基本的なハウスキーピング\n",
        "\n",
        "以下は、トレーニングに使用される定数とハイパーパラメータです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WHyOkC9uL71"
      },
      "outputs": [],
      "source": [
        "NUM_OOV_BUCKETS = 1\n",
        "\n",
        "EPOCH_SPLITS = 10\n",
        "TRAIN_NUM_EPOCHS = 2*EPOCH_SPLITS\n",
        "NUM_TRAIN_INSTANCES = len(pandas_train)\n",
        "NUM_TEST_INSTANCES = len(pandas_test)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "STEPS_PER_TRAIN_EPOCH = tf.math.ceil(NUM_TRAIN_INSTANCES/BATCH_SIZE/EPOCH_SPLITS)\n",
        "EVALUATION_STEPS = tf.math.ceil(NUM_TEST_INSTANCES/BATCH_SIZE)\n",
        "\n",
        "# Names of temp files\n",
        "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
        "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
        "EXPORTED_MODEL_DIR = 'exported_model_dir'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG2uO-88c6R9"
      },
      "outputs": [],
      "source": [
        "if testing:\n",
        "  TRAIN_NUM_EPOCHS = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a1ns5KswDb2"
      },
      "source": [
        "##`tf.Transform` による前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKd3mCLNVYmg"
      },
      "source": [
        "###`tf.Transform` preprocessing_fnを作成します。*前処理関数*は、tf.Transform の最も重要な概念です。前処理関数では、データセットの変換が実際に行われます。テンソルのディクショナリーを受け入れて返します。ここで、テンソルは [`Tensor`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor) または [`SparseTensor`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/SparseTensor) を意味します。通常、前処理関数の中心となる API 呼び出しには 2 つの主要なグループがあります。\n",
        "\n",
        "1. **TensorFlow 演算子:** テンソルを受け入れて返す関数。通常は TensorFlow 演算子を意味します。これらは、生データを一度に 1 つの特徴量ベクトルで変換されたデータに変換するグラフに TensorFlow 演算子を追加します。これらは、トレーニングとサービングの両方で、すべての例で実行されます。\n",
        "2. **Tensorflow Transform アナライザー/マッパー:** tf.Transform によって提供されるアナライザー/マッパーのいずれか。これらもテンソルを受け入れて返し、通常は Tensorflow 演算子と Beam 計算の組み合わせを含みますが、TensorFlow 演算子とは異なり、分析中はビームパイプラインでのみ実行され、トレーニングデータセット全体を通じた処理が必要になります。Beam 計算は（トレーニング前、分析中に）1 回だけ実行され、通常はトレーニングデータセット全体を処理します。`tf.constant` テンソルが作成され、グラフに追加されます。たとえば、 `tft.min` は、トレーニングデータセットのテンソルの最小値を計算します。\n",
        "\n",
        "注意: 前処理関数をサービング推論に適用する場合、トレーニング中にアナライザーにより作成された定数は変更されません。データに傾向または季節性の要素がある場合は、それに応じて計画します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZopPfpaH4sB"
      },
      "source": [
        "以下は、このデータセットの `preprocessing_fn` です。以下のことを実行します。\n",
        "\n",
        "1. `tft.scale_to_0_1` を使用して、数値特徴量を `[0,1]` の範囲にスケーリングします。\n",
        "2. `tft.compute_and_apply_vocabulary` を使って、カテゴリカル特徴量ごとの語彙を計算し、各入力の整数 ID を `tf.int64` として返します。これは、文字列と整数のどちらのカテゴリカル入力にも適用されます。\n",
        "3. 標準の TensorFlow 演算を使ってデータに手動変換を適用します。ここでは、ラベルに対して演算を適用しますが、特徴量も変換することが可能です。TensorFlow 演算は以下を実行します。\n",
        "    - ラベルのルックアップテーブルをビルドします（`tf.init_scope` は、関数が初めて呼び出された時にのみテーブルを作成します）。\n",
        "    - ラベルのテキストを正規化します。\n",
        "    - ラベルをワンホットに変換します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDrzuYH0WFc2"
      },
      "outputs": [],
      "source": [
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "  # Since we are modifying some features and leaving others unchanged, we\n",
        "  # start by setting `outputs` to a copy of `inputs.\n",
        "  outputs = inputs.copy()\n",
        "\n",
        "  # Scale numeric columns to have range [0, 1].\n",
        "  for key in NUMERIC_FEATURE_KEYS:\n",
        "    outputs[key] = tft.scale_to_0_1(inputs[key])\n",
        "\n",
        "  # For all categorical columns except the label column, we generate a\n",
        "  # vocabulary but do not modify the feature.  This vocabulary is instead\n",
        "  # used in the trainer, by means of a feature column, to convert the feature\n",
        "  # from a string to an integer id.\n",
        "  for key in CATEGORICAL_FEATURE_KEYS:\n",
        "    outputs[key] = tft.compute_and_apply_vocabulary(\n",
        "        tf.strings.strip(inputs[key]),\n",
        "        num_oov_buckets=NUM_OOV_BUCKETS,\n",
        "        vocab_filename=key)\n",
        "\n",
        "  # For the label column we provide the mapping from string to index.\n",
        "  table_keys = ['>50K', '<=50K']\n",
        "  with tf.init_scope():\n",
        "    initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=table_keys,\n",
        "        values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
        "        key_dtype=tf.string,\n",
        "        value_dtype=tf.int64)\n",
        "    table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
        "\n",
        "  # Remove trailing periods for test data when the data is read with tf.data.\n",
        "  # label_str  = tf.sparse.to_dense(inputs[LABEL_KEY])\n",
        "  label_str = inputs[LABEL_KEY]\n",
        "  label_str = tf.strings.regex_replace(label_str, r'\\.$', '')\n",
        "  label_str = tf.strings.strip(label_str)\n",
        "  data_labels = table.lookup(label_str)\n",
        "  transformed_label = tf.one_hot(\n",
        "      indices=data_labels, depth=len(table_keys), on_value=1.0, off_value=0.0)\n",
        "  outputs[LABEL_KEY] = tf.reshape(transformed_label, [-1, len(table_keys)])\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA1Eg2JXFzzZ"
      },
      "source": [
        "## 構文\n",
        "\n",
        "これで、すべてをまとめて <a target=\"_blank\" href=\"https://beam.apache.org/\">Apache Beam</a> を使用して実行する準備がほぼ整いました。\n",
        "\n",
        "Apache Beam は、<a target=\"_blank\" href=\"https://beam.apache.org/documentation/programming-guide/#applying-transforms\">特別な構文を使用して変換を定義および呼び出します</a>。たとえば、次の行をご覧ください。\n",
        "\n",
        "```\n",
        "result = pass_this | 'name this step' >> to_this_call\n",
        "```\n",
        "\n",
        "メソッド <code>to_this_call</code> が呼び出され、<code>pass_this</code> というオブジェクトが渡されます。この演算は、<a target=\"_blank\" href=\"https://stackoverflow.com/questions/50519662/what-does-the-redirection-mean-in-apache-beam-python\">スタックトレースで <code>name this step</code> と呼ばれます</a>。<code>to_this_call</code> の呼び出しの結果は、<code>result</code> に返されます。 頻繁にパイプラインのステージは次のようにチェーンされます。\n",
        "\n",
        "```\n",
        "result = apache_beam.Pipeline() | 'first step' >> do_this_first() | 'second step' >> do_this_last()\n",
        "```\n",
        "\n",
        "そして、新しいパイプラインで始まったので、以下のように続行できます。\n",
        "\n",
        "```\n",
        "next_result = result | 'doing more stuff' >> another_function()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgAGOAdFWRn2"
      },
      "source": [
        "### データを変換する\n",
        "\n",
        "Apache Beam パイプラインでデータを変換し始める準備が整いました。\n",
        "\n",
        "1. `tfxio.CsvTFXIO` CSV リーダーを使用してデータを読み取ります（パイプラインでテキストの行を処理するには、代わりに `tfxio.BeamRecordCsvTFXIO` を使用します）。\n",
        "2. 上記で定義した `preprocessing_fn` を使ってデータの分析と変換を行います。\n",
        "3. 結果を `Example` プロトの `TFRecord` として書き出します。これは、後でモデルのトレーニングに使用します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCeYucVoRRfo"
      },
      "outputs": [],
      "source": [
        "def transform_data(train_data_file, test_data_file, working_dir):\n",
        "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
        "\n",
        "  Read in the data using the CSV reader, and transform it using a\n",
        "  preprocessing pipeline that scales numeric data and converts categorical data\n",
        "  from strings to int64 values indices, by creating a vocabulary for each\n",
        "  category.\n",
        "\n",
        "  Args:\n",
        "    train_data_file: File containing training data\n",
        "    test_data_file: File containing test data\n",
        "    working_dir: Directory to write transformed data and metadata to\n",
        "  \"\"\"\n",
        "\n",
        "  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
        "  # of the block.\n",
        "  with beam.Pipeline() as pipeline:\n",
        "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "      # Create a TFXIO to read the census data with the schema. To do this we\n",
        "      # need to list all columns in order since the schema doesn't specify the\n",
        "      # order of columns in the csv.\n",
        "      # We first read CSV files and use BeamRecordCsvTFXIO whose .BeamSource()\n",
        "      # accepts a PCollection[bytes] because we need to patch the records first\n",
        "      # (see \"FixCommasTrainData\" below). Otherwise, tfxio.CsvTFXIO can be used\n",
        "      # to both read the CSV files and parse them to TFT inputs:\n",
        "      # csv_tfxio = tfxio.CsvTFXIO(...)\n",
        "      # raw_data = (pipeline | 'ToRecordBatches' >> csv_tfxio.BeamSource())\n",
        "      train_csv_tfxio = tfxio.CsvTFXIO(\n",
        "          file_pattern=train_data_file,\n",
        "          telemetry_descriptors=[],\n",
        "          column_names=ORDERED_CSV_COLUMNS,\n",
        "          schema=SCHEMA)\n",
        "\n",
        "      # Read in raw data and convert using CSV TFXIO.\n",
        "      raw_data = (\n",
        "          pipeline |\n",
        "          'ReadTrainCsv' >> train_csv_tfxio.BeamSource())\n",
        "\n",
        "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
        "      # the schema to read the CSV data, but we also need it to interpret\n",
        "      # raw_data.\n",
        "      cfg = train_csv_tfxio.TensorAdapterConfig()\n",
        "      raw_dataset = (raw_data, cfg)\n",
        "\n",
        "      # The TFXIO output format is chosen for improved performance.\n",
        "      transformed_dataset, transform_fn = (\n",
        "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(\n",
        "              preprocessing_fn, output_record_batches=True))\n",
        "\n",
        "      # Transformed metadata is not necessary for encoding.\n",
        "      transformed_data, _ = transformed_dataset\n",
        "\n",
        "      # Extract transformed RecordBatches, encode and write them to the given\n",
        "      # directory.\n",
        "      coder = RecordBatchToExamplesEncoder()\n",
        "      _ = (\n",
        "          transformed_data\n",
        "          | 'EncodeTrainData' >>\n",
        "          beam.FlatMapTuple(lambda batch, _: coder.encode(batch))\n",
        "          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n",
        "\n",
        "      # Now apply transform function to test data.  In this case we remove the\n",
        "      # trailing period at the end of each line, and also ignore the header line\n",
        "      # that is present in the test data file.\n",
        "      test_csv_tfxio = tfxio.CsvTFXIO(\n",
        "          file_pattern=test_data_file,\n",
        "          skip_header_lines=1,\n",
        "          telemetry_descriptors=[],\n",
        "          column_names=ORDERED_CSV_COLUMNS,\n",
        "          schema=SCHEMA)\n",
        "      raw_test_data = (\n",
        "          pipeline\n",
        "          | 'ReadTestCsv' >> test_csv_tfxio.BeamSource())\n",
        "\n",
        "      raw_test_dataset = (raw_test_data, test_csv_tfxio.TensorAdapterConfig())\n",
        "\n",
        "      # The TFXIO output format is chosen for improved performance.\n",
        "      transformed_test_dataset = (\n",
        "          (raw_test_dataset, transform_fn)\n",
        "          | tft_beam.TransformDataset(output_record_batches=True))\n",
        "\n",
        "      # Transformed metadata is not necessary for encoding.\n",
        "      transformed_test_data, _ = transformed_test_dataset\n",
        "\n",
        "      # Extract transformed RecordBatches, encode and write them to the given\n",
        "      # directory.\n",
        "      _ = (\n",
        "          transformed_test_data\n",
        "          | 'EncodeTestData' >>\n",
        "          beam.FlatMapTuple(lambda batch, _: coder.encode(batch))\n",
        "          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n",
        "\n",
        "      # Will write a SavedModel and metadata to working_dir, which can then\n",
        "      # be read by the tft.TFTransformOutput class.\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huaj5EgCVRD9"
      },
      "source": [
        "パイプラインを実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjC7eDWFyA8K"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import pathlib\n",
        "\n",
        "output_dir = os.path.join(tempfile.mkdtemp(), 'keras')\n",
        "\n",
        "\n",
        "transform_data(train_path, test_path, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqln2AClsA0z"
      },
      "source": [
        "出力ディレクトリを `tft.TFTransformOutput` としてラップします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXd4Mgj6sAGB"
      },
      "outputs": [],
      "source": [
        "tf_transform_output = tft.TFTransformOutput(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59hNe7oY9vqG"
      },
      "outputs": [],
      "source": [
        "tf_transform_output.transformed_feature_spec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBlL2EIVVF8"
      },
      "source": [
        "ディレクトリを確認すると、以下の 3 つの項目があります。\n",
        "\n",
        "1. `train_transformed` と `test_transformed` データファイル\n",
        "2. `transform_fn` ディレクトリ（`tf.saved_model`）\n",
        "3. transformed_metadata\n",
        "\n",
        "次のセクションでは、これらのアーティファクトを使ってモデルをトレーニングする方法を説明します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG6nrHEP2L65"
      },
      "outputs": [],
      "source": [
        "!ls -l {output_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnaMyRMJ03bR"
      },
      "source": [
        "##前処理されたデータを使用して、tf.keras を使用してモデルをトレーニングします\n",
        "\n",
        "`tf.Transform` を使用して、トレーニングとサービングの両方に同じコードを使用し、スキューを防ぐ方法を示すためにモデルをトレーニングします。モデルをトレーニングし、トレーニングしたモデルを本番用に準備するには、入力関数を作成する必要があります。トレーニング入力関数とサービング入力関数の主な違いは、トレーニングデータにはラベルが含まれ、本番環境のデータには含まれないことです。引数と戻り値も多少異なります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8xCZKNc2wAS"
      },
      "source": [
        "###トレーニング用の入力関数を作成します"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StezlX-Uv0ae"
      },
      "source": [
        "前のセクションのパイプラインを実行すると、変換済みのデータを含む `TFRecord` ファイルが作成されました。\n",
        "\n",
        "次のコードは、`tf.data.experimental.make_batched_features_dataset` と `tft.TFTransformOutput.transformed_feature_spec` を使用して、データファイルを `tf.data.Dataset` として読み取ります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775Y7BTpHBmb"
      },
      "outputs": [],
      "source": [
        "def _make_training_input_fn(tf_transform_output, train_file_pattern,\n",
        "                            batch_size):\n",
        "  \"\"\"An input function reading from transformed data, converting to model input.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    transformed_examples: Base filename of examples.\n",
        "    batch_size: Batch size.\n",
        "\n",
        "  Returns:\n",
        "    The input data for training or eval, in the form of k.\n",
        "  \"\"\"\n",
        "  def input_fn():\n",
        "    return tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=train_file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=tf_transform_output.transformed_feature_spec(),\n",
        "        reader=tf.data.TFRecordDataset,\n",
        "        label_key=LABEL_KEY,\n",
        "        shuffle=True)\n",
        "\n",
        "  return input_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b8BgvBvkCnX"
      },
      "outputs": [],
      "source": [
        "train_file_pattern = pathlib.Path(output_dir)/f'{TRANSFORMED_TRAIN_DATA_FILEBASE}*'\n",
        "\n",
        "input_fn = _make_training_input_fn(\n",
        "    tf_transform_output=tf_transform_output,\n",
        "    train_file_pattern = str(train_file_pattern),\n",
        "    batch_size = 10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0PwPLBqxsg2"
      },
      "source": [
        "以下では、変換済みのデータサンプルを確認できます。`education-num` や `hourd-per-week` などの数値カラムは範囲 [0,1] の浮動小数点数に、文字列カラムは ID に変換されていることに注目してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpiS26IWlD-1"
      },
      "outputs": [],
      "source": [
        "for example, label in input_fn().take(1):\n",
        "  break\n",
        "\n",
        "pd.DataFrame(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaMzMnij88_v"
      },
      "outputs": [],
      "source": [
        "label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyNTX7CO8AAz"
      },
      "source": [
        "### モデルのトレーニングと評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdg9jXuLWuyK"
      },
      "source": [
        "モデルを構築する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK4brUuDTAJ4"
      },
      "outputs": [],
      "source": [
        "def build_keras_model(working_dir):\n",
        "  inputs = build_keras_inputs(working_dir)\n",
        "\n",
        "  encoded_inputs = encode_inputs(inputs)\n",
        "\n",
        "  stacked_inputs = tf.concat(tf.nest.flatten(encoded_inputs), axis=1)\n",
        "  output = tf.keras.layers.Dense(100, activation='relu')(stacked_inputs)\n",
        "  output = tf.keras.layers.Dense(50, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dense(2)(output)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fJwIbdCRFER"
      },
      "outputs": [],
      "source": [
        "def build_keras_inputs(working_dir):\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
        "  feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  # Build the `keras.Input` objects.\n",
        "  inputs = {}\n",
        "  for key, spec in feature_spec.items():\n",
        "    if isinstance(spec, tf.io.VarLenFeature):\n",
        "      inputs[key] = tf.keras.layers.Input(\n",
        "          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n",
        "    elif isinstance(spec, tf.io.FixedLenFeature):\n",
        "      inputs[key] = tf.keras.layers.Input(\n",
        "          shape=spec.shape, name=key, dtype=spec.dtype)\n",
        "    else:\n",
        "      raise ValueError('Spec type is not supported: ', key, spec)\n",
        "\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dHD5SoqRqOh"
      },
      "outputs": [],
      "source": [
        "def encode_inputs(inputs):\n",
        "  encoded_inputs = {}\n",
        "  for key in inputs:\n",
        "    feature = tf.expand_dims(inputs[key], -1)\n",
        "    if key in CATEGORICAL_FEATURE_KEYS:\n",
        "      num_buckets = tf_transform_output.num_buckets_for_transformed_feature(key)\n",
        "      encoding_layer = (\n",
        "          tf.keras.layers.CategoryEncoding(\n",
        "              num_tokens=num_buckets, output_mode='binary', sparse=False))\n",
        "      encoded_inputs[key] = encoding_layer(feature)\n",
        "    else:\n",
        "      encoded_inputs[key] = feature\n",
        "  \n",
        "  return encoded_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xNhSq8lTTx3"
      },
      "outputs": [],
      "source": [
        "model = build_keras_model(output_dir)\n",
        "\n",
        "tf.keras.utils.plot_model(model,rankdir='LR', show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQSpw_XzXVn1"
      },
      "source": [
        "データセットを構築します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afi3NOC0OMUa"
      },
      "outputs": [],
      "source": [
        "def get_dataset(working_dir, filebase):\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  data_path_pattern = os.path.join(\n",
        "      working_dir,\n",
        "      filebase + '*')\n",
        "  \n",
        "  input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      data_path_pattern,\n",
        "      batch_size=BATCH_SIZE)\n",
        "  \n",
        "  dataset = input_fn()\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fE_3jyzX_h2"
      },
      "source": [
        "モデルをトレーニングして評価します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i_lhWH8IZrk"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(\n",
        "    model,\n",
        "    working_dir):\n",
        "  \"\"\"Train the model on training data and evaluate on test data.\n",
        "\n",
        "  Args:\n",
        "    working_dir: The location of the Transform output.\n",
        "    num_train_instances: Number of instances in train set\n",
        "    num_test_instances: Number of instances in test set\n",
        "\n",
        "  Returns:\n",
        "    The results from the estimator's 'evaluate' method\n",
        "  \"\"\"\n",
        "  train_dataset = get_dataset(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)\n",
        "  validation_dataset = get_dataset(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)\n",
        "\n",
        "  model = build_keras_model(working_dir)\n",
        "\n",
        "  history = train_model(model, train_dataset, validation_dataset)\n",
        "\n",
        "  metric_values = model.evaluate(validation_dataset,\n",
        "                                 steps=EVALUATION_STEPS,\n",
        "                                 return_dict=True)\n",
        "  return model, history, metric_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcVsByIsViRy"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataset, validation_dataset):\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_dataset, validation_data=validation_dataset,\n",
        "      epochs=TRAIN_NUM_EPOCHS,\n",
        "      steps_per_epoch=STEPS_PER_TRAIN_EPOCH,\n",
        "      validation_steps=EVALUATION_STEPS)\n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5xoioogYTle"
      },
      "outputs": [],
      "source": [
        "model, history, metric_values = train_and_evaluate(model, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQCbdPIQeXeZ"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Eval')\n",
        "plt.ylim(0,max(plt.ylim()))\n",
        "plt.legend()\n",
        "plt.title('Loss');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYeuthrs27vl"
      },
      "source": [
        "### 新しいデータを変換する\n",
        "\n",
        "前のセクションのトレーニングプロセスでは、`transform_dataset` 関数の `tft_beam.AnalyzeAndTransformDataset` によって生成された変換済みデータのハードコピーを使用しました。\n",
        "\n",
        "新しいデータを操作するには、`tft_beam.WriteTransformFn` が保存した最終バージョンの `preprocessing_fn` を読み込む必要があります。\n",
        "\n",
        "`TFTransformOutput.transform_features_layer` メソッドは、出力ディレクトリから `preprocessing_fn` SavedModel を読み込みます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxi9aS106CLd"
      },
      "source": [
        "以下は、新しい未加工のバッチをソースファイルから読み込む関数です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMHDZhp82ZjM"
      },
      "outputs": [],
      "source": [
        "def read_csv(file_name, batch_size):\n",
        "  return tf.data.experimental.make_csv_dataset(\n",
        "        file_pattern=file_name,\n",
        "        batch_size=batch_size,\n",
        "        column_names=ORDERED_CSV_COLUMNS,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        prefetch_buffer_size=0,\n",
        "        ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AradAjmW2vyd"
      },
      "outputs": [],
      "source": [
        "for ex in read_csv(test_path, batch_size=5):\n",
        "  break\n",
        "\n",
        "pd.DataFrame(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX1f6SgM6LZc"
      },
      "source": [
        "`tft.TransformFeaturesLayer` を読み込んで、このデータを `preprocessing_fn` で変換します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nma2Bzi--11x"
      },
      "outputs": [],
      "source": [
        "ex2 = ex.copy()\n",
        "ex2.pop('fnlwgt')\n",
        "\n",
        "tft_layer = tf_transform_output.transform_features_layer()\n",
        "t_ex = tft_layer(ex2)\n",
        "\n",
        "label = t_ex.pop(LABEL_KEY)\n",
        "pd.DataFrame(t_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P43ixyQNz1zq"
      },
      "source": [
        "`tft_layer` は、特徴量のサブセットのみが渡された場合でも変換を実行できるほどスマートな関数です。たとえば、2 つの特徴量のみを渡しても、変換済みの特徴量を得ることができます。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swEPuZsR0Y5S"
      },
      "outputs": [],
      "source": [
        "ex2 = pd.DataFrame(ex)[['education', 'hours-per-week']]\n",
        "ex2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s4SxutV1DTI"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(tft_layer(dict(ex2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5wo3dN-vhFL"
      },
      "source": [
        "以下はより堅牢なバージョンで、特徴量の仕様に含まれない特徴量をドロップし、提供された特徴量にラベルが存在する場合に `(features, label)` ペアを返します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdMKDnafJh64"
      },
      "outputs": [],
      "source": [
        "class Transform(tf.Module):\n",
        "  def __init__(self, working_dir):\n",
        "    self.working_dir = working_dir\n",
        "    self.tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "    self.tft_layer = tf_transform_output.transform_features_layer()\n",
        "  \n",
        "  @tf.function\n",
        "  def __call__(self, features):\n",
        "    raw_features = {}\n",
        "\n",
        "    for key, val in features.items():\n",
        "      # Skip unused keys\n",
        "      if key not in RAW_DATA_FEATURE_SPEC:\n",
        "        continue\n",
        "\n",
        "      raw_features[key] = val\n",
        "\n",
        "    # Apply the `preprocessing_fn`.\n",
        "    transformed_features = tft_layer(raw_features)\n",
        "    \n",
        "    if LABEL_KEY in transformed_features:\n",
        "      # Pop the label and return a (features, labels) pair.\n",
        "      data_labels = transformed_features.pop(LABEL_KEY)\n",
        "      return (transformed_features, data_labels)\n",
        "    else:\n",
        "      return transformed_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm5HI578Ku1B"
      },
      "outputs": [],
      "source": [
        "transform = Transform(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jeenwN_3ZRj"
      },
      "outputs": [],
      "source": [
        "t_ex, t_label = transform(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIavZAqALO8H"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(t_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVQead0fwVuy"
      },
      "source": [
        "次に、`Dataset.map` を使用して、その変換をオンザフライで新しいデータに適用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN3IO6u1Mk83"
      },
      "outputs": [],
      "source": [
        "model.evaluate(\n",
        "    read_csv(test_path, batch_size=5).map(transform),\n",
        "    steps=EVALUATION_STEPS,\n",
        "    return_dict=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymlco3hfU_-E"
      },
      "source": [
        "### モデルをエクスポートする\n",
        "\n",
        "トレーニング済みのモデルと、新しいデータに `preporcessing_fn` を適用するメソッドの準備ができました。これらを、シリアル化された `tf.train.Example` proto を入力として受け取る新しいモデルにまとめます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ2WICuwEwqC"
      },
      "outputs": [],
      "source": [
        "class ServingModel(tf.Module):\n",
        "  def __init__(self, model, working_dir):\n",
        "    self.model = model\n",
        "    self.working_dir = working_dir\n",
        "    self.transform = Transform(working_dir)\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\n",
        "  def __call__(self, serialized_tf_examples):\n",
        "    # parse the tf.train.Example\n",
        "    feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
        "    feature_spec.pop(LABEL_KEY)\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "    # Apply the `preprocessing_fn`\n",
        "    transformed_features = self.transform(parsed_features)\n",
        "    # Run the model\n",
        "    outputs = self.model(transformed_features)\n",
        "    # Format the output\n",
        "    classes_names = tf.constant([['0', '1']])\n",
        "    classes = tf.tile(classes_names, [tf.shape(outputs)[0], 1])\n",
        "    return {'classes': classes, 'scores': outputs}\n",
        "\n",
        "  def export(self, output_dir):\n",
        "    # Increment the directory number. This is required in order to make this\n",
        "    # model servable with model_server.\n",
        "    save_model_dir = pathlib.Path(output_dir)/'model'\n",
        "    number_dirs = [int(p.name) for p in save_model_dir.glob('*')\n",
        "                  if p.name.isdigit()]\n",
        "    id = max([0] + number_dirs)+1\n",
        "    save_model_dir = save_model_dir/str(id)\n",
        "\n",
        "    # Set the signature to make it visible for serving.\n",
        "    concrete_serving_fn = self.__call__.get_concrete_function()\n",
        "    signatures = {'serving_default': concrete_serving_fn}\n",
        "\n",
        "    # Export the model.\n",
        "    tf.saved_model.save(\n",
        "        self,\n",
        "        str(save_model_dir),\n",
        "        signatures=signatures)\n",
        "    \n",
        "    return save_model_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8TZf2di24L2"
      },
      "source": [
        "モデルをビルドし、シリアル化した Example のバッチでテストランを実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2mSC1UMGAwJ"
      },
      "outputs": [],
      "source": [
        "serving_model = ServingModel(model, output_dir)\n",
        "\n",
        "serving_model(serialized_example_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWhighof3AK8"
      },
      "source": [
        "モデルを SavedModel としてエクスポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kodDWTJIEr77"
      },
      "outputs": [],
      "source": [
        "saved_model_dir = serving_model.export(output_dir)\n",
        "saved_model_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohbWxp3-3aQu"
      },
      "source": [
        "モデルを再読み込みし、同じ Example のバッチでテストします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nShh6GqcEr78"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load(str(saved_model_dir))\n",
        "run_model = reloaded.signatures['serving_default']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiYJhQySEr78"
      },
      "outputs": [],
      "source": [
        "run_model(serialized_example_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICqetCnSjwp1"
      },
      "source": [
        "##この例では、`tf.Transform` を使用して国勢調査データのデータセットを前処理し、クリーンアップおよび変換されたデータを使用してモデルをトレーニングしました。また、トレーニング済みモデルを本番環境にデプロイして推論を実行する際に使用する入力関数も作成しました。トレーニングと推論の両方に同じコードを使用することで、データのスキューに関する問題を回避します。その過程で、データのクリーンアップに必要な変換を実行するための Apache Beam 変換の作成について学習しました。また、この変換されたデータを使用して、`tf.keras` を使用してモデルをトレーニングする方法も確認しました。これは、TensorFlow Transform でできることのほんの一部です。`tf.Transform` についての知識を深めることをお勧めします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APEUSA9boKgT"
      },
      "source": [
        "## ［オプション］前処理されたデータを使用して tf.estimator でモデルをトレーニングする\n",
        "\n",
        "> 警告: 新しいコードには Estimators は推奨されません。Estimators は <code>v1.Session</code> スタイルのコードを実行しますが、これは正しく記述するのはより難しく、特に TF 2 コードと組み合わせると予期しない動作をする可能性があります。Estimators は、<a>互換性保証</a>の対象となりますが、セキュリティの脆弱性以外の修正は行われません。詳細については、[移行ガイド](https://tensorflow.org/guide/versions)を参照してください。\n",
        "\n",
        " <!-- <div class=\"tfo-display-only-on-site\"><devsite-expandable>\n",
        "  <button type=\"button\" class=\"button-red button expand-control\">Show Section</button> -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcBWjr3ioZbl"
      },
      "source": [
        "###トレーニング用の入力関数を作成します"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFO0MeWQ228a"
      },
      "outputs": [],
      "source": [
        "def _make_training_input_fn(tf_transform_output, transformed_examples,\n",
        "                            batch_size):\n",
        "  \"\"\"Creates an input function reading from transformed data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    transformed_examples: Base filename of examples.\n",
        "    batch_size: Batch size.\n",
        "\n",
        "  Returns:\n",
        "    The input function for training or eval.\n",
        "  \"\"\"\n",
        "  def input_fn():\n",
        "    \"\"\"Input function for training and eval.\"\"\"\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=transformed_examples,\n",
        "        batch_size=batch_size,\n",
        "        features=tf_transform_output.transformed_feature_spec(),\n",
        "        reader=tf.data.TFRecordDataset,\n",
        "        shuffle=True)\n",
        "\n",
        "    transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n",
        "        dataset).get_next()\n",
        "\n",
        "    # Extract features and label from the transformed tensors.\n",
        "    transformed_labels = tf.where(\n",
        "        tf.equal(transformed_features.pop(LABEL_KEY), 1))\n",
        "\n",
        "    return transformed_features, transformed_labels[:,1]\n",
        "\n",
        "  return input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22XOsZ-noez-"
      },
      "source": [
        "###サービングするための入力関数を作成します\n",
        "\n",
        "本番環境で使用できる入力関数を作成し、トレーニング済みのモデルをサービングできるように準備します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "NN5FVg343Jea"
      },
      "outputs": [],
      "source": [
        "def _make_serving_input_fn(tf_transform_output):\n",
        "  \"\"\"Creates an input function reading from raw data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "\n",
        "  Returns:\n",
        "    The serving input function.\n",
        "  \"\"\"\n",
        "  raw_feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
        "  # Remove label since it is not available during serving.\n",
        "  raw_feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  def serving_input_fn():\n",
        "    \"\"\"Input function for serving.\"\"\"\n",
        "    # Get raw features by generating the basic serving input_fn and calling it.\n",
        "    # Here we generate an input_fn that expects a parsed Example proto to be fed\n",
        "    # to the model at serving time.  See also\n",
        "    # tf.estimator.export.build_raw_serving_input_receiver_fn.\n",
        "    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "        raw_feature_spec, default_batch_size=None)\n",
        "    serving_input_receiver = raw_input_fn()\n",
        "\n",
        "    # Apply the transform function that was used to generate the materialized\n",
        "    # data.\n",
        "    raw_features = serving_input_receiver.features\n",
        "    transformed_features = tf_transform_output.transform_raw_features(\n",
        "        raw_features)\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(\n",
        "        transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "  return serving_input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc9Edp8A7dsI"
      },
      "source": [
        "###入力データを FeatureColumns でラップします。モデルは TensorFlow FeatureColumns でデータを期待します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qOFOvBk7oJX"
      },
      "outputs": [],
      "source": [
        "def get_feature_columns(tf_transform_output):\n",
        "  \"\"\"Returns the FeatureColumns for the model.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: A `TFTransformOutput` object.\n",
        "\n",
        "  Returns:\n",
        "    A list of FeatureColumns.\n",
        "  \"\"\"\n",
        "  # Wrap scalars as real valued columns.\n",
        "  real_valued_columns = [tf.feature_column.numeric_column(key, shape=())\n",
        "                         for key in NUMERIC_FEATURE_KEYS]\n",
        "\n",
        "  # Wrap categorical columns.\n",
        "  one_hot_columns = [\n",
        "      tf.feature_column.indicator_column(\n",
        "          tf.feature_column.categorical_column_with_identity(\n",
        "              key=key,\n",
        "              num_buckets=(NUM_OOV_BUCKETS +\n",
        "                  tf_transform_output.vocabulary_size_by_name(\n",
        "                      vocab_filename=key))))\n",
        "      for key in CATEGORICAL_FEATURE_KEYS]\n",
        "\n",
        "  return real_valued_columns + one_hot_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6FyMzMcpOgT"
      },
      "source": [
        "###モデルをトレーニング、評価、エクスポートします"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iGQ0jeq8IWr"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,\n",
        "                       num_test_instances=NUM_TEST_INSTANCES):\n",
        "  \"\"\"Train the model on training data and evaluate on test data.\n",
        "\n",
        "  Args:\n",
        "    working_dir: Directory to read transformed data and metadata from and to\n",
        "        write exported model to.\n",
        "    num_train_instances: Number of instances in train set\n",
        "    num_test_instances: Number of instances in test set\n",
        "\n",
        "  Returns:\n",
        "    The results from the estimator's 'evaluate' method\n",
        "  \"\"\"\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  run_config = tf.estimator.RunConfig()\n",
        "\n",
        "  estimator = tf.estimator.LinearClassifier(\n",
        "      feature_columns=get_feature_columns(tf_transform_output),\n",
        "      config=run_config,\n",
        "      loss_reduction=tf.losses.Reduction.SUM)\n",
        "\n",
        "  # Fit the model using the default optimizer.\n",
        "  train_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
        "      batch_size=BATCH_SIZE)\n",
        "  estimator.train(\n",
        "      input_fn=train_input_fn,\n",
        "      max_steps=TRAIN_NUM_EPOCHS * num_train_instances / BATCH_SIZE)\n",
        "\n",
        "  # Evaluate model on test dataset.\n",
        "  eval_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n",
        "      batch_size=1)\n",
        "\n",
        "  # Export the model.\n",
        "  serving_input_fn = _make_serving_input_fn(tf_transform_output)\n",
        "  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
        "  estimator.export_saved_model(exported_model_dir, serving_input_fn)\n",
        "\n",
        "  return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k8LrDPZpZsK"
      },
      "source": [
        "###すべてをまとめます。以上で国勢調査データを前処理し、モデルをトレーニングして、サービングする準備が完了しました。次に実行します。\n",
        "\n",
        "注意: このセルからの出力をスクロールして、プロセス全体を表示します。結果は一番下に表示されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_1_2dB6pdc2"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "temp = temp = os.path.join(tempfile.mkdtemp(),'estimator')\n",
        "\n",
        "transform_data(train_path, test_path, temp)\n",
        "results = train_and_evaluate(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_IqGL90GCIq"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6T3aHoRsjgR"
      },
      "source": [
        " </devsite-expandable></div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "APEUSA9boKgT"
      ],
      "name": "census.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
