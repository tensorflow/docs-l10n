{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24gYiJcWNlpA"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Neural Structured Learning Authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ioaprt5q5US7"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# 合成グラフを使ってセンチメント分類を実施するためのグラフ正則化\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TFハブモデルを参照してください</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3otbdCMmJiJ"
      },
      "source": [
        "## 概要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "このノートブックでは、映画レビューのテキストを使用して、それが*肯定的*であるか*否定的*であるかに分類します。これは*二項*分類の例で、機械学習問題では重要な分類法として広く適用されています。\n",
        "\n",
        "このノートブックでは、特定の入力からグラフを構築することでグラフ正則化の使用方法を示しています。入力に明示的なグラフが含まれない場合に、Neural Structured Learning（NSL）フレームワークを使ってグラフ正則化モデルを構築するための一般的なレシピは以下のとおりです。\n",
        "\n",
        "1. 入力のテキストサンプルに埋め込みを作成します。これは、[word2vec](https://arxiv.org/pdf/1310.4546.pdf)、[Swivel](https://arxiv.org/abs/1602.02215)、[BERT](https://arxiv.org/abs/1810.04805) などの事前トレーニング済みのモデルを使って行えます。\n",
        "2. 'L2' distance、'cosine' distance などの類似度メトリクスを使って、これらの埋め込みに基づくグラフを構築します。グラフ内のノードはサンプルに対応し、グラフ内のエッジはサンプルペア間の類似度に対応します。\n",
        "3. 上記の合成グラフとサンプル特徴量からトレーニングデータを生成します。生成されたトレーニングデータには、元のノード特徴量のほかに、近傍する特徴量が含まれます。\n",
        "4. Keras Sequential API、Functional API、または Subclass API を使用して、基本モデルとしてニューラルネットワークを作成します。\n",
        "5. NSL フレームワークが提供する GraphRegularization ラッパークラスで基本モデルをラップし、新しいグラフ Keras モデルを作成します。この新しいモデルは、トレーニング目的の正則化項にグラフ正則化損失を含みます。\n",
        "6. グラフ Keras モデルをトレーニングして評価します。\n",
        "\n",
        "**注意**: このチュートリアルにかかる時間はおよそ 1 時間を想定しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOFbB34KY1R"
      },
      "source": [
        "## 要件\n",
        "\n",
        "1. Neural Structured Learning パッケージをインストールします。\n",
        "2. tensorflow-hub をインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVnjPmOaQlnH"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet neural-structured-learning\n",
        "!pip install --quiet tensorflow-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6FJ64qMNLez"
      },
      "source": [
        "## 依存関係とインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ew7HTbPpCJH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import neural_structured_learning as nsl\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Resets notebook state\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\n",
        "    \"GPU is\",\n",
        "    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGwwFd99n42P"
      },
      "source": [
        "## IMDB データセット\n",
        "\n",
        "[IMDB データセット](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb)には、[Internet Movie Database](https://www.imdb.com/) から抽出した 50,000 件の映画レビューのテキストが含まれています。これらはトレーニング用とテスト用に 25,000 件ずつに分割されています。トレーニング用とテスト用のセットは*均衡*しています。つまり、これらに含まれる肯定的なレビューと否定的なレビューの件数が同数であるということです。\n",
        "\n",
        "このチュートリアルでは、事前処理済みの IMDB データセットを使用します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "source": [
        "### 事前処理済みの IMDB データセットをダウンロードする\n",
        "\n",
        "IMDB データセットは TensorFlow にパッケージ化されています。事前に処理済みであるため、レビュー（単語のシーケンス）は整数のシーケンスに変換されています。各整数は、ディクショナリ内の特定の単語を表現します。\n",
        "\n",
        "以下のコードを使って IMDB データセットをダウンロードします（または、ダウンロード済みの場合はキャッシュされたコピーを使用します）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXXx5Oc3pOmN"
      },
      "outputs": [],
      "source": [
        "imdb = tf.keras.datasets.imdb\n",
        "(pp_train_data, pp_train_labels), (pp_test_data, pp_test_labels) = (\n",
        "    imdb.load_data(num_words=10000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odr-KlzO-lkL"
      },
      "source": [
        "引数 `num_words=10000` によって、トレーニングデータ内で最も頻繁に出現する単語の上位 10,000 語が保持されます。語彙を管理しやすいサイズに維持するために、まれに出現する単語は破棄されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l50X3GfjpU4r"
      },
      "source": [
        "### データを調べる\n",
        "\n",
        "データの形式を確認してみましょう。データセットは事前処理が行われているため、各サンプルは、映画レビューの単語を表現する整数の配列です。各ラベルは 0 または 1 の整数値で、0 は否定的なレビュー、1 は肯定的なレビューを示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8qCnve_-lkO"
      },
      "outputs": [],
      "source": [
        "print('Training entries: {}, labels: {}'.format(\n",
        "    len(pp_train_data), len(pp_train_labels)))\n",
        "training_samples_count = len(pp_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnKvHWW4-lkW"
      },
      "source": [
        "レビューのテキストは整数に変換済みであり、これらの各整数はディクショナリ内の特定の単語を表現します。以下は、最初のレビューです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtTS4kpEpjbi"
      },
      "outputs": [],
      "source": [
        "print(pp_train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIE4l_72x7DP"
      },
      "source": [
        "映画レビューの長さは異なります。以下のコードでは、1 番目と 2 番目の映画レビューの語数を示します。ニューラルネットワークへの入力は同じ長さである必要があるため、これについて後で解決する必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-6Ii9Pfx6Nr"
      },
      "outputs": [],
      "source": [
        "len(pp_train_data[0]), len(pp_train_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wJg2FiYpuoX"
      },
      "source": [
        "### 整数を単語に変換し直す\n",
        "\n",
        "整数を対応するテキストに変換する方法を知っておくと便利かもしれません。ここでは、整数と文字列のマッピングを含むディクショナリオブジェクトをクエリするヘルパー関数を作成することにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr5s_1alpzop"
      },
      "outputs": [],
      "source": [
        "def build_reverse_word_index():\n",
        "  # A dictionary mapping words to an integer index\n",
        "  word_index = imdb.get_word_index()\n",
        "\n",
        "  # The first indices are reserved\n",
        "  word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "  word_index['<PAD>'] = 0\n",
        "  word_index['<START>'] = 1\n",
        "  word_index['<UNK>'] = 2  # unknown\n",
        "  word_index['<UNUSED>'] = 3\n",
        "  return dict((value, key) for (key, value) in word_index.items())\n",
        "\n",
        "reverse_word_index = build_reverse_word_index()\n",
        "\n",
        "def decode_review(text):\n",
        "  return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3CNRvEZVppl"
      },
      "source": [
        "これで `decode_review` 関数を使用して、最初のレビューのテキストを表示できるようになりました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_OqxmH6-lkn"
      },
      "outputs": [],
      "source": [
        "decode_review(pp_train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVmqL-zcWm8v"
      },
      "source": [
        "## グラフの構築\n",
        "\n",
        "グラフの構築では、テキストサンプル用の埋め込みを作成してから、類似度関数を使って埋め込みの比較が行われます。\n",
        "\n",
        "先に進む前にまず、このチュートリアルで作成されるアーティファクトを保存するためのディクショナリを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZicFxFOeL2J"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /tmp/imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUyHEa-3TB2X"
      },
      "source": [
        "### サンプルの埋め込みを作成する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCe9vOy7-Br9"
      },
      "source": [
        "事前トレーニング済みの Swivel 埋め込みを使用して、入力の各サンプルに使用する埋め込みを `tf.train.Example` 形式で作成します。作成した埋め込みを各サンプルの ID とともに `TFRecord` 形式で保存します。これは重要な手順であり、後でサンプルの埋め込みと対応するノードをグラフで一致させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq2Ohd9CuZv_"
      },
      "outputs": [],
      "source": [
        "pretrained_embedding = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\n",
        "\n",
        "hub_layer = hub.KerasLayer(\n",
        "    pretrained_embedding, input_shape=[], dtype=tf.string, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXJ3RaboTSKQ"
      },
      "outputs": [],
      "source": [
        "def _int64_feature(value):\n",
        "  \"\"\"Returns int64 tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value.tolist()))\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns bytes tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(\n",
        "      bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
        "\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns float tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=value.tolist()))\n",
        "\n",
        "\n",
        "def create_embedding_example(word_vector, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's embedding and its ID.\"\"\"\n",
        "\n",
        "  text = decode_review(word_vector)\n",
        "\n",
        "  # Shape = [batch_size,].\n",
        "  sentence_embedding = hub_layer(tf.reshape(text, shape=[-1,]))\n",
        "\n",
        "  # Flatten the sentence embedding back to 1-D.\n",
        "  sentence_embedding = tf.reshape(sentence_embedding, shape=[-1])\n",
        "\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'embedding': _float_feature(sentence_embedding.numpy())\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "\n",
        "def create_embeddings(word_vectors, output_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(output_path) as writer:\n",
        "    for word_vector in word_vectors:\n",
        "      example = create_embedding_example(word_vector, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "\n",
        "# Persist TF.Example features containing embeddings for training data in\n",
        "# TFRecord format.\n",
        "create_embeddings(pp_train_data, '/tmp/imdb/embeddings.tfr', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8s06RuI_vKs"
      },
      "source": [
        "### グラフを構築する\n",
        "\n",
        "サンプルの埋め込みを作成したので、それを使用して類似度グラフを構築します。つまり、このグラフのノードはサンプルに対応し、エッジはノードペア間の類似度に対応します。\n",
        "\n",
        "Neural Structured Learning にはグラフ構築用のライブラリが備わっており、サンプルの埋め込みに基づいてグラフを作成することができます。類似度の測定として[**コサイン類似度**](https://en.wikipedia.org/wiki/Cosine_similarity)を使用して埋め込みを比較し、それらの間にエッジを作成します。また、類似度のしきい値を指定できるため、それを使用して、類似しないエッジを最終グラフから破棄することができます。次の例では、0.99 を類似度のしきい値として、12345 をランダムシードとして使用し、429,415 個の双方向エッジを持つグラフが構築されています。ここでは、グラフビルダーの[局所性鋭敏型ハッシュ](https://en.wikipedia.org/wiki/Locality-sensitive_hashing)（LSH）サポートを使用して、グラフの構築を高速化しています。グラフビルダーの LSH サポートについては、[`build_graph_from_config`](https://www.tensorflow.org/neural_structured_learning/api_docs/python/nsl/tools/build_graph_from_config) API ドキュメントをご覧ください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY6lqhNkBh2Q"
      },
      "outputs": [],
      "source": [
        "graph_builder_config = nsl.configs.GraphBuilderConfig(\n",
        "    similarity_threshold=0.99, lsh_splits=32, lsh_rounds=15, random_seed=12345)\n",
        "nsl.tools.build_graph_from_config(['/tmp/imdb/embeddings.tfr'],\n",
        "                                  '/tmp/imdb/graph_99.tsv',\n",
        "                                  graph_builder_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dk9xfQcK553"
      },
      "source": [
        "それぞれの双方向エッジは、出力 TSV ファイルの 2 つの有向エッジで表現されているため、ファイルには、合計 429,415 * 2 = 858,830 行が含まれます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDPwTpZcJ3zF"
      },
      "outputs": [],
      "source": [
        "!wc -l /tmp/imdb/graph_99.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06QrEVCIlTvV"
      },
      "source": [
        "**注意:** グラフの品質と、その延長として埋め込みの品質はグラフの正則化において非常に重要です。このノードブックでは Swivel 埋め込みを使用しましたが、たとえば BERT 埋め込みを使用した場合には、レビューのセマンティクスをより正確に捉えられる可能性があります。それぞれのニーズに合ったさまざまな埋め込みを使用することをお勧めします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USkfut69gNW"
      },
      "source": [
        "## サンプル特徴量\n",
        "\n",
        "この問題のサンプル特徴量を `tf.train.Example` 形式で作成し、`TFRecord` 形式で永続化します。各サンプルには、次の 3 つの特徴量が含まれます。\n",
        "\n",
        "1. **id**: サンプルのノード ID。\n",
        "2. **words**: 単語 ID を含む int64 リスト。\n",
        "3. **label**: レビューのターゲットクラスを識別するシングルトン int64。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PcUF4_B9grB"
      },
      "outputs": [],
      "source": [
        "def create_example(word_vector, label, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's word vector, label, and ID.\"\"\"\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'words': _int64_feature(np.asarray(word_vector)),\n",
        "      'label': _int64_feature(np.asarray([label])),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "def create_records(word_vectors, labels, record_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(record_path) as writer:\n",
        "    for word_vector, label in zip(word_vectors, labels):\n",
        "      example = create_example(word_vector, label, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "# Persist TF.Example features (word vectors and labels) for training and test\n",
        "# data in TFRecord format.\n",
        "next_record_id = create_records(pp_train_data, pp_train_labels,\n",
        "                                '/tmp/imdb/train_data.tfr', 0)\n",
        "create_records(pp_test_data, pp_test_labels, '/tmp/imdb/test_data.tfr',\n",
        "               next_record_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhFO9sZ8Aa_g"
      },
      "source": [
        "## グラフ近傍を使ってトレーニングデータを拡張する\n",
        "\n",
        "サンプルの特徴量と合成したグラフがあるため、Neural Structured Learning 用の拡張トレーニングデータを生成することができます。NSL フレームワークにはグラフとサンプル特徴量を合成し、最終的なトレーニングデータを作成してグラフの正則化を得るためのライブラリがあります。作成されたトレーニングデータには元のサンプル特徴量とそれに対応する近傍値が含まれます。\n",
        "\n",
        "このチュートリアルでは、無向エッジを考慮し、サンプルごとに最大 3 つの近傍値を使用して、グラフ近傍でトレーニングデータを拡張します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSCHj4rIBj_A"
      },
      "outputs": [],
      "source": [
        "nsl.tools.pack_nbrs(\n",
        "    '/tmp/imdb/train_data.tfr',\n",
        "    '',\n",
        "    '/tmp/imdb/graph_99.tsv',\n",
        "    '/tmp/imdb/nsl_train_data.tfr',\n",
        "    add_undirected_edges=True,\n",
        "    max_nbrs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzBWdWkBqlMy"
      },
      "source": [
        "## 基本モデル\n",
        "\n",
        "グラフの正則化を行わずに基本モデルを構築する準備が整いました。このモデルを構築するには、グラフを構築するために使用された埋め込みを使用するか、分類タスクと同時に新しい埋め込みを学習することができます。このノートブックの目的により、ここでは後者を行うことにします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSbRFguBUNl"
      },
      "source": [
        "### グローバル変数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsA8HuvvwGri"
      },
      "outputs": [],
      "source": [
        "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
        "NBR_WEIGHT_SUFFIX = '_weight'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8gMVBw6t6CI"
      },
      "source": [
        "### ハイパーパラメータ\n",
        "\n",
        "`HParams` のインスタンスを使用して、トレーニングと評価に使用する様々なハイパーパラメータと定数をインクルードします。それぞれについての簡単な説明を以下に示します。\n",
        "\n",
        "- **num_classes**: *positive* と *negative* の 2 つのクラスがあります。\n",
        "\n",
        "- **max_seq_length**: これはこの例のそれぞれの映画レビューから考慮される単語の最大数です。\n",
        "\n",
        "- **vocab_size**: これは、この例で考慮される語彙のサイズです。\n",
        "\n",
        "- **distance_type**：これはサンプルをその近傍と正則化する際に使用する距離メトリックです。\n",
        "\n",
        "- **graph_regularization_multiplier**：これは全体の損失関数においてグラフ正則化項の相対的な重みを制御します。\n",
        "\n",
        "- **num_neighbors**: グラフ正則化に使用する近傍の数です。この値は、`nsl.tools.pack_nbrs` を呼び出す際に上記で使用した `max_nbrs` 以下である必要があります。\n",
        "\n",
        "- **num_fc_units**: ニューラルネットワークの全結合レイヤーのユニット数。\n",
        "\n",
        "- **train_epochs**：トレーニングのエポック数。\n",
        "\n",
        "- **<code>batch_size</code>**: トレーニングや評価に使用するバッチサイズ。\n",
        "\n",
        "- **eval_steps**：評価が完了したと判断するまでに処理を行うバッチ数。`None` 設定にすると、テストセット内の全てのインスタンスを評価します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlTmug7auQ2r"
      },
      "outputs": [],
      "source": [
        "class HParams(object):\n",
        "  \"\"\"Hyperparameters used for training.\"\"\"\n",
        "  def __init__(self):\n",
        "    ### dataset parameters\n",
        "    self.num_classes = 2\n",
        "    self.max_seq_length = 256\n",
        "    self.vocab_size = 10000\n",
        "    ### neural graph learning parameters\n",
        "    self.distance_type = nsl.configs.DistanceType.L2\n",
        "    self.graph_regularization_multiplier = 0.1\n",
        "    self.num_neighbors = 2\n",
        "    ### model architecture\n",
        "    self.num_embedding_dims = 16\n",
        "    self.num_lstm_dims = 64\n",
        "    self.num_fc_units = 64\n",
        "    ### training parameters\n",
        "    self.train_epochs = 10\n",
        "    self.batch_size = 128\n",
        "    ### eval parameters\n",
        "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
        "\n",
        "HPARAMS = HParams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFP_XKVRp4_S"
      },
      "source": [
        "### データを準備する\n",
        "\n",
        "整数の配列で表現されたレビューをニューラルネットワークにフィードする前にテンソルに変換する必要があります。この変換は、以下の 2 つの方法で行われます。\n",
        "\n",
        "- 配列を、ワンホットエンコーディングと同様に、単語の出現を示す `0` と `1` のベクトルに変換します。たとえば、シーケンス `[3, 5]` は、1 を示す `3` と `5` を除き、すべてゼロの `10000` 次元のベクトルになります。次に、これをネットワークの最初のレイヤーである、浮動小数点のベクトルデータを処理できる `Dense` レイヤーにします。ただし、このアプローチはメモリを集中的に使用するため、`num_words * num_reviews`     サイズの行列が必要です。\n",
        "\n",
        "- または、配列の長さが同じになるように配列にパディングを行い、形状 `max_length * num_reviews` の整数テンソルを作成することができます。この形状を処理できる埋め込みレイヤーをネットワークの最初のレイヤーとして使用します。\n",
        "\n",
        "このチュートリアルでは、後者のアプローチを使用します。\n",
        "\n",
        "映画レビューの長さは同じである必要があるため、以下に定義される `pad_sequence` 関数を使用して、長さを標準化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5lkZVynuHWs"
      },
      "outputs": [],
      "source": [
        "def make_dataset(file_path, training=False):\n",
        "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
        "\n",
        "  Args:\n",
        "    file_path: Name of the file in the `.tfrecord` format containing\n",
        "      `tf.train.Example` objects.\n",
        "    training: Boolean indicating if we are in training mode.\n",
        "\n",
        "  Returns:\n",
        "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
        "    objects.\n",
        "  \"\"\"\n",
        "\n",
        "  def pad_sequence(sequence, max_seq_length):\n",
        "    \"\"\"Pads the input sequence (a `tf.SparseTensor`) to `max_seq_length`.\"\"\"\n",
        "    pad_size = tf.maximum([0], max_seq_length - tf.shape(sequence)[0])\n",
        "    padded = tf.concat(\n",
        "        [sequence.values,\n",
        "         tf.fill((pad_size), tf.cast(0, sequence.dtype))],\n",
        "        axis=0)\n",
        "    # The input sequence may be larger than max_seq_length. Truncate down if\n",
        "    # necessary.\n",
        "    return tf.slice(padded, [0], [max_seq_length])\n",
        "\n",
        "  def parse_example(example_proto):\n",
        "    \"\"\"Extracts relevant fields from the `example_proto`.\n",
        "\n",
        "    Args:\n",
        "      example_proto: An instance of `tf.train.Example`.\n",
        "\n",
        "    Returns:\n",
        "      A pair whose first value is a dictionary containing relevant features\n",
        "      and whose second value contains the ground truth labels.\n",
        "    \"\"\"\n",
        "    # The 'words' feature is a variable length word ID vector.\n",
        "    feature_spec = {\n",
        "        'words': tf.io.VarLenFeature(tf.int64),\n",
        "        'label': tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
        "    }\n",
        "    # We also extract corresponding neighbor features in a similar manner to\n",
        "    # the features above during training.\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i,\n",
        "                                         NBR_WEIGHT_SUFFIX)\n",
        "        feature_spec[nbr_feature_key] = tf.io.VarLenFeature(tf.int64)\n",
        "\n",
        "        # We assign a default value of 0.0 for the neighbor weight so that\n",
        "        # graph regularization is done on samples based on their exact number\n",
        "        # of neighbors. In other words, non-existent neighbors are discounted.\n",
        "        feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
        "            [1], tf.float32, default_value=tf.constant([0.0]))\n",
        "\n",
        "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "\n",
        "    # Since the 'words' feature is a variable length word vector, we pad it to a\n",
        "    # constant maximum length based on HPARAMS.max_seq_length\n",
        "    features['words'] = pad_sequence(features['words'], HPARAMS.max_seq_length)\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        features[nbr_feature_key] = pad_sequence(features[nbr_feature_key],\n",
        "                                                 HPARAMS.max_seq_length)\n",
        "\n",
        "    labels = features.pop('label')\n",
        "    return features, labels\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset([file_path])\n",
        "  if training:\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.map(parse_example)\n",
        "  dataset = dataset.batch(HPARAMS.batch_size)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = make_dataset('/tmp/imdb/nsl_train_data.tfr', True)\n",
        "test_dataset = make_dataset('/tmp/imdb/test_data.tfr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "### モデルを構築する\n",
        "\n",
        "ニューラルネットワークは、レイヤーをスタックして作成されており、これには、2 つの主なアーキテクチャ上の決定が必要です。\n",
        "\n",
        "- モデルにはいくつのレイヤーを使用するか。\n",
        "- 各レイヤーにはいくつの*非表示ユニット*を使用するか。\n",
        "\n",
        "この例では、入力データは単語のインデックスの配列で構成されています。予測するラベルは 0 または 1 です。\n",
        "\n",
        "このチュートリアルでは、基本モデルとして双方向 LSTM を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpKOoWgu-llD"
      },
      "outputs": [],
      "source": [
        "# This function exists as an alternative to the bi-LSTM model used in this\n",
        "# notebook.\n",
        "def make_feed_forward_model():\n",
        "  \"\"\"Builds a simple 2 layer feed forward neural network.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "  embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size, 16)(inputs)\n",
        "  pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
        "  dense_layer = tf.keras.layers.Dense(16, activation='relu')(pooling_layer)\n",
        "  outputs = tf.keras.layers.Dense(1)(dense_layer)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "def make_bilstm_model():\n",
        "  \"\"\"Builds a bi-directional LSTM model.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "  embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size,\n",
        "                                              HPARAMS.num_embedding_dims)(\n",
        "                                                  inputs)\n",
        "  lstm_layer = tf.keras.layers.Bidirectional(\n",
        "      tf.keras.layers.LSTM(HPARAMS.num_lstm_dims))(\n",
        "          embedding_layer)\n",
        "  dense_layer = tf.keras.layers.Dense(\n",
        "      HPARAMS.num_fc_units, activation='relu')(\n",
        "          lstm_layer)\n",
        "  outputs = tf.keras.layers.Dense(1)(dense_layer)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "# Feel free to use an architecture of your choice.\n",
        "model = make_bilstm_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "レイヤーは分類器を構成するため効果的に一列に積み重ねられます。\n",
        "\n",
        "1. 最初のレイヤーは、整数でエンコーディングされた語彙を取る `Input` レイヤーです。\n",
        "2. 次のレイヤーは、整数でエンコーディングされた語彙を受け取って、埋め込みベクトルで各単語インデックスをルックアップする `Embedding` レイヤーです。これらのベクトルはモデルのトレーニングの過程で学習されます。ベクトルは出力配列に次元を追加します。生成される次元は、`(batch, sequence, embedding)` です。\n",
        "3. 次に、双方向 LSTM レイヤーがサンプルごとに固定長の出力ベクトルを返します。\n",
        "4. この固定長の出力ベクトルは、64 個の非表示ユニットを持つ全結合（`Dense`）レイヤーに受け渡されます。\n",
        "5. 最後のレイヤーは、単一の出力ノードに密に接続されています。`sigmoid` 活性化関数を使用し、この値は、確率または信頼水準を表す 0 と 1 の間の浮動小数となります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XMwnDOp-llH"
      },
      "source": [
        "### 非表示ユニット\n",
        "\n",
        "上記のモデルには、`Embedding` を除き、入力と出力の間に 2 つの中間または「非表示」レイヤーがあります。出力数（ユニット、ノード、またはニューロン）はレイヤーの表現空間の次元で、言い換えると、内部表現を学習する際にネットワークに許可された自由度です。\n",
        "\n",
        "モデルにより大きい非表示ユニット数（より高次元の表現空間）がある場合や、レイヤー数が増えるほど、ネットワークはよく複雑な表現を学習できますが、ネットワークの計算コストが高まり、不要なパターンが学習される可能性があります。これらのパターンはトレーニングデータのパフォーマンスを改善しても、テストデータのパフォーマンスは改善しません。この現象は「*過適合*」と呼ばれています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### 損失関数とオプティマイザ\n",
        "\n",
        "モデルをトレーニングするには、損失関数とオプティマイザが必要です。これは二項分類問題であり、モデルは確率（シグモイド活性を持つ単一ユニットレイヤー）を出力するため、`binary_crossentropy` 損失関数を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCWYwkug-llQ"
      },
      "source": [
        "### 検証セットを作成する\n",
        "\n",
        "トレーニングの際に、モデルが遭遇したことのないデータで、モデルの精度を確認します。この場合、元のトレーニングデータの一部を分割し、*検証セット*を作成します。（ここでテストセットを使用しないのは、トレーニングデータのみを使用してモデルの開発とチューニングを行い、その後でテストデータを一度だけ使用して精度を評価することが目標であるためです）。\n",
        "\n",
        "このチュートリアルでは、最初のトレーニングサンプルのおよそ 10%（25000 の 10%）をトレーニングのラベル付きデータとして取り、残りを検証データとしています。最初のトレーニングデータとテストデータの割合は 50:50（それぞれ 25000 個のサンプル）であったため、実際のトレーニング/検証/テストの分割率は、5:45:50 です。\n",
        "\n",
        "'train_dataset' にはすでにバッチ化とシャッフルが行われいます。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYTf7zkZQ-Dl"
      },
      "outputs": [],
      "source": [
        "validation_fraction = 0.9\n",
        "validation_size = int(validation_fraction *\n",
        "                      int(training_samples_count / HPARAMS.batch_size))\n",
        "print(validation_size)\n",
        "validation_dataset = train_dataset.take(validation_size)\n",
        "train_dataset = train_dataset.skip(validation_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "### モデルをトレーニングする\n",
        "\n",
        "モデルをミニバッチでトレーニングします。トレーニング中に、検証セットでのモデルの損失と精度を監視します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLWzgfF1xpDu"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "### モデルを評価する\n",
        "\n",
        "モデルがどのように実行するかを確認しましょう。損失（誤差を表す数値で、低いほど良です）と精度の 2 つの値が返されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q7CoDfoCJ5h"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KggXVeL-llZ"
      },
      "source": [
        "### 精度と損失の経時的なグラフを作成する\n",
        "\n",
        "`model.fit()` は、トレーニング中に発生したすべての情報を収めたディクショナリを含む `History` オブジェクトを返します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcvSXvhp-llb"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRKsqL40-lle"
      },
      "source": [
        "トレーニングと検証中に監視されている各メトリックに対して 1 つずつ、計 4 つのエントリがあります。このエントリを使用して、トレーニングと検証の損失とトレーニングと検証の精度を比較したグラフを作成することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGoYf2Js-lle"
      },
      "outputs": [],
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hXx-xOv-llh"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEmZ5zq-llk"
      },
      "source": [
        "トレーニングの損失がエポックごとに*下降*し、トレーニングの精度がエポックごとに*上昇*していることに注目してください。これは、勾配下降最適化を使用しているときに見られる現象で、イテレーションごとに希望する量を最小化します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SymtYWWiMUum"
      },
      "source": [
        "## グラフの正則化\n",
        "\n",
        "上記で構築した基本モデルを使用して、グラフの正則化を試す準備が整いました。Neural Structured Learning フレームワークが提供する `GraphRegularization` ラッパークラスを使用して基本（bi-LSTM）モデルをラップし、グラフの正則化を含めます。グラフ正則化のトレーニングと評価の残りのステップは、基本モデルのトレーニングと評価と同じです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCIkVe_QFX38"
      },
      "source": [
        "### グラフ正則化モデルを作成する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIGN8KQH0jR"
      },
      "source": [
        "グラフ正則化の増分効果を評価するために、基本モデルの新しいインスタンスを作成します。これは、`model` がすでに数回のイテレーションでトレーニングされており、このトレーニング済みのモデルを再利用してグラフ正則化モデルを作成しても、`model`の公平な比較にならないためです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOEElnbtPzSr"
      },
      "outputs": [],
      "source": [
        "# Build a new base LSTM model.\n",
        "base_reg_model = make_bilstm_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGaDeyjEOMLC"
      },
      "outputs": [],
      "source": [
        "# Wrap the base model with graph regularization.\n",
        "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
        "    max_neighbors=HPARAMS.num_neighbors,\n",
        "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
        "    distance_type=HPARAMS.distance_type,\n",
        "    sum_over_axis=-1)\n",
        "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
        "                                                graph_reg_config)\n",
        "graph_reg_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSZSqJOKFdgX"
      },
      "source": [
        "### モデルをトレーニングする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aONZhwc9FWoo"
      },
      "outputs": [],
      "source": [
        "graph_reg_history = graph_reg_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD1oHiGHFjPB"
      },
      "source": [
        "### モデルを評価する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdFMEfe2e5JY"
      },
      "outputs": [],
      "source": [
        "graph_reg_results = graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(graph_reg_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BshURAbF49R"
      },
      "source": [
        "### 精度と損失の経時的なグラフを作成する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHxshrYLah9v"
      },
      "outputs": [],
      "source": [
        "graph_reg_history_dict = graph_reg_history.history\n",
        "graph_reg_history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBrp0Y0jHu5k"
      },
      "source": [
        "ディクショナリには、トレーニング損失、トレーニング精度、トレーニンググラフ損失、検証損失、および検証精度の 5 つのエントリがあります。これらをまとめてプロットし、比較に使用することができます。グラフ損失はトレーニング中にのみ計算されることに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhjhH4n_aprb"
      },
      "outputs": [],
      "source": [
        "acc = graph_reg_history_dict['accuracy']\n",
        "val_acc = graph_reg_history_dict['val_accuracy']\n",
        "loss = graph_reg_history_dict['loss']\n",
        "graph_loss = graph_reg_history_dict['scaled_graph_loss']\n",
        "val_loss = graph_reg_history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.clf()   # clear figure\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-gD\" is for solid green line with diamond markers.\n",
        "plt.plot(epochs, graph_loss, '-gD', label='Training graph loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE0vcDiqa1Id"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su1TOgT3mgrk"
      },
      "source": [
        "## 半教師あり学習の性能\n",
        "\n",
        "半教師あり学習、さらに具体的に言えば、このチュートリアルの文脈でのグラフ正則化は、トレーニングデータの量が少ない場合に非常に強力です。トレーニングデータの不足分は、トレーニングサンプル間の類似度を利用して補完されます。これは、従来の教師あり学習では実現できません。\n",
        "\n",
        "***supervision ratio***（教師率）を、トレーニング、検証、およびテストサンプルを含むサンプル総数に対するトレーニングサンプルの比率として定義します。このノートブックでは、基本モデルとグラフ正則化モデルの両方のトレーニングに 0.05 の教師率（ラベル付きデータの 5%）を使用しました。教師率がモデルの精度に与える影響を以下のセルで説明します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWWa384R5vSm"
      },
      "outputs": [],
      "source": [
        "# Accuracy values for both the Bi-LSTM model and the feed forward NN model have\n",
        "# been precomputed for the following supervision ratios.\n",
        "\n",
        "supervision_ratios = [0.3, 0.15, 0.05, 0.03, 0.02, 0.01, 0.005]\n",
        "\n",
        "model_tags = ['Bi-LSTM model', 'Feed Forward NN model']\n",
        "base_model_accs = [[84, 84, 83, 80, 65, 52, 50], [87, 86, 76, 74, 67, 52, 51]]\n",
        "graph_reg_model_accs = [[84, 84, 83, 83, 65, 63, 50],\n",
        "                        [87, 86, 80, 75, 67, 52, 50]]\n",
        "\n",
        "plt.clf()  # clear figure\n",
        "\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "fig.set_size_inches((12, 5))\n",
        "\n",
        "for ax, model_tag, base_model_acc, graph_reg_model_acc in zip(\n",
        "    axes, model_tags, base_model_accs, graph_reg_model_accs):\n",
        "\n",
        "  # \"-r^\" is for solid red line with triangle markers.\n",
        "  ax.plot(base_model_acc, '-r^', label='Base model')\n",
        "  # \"-gD\" is for solid green line with diamond markers.\n",
        "  ax.plot(graph_reg_model_acc, '-gD', label='Graph-regularized model')\n",
        "  ax.set_title(model_tag)\n",
        "  ax.set_xlabel('Supervision ratio')\n",
        "  ax.set_ylabel('Accuracy(%)')\n",
        "  ax.set_ylim((25, 100))\n",
        "  ax.set_xticks(range(len(supervision_ratios)))\n",
        "  ax.set_xticklabels(supervision_ratios)\n",
        "  ax.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tukoIryKugX_"
      },
      "source": [
        "教師率が低下すると、モデルの精度も低下することが観測できます。これは、使用されているモデルのアーキテクチャに関係なく、基本モデルとグラフ正則化モデルの両方に当てはまることです。ただし、両方のアーキテクチャにおいて、グラフ正則化モデルのパフォーマンスが基本モデルよりも高いことに注目してください。特に Bi-LSTM モデルの場合、教師率が 0.01 の時のグラフ正則化モデルの精度は、基本モデルの精度よりも **約 20%** 高くなっています。これは主に、グラフ正則化モデルの半教師あり学習において、トレーニングサンプルのほかにトレーニングサンプル間の構造的類似度が使用されたためです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4zCEyPhIp-"
      },
      "source": [
        "## 結論\n",
        "\n",
        "入力に明示的なグラフが含まれていない場合でも、Newral Structured Leaning（NSL）フレームワークを使用したグラフ正則化の使用を説明しました。IMDB 映画レビューのセンチメント分類タスクを考察する上で、レビューの埋め込みに基づく類似度グラフを合成しました。ハイパーパラメータや教師率を変化させ、異なるモデルアーキテクチャを使用することで、さらに実験することをお勧めします。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "24gYiJcWNlpA"
      ],
      "name": "graph_keras_lstm_imdb.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
