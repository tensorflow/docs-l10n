{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTFj8ft5dlbS"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lzyBOpYMdp3F"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m_x4KfSJ7Vt7"
      },
      "outputs": [],
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9HmC2T4ld5B"
      },
      "source": [
        "# 過学習と学習不足について知る"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRTxFhXAlnl1"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19rPukKZsPG6"
      },
      "source": [
        "いつものように、この例のプログラムは `tf.keras` APIを使用します。詳しくは TensorFlow の [Keras ガイド](https://www.tensorflow.org/guide/keras)を参照してください。\n",
        "\n",
        "これまでの例、つまり、映画レビューの分類と燃費の推定では、検証用データでのモデルの精度が、数エポックでピークを迎え、その後低下するという現象が見られました。\n",
        "\n",
        "言い換えると、モデルがトレーニング用データを<strong>過学習</strong>したと考えられます。過学習への対処の仕方を学ぶことは重要です。<strong>トレーニング用データセット</strong>で高い精度を達成することは難しくありませんが、（これまで見たこともない）<strong>テスト用データ</strong>に汎化したモデルを開発したいのです。\n",
        "\n",
        "過学習の反対語は**学習不足**（underfitting）です。学習不足は、モデルがテストデータに対してまだ改善の余地がある場合に発生します。学習不足の原因は様々です。モデルが十分強力でないとか、正則化のしすぎだとか、単にトレーニング時間が短すぎるといった理由があります。学習不足は、トレーニング用データの中の関連したパターンを学習しきっていないということを意味します。\n",
        "\n",
        "モデルのトレーニングをやりすぎると、モデルは過学習を始め、トレーニング用データの中のパターンで、テストデータには一般的ではないパターンを学習します。そのため、過学習と学習不足の中間を目指す必要があります。これから見ていくように、ちょうどよいエポック数だけトレーニングを行う必要があります。\n",
        "\n",
        "過学習を防止するための、最良の解決策は、より多くのトレーニング用データを使うことです。データセットには、モデルが処理するあらゆる入力が含まれる必要があります。追加のデータは、新しく興味深いケースに対応する場合にのみ役立ちます。\n",
        "\n",
        "多くのデータでトレーニングを行えば行うほど、当然のことながらモデルの汎化​性能が高くなります。これが不可能な場合、次善の策は正則化のようなテクニックを使うことです。正則化は、モデルに保存される情報の量とタイプに制約をを課します。ネットワークが少数のパターンしか記憶できない場合、最適化プロセスにより、最も顕著なパターンに焦点を合わせるように強制されます。これにより、汎化​性能が高くなる可能性があります。\n",
        "\n",
        "このノートブックでは、いくつかの一般的な正則化手法を使用して分類モデルを改善する方法を見ていきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL8UoOTmGGsL"
      },
      "source": [
        "## セットアップ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FklhSI0Gg9R"
      },
      "source": [
        "まず、必要なパッケージをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pZ8A2liqvgk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnAtAjqRYVXe"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/tensorflow/docs\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.modeling\n",
        "import tensorflow_docs.plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pnOU-ctX27Q"
      },
      "outputs": [],
      "source": [
        "from  IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pathlib\n",
        "import shutil\n",
        "import tempfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj6I4dvTtbUe"
      },
      "outputs": [],
      "source": [
        "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
        "shutil.rmtree(logdir, ignore_errors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cweoTiruj8O"
      },
      "source": [
        "## Higgs データセット\n",
        "\n",
        "このチュートリアルの目的は素粒子物理学を行うことではないので、データセットの詳細にこだわる必要はありませんが、これには 11,000,000 のサンプルが含まれていて、各サンプルには 28 の特徴量とバイナリクラスラベルがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPjAvwb-6dFd"
      },
      "outputs": [],
      "source": [
        "gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'http://mlphysics.ics.uci.edu/data/higgs/HIGGS.csv.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkiyUdaWIrww"
      },
      "outputs": [],
      "source": [
        "FEATURES = 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFggl9gYKKRJ"
      },
      "source": [
        "`tf.data.experimental.CsvDataset` クラスを使用すると、中間の解凍手順なしで、gzip ファイルから直接 csv レコードを読み取ることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHz4sLVQEVIU"
      },
      "outputs": [],
      "source": [
        "ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzahEELTKlSV"
      },
      "source": [
        "csv リーダークラスは、各レコードのスカラーのリストを返します。次の関数は、そのスカラーのリストを (feature_vector, label) ペアに再度パックします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPD6ICDlF6Wf"
      },
      "outputs": [],
      "source": [
        "def pack_row(*row):\n",
        "  label = row[0]\n",
        "  features = tf.stack(row[1:],1)\n",
        "  return features, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oa8tLuwLsbO"
      },
      "source": [
        "TensorFlow は、大規模なバッチのデータで演算する場合に最も効率的です。\n",
        "\n",
        "したがって、各行を個別に再パックする代わりに、10,000 サンプルのバッチを取得する新しい `Dataset` を作成し、`pack_row` 関数を各バッチに適用してから、バッチを個々のレコードに分割します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w-VHTwwGVoZ"
      },
      "outputs": [],
      "source": [
        "packed_ds = ds.batch(10000).map(pack_row).unbatch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUbxc5bxNSXV"
      },
      "source": [
        "この新しい `packed_ds` のレコードのいくつかを確認します。\n",
        "\n",
        "特徴は完全に正規化されていませんが、このチュートリアルには十分です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfcXuv33Fvka"
      },
      "outputs": [],
      "source": [
        "for features,label in packed_ds.batch(1000).take(1):\n",
        "  print(features[0])\n",
        "  plt.hist(features.numpy().flatten(), bins = 101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICKZRY7gN-QM"
      },
      "source": [
        "この短いチュートリアルでは、検証に最初の 1,000 サンプルのみを使用し、トレーニングに次の 10,000 サンプルを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmk49OqZIFZP"
      },
      "outputs": [],
      "source": [
        "N_VALIDATION = int(1e3)\n",
        "N_TRAIN = int(1e4)\n",
        "BUFFER_SIZE = int(1e4)\n",
        "BATCH_SIZE = 500\n",
        "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP3M9DmvON32"
      },
      "source": [
        "`Dataset.skip` と `Dataset.take` メソッドを使うと簡単に実行できます。\n",
        "\n",
        "また、`Dataset.cache` メソッドを使用して、ローダーが各エポックでファイルからデータを再読み取りする必要がないようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8H_ZzpBOOk-"
      },
      "outputs": [],
      "source": [
        "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
        "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zAOqk2_Px7K"
      },
      "outputs": [],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PMliHoVO3OL"
      },
      "source": [
        "これらのデータセットは、個々のサンプルを返します。`Dataset.batch` メソッドを使用して、トレーニングに適したサイズのバッチを作成します。バッチ処理する前に、トレーニングセットを `Dataset.shuffle` および `Dataset.repeat` することも忘れないでください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7I4J355O223"
      },
      "outputs": [],
      "source": [
        "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lglk41MwvU5o"
      },
      "source": [
        "## 過学習のデモ\n",
        "\n",
        "過学習を防止するための最も単純な方法は、モデルのサイズ、すなわち、モデル内の学習可能なパラメータの数を小さくすることです（学習パラメータの数は、レイヤーの数とレイヤーごとのユニット数で決まります）。ディープラーニングでは、モデルの学習可能なパラメータ数を、しばしばモデルの「容量」と呼びます。\n",
        "\n",
        "直感的に考えれば、パラメータ数の多いモデルほど「記憶容量」が大きくなり、トレーニング用のサンプルとその目的変数の間のディクショナリのようなマッピングをたやすく学習することができます。このマッピングには汎化能力がまったくなく、これまで見たことがないデータを使って予測をする際には役に立ちません。\n",
        "\n",
        "ディープラーニングのモデルはトレーニング用データに適応しやすいけれど、本当のチャレレンジは汎化であって適応ではありません。\n",
        "\n",
        "一方、ネットワークの記憶容量が限られている場合、前述のようなマッピングを簡単に学習することはできません。損失を減らすためには、より予測能力が高い圧縮された表現を学習しなければなりません。同時に、モデルを小さくしすぎると、トレーニング用データに適応するのが難しくなります。「多すぎる容量」と「容量不足」の間にちょうどよい容量があるのです。\n",
        "\n",
        "残念ながら、（レイヤーの数や、レイヤーごとの大きさといった）モデルの適切なサイズやアーキテクチャを決める魔法の方程式はありません。一連の異なるアーキテクチャを使って実験を行う必要があります。\n",
        "\n",
        "適切なモデルのサイズを見つけるには、比較的少ないレイヤーの数とパラメータから始めるのがベストです。それから、検証用データでの損失値の改善が見られなくなるまで、徐々にレイヤーの大きさを増やしたり、新たなレイヤーを加えたりします。\n",
        "\n",
        "比較基準として、密に接続されたレイヤー（`tf.keras.layers.Dense`）だけを使ったシンプルなモデルを構築し、その後、大規模なバージョンを作って比較します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ReKHdC2EgVu"
      },
      "source": [
        "### 比較基準を作る"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNzkSkkXSP5l"
      },
      "source": [
        "トレーニング中に学習率を徐々に下げると、多くのモデルのトレーニングが向上します。`tf.keras.optimizers.schedules` を使用して、時間の経過とともに学習率を下げます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwQp-ERhAD6F"
      },
      "outputs": [],
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "  0.001,\n",
        "  decay_steps=STEPS_PER_EPOCH*1000,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "\n",
        "def get_optimizer():\n",
        "  return tf.keras.optimizers.Adam(lr_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kANLx6OYTQ8B"
      },
      "source": [
        "上記のコードは、`tf.keras.optimizers.schedules.InverseTimeDecay` を設定し、学習率を 1000 エポックで基本率の 1/2 に、2000 エポックで 1/3 に双曲線的に減少させます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIo_yPjEAFgn"
      },
      "outputs": [],
      "source": [
        "step = np.linspace(0,100000)\n",
        "lr = lr_schedule(step)\n",
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "_ = plt.ylabel('Learning Rate')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya7x7gr9UjU0"
      },
      "source": [
        "このチュートリアルの各モデルは、同じトレーニング構成を使用します。したがって、コールバックのリストから始めて、再利用可能な方法でこれらを設定します。\n",
        "\n",
        "このチュートリアルのトレーニングは、多くの短いエポックで実行されます。不要なログ情報を減らすためには、`tfdocs.EpochDots` を使用します。これは、エポックごとに `.` を出力し、100 エポックごとにメトリックのフルセットを出力します。\n",
        "\n",
        "次に、`tf.keras.callbacks.EarlyStopping` を含めて、トレーニング時間が不必要に長くならないようにます。このコールバックは、`val_loss` ではなく、`val_binary_crossentropy` を監視するように設定されていることに注意してください。この違いは後で重要になります。\n",
        "\n",
        "`callbacks.TensorBoard` を使用して、トレーニング用の TensorBoard ログを生成します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSv8rfw_T85n"
      },
      "outputs": [],
      "source": [
        "def get_callbacks(name):\n",
        "  return [\n",
        "    tfdocs.modeling.EpochDots(),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
        "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhctzKhBWVDD"
      },
      "source": [
        "同様に、各モデルは同じ `Model.compile` および `Model.fit` 設定を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRCGwU3YH5sT"
      },
      "outputs": [],
      "source": [
        "def compile_and_fit(model, name, optimizer=None, max_epochs=10000):\n",
        "  if optimizer is None:\n",
        "    optimizer = get_optimizer()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                metrics=[\n",
        "                  tf.keras.losses.BinaryCrossentropy(\n",
        "                      from_logits=True, name='binary_crossentropy'),\n",
        "                  'accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "    train_ds,\n",
        "    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "    epochs=max_epochs,\n",
        "    validation_data=validate_ds,\n",
        "    callbacks=get_callbacks(name),\n",
        "    verbose=0)\n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxBeiLUiWHJV"
      },
      "source": [
        "### 非常に小規模のモデル（Tiny）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6JDv12scLTI"
      },
      "source": [
        "まず、モデルをトレーニングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZh-QFjKHb70"
      },
      "outputs": [],
      "source": [
        "tiny_model = tf.keras.Sequential([\n",
        "    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X72IUdWYipIS"
      },
      "outputs": [],
      "source": [
        "size_histories = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdOcJtPGHhJ5"
      },
      "outputs": [],
      "source": [
        "size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS_QGT6icwdI"
      },
      "source": [
        "次に、モデルがどのように機能したかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkEvb2x5XsjE"
      },
      "outputs": [],
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGxGzh_FWOJ8"
      },
      "source": [
        "### 小規模のモデル（Small）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjMb6E72f2pN"
      },
      "source": [
        "小規模なモデルのパフォーマンスを上回ることができるかどうかを確認するには、いくつかの大規模なモデルを段階的にトレーニングします。\n",
        "\n",
        "隠れレイヤーが 2 つ、 1 つのレイヤー内のユニットが 16 あるモデルを構築します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKgdXPx9usBa"
      },
      "outputs": [],
      "source": [
        "baseline_model = keras.Sequential([\n",
        "    # `.summary` を見るために`input_shape`が必要 \n",
        "    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "baseline_model.compile(optimizer='adam',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['accuracy', 'binary_crossentropy'])\n",
        "\n",
        "baseline_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqG3MXF5xSjR"
      },
      "outputs": [],
      "source": [
        "baseline_history = baseline_model.fit(train_data,\n",
        "                                      train_labels,\n",
        "                                      epochs=20,\n",
        "                                      batch_size=512,\n",
        "                                      validation_data=(test_data, test_labels),\n",
        "                                      verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-DGRBbGxI6G"
      },
      "source": [
        "### 中規模のモデル（Medium）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrfoVQheYSO5"
      },
      "source": [
        "次に、隠れレイヤーが 3 つ、 1 つのレイヤー内のユニットが​ 64 あるモデルを構築します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jksi-XtaxDAh"
      },
      "outputs": [],
      "source": [
        "smaller_model = keras.Sequential([\n",
        "    keras.layers.Dense(4, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(4, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "smaller_model.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy', 'binary_crossentropy'])\n",
        "\n",
        "smaller_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbngCZliYdma"
      },
      "source": [
        "同じデータを使って訓練します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofn1AwDhx-Fe"
      },
      "outputs": [],
      "source": [
        "smaller_history = smaller_model.fit(train_data,\n",
        "                                    train_labels,\n",
        "                                    epochs=20,\n",
        "                                    batch_size=512,\n",
        "                                    validation_data=(test_data, test_labels),\n",
        "                                    verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIPuf23FFaVn"
      },
      "source": [
        "### 大規模のモデル（Large）\n",
        "\n",
        "演習として、より大規模なモデルを作成し、それがどれだけ迅速に過適合し始めるかを確認してみましょう。次に、このベンチマークに、ここで必要とされる容量を大幅に上回るネットワークを追加します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghQwwqwqvQM9"
      },
      "outputs": [],
      "source": [
        "bigger_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(512, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "bigger_model.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy','binary_crossentropy'])\n",
        "\n",
        "bigger_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-d-i5DaYmr7"
      },
      "source": [
        "このモデルもまた同じデータを使って訓練します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1A99dhqvepf"
      },
      "outputs": [],
      "source": [
        "bigger_history = bigger_model.fit(train_data, train_labels,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=512,\n",
        "                                  validation_data=(test_data, test_labels),\n",
        "                                  verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy3CMUZpzH3d"
      },
      "source": [
        "### 訓練時と検証時の損失をグラフにする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSlo1F4xHuuM"
      },
      "source": [
        "実線はトレーニング用データセットの損失、破線は検証用データセットでの損失です（検証用データでの損失が小さい方が良いモデルです）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLhL1AszdLfM"
      },
      "source": [
        "より大きなモデルを構築すると、より多くのパワーが得られますが、このパワーが何らかの形で制約されていない場合、トレーニングセットに簡単に過適合する可能性があります。\n",
        "\n",
        "この例では、通常、`\"Tiny\"` モデルのみが過適合を完全に回避し、より大規模なモデルはデータをより迅速に過適合します。過適合は、`\"large\"` モデルでは非常に深刻になるため、実際に何が起こっているかを確認するには、プロットを対数スケールに切り替える必要があります。\n",
        "\n",
        "これは、検証メトリックをトレーニングメトリックとプロットして比較すると明らかです。\n",
        "\n",
        "- わずかな違いがあるのは正常です。\n",
        "- 両方のメトリックが同じ方向に移動している場合、すべて正常です。\n",
        "- トレーニングメトリックが改善し続けているのに検証メトリックが停滞し始めた場合は、おそらく過適合に近づいています。\n",
        "- 検証メトリックが反対方向に進んでいる場合、モデルは明らかに過適合しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XmKDtOWzOpk"
      },
      "outputs": [],
      "source": [
        "def plot_history(histories, key='binary_crossentropy'):\n",
        "  plt.figure(figsize=(16,10))\n",
        "    \n",
        "  for name, history in histories:\n",
        "    val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                   '--', label=name.title()+' Val')\n",
        "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "             label=name.title()+' Train')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(key.replace('_',' ').title())\n",
        "  plt.legend()\n",
        "\n",
        "  plt.xlim([0,max(history.epoch)])\n",
        "\n",
        "\n",
        "plot_history([('baseline', baseline_history),\n",
        "              ('smaller', smaller_history),\n",
        "              ('bigger', bigger_history)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UekcaQdmZxnW"
      },
      "source": [
        "注意: 上記のすべてのトレーニング実行では、`callbacks.EarlyStopping` を使用して、モデルが進行していないことが明らかになった時点でトレーニングを終了しました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEQNKadHA0M3"
      },
      "source": [
        "### TensorBoard で表示する\n",
        "\n",
        "これらのモデルはすべて、トレーニング中に TensorBoard ログを書き込みました。\n",
        "\n",
        "ノートブック内に埋め込まれた TensorBoard ビューアを開きます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oa1lkJddZ-m"
      },
      "outputs": [],
      "source": [
        "#docs_infra: no_execute\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Open an embedded TensorBoard viewer\n",
        "%tensorboard --logdir {logdir}/sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjqx3bywDPjf"
      },
      "source": [
        "[TensorBoard.dev](https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97) で、このノートブックの[前回の実行結果](https://tensorboard.dev/)を閲覧できます。\n",
        "\n",
        "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
        "\n",
        "便宜上、`<iframe>` にも含まれています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX5fcgrADwym"
      },
      "outputs": [],
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\",\n",
        "    width=\"100%\", height=\"800px\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDQDBKYZBXF_"
      },
      "source": [
        "TensorBoard の結果を共有することを希望する場合は、以下のコードをコードセルに張り付けて、<a>TensorBoard.dev</a> にログをアップロードできます。\n",
        "\n",
        "注意: Google アカウントが必要です。\n",
        "\n",
        "```\n",
        "!tensorboard dev upload --logdir  {logdir}/sizes\n",
        "```\n",
        "\n",
        "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASdv7nsgEFhx"
      },
      "source": [
        "## 過学習防止の戦略"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN512ksslaxJ"
      },
      "source": [
        "このセクションの内容に入る前に、上記の `\"Tiny\"` モデルからトレーニングログをコピーして、比較のベースラインとして使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40k1eBtnQzNo"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(logdir/'regularizers/Tiny', ignore_errors=True)\n",
        "shutil.copytree(logdir/'sizes/Tiny', logdir/'regularizers/Tiny')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFWMeFo7jLpN"
      },
      "outputs": [],
      "source": [
        "regularizer_histories = {}\n",
        "regularizer_histories['Tiny'] = size_histories['Tiny']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rHoVWcswFLa"
      },
      "source": [
        "### 重みの正則化を加える\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRxWepNawbBK"
      },
      "source": [
        "「オッカムの剃刀」の原則をご存知でしょうか。何かの説明が2つあるとすると、最も正しいと考えられる説明は、仮定の数が最も少ない「一番単純な」説明だというものです。この原則は、ニューラルネットワークを使って学習されたモデルにも当てはまります。ある訓練用データとネットワーク構造があって、そのデータを説明できる重みの集合が複数ある時（つまり、複数のモデルがある時）、単純なモデルのほうが複雑なものよりも過学習しにくいのです。\n",
        "\n",
        "ここで言う「単純なモデル」とは、パラメータ値の分布のエントロピーが小さいもの（あるいは、上記で見たように、そもそもパラメータの数が少ないもの）です。したがって、過学習を緩和するための一般的な手法は、重みが小さい値のみをとることで、重み値の分布がより整然となる（正則）ように制約を与えるものです。これを「重みの正則化」と呼ばれ、ネットワークの損失関数に、重みの大きさに関連するコストを加えることで行われます。このコストには 2 つの種類があります。\n",
        "\n",
        "- [L1 正則化](https://developers.google.com/machine-learning/glossary/#L1_regularization): 重み係数の絶対値に比例するコストを加える（重みの「L1 ノルム」と呼ばれる）。\n",
        "\n",
        "- [L2 正則化](https://developers.google.com/machine-learning/glossary/#L2_regularization): 重み係数の二乗に比例するコストを加える（重み係数の二乗「L2 ノルム」と呼ばれる）。L2 正則化はニューラルネットワーク用語では重み減衰（Weight Decay）と呼ばれる。呼び方が違うので混乱しないように。重み減衰は数学的には L2 正則化と同義である。\n",
        "\n",
        "L1 正則化は重みパラメータの一部を 0 にすることでモデルを疎にする効果があります。L2 正則化は重みパラメータにペナルティを加えますがモデルを疎にすることはありません。そのため、L2 正則化のほうが一般的です。\n",
        "\n",
        "`tf.keras`では、重みの正則化をするために、重み正則化のインスタンスをキーワード引数としてレイヤーに加えます。ここでは、L2 正則化を追加してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFGmcwduwVyQ"
      },
      "outputs": [],
      "source": [
        "l2_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                       activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                       activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "l2_model.compile(optimizer='adam',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy', 'binary_crossentropy'])\n",
        "\n",
        "l2_model_history = l2_model.fit(train_data, train_labels,\n",
        "                                epochs=20,\n",
        "                                batch_size=512,\n",
        "                                validation_data=(test_data, test_labels),\n",
        "                                verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUUHoXb7w-_C"
      },
      "source": [
        "`l2(0.001)` というのは、レイヤーの重み行列の係数全てに対して `0.001 * weight_coefficient_value**2` をネットワークの損失値合計に加えることを意味します。\n",
        "\n",
        "そのため、`binary_crossentropy` を直接監視しています。この正則化コンポーネントが混在していないためです。\n",
        "\n",
        "したがって、`L2` 正則化ペナルティが設けられた同じ `\"Large\"` モデルのパフォーマンスははるかに優れています。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wkfLyxBZdh_"
      },
      "outputs": [],
      "source": [
        "plot_history([('baseline', baseline_history),\n",
        "              ('l2', l2_model_history)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx1YHMsVxWjP"
      },
      "source": [
        "ご覧のように、`\"L2\"` 正則化ありのモデルは `\"Tiny\"` モデルとほぼ同等になりました。`\"L2\"` モデルは `\"Large\"` モデルと比べて過学習しにくくなっています。両方のモデルのパラメータ数は同じであるにもかかわらずです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JheBk6f8jMQ7"
      },
      "source": [
        "#### 詳細情報\n",
        "\n",
        "このような正則化について注意すべき重要事項が 2 つあります。\n",
        "\n",
        "1. 独自のトレーニングループを作成している場合は、モデルに正則化の損失を必ず確認する必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apDHQNybjaML"
      },
      "outputs": [],
      "source": [
        "result = l2_model(features)\n",
        "regularization_loss=tf.add_n(l2_model.losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLhG6fMSjE-J"
      },
      "source": [
        "1. この実装は、モデルの損失に対して重みペナルティを与えてから標準の最適化手順を適用します。\n",
        "\n",
        "2 番目のアプローチでは、代わりに、生の損失に対してのみオプティマイザを実行します。オプティマイザは計算されたステップを適用しながら、重みの減衰も適用します。この「分離された重みの減衰」は、`tf.keras.optimizers.Ftrl` や `tfa.optimizers.AdamW` などのオプティマイザで使用されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmnBNOOVxiG8"
      },
      "source": [
        "### ドロップアウトを追加する\n",
        "\n",
        "ドロップアウトは、ニューラルネットワークの正則化テクニックとして最もよく使われる手法の一つです。この手法は、トロント大学のヒントンと彼の学生が開発したものです。ドロップアウトは層に適用するもので、訓練時に層から出力された特徴量に対してランダムに「ドロップアウト（つまりゼロ化）」を行うものです。例えば、ある層が訓練時にある入力サンプルに対して、普通は`[0.2, 0.5, 1.3, 0.8, 1.1]` というベクトルを出力するとします。ドロップアウトを適用すると、このベクトルは例えば`[0, 0.5, 1.3, 0, 1.1]`のようにランダムに散らばったいくつかのゼロを含むようになります。「ドロップアウト率」はゼロ化される特徴の割合で、通常は0.2から0.5の間に設定します。テスト時は、どのユニットもドロップアウトされず、代わりに出力値がドロップアウト率と同じ比率でスケールダウンされます。これは、訓練時に比べてたくさんのユニットがアクティブであることに対してバランスをとるためです。\n",
        "\n",
        "ドロップアウトを簡単に説明すると、ネットワーク内の個々のノードは他のノードの出力に依存できないため、各ノードはそれ自体で役立つ特徴を出力する必要があるということです。\n",
        "\n",
        "ドロップアウトはレイヤーに適用するもので、トレーニング時にレイヤーから出力された特徴量に対してランダムに「ドロップアウト（つまりゼロ化）」を行うものです。例えば、あるレイヤーがトレーニング時にある入力サンプルに対して、普通は`[0.2, 0.5, 1.3, 0.8, 1.1]` というベクトルを出力するとします。ドロップアウトを適用すると、このベクトルは例えば`[0, 0.5, 1.3, 0, 1.1]`のようにランダムに散らばったいくつかのゼロを含むようになります。\n",
        "\n",
        "「ドロップアウト率」はゼロ化される特徴の割合で、通常は 0.2 から 0.5 の間に設定します。テスト時は、どのユニットもドロップアウトされず、代わりに出力値がドロップアウト率と同じ比率でスケールダウンされます。これは、トレーニング時に比べてたくさんのユニットがアクティブであることに対してバランスをとるためです。\n",
        "\n",
        "Keras では、`tf.keras.layers.Dropout` レイヤーを使ってドロップアウトをネットワークに導入できます。ドロップアウトレイヤーは、その直前のレイヤーの出力に対してドロップアウトを適用します。\n",
        "\n",
        "ネットワークに 2 つのドロップアウトレイヤーを追加して、過適合を減らすのにどれだけ効果を発揮するか見てみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFEYvtrHxSWS"
      },
      "outputs": [],
      "source": [
        "dpt_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "dpt_model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy','binary_crossentropy'])\n",
        "\n",
        "dpt_model_history = dpt_model.fit(train_data, train_labels,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=512,\n",
        "                                  validation_data=(test_data, test_labels),\n",
        "                                  verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPZqwVchx5xp"
      },
      "outputs": [],
      "source": [
        "plot_history([('baseline', baseline_history),\n",
        "              ('dropout', dpt_model_history)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zlHr4iaI1U6"
      },
      "source": [
        "このプロットから、これらの正則化アプローチは両方とも `\"Large\"` モデルの動作を改善することが分かります。しかし、`\"Tiny\"` のベースラインと比較すると勝るものはありません。\n",
        "\n",
        "次に、両方を一緒に試して、改善するかどうかを確認します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7qMg_7Nwy5t"
      },
      "source": [
        "###  L2 とドロップアウトを組み合わせる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zfs_qQIw1cz"
      },
      "outputs": [],
      "source": [
        "combined_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "regularizer_histories['combined'] = compile_and_fit(combined_model, \"regularizers/combined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDqBBxfI0Yd8"
      },
      "outputs": [],
      "source": [
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE0OoNCQNTJv"
      },
      "source": [
        "`\"Combined\"` 正則化を使用したモデルは、明らかに最も優れたモデルです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dw23T03FEO1"
      },
      "source": [
        "### TensorBoard で表示する\n",
        "\n",
        "これらのモデルは、TensorBoard ログも記録しました。\n",
        "\n",
        "ノートブック内に埋め込まれたテンソルボードビューアを開くには、以下をコードセルにコピーします。\n",
        "\n",
        "```\n",
        "%tensorboard --logdir {logdir}/regularizers\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX3Voac-FEO4"
      },
      "source": [
        "[TensorBoard.dev](https://tensorboard.dev/experiment/fGInKDo8TXes1z7HQku9mw/#scalars&_smoothingWeight=0.97) で、このノートブックの[前回の実行結果](https://tensorboard.dev/)を閲覧できます。\n",
        "\n",
        "便宜上、`<iframe>` にも含まれています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doMtyYoqFEO5"
      },
      "outputs": [],
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/fGInKDo8TXes1z7HQku9mw/#scalars&_smoothingWeight=0.97\",\n",
        "    width = \"100%\",\n",
        "    height=\"800px\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mds5RXGjIcSu"
      },
      "source": [
        "次のようにアップロードされました。\n",
        "\n",
        "```\n",
        "!tensorboard dev upload --logdir  {logdir}/regularizers\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXJxtwBWIhjG"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjfnkEeQyAFG"
      },
      "source": [
        "ニューラルネットワークの過適合を防ぐための最も一般的な方法は次のとおりです。\n",
        "\n",
        "- より多くのトレーニングデータを取得します。\n",
        "- ネットワークの容量を減らします。\n",
        "- 重みの正則化を追加します。\n",
        "- ドロップアウトを追加します。\n",
        "\n",
        "このガイドで説明されていない 2 つの重要なアプローチは次のとおりです。\n",
        "\n",
        "- [データ拡張](../images/data_augmentation.ipynb)\n",
        "- バッチ正規化 (`tf.keras.layers.BatchNormalization`)\n",
        "\n",
        "それぞれの方法は個別に利用しても役立つ可能性がありますが、多くの場合、組み合わせるとさらに効果的になります。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "overfit_and_underfit.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
