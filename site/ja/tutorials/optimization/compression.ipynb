{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Compression Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# スケーラブルなモデル圧縮"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/optimization/compression\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     TensorFlow.org で表示</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tutorials/optimization/compression.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Google Colab で実行</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/optimization/compression.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     GitHubでソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tutorials/optimization/compression.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## 概要\n",
        "\n",
        "このノートブックでは、[TensorFlow Compression](https://github.com/tensorflow/compression) を使用してモデルを圧縮する方法を説明します。\n",
        "\n",
        "以下の例では、分類精度を維持しながら、MNIST 分類器の重みを浮動小数点表現よりもはるかに小さいサイズに圧縮します。これは、論文 [Scalable Model Compression by Entropy Penalized Reparameterization](https://arxiv.org/abs/1906.06624) に基づく 2 段階のプロセスによって行われます。\n",
        "\n",
        "- トレーニング中に明示的な**エントロピーペナルティ**を使用して「圧縮可能な」モデルをトレーニングします。これにより、モデルパラメータの圧縮性が促進されます。このペナルティの重み $\\lambda$ により、圧縮されたモデルのサイズとその精度の間のトレードオフを継続的に制御できます。\n",
        "\n",
        "- ペナルティと一致するコーディングスキームを使用して、圧縮可能なモデルを圧縮モデルにエンコードします。つまり、ペナルティはモデルサイズの予測因子となります。これにより、微調整するためにモデルのトレーニング、圧縮、再トレーニングを複数回繰り返す必要がなくなります。\n",
        "\n",
        "この方法は、計算の複雑さではなく、圧縮されたモデルのサイズに厳密に関係しています。モデルのプルーニングなどの手法と組み合わせて、サイズと複雑さを軽減できます。\n",
        "\n",
        "さまざまなモデルでの圧縮結果の例:\n",
        "\n",
        "モデル (データセット) | モデルサイズ | 圧縮比 | トップ-1 エラー圧縮(非圧縮)\n",
        "--- | --- | --- | ---\n",
        "LeNet300-100 (MNIST) | 8.56 KB | 124x | 1.9%  (1.6%)\n",
        "LeNet5-Caffe (MNIST) | 2.84 KB | 606x | 1.0%  (0.7%)\n",
        "VGG-16 (CIFAR-10) | 101 KB | 590x | 10.0%  (6.6%)\n",
        "ResNet-20-4 (CIFAR-10) | 128 KB | 134x | 8.8%  (5.0%)\n",
        "ResNet-18 (ImageNet) | 1.97 MB | 24x | 30.0% (30.0%)\n",
        "ResNet-50 (ImageNet) | 5.49 MB | 19x | 26.0% (25.0%)\n",
        "\n",
        "使用例:\n",
        "\n",
        "- モデルを大規模にエッジデバイスにデプロイ/ブロードキャスト。転送中の帯域幅の使用量を低減できます。\n",
        "- フェデレーテッドラーニングでグローバルモデルの状態をクライアントに伝える。モデルのアーキテクチャ (隠れユニットの数など) は初期モデルから変更されておらず、クライアントは解凍されたモデルで学習を続けることができます。\n",
        "- 非常にメモリが制限されたクライアントでの推論の実行。推論中、各レイヤーの重みを順次解凍し、アクティベーションが計算された直後に破棄することができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## セットアップ\n",
        "\n",
        "`pip` で Tensorflow Compression をインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K489KsEgxuLI"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Installs the latest version of TFC compatible with the installed TF version.\n",
        "\n",
        "read MAJOR MINOR <<< \"$(pip show tensorflow | perl -p -0777 -e 's/.*Version: (\\d+)\\.(\\d+).*/\\1 \\2/sg')\"\n",
        "pip install \"tensorflow-compression<$MAJOR.$(($MINOR+1))\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfVAmHCVxpTS"
      },
      "source": [
        "ライブラリ依存関係をインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_compression as tfc\n",
        "import tensorflow_datasets as tfds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsncKT2iymgQ"
      },
      "source": [
        "## 基本的な MNIST 分類器を定義してトレーニングする\n",
        "\n",
        "密層と畳み込み層を効果的に圧縮するには、カスタム層クラスを定義する必要があります。これらは `tf.keras.layers` の下のレイヤーに似ていますが、Entropy Penalized Reparameterization (EPR) を効果的に実装するために後でサブクラス化します。このために、コピーコンストラクタも追加します。\n",
        "\n",
        "まず、標準の密層を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_7ZRqiaO1WQ"
      },
      "outputs": [],
      "source": [
        "class CustomDense(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, filters, name=\"dense\"):\n",
        "    super().__init__(name=name)\n",
        "    self.filters = filters\n",
        "\n",
        "  @classmethod\n",
        "  def copy(cls, other, **kwargs):\n",
        "    \"\"\"Returns an instantiated and built layer, initialized from `other`.\"\"\"\n",
        "    self = cls(filters=other.filters, name=other.name, **kwargs)\n",
        "    self.build(None, other=other)\n",
        "    return self\n",
        "\n",
        "  def build(self, input_shape, other=None):\n",
        "    \"\"\"Instantiates weights, optionally initializing them from `other`.\"\"\"\n",
        "    if other is None:\n",
        "      kernel_shape = (input_shape[-1], self.filters)\n",
        "      kernel = tf.keras.initializers.GlorotUniform()(shape=kernel_shape)\n",
        "      bias = tf.keras.initializers.Zeros()(shape=(self.filters,))\n",
        "    else:\n",
        "      kernel, bias = other.kernel, other.bias\n",
        "    self.kernel = tf.Variable(\n",
        "        tf.cast(kernel, self.variable_dtype), name=\"kernel\")\n",
        "    self.bias = tf.Variable(\n",
        "        tf.cast(bias, self.variable_dtype), name=\"bias\")\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = tf.linalg.matvec(self.kernel, inputs, transpose_a=True)\n",
        "    outputs = tf.nn.bias_add(outputs, self.bias)\n",
        "    return tf.nn.leaky_relu(outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUZkcXegc0yR"
      },
      "source": [
        "同様に、2D 畳み込み層を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDibtb8EWCSj"
      },
      "outputs": [],
      "source": [
        "class CustomConv2D(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, filters, kernel_size,\n",
        "               strides=1, padding=\"SAME\", name=\"conv2d\"):\n",
        "    super().__init__(name=name)\n",
        "    self.filters = filters\n",
        "    self.kernel_size = kernel_size\n",
        "    self.strides = strides\n",
        "    self.padding = padding\n",
        "\n",
        "  @classmethod\n",
        "  def copy(cls, other, **kwargs):\n",
        "    \"\"\"Returns an instantiated and built layer, initialized from `other`.\"\"\"\n",
        "    self = cls(filters=other.filters, kernel_size=other.kernel_size,\n",
        "               strides=other.strides, padding=other.padding, name=other.name,\n",
        "               **kwargs)\n",
        "    self.build(None, other=other)\n",
        "    return self\n",
        "\n",
        "  def build(self, input_shape, other=None):\n",
        "    \"\"\"Instantiates weights, optionally initializing them from `other`.\"\"\"\n",
        "    if other is None:\n",
        "      kernel_shape = 2 * (self.kernel_size,) + (input_shape[-1], self.filters)\n",
        "      kernel = tf.keras.initializers.GlorotUniform()(shape=kernel_shape)\n",
        "      bias = tf.keras.initializers.Zeros()(shape=(self.filters,))\n",
        "    else:\n",
        "      kernel, bias = other.kernel, other.bias\n",
        "    self.kernel = tf.Variable(\n",
        "        tf.cast(kernel, self.variable_dtype), name=\"kernel\")\n",
        "    self.bias = tf.Variable(\n",
        "        tf.cast(bias, self.variable_dtype), name=\"bias\")\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = tf.nn.convolution(\n",
        "        inputs, self.kernel, strides=self.strides, padding=self.padding)\n",
        "    outputs = tf.nn.bias_add(outputs, self.bias)\n",
        "    return tf.nn.leaky_relu(outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xWa1hHMdCpG"
      },
      "source": [
        "モデルの圧縮に進む前に、通常の分類器を正常にトレーニングできることを確認します。\n",
        "\n",
        "モデルアーキテクチャを定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yZESLgW-vp1"
      },
      "outputs": [],
      "source": [
        "classifier = tf.keras.Sequential([\n",
        "    CustomConv2D(20, 5, strides=2, name=\"conv_1\"),\n",
        "    CustomConv2D(50, 5, strides=2, name=\"conv_2\"),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    CustomDense(500, name=\"fc_1\"),\n",
        "    CustomDense(10, name=\"fc_2\"),\n",
        "], name=\"classifier\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iRSvt_CdUuY"
      },
      "source": [
        "トレーニングデータを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4bsA3HFF2k0"
      },
      "outputs": [],
      "source": [
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "training_dataset, validation_dataset = tfds.load(\n",
        "    \"mnist\",\n",
        "    split=[\"train\", \"test\"],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=False,\n",
        ")\n",
        "training_dataset = training_dataset.map(normalize_img)\n",
        "validation_dataset = validation_dataset.map(normalize_img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR9WYjt_daRG"
      },
      "source": [
        "最後に、モデルをトレーニングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROn2DbzsBirI"
      },
      "outputs": [],
      "source": [
        "def train_model(model, training_data, validation_data, **kwargs):\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        "      # Uncomment this to ease debugging:\n",
        "      # run_eagerly=True,\n",
        "  )\n",
        "  kwargs.setdefault(\"epochs\", 5)\n",
        "  kwargs.setdefault(\"verbose\", 1)\n",
        "  log = model.fit(\n",
        "      training_data.batch(128).prefetch(8),\n",
        "      validation_data=validation_data.batch(128).cache(),\n",
        "      validation_freq=1,\n",
        "      **kwargs,\n",
        "  )\n",
        "  return log.history[\"val_sparse_categorical_accuracy\"][-1]\n",
        "\n",
        "classifier_accuracy = train_model(\n",
        "    classifier, training_dataset, validation_dataset)\n",
        "\n",
        "print(f\"Accuracy: {classifier_accuracy:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QupWKZ91di-y"
      },
      "source": [
        "成功しました！ モデルは適切にトレーニングされ、5 エポック以内に検証セットで 98% を超える精度に達しました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRqZFwb5dqQm"
      },
      "source": [
        "## 圧縮可能な分類器をトレーニングする\n",
        "\n",
        "Entropy Penalized Reparameterization (EPR) には、次の 2 つの主要な要素があります。\n",
        "\n",
        "- トレーニング中にモデルの重みに**ペナルティ**を適用します。これは、確率モデルの下でのエントロピーに対応し、重みのエンコードスキームと一致します。以下では、このペナルティを実装する Keras `Regularizer` を定義します。\n",
        "\n",
        "- 重みを**再パラメータ化**します。つまり、重みをより圧縮可能な潜在表現にします (圧縮性とモデルのパフォーマンスのトレードオフを改善します)。畳み込みカーネルの場合、フーリエドメインが適切な表現であることが[示されています](https://arxiv.org/abs/1906.06624)。他のパラメータについては、以下の例では、さまざまな量子化ステップサイズで単純にスカラー量子化 (丸め) を使用しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4jmnqEmO6eB"
      },
      "source": [
        "まず、ペナルティを定義します。\n",
        "\n",
        "以下の例では、論文 [Optimizing the Communication-Accuracy Trade-off in Federated Learning with Rate-Distortion Theory](https://arxiv.org/abs/2201.02664) に着想を得て、`tfc.PowerLawEntropyModel` クラスに実装されたコード/確率モデルを使用しています。ペナルティは$$ \\log \\Bigl(\\frac {|x| + \\alpha} \\alpha\\Bigr), $$ と定義されます。$x$ はモデルパラメータまたはその潜在表現の 1 つの要素であり、$\\alpha$ は小さい定数で、数値は 0 付近で安定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh57nxjuwocc"
      },
      "outputs": [],
      "source": [
        "_ = tf.linspace(-5., 5., 501)\n",
        "plt.plot(_, tfc.PowerLawEntropyModel(0).penalty(_));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr3-6vLrwo-H"
      },
      "source": [
        "ペナルティは正則化の損失です (「重み損失」と呼ばれることもあります)。ゼロの場合、カスプが凹形状であり、重みのスパース性を助長します。重みを圧縮するために適用されるコーディングスキームである [Elias ガンマ コード](https://en.wikipedia.org/wiki/Elias_gamma_coding)は、要素の大きさの長さが $ 1 + \\lfloor \\log_2 |x| \\rfloor $ ビットのコードを生成します。つまり、それはペナルティに一致し、ペナルティを適用すると、予想されるコード長が最小化されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1Yt6e1ub6pU"
      },
      "outputs": [],
      "source": [
        "class PowerLawRegularizer(tf.keras.regularizers.Regularizer):\n",
        "\n",
        "  def __init__(self, lmbda):\n",
        "    super().__init__()\n",
        "    self.lmbda = lmbda\n",
        "\n",
        "  def __call__(self, variable):\n",
        "    em = tfc.PowerLawEntropyModel(coding_rank=variable.shape.rank)\n",
        "    return self.lmbda * em.penalty(variable)\n",
        "\n",
        "# Normalizing the weight of the penalty by the number of model parameters is a\n",
        "# good rule of thumb to produce comparable results across models.\n",
        "regularizer = PowerLawRegularizer(lmbda=2./classifier.count_params())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyQc35QTf8Aq"
      },
      "source": [
        "次に、次の追加機能を持つ `CustomDense` と `Custom Conv2D` のサブクラスを定義します。\n",
        "\n",
        "- これらは上記の正則化のインスタンスを取得し、トレーニング中にそれをカーネルとバイアスに適用します。\n",
        "- これらは、カーネルとバイアスを `@property` として定義し、変数にアクセスするたびに直接勾配で量子化を実行します。これは、後で圧縮モデルで実行される計算を正確に反映します。\n",
        "- これらは、量子化ステップサイズの対数を表す追加の `log_step` 変数を定義します。量子化が粗いほど、モデルのサイズは小さくなりますが、精度は低くなります。量子化ステップサイズは各モデルパラメータに対してトレーニング可能であるため、ペナルティ付き損失関数で最適化を実行すると、最適な量子化ステップサイズが決定されます。\n",
        "\n",
        "量子化ステップは次のように定義されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60fMt3avgSFw"
      },
      "outputs": [],
      "source": [
        "def quantize(latent, log_step):\n",
        "  step = tf.exp(log_step)\n",
        "  return tfc.round_st(latent / step) * step\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stKrchp7mB0b"
      },
      "source": [
        "これで、密層を定義できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ciz1F1WsXre_"
      },
      "outputs": [],
      "source": [
        "class CompressibleDense(CustomDense):\n",
        "\n",
        "  def __init__(self, regularizer, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.regularizer = regularizer\n",
        "\n",
        "  def build(self, input_shape, other=None):\n",
        "    \"\"\"Instantiates weights, optionally initializing them from `other`.\"\"\"\n",
        "    super().build(input_shape, other=other)\n",
        "    if other is not None and hasattr(other, \"kernel_log_step\"):\n",
        "      kernel_log_step = other.kernel_log_step\n",
        "      bias_log_step = other.bias_log_step\n",
        "    else:\n",
        "      kernel_log_step = bias_log_step = -4.\n",
        "    self.kernel_log_step = tf.Variable(\n",
        "        tf.cast(kernel_log_step, self.variable_dtype), name=\"kernel_log_step\")\n",
        "    self.bias_log_step = tf.Variable(\n",
        "        tf.cast(bias_log_step, self.variable_dtype), name=\"bias_log_step\")\n",
        "    self.add_loss(lambda: self.regularizer(\n",
        "        self.kernel_latent / tf.exp(self.kernel_log_step)))\n",
        "    self.add_loss(lambda: self.regularizer(\n",
        "        self.bias_latent / tf.exp(self.bias_log_step)))\n",
        "\n",
        "  @property\n",
        "  def kernel(self):\n",
        "    return quantize(self.kernel_latent, self.kernel_log_step)\n",
        "\n",
        "  @kernel.setter\n",
        "  def kernel(self, kernel):\n",
        "    self.kernel_latent = tf.Variable(kernel, name=\"kernel_latent\")\n",
        "\n",
        "  @property\n",
        "  def bias(self):\n",
        "    return quantize(self.bias_latent, self.bias_log_step)\n",
        "\n",
        "  @bias.setter\n",
        "  def bias(self, bias):\n",
        "    self.bias_latent = tf.Variable(bias, name=\"bias_latent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsykbQO0hxzW"
      },
      "source": [
        "畳み込み層も同様です。さらに、畳み込みカーネルは、カーネルが設定されるたびに実数値の離散フーリエ変換 (RDFT) として格納され、カーネルが使用されるたびに変換が反転されます。カーネルのさまざまな周波数成分は多かれ少なかれ圧縮可能である傾向があるため、それぞれに独自の量子化ステップサイズが割り当てられます。\n",
        "\n",
        "フーリエ変換とその逆を次のように定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUFMKGHDguJS"
      },
      "outputs": [],
      "source": [
        "def to_rdft(kernel, kernel_size):\n",
        "  # The kernel has shape (H, W, I, O) -> transpose to take DFT over last two\n",
        "  # dimensions.\n",
        "  kernel = tf.transpose(kernel, (2, 3, 0, 1))\n",
        "  # The RDFT has type complex64 and shape (I, O, FH, FW).\n",
        "  kernel_rdft = tf.signal.rfft2d(kernel)\n",
        "  # Map real and imaginary parts into regular floats. The result is float32\n",
        "  # and has shape (I, O, FH, FW, 2).\n",
        "  kernel_rdft = tf.stack(\n",
        "      [tf.math.real(kernel_rdft), tf.math.imag(kernel_rdft)], axis=-1)\n",
        "  # Divide by kernel size to make the DFT orthonormal (length-preserving).\n",
        "  return kernel_rdft / kernel_size\n",
        "\n",
        "def from_rdft(kernel_rdft, kernel_size):\n",
        "  # Undoes the transformations in to_rdft.\n",
        "  kernel_rdft *= kernel_size\n",
        "  kernel_rdft = tf.dtypes.complex(*tf.unstack(kernel_rdft, axis=-1))\n",
        "  kernel = tf.signal.irfft2d(kernel_rdft, fft_length=2 * (kernel_size,))\n",
        "  return tf.transpose(kernel, (2, 3, 0, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZZrJ5ImVDY"
      },
      "source": [
        "次に畳み込み層を以下のように定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKzXBNCO7bjB"
      },
      "outputs": [],
      "source": [
        "class CompressibleConv2D(CustomConv2D):\n",
        "\n",
        "  def __init__(self, regularizer, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.regularizer = regularizer\n",
        "\n",
        "  def build(self, input_shape, other=None):\n",
        "    \"\"\"Instantiates weights, optionally initializing them from `other`.\"\"\"\n",
        "    super().build(input_shape, other=other)\n",
        "    if other is not None and hasattr(other, \"kernel_log_step\"):\n",
        "      kernel_log_step = other.kernel_log_step\n",
        "      bias_log_step = other.bias_log_step\n",
        "    else:\n",
        "      kernel_log_step = tf.fill(self.kernel_latent.shape[2:], -4.)\n",
        "      bias_log_step = -4.\n",
        "    self.kernel_log_step = tf.Variable(\n",
        "        tf.cast(kernel_log_step, self.variable_dtype), name=\"kernel_log_step\")\n",
        "    self.bias_log_step = tf.Variable(\n",
        "        tf.cast(bias_log_step, self.variable_dtype), name=\"bias_log_step\")\n",
        "    self.add_loss(lambda: self.regularizer(\n",
        "        self.kernel_latent / tf.exp(self.kernel_log_step)))\n",
        "    self.add_loss(lambda: self.regularizer(\n",
        "        self.bias_latent / tf.exp(self.bias_log_step)))\n",
        "\n",
        "  @property\n",
        "  def kernel(self):\n",
        "    kernel_rdft = quantize(self.kernel_latent, self.kernel_log_step)\n",
        "    return from_rdft(kernel_rdft, self.kernel_size)\n",
        "\n",
        "  @kernel.setter\n",
        "  def kernel(self, kernel):\n",
        "    kernel_rdft = to_rdft(kernel, self.kernel_size)\n",
        "    self.kernel_latent = tf.Variable(kernel_rdft, name=\"kernel_latent\")\n",
        "\n",
        "  @property\n",
        "  def bias(self):\n",
        "    return quantize(self.bias_latent, self.bias_log_step)\n",
        "\n",
        "  @bias.setter\n",
        "  def bias(self, bias):\n",
        "    self.bias_latent = tf.Variable(bias, name=\"bias_latent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-ekDDQ9jidI"
      },
      "source": [
        "上記と同じアーキテクチャで分類器モデルを定義しますが、これらの変更されたレイヤーを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQgp84L7qalw"
      },
      "outputs": [],
      "source": [
        "def make_mnist_classifier(regularizer):\n",
        "  return tf.keras.Sequential([\n",
        "      CompressibleConv2D(regularizer, 20, 5, strides=2, name=\"conv_1\"),\n",
        "      CompressibleConv2D(regularizer, 50, 5, strides=2, name=\"conv_2\"),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      CompressibleDense(regularizer, 500, name=\"fc_1\"),\n",
        "      CompressibleDense(regularizer, 10, name=\"fc_2\"),\n",
        "  ], name=\"classifier\")\n",
        "\n",
        "compressible_classifier = make_mnist_classifier(regularizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ-TMHE1kNFc"
      },
      "source": [
        "モデルをトレーニングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L5ZJAX4EiXW"
      },
      "outputs": [],
      "source": [
        "penalized_accuracy = train_model(\n",
        "    compressible_classifier, training_dataset, validation_dataset)\n",
        "\n",
        "print(f\"Accuracy: {penalized_accuracy:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuE4NeY_kTDz"
      },
      "source": [
        "圧縮可能なモデルは、単純な分類器と同様の精度に達しています。\n",
        "\n",
        "ただし、モデルは実際にはまだ圧縮されていません。これを行うためには、カーネルとバイアスを圧縮された形式 (一連のビット) で格納するサブクラスの別のセットを定義します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZhj8A2gnBkD"
      },
      "source": [
        "## 分類器を圧縮する\n",
        "\n",
        "以下に定義されている `CustomDense` と `Custom Conv2D` のサブクラスは、圧縮可能な高密度レイヤーの重みをバイナリ文字列に変換します。さらに、スペースを節約するために、量子化ステップサイズの対数を半精度で格納します。カーネルまたはバイアスが `@property` を介してアクセスされるたびに、それらは文字列表現から解凍され、逆量子化されます。\n",
        "\n",
        "まず、モデルパラメータを圧縮および解凍する関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS19FhDajeto"
      },
      "outputs": [],
      "source": [
        "def compress_latent(latent, log_step, name):\n",
        "  em = tfc.PowerLawEntropyModel(latent.shape.rank)\n",
        "  compressed = em.compress(latent / tf.exp(log_step))\n",
        "  compressed = tf.Variable(compressed, name=f\"{name}_compressed\")\n",
        "  log_step = tf.cast(log_step, tf.float16)\n",
        "  log_step = tf.Variable(log_step, name=f\"{name}_log_step\")\n",
        "  return compressed, log_step\n",
        "\n",
        "def decompress_latent(compressed, shape, log_step):\n",
        "  latent = tfc.PowerLawEntropyModel(len(shape)).decompress(compressed, shape)\n",
        "  step = tf.exp(tf.cast(log_step, latent.dtype))\n",
        "  return latent * step\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPPABE9fjqHJ"
      },
      "source": [
        "これらを使用して、`CompressedDense` を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnaiNzhgaZ7s"
      },
      "outputs": [],
      "source": [
        "class CompressedDense(CustomDense):\n",
        "\n",
        "  def build(self, input_shape, other=None):\n",
        "    assert isinstance(other, CompressibleDense)\n",
        "    self.input_channels = other.kernel.shape[0]\n",
        "    self.kernel_compressed, self.kernel_log_step = compress_latent(\n",
        "        other.kernel_latent, other.kernel_log_step, \"kernel\")\n",
        "    self.bias_compressed, self.bias_log_step = compress_latent(\n",
        "        other.bias_latent, other.bias_log_step, \"bias\")\n",
        "    self.built = True\n",
        "\n",
        "  @property\n",
        "  def kernel(self):\n",
        "    kernel_shape = (self.input_channels, self.filters)\n",
        "    return decompress_latent(\n",
        "        self.kernel_compressed, kernel_shape, self.kernel_log_step)\n",
        "\n",
        "  @property\n",
        "  def bias(self):\n",
        "    bias_shape = (self.filters,)\n",
        "    return decompress_latent(\n",
        "        self.bias_compressed, bias_shape, self.bias_log_step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzvMCM0El2iW"
      },
      "source": [
        "畳み込み層クラスは上記に類似しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS-2ADA6iWeQ"
      },
      "outputs": [],
      "source": [
        "class CompressedConv2D(CustomConv2D):\n",
        "\n",
        "  def build(self, input_shape, other=None):\n",
        "    assert isinstance(other, CompressibleConv2D)\n",
        "    self.input_channels = other.kernel.shape[2]\n",
        "    self.kernel_compressed, self.kernel_log_step = compress_latent(\n",
        "        other.kernel_latent, other.kernel_log_step, \"kernel\")\n",
        "    self.bias_compressed, self.bias_log_step = compress_latent(\n",
        "        other.bias_latent, other.bias_log_step, \"bias\")\n",
        "    self.built = True\n",
        "\n",
        "  @property\n",
        "  def kernel(self):\n",
        "    rdft_shape = (self.input_channels, self.filters,\n",
        "                  self.kernel_size, self.kernel_size // 2 + 1, 2)\n",
        "    kernel_rdft = decompress_latent(\n",
        "        self.kernel_compressed, rdft_shape, self.kernel_log_step)\n",
        "    return from_rdft(kernel_rdft, self.kernel_size)\n",
        "\n",
        "  @property\n",
        "  def bias(self):\n",
        "    bias_shape = (self.filters,)\n",
        "    return decompress_latent(\n",
        "        self.bias_compressed, bias_shape, self.bias_log_step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJLCPoe3l8jG"
      },
      "source": [
        "圧縮可能なモデルを圧縮モデルに変換するには、便利な `clone_model` 関数を使用できます。`compress_layer` は、圧縮可能なレイヤーを圧縮レイヤーに変換し、他のタイプのレイヤー (`Flatten` など) を単純に通過させます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEHroUyhG56m"
      },
      "outputs": [],
      "source": [
        "def compress_layer(layer):\n",
        "  if isinstance(layer, CompressibleDense):\n",
        "    return CompressedDense.copy(layer)\n",
        "  if isinstance(layer, CompressibleConv2D):\n",
        "    return CompressedConv2D.copy(layer)\n",
        "  return type(layer).from_config(layer.get_config())\n",
        "\n",
        "compressed_classifier = tf.keras.models.clone_model(\n",
        "    compressible_classifier, clone_function=compress_layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3wbN1XQmkDg"
      },
      "source": [
        "ここで、圧縮されたモデルの精度が期待どおりであることを検証します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R95kuURITpa9"
      },
      "outputs": [],
      "source": [
        "compressed_classifier.compile(metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "_, compressed_accuracy = compressed_classifier.evaluate(validation_dataset.batch(128))\n",
        "\n",
        "print(f\"Accuracy of the compressible classifier: {penalized_accuracy:0.4f}\")\n",
        "print(f\"Accuracy of the compressed classifier: {compressed_accuracy:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtFhpXh6uaIY"
      },
      "source": [
        "圧縮されたモデルの分類精度は、トレーニング中に達成されたものと同じです!\n",
        "\n",
        "さらに、圧縮されたモデルの重みのサイズは、元のモデルのサイズよりもはるかに小さくなっています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp-ecfuYufbs"
      },
      "outputs": [],
      "source": [
        "def get_weight_size_in_bytes(weight):\n",
        "  if weight.dtype == tf.string:\n",
        "    return tf.reduce_sum(tf.strings.length(weight, unit=\"BYTE\"))\n",
        "  else:\n",
        "    return tf.size(weight) * weight.dtype.size\n",
        "\n",
        "original_size = sum(map(get_weight_size_in_bytes, classifier.weights))\n",
        "compressed_size = sum(map(get_weight_size_in_bytes, compressed_classifier.weights))\n",
        "\n",
        "print(f\"Size of original model weights: {original_size} bytes\")\n",
        "print(f\"Size of compressed model weights: {compressed_size} bytes\")\n",
        "print(f\"Compression ratio: {(original_size/compressed_size):0.0f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8A8v0df6TR2"
      },
      "source": [
        "モデルをディスクに保存するには、モデルアーキテクチャ、関数グラフなどを保存するためのオーバーヘッドが必要です。\n",
        "\n",
        "ZIP などの可逆圧縮方法は、このようなデータの圧縮には適していますが、重み自体の圧縮には適していません。そのため、ZIP 圧縮を適用した後でも、そのオーバーヘッドを含めたモデルのサイズを数えると、EPR には大きな利点があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hunDYxH1zqb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def get_disk_size(model, path):\n",
        "  model.save(path)\n",
        "  zip_path = shutil.make_archive(path, \"zip\", path)\n",
        "  return os.path.getsize(zip_path)\n",
        "\n",
        "original_zip_size = get_disk_size(classifier, \"/tmp/classifier\")\n",
        "compressed_zip_size = get_disk_size(\n",
        "    compressed_classifier, \"/tmp/compressed_classifier\")\n",
        "\n",
        "print(f\"Original on-disk size (ZIP compressed): {original_zip_size} bytes\")\n",
        "print(f\"Compressed on-disk size (ZIP compressed): {compressed_zip_size} bytes\")\n",
        "print(f\"Compression ratio: {(original_zip_size/compressed_zip_size):0.0f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSITvJrlAhZs"
      },
      "source": [
        "## 正則化効果とサイズと精度のトレードオフ\n",
        "\n",
        "上記では、$\\lambda$ ハイパーパラメータが 2 に設定されています (モデル内のパラメータの数によって正規化されています)。 $\\lambda$ を大きくすると、モデルの重みは圧縮性に対してますます不利になります。\n",
        "\n",
        "低い値の場合、ペナルティは重みの正則化のように機能します。実際には、分類器の一般化パフォーマンスに有益な効果があり、検証データセットの精度がわずかに高くなる可能性があります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4rhmKu98FdPJ"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "print(f\"Accuracy of the vanilla classifier: {classifier_accuracy:0.4f}\")\n",
        "print(f\"Accuracy of the penalized classifier: {penalized_accuracy:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UCfC4LQFdjL"
      },
      "source": [
        "値が高いほど、モデルのサイズは小さくなりますが、精度は徐々に低下します。このことを確認するために、いくつかのモデルをトレーニングして、サイズと精度をプロットしてみます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diApPKHbAIqa"
      },
      "outputs": [],
      "source": [
        "def compress_and_evaluate_model(lmbda):\n",
        "  print(f\"lambda={lmbda:0.0f}: training...\", flush=True)\n",
        "  regularizer = PowerLawRegularizer(lmbda=lmbda/classifier.count_params())\n",
        "  compressible_classifier = make_mnist_classifier(regularizer)\n",
        "  train_model(\n",
        "      compressible_classifier, training_dataset, validation_dataset, verbose=0)\n",
        "  print(\"compressing...\", flush=True)\n",
        "  compressed_classifier = tf.keras.models.clone_model(\n",
        "      compressible_classifier, clone_function=compress_layer)\n",
        "  compressed_size = sum(map(\n",
        "      get_weight_size_in_bytes, compressed_classifier.weights))\n",
        "  compressed_zip_size = float(get_disk_size(\n",
        "      compressed_classifier, \"/tmp/compressed_classifier\"))\n",
        "  print(\"evaluating...\", flush=True)\n",
        "  compressed_classifier = tf.keras.models.load_model(\n",
        "      \"/tmp/compressed_classifier\")\n",
        "  compressed_classifier.compile(\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "  _, compressed_accuracy = compressed_classifier.evaluate(\n",
        "      validation_dataset.batch(128), verbose=0)\n",
        "  print()\n",
        "  return compressed_size, compressed_zip_size, compressed_accuracy\n",
        "\n",
        "lambdas = (2., 5., 10., 20., 50.)\n",
        "metrics = [compress_and_evaluate_model(l) for l in lambdas]\n",
        "metrics = tf.convert_to_tensor(metrics, tf.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bhAi85KzGqTz"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "def plot_broken_xaxis(ax, compressed_sizes, original_size, original_accuracy):\n",
        "  xticks = list(range(\n",
        "      int(tf.math.floor(min(compressed_sizes) / 5) * 5),\n",
        "      int(tf.math.ceil(max(compressed_sizes) / 5) * 5) + 1,\n",
        "      5))\n",
        "  xticks.append(xticks[-1] + 10)\n",
        "  ax.set_xlim(xticks[0], xticks[-1] + 2)\n",
        "  ax.set_xticks(xticks[1:])\n",
        "  ax.set_xticklabels(xticks[1:-1] + [f\"{original_size:0.2f}\"])\n",
        "  ax.plot(xticks[-1], original_accuracy, \"o\", label=\"float32\")\n",
        "\n",
        "sizes, zip_sizes, accuracies = tf.transpose(metrics)\n",
        "sizes /= 1024\n",
        "zip_sizes /= 1024\n",
        "\n",
        "fig, (axl, axr) = plt.subplots(1, 2, sharey=True, figsize=(10, 4))\n",
        "axl.plot(sizes, accuracies, \"o-\", label=\"EPR compressed\")\n",
        "axr.plot(zip_sizes, accuracies, \"o-\", label=\"EPR compressed\")\n",
        "plot_broken_xaxis(axl, sizes, original_size/1024, classifier_accuracy)\n",
        "plot_broken_xaxis(axr, zip_sizes, original_zip_size/1024, classifier_accuracy)\n",
        "\n",
        "axl.set_xlabel(\"size of model weights [kbytes]\")\n",
        "axr.set_xlabel(\"ZIP compressed on-disk model size [kbytes]\")\n",
        "axl.set_ylabel(\"accuracy\")\n",
        "axl.legend(loc=\"lower right\")\n",
        "axr.legend(loc=\"lower right\")\n",
        "axl.grid()\n",
        "axr.grid()\n",
        "for i in range(len(lambdas)):\n",
        "  axl.annotate(f\"$\\lambda = {lambdas[i]:0.0f}$\", (sizes[i], accuracies[i]),\n",
        "               xytext=(10, -5), xycoords=\"data\", textcoords=\"offset points\")\n",
        "  axr.annotate(f\"$\\lambda = {lambdas[i]:0.0f}$\", (zip_sizes[i], accuracies[i]),\n",
        "               xytext=(10, -5), xycoords=\"data\", textcoords=\"offset points\")\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajrHaFTAaLd2"
      },
      "source": [
        "プロットは、理想的にはエルボー型のサイズと精度のトレードオフを示す必要がありますが、精度メトリクスに多少のノイズがあるのは正常です。初期化によっては、曲線にねじれが生じる場合があります。\n",
        "\n",
        "正則化効果により、EPR 圧縮モデルは、$\\lambda$ の値が小さい場合、元のモデルよりもテストセットでより正確です。追加の ZIP 圧縮後のサイズを比較しても、EPR 圧縮モデルは何倍も小さくなっています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RBhdXZTzoWw"
      },
      "source": [
        "## 分類器を解凍する\n",
        "\n",
        "`CompressedDense` と `CompressedConv2D` は、フォワードパスごとに重みを解凍します。そのため、メモリが制限されたデバイスに最適ですが、圧縮解凍は、特に小さなバッチサイズの場合、計算コストが高くなる可能性があります。\n",
        "\n",
        "モデルを一度解凍し、それをさらにトレーニングや推論に使用するには、通常のレイヤーまたは圧縮可能なレイヤーを使用してモデルに戻すことができます。これは、モデルのデプロイまたはフェデレーションラーニングのシナリオで役立ちます。\n",
        "\n",
        "まず、単純なモデルに戻すと、推論を実行したり、圧縮ペナルティなしで通常のトレーニングを継続したりできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBB2-X5XzvwB"
      },
      "outputs": [],
      "source": [
        "def decompress_layer(layer):\n",
        "  if isinstance(layer, CompressedDense):\n",
        "    return CustomDense.copy(layer)\n",
        "  if isinstance(layer, CompressedConv2D):\n",
        "    return CustomConv2D.copy(layer)\n",
        "  return type(layer).from_config(layer.get_config())\n",
        "\n",
        "decompressed_classifier = tf.keras.models.clone_model(\n",
        "    compressed_classifier, clone_function=decompress_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehE2ov8U0p0G"
      },
      "outputs": [],
      "source": [
        "decompressed_accuracy = train_model(\n",
        "    decompressed_classifier, training_dataset, validation_dataset, epochs=1)\n",
        "\n",
        "print(f\"Accuracy of the compressed classifier: {compressed_accuracy:0.4f}\")\n",
        "print(f\"Accuracy of the decompressed classifier after one more epoch of training: {decompressed_accuracy:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiSCvemQ04o8"
      },
      "source": [
        "トレーニングは正則化なしで行われるため、追加エポックのトレーニング後に検証精度が低下することに注意してください。\n",
        "\n",
        "または、モデルを「圧縮可能な」モデルに変換して、圧縮ペナルティを使用して推論やさらなるトレーニングを行うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDppVUdx1BvY"
      },
      "outputs": [],
      "source": [
        "def decompress_layer_with_penalty(layer):\n",
        "  if isinstance(layer, CompressedDense):\n",
        "    return CompressibleDense.copy(layer, regularizer=regularizer)\n",
        "  if isinstance(layer, CompressedConv2D):\n",
        "    return CompressibleConv2D.copy(layer, regularizer=regularizer)\n",
        "  return type(layer).from_config(layer.get_config())\n",
        "\n",
        "decompressed_classifier = tf.keras.models.clone_model(\n",
        "    compressed_classifier, clone_function=decompress_layer_with_penalty)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJcnyOFW2IcK"
      },
      "outputs": [],
      "source": [
        "decompressed_accuracy = train_model(\n",
        "    decompressed_classifier, training_dataset, validation_dataset, epochs=1)\n",
        "\n",
        "print(f\"Accuracy of the compressed classifier: {compressed_accuracy:0.4f}\")\n",
        "print(f\"Accuracy of the decompressed classifier after one more epoch of training: {decompressed_accuracy:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ciol315T_TwQ"
      },
      "source": [
        "ここでは、追加のエポックのトレーニング後に精度が向上しています。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L",
        "xHxb-dlhMIzW"
      ],
      "name": "compression.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
