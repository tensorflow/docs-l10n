{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg02FZzDyEqd"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2mapZ9afGJ69"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMYQvJuBi7MS"
      },
      "source": [
        "# Keras 前処理レイヤーを使って構造化データを分類する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FaL4wnr22oy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">    TensorFlow.org で表示</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tutorials/structured_data/preprocessing_layers.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">    Google Colab で実行</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/structured_data/preprocessing_layers.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">    GitHub でソースを表示</a>   </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tutorials/structured_data/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a>   </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nna1tOKxyEqe"
      },
      "source": [
        "このチュートリアルでは、構造化データ（CSV のタブ区切りデータ）を分類する方法を実演します。モデルの定義には [Keras](https://www.tensorflow.org/guide/keras) を使用し、CSV の列からモデルのトレーニングに使用する特徴量にマッピングするための懸け橋として[前処理レイヤー](https://www.tensorflow.org/guide/keras/preprocessing_layers)を使用します。このチュートリアルに含まれるコードは、次のことを行います。\n",
        "\n",
        "- [Pandas](https://pandas.pydata.org/) を使って CSV ファイルを読み込みます。\n",
        "- [tf.data](https://www.tensorflow.org/guide/datasets) を使用して、行をバッチ化してシャッフルする入力パイプラインを構築します。\n",
        "- Keras 前処理レイヤーを使ってモデルをトレーニングするために使用する特徴量に、CSV のカラムをマッピングします。\n",
        "- Keras を使用して、モデルを構築、トレーニング、および評価します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5xkXCicjFQD"
      },
      "source": [
        "注意: このチュートリアルは、「[特徴量カラムを使って構造化データを分類する](https://www.tensorflow.org/tutorials/structured_data/feature_columns)」に類似しています。このバージョンでは、新しい実験的 Keras [前処理レイヤー](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing)を使用しており、`tf.feature_column` を使っていません。Keras 前処理レイヤーはより直感的であり、デプロイを単純化できるようにモデル内に簡単に含めることができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHxU1FMNpomc"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "PetFinder [データセット](https://www.kaggle.com/c/petfinder-adoption-prediction)の簡易バージョンを使用します。CSV には数千行のデータが含まれており各行にペットに関する記述、各列にその属性が含まれています。この情報を使用して、ペットが引き取り可能であるかどうかを予測します。\n",
        "\n",
        "以下は、このデータセットの説明です。数値とカテゴリカルのカラムがあることに注意してください。自由テキストのカラムもありアンスが、このチュートリアルでは使用しません。\n",
        "\n",
        "カラム | 説明 | 特徴量タイプ | データ型\n",
        "--- | --- | --- | ---\n",
        "Type | 動物の種類（犬、猫） | カテゴリカル | 文字列\n",
        "Age | ペットの年齢 | 数値 | 整数\n",
        "Breed1 | ペットの主な品種 | カテゴリカル | 文字列\n",
        "Color1 | ペットの毛色 1 | カテゴリカル | 文字列\n",
        "Color2 | ペットの毛色 2 | カテゴリカル | 文字列\n",
        "MaturitySize | 成獣時のサイズ | カテゴリカル | 文字列\n",
        "FurLength | 毛の長さ | カテゴリカル | 文字列\n",
        "Vaccinated | 予防接種済み | カテゴリカル | 文字列\n",
        "Sterilized | 不妊手術済み | カテゴリカル | 文字列\n",
        "Health | 健康状態 | カテゴリカル | 文字列\n",
        "Fee | 引き取り料 | 数値 | 整数\n",
        "Description | ペットのプロフィール | テキスト | 文字列\n",
        "PhotoAmt | アップロードされたペットの写真数 | 数値 | 整数\n",
        "AdoptionSpeed | 引き取りまでの期間 | 分類 | 整数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjFbdBldyEqf"
      },
      "source": [
        "## TensorFlow とその他のライブラリをインポートする\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_BdyQlPjfDW"
      },
      "outputs": [],
      "source": [
        "!pip install -q sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LklnLlt6yEqf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKU7RyoQGVKB"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXvBvobayEqi"
      },
      "source": [
        "## Pandas を使用してデータフレームを作成する\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/) は、構造化データの読み込みと処理を支援するユーティリティが多数含まれる Python ライブラリです。Pandas を使用し、URL からデータセットをダウンロードしてデータフレームに読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ4Ajn-YyEqj"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
        "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
        "\n",
        "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
        "                        extract=True, cache_dir='.')\n",
        "dataframe = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uiq4hoIGyXI"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3zDbrozyEqq"
      },
      "source": [
        "## ターゲット変数を作成する\n",
        "\n",
        "Kaggle コンペティションでは、ペットが引き取られるまでの期間（1 週目、1 か月目、3 か月目など）を予測することがタスクとなっていますが、このチュートリアルでは、このタスクを単純化しましょう。ここでは、このタスクを二項分類問題にし、単にペットが引き取られるかどうかのみを予測します。\n",
        "\n",
        "ラベルカラムを変更すると、0 は引き取られなかった、1 は引き取られたことを示すようになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmMDc46-yEqq"
      },
      "outputs": [],
      "source": [
        "# In the original dataset \"4\" indicates the pet was not adopted.\n",
        "dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
        "\n",
        "# Drop un-used columns.\n",
        "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp0NCbswyEqs"
      },
      "source": [
        "## データフレームを train、validation、および test に分割する\n",
        "\n",
        "ダウンロードしたデータセットは単純な CSV ファイルです。これを train、validation、および test セットに分割します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT6HdyEwyEqt"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_7uVu-xyEqv"
      },
      "source": [
        "## tf.data を使用して入力パイプラインを作成する\n",
        "\n",
        "次に、データをシャッフルしてバッチ化するために、データフレームを [tf.data](https://www.tensorflow.org/guide/datasets) でラップします。非常に大型（メモリに収まらないほどの規模）の CSV ファイルを処理している場合は、tf.data を使用してディスクから直接読み取ります。この方法は、このチュートリアルでは説明していません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r4j-1lRyEqw"
      },
      "outputs": [],
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYxIXH579uS9"
      },
      "source": [
        "入力パイプラインを作成したので、それを呼び出して、戻されるデータのフォーマットを確認しましょう。出力の可読性を維持するために、小さなバッチを使用しました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYiNH-QI96Jo"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFYir6S8HgIJ"
      },
      "outputs": [],
      "source": [
        "[(train_features, label_batch)] = train_ds.take(1)\n",
        "print('Every feature:', list(train_features.keys()))\n",
        "print('A batch of ages:', train_features['Age'])\n",
        "print('A batch of targets:', label_batch )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geqHWW54Hmte"
      },
      "source": [
        "ご覧のとおり、データセットは、データフレームの行からカラムの値にマップしているカラム名の（データフレームのカラム名）のディクショナリを返しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v50jBIuj4gb"
      },
      "source": [
        "## 前処理レイヤーの使用を実演する\n",
        "\n",
        "Keras Preprocessing Layers API を使うと、Keras ネイティブの入力処理パイプラインを構築することができます。特徴量の前処理コードを実演するために、3 つの前処理レイヤーを使用します。\n",
        "\n",
        "- [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) - データの特徴量方向の正規化。\n",
        "- [`CategoryEncoding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding) - カテゴリのエンコーディングレイヤー。\n",
        "- [`StringLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/StringLookup) - ボキャブラリから整数インデックスに文字列をマッピングします。\n",
        "- [`IntegerLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/IntegerLookup) - ボキャブラリから整数インデックスに整数をマッピングします。\n",
        "\n",
        "使用できる前処理レイヤーのリストは、[こちら](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing)をご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twXBSxnT66o8"
      },
      "source": [
        "### 数値カラム\n",
        "\n",
        "数値特徴量ごとに、各特徴量の平均が 0、標準偏差が 1 となるように Normalization() レイヤーを使用します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OosUh4kTsK_q"
      },
      "source": [
        "`get_normalization_layer` 関数は、特徴量方向の正規化を数値特徴量に適用するレイヤーを返します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6OuEKMMyEq1"
      },
      "outputs": [],
      "source": [
        "def get_normalization_layer(name, dataset):\n",
        "  # Create a Normalization layer for our feature.\n",
        "  normalizer = preprocessing.Normalization(axis=None)\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the statistics of the data.\n",
        "  normalizer.adapt(feature_ds)\n",
        "\n",
        "  return normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpKgUDyk69bM"
      },
      "outputs": [],
      "source": [
        "photo_count_col = train_features['PhotoAmt']\n",
        "layer = get_normalization_layer('PhotoAmt', train_ds)\n",
        "layer(photo_count_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foWY00YBUx9N"
      },
      "source": [
        "注意: 多数の特徴量（数百個以上）がある場合は、先にそれらを連結してから単一の [normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) レイヤーを使用するとより効率的です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVD--2WZ7vmh"
      },
      "source": [
        "### カテゴリカルカラム\n",
        "\n",
        "このデータセットでは、Type は文字列として表現されています（'Dog' または 'Cat'）。文字列を直接モデルに注入することはできないため、前処理レイヤーを使って文字列をワンホットベクトルとして表現します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWlkOPwMsxdv"
      },
      "source": [
        "`get_category_encoding_layer` 関数は、ボキャブラリの値を整数インデックスにマッピングして特徴量をワンホットエンコーディングするレイヤーを返します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmgaeRjlDoUO"
      },
      "outputs": [],
      "source": [
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a StringLookup layer which will turn strings into integer indices\n",
        "  if dtype == 'string':\n",
        "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
        "  else:\n",
        "    index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Create a Discretization for our integer indices.\n",
        "  encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
        "\n",
        "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
        "  # layer so we can use them, or include them in the functional model later.\n",
        "  return lambda feature: encoder(index(feature))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2t2ff9K8PcT"
      },
      "outputs": [],
      "source": [
        "type_col = train_features['Type']\n",
        "layer = get_category_encoding_layer('Type', train_ds, 'string')\n",
        "layer(type_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6eDongw8knz"
      },
      "source": [
        "数値を直接モデルに注入せずに、それらの入力のワンホットエンコーディングを使用することがよくあります。ペットの年齢を表す未加工のデータを考察しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FjBioQ38oNE"
      },
      "outputs": [],
      "source": [
        "type_col = train_features['Age']\n",
        "category_encoding_layer = get_category_encoding_layer('Age', train_ds,\n",
        "                                                      'int64', 5)\n",
        "category_encoding_layer(type_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiE0glOPkMyh"
      },
      "source": [
        "## 使用するカラムを選択する\n",
        "\n",
        "さまざまな種類の前処理レイヤーが使用される様子を見てきましたが、今後は、それを使ってモデルをトレーニングする方法を見てみましょう。[Keras-functional API](https://www.tensorflow.org/guide/keras/functional) を使用して、モデルを構築します。Keras functional API は、[tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) API より柔軟なモデルを作成するのに適しています。\n",
        "\n",
        "このチュートリアルでは、前処理レイヤーを使用するために必要な全コード（mechanic）などを示すことを目的としています。モデルをトレーニングするためにいくつかのカラムが任意に選択されています。\n",
        "\n",
        "重要ポイント: 正確なモデルの構築を目的としている場合は、より大きなデータセットを独自に用意し、どの特徴量を含めるのが最も意義が高く、どのように表現すrべきかについてよく考えましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj1GoHSZ9R3H"
      },
      "source": [
        "最初の方で、入力パイプラインを実演するために小さなバッチを使用しました。今度はより大きなバッチサイズで新しい入力パイプラインを作成してみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rcv2kQTTo23h"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3RBa51VkaAn"
      },
      "outputs": [],
      "source": [
        "all_inputs = []\n",
        "encoded_features = []\n",
        "\n",
        "# Numeric features.\n",
        "for header in ['PhotoAmt', 'Fee']:\n",
        "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "  normalization_layer = get_normalization_layer(header, train_ds)\n",
        "  encoded_numeric_col = normalization_layer(numeric_col)\n",
        "  all_inputs.append(numeric_col)\n",
        "  encoded_features.append(encoded_numeric_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FOMGfZflhoA"
      },
      "outputs": [],
      "source": [
        "# Categorical features encoded as integers.\n",
        "age_col = tf.keras.Input(shape=(1,), name='Age', dtype='int64')\n",
        "encoding_layer = get_category_encoding_layer('Age', train_ds, dtype='int64',\n",
        "                                             max_tokens=5)\n",
        "encoded_age_col = encoding_layer(age_col)\n",
        "all_inputs.append(age_col)\n",
        "encoded_features.append(encoded_age_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8C8xyiXm-Ie"
      },
      "outputs": [],
      "source": [
        "# Categorical features encoded as string.\n",
        "categorical_cols = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
        "                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1']\n",
        "for header in categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string',\n",
        "                                               max_tokens=5)\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHSnhz2fyEq3"
      },
      "source": [
        "## モデルを作成、コンパイル、およびトレーニングする\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDGyN_wpo0XS"
      },
      "source": [
        "エンドツーエンドのモデルを作成できるようになりました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yrj-_pr6jyL"
      },
      "outputs": [],
      "source": [
        "all_features = tf.keras.layers.concatenate(encoded_features)\n",
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "model = tf.keras.Model(all_inputs, output)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6mNMfG6yEq5"
      },
      "source": [
        "接続性グラフを視覚化しましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7Bkx4c7yEq5"
      },
      "outputs": [],
      "source": [
        "# rankdir='LR' is used to make the graph horizontal.\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CED6OStLyEq7"
      },
      "source": [
        "### モデルをトレーニングする\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQfE3PC6yEq8"
      },
      "outputs": [],
      "source": [
        "model.fit(train_ds, epochs=10, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8N2uAdU2Cni"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmZMnTKaCZda"
      },
      "source": [
        "## 新しいデータの推論\n",
        "\n",
        "重要ポイント: 開発したモデルには前処理コードが含まれているため、直接 CSV ファイルから行を分類できるようになりました。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xkOlK8Zweeh"
      },
      "source": [
        "Keras モデルを保存して、再読み込みすることができます。TensorFlow モデルの詳細については、[こちら](https://www.tensorflow.org/tutorials/keras/save_and_load)のチュートリアルをご覧ください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH9Zy1sBvwOH"
      },
      "outputs": [],
      "source": [
        "model.save('my_pet_classifier')\n",
        "reloaded_model = tf.keras.models.load_model('my_pet_classifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D973plJrdwQ9"
      },
      "source": [
        "`model.predict()` を呼び出すだけで、新しいサンプルの予測を得ることができます。以下の 2 つを行ってください。\n",
        "\n",
        "1. バッチに次元をを持たせるために、スカラーをリストにラップします（モデルは、単一のサンプルではなく、データのバッチのみを処理します）。\n",
        "2. 各特徴量で `convert_to_tensor` を呼び出します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKq4pxtdDa7i"
      },
      "outputs": [],
      "source": [
        "sample = {\n",
        "    'Type': 'Cat',\n",
        "    'Age': 3,\n",
        "    'Breed1': 'Tabby',\n",
        "    'Gender': 'Male',\n",
        "    'Color1': 'Black',\n",
        "    'Color2': 'White',\n",
        "    'MaturitySize': 'Small',\n",
        "    'FurLength': 'Short',\n",
        "    'Vaccinated': 'No',\n",
        "    'Sterilized': 'No',\n",
        "    'Health': 'Healthy',\n",
        "    'Fee': 100,\n",
        "    'PhotoAmt': 2,\n",
        "}\n",
        "\n",
        "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "predictions = reloaded_model.predict(input_dict)\n",
        "prob = tf.nn.sigmoid(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This particular pet had a %.1f percent probability \"\n",
        "    \"of getting adopted.\" % (100 * prob)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJQQZEiH2FaB"
      },
      "source": [
        "重要ポイント: 通常、データベースの規模が大きく複雑であるほど、ディープラーニングの結果がよくなります。このチュートリアルのデータセットのように、小さなデータセットを使用する場合は、決定木またはランダムフォレストを強力なベースラインとして使用することをお勧めします。このチュートリアルでは、構造化データとの連携の仕組みを実演することが目的であるため、コードは将来的に独自のデータセットを使用する際の出発点として使用することができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0QAY2Tb2HYG"
      },
      "source": [
        "## 次のステップ\n",
        "\n",
        "構造化データの分類をさらに学習するには、自分で試すのが最善です。別のデータセットを使用し、上記に似たコードを使用し、モデルのトレーニングと分類をおこなうと良いでしょう。精度を改善するには、モデルに含める特徴量とその表現方法を吟味ししてください。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocessing_layers.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
