{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4espuLKJiA"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DjZQV2njKJ3U"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTL0TERThT6z"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/transfer_learning_audio\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.orgで表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">GoogleColabで実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHubで表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロードする</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TFハブモデルを参照してください</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2madPFAGHb3"
      },
      "source": [
        "# YAMNetを用いた転移学習による環境音分類\n",
        "\n",
        "[YAMNet](https://tfhub.dev/google/yamnet/1) は、笑い声、動物の吠える声、サイレン音などを含む [521 種](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv)の音声イベントを予測できるトレーニング済みのディープニューラルネットワークです。\n",
        "\n",
        "このチュートリアルでは次の方法について学ぶことができます:\n",
        "\n",
        "- YAMNetをロードし、推論に利用する\n",
        "- YAMNetのエンベディングを利用した新しいモデルを作成し、猫と犬の音を分類する\n",
        "- 作成したモデルを評価しエクスポートする\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mdp2TpBh96Y"
      },
      "source": [
        "## <br>TensorFlow およびその他のライブラリのインポート\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCcKYqu_hvKe"
      },
      "source": [
        "まず、[TensorFlow I / Oを](https://www.tensorflow.org/io)インストールすることから始めます。これにより、オーディオファイルをディスクから簡単にロードできるようになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urBpRWDHTHHU"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l3nqdWVF-kC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_io as tfio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ZhybCnt_bM"
      },
      "source": [
        "## YAMNetについて\n",
        "\n",
        "[YAMNet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet) は、[MobileNetV1](https://arxiv.org/abs/1704.04861) という深さ方向に分離可能な畳み込みアーキテクチャを使用するトレーニング済みのニューラルネットワークです。音声の波形を入力として使用し、[AudioSet](http://g.co/audioset) コーパスの 521 種の各音声イベントに対して個別の予測を行えます。\n",
        "\n",
        "内部的には、モデルは音声信号から「フレーム」を抽出し、これらのフレームをバッチ処理します。このバージョンのモデルは長さが 0.96 秒のフレームを使用し、0.48 秒ごとに 1 つのフレームを抽出します。\n",
        "\n",
        "モデルは、値域 `[-1.0, +1.0]` の単精度 16 kHz サンプルとして表される、任意の長さの波形を、1-D float32 テンソルまたは NumPy 配列で受け入れます。このチュートリアルには、WAV ファイルをサポートされたフォーマットに変換するのに役立つコードが含まれています。\n",
        "\n",
        "モデルは、クラススコア、埋め込み（転移学習に使用）、およびログメル[スペクトログラム](https://www.tensorflow.org/tutorials/audio/simple_audio#spectrogram)を含む 3 つの出力を返します。詳細については、[こちら](https://tfhub.dev/google/yamnet/1)をご覧ください。\n",
        "\n",
        "YAMNet には、高レベル特徴量抽出器（1,024 次元埋め込み出力）としての特定の使用方法があります。ベース（YAMNet）モデルの入力特徴量を使用して、それらを、1 つの `tf.keras.layers.Dense` という非表示レイヤーで構成されるより浅いモデルにフィードします。その後、ネットワークを*多数のラベル付きデータを使ったりエンドツーエンドでトレーニングすることなく*、少量のデータで音声分類トレーニングを行います。（これは[TensorFlow Hub を使った画像分類の転移学習](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)に似ています。詳しくはそちらをご覧ください。）\n",
        "\n",
        "それでは、モデルをテストし、音声の分類結果を確認してみましょう。その後、データの前処理パイプラインを構築していきます。\n",
        "\n",
        "### TensorFlowハブからYAMNetを読み込む\n",
        "\n",
        "[Tensorflow Hub](https://tfhub.dev/) にある事前トレーニング済みの YAMNet を使用して、サウンドファイルから埋め込みを抽出します。\n",
        "\n",
        "TensorFlow Hubからモデルをロードするのは簡単です。モデルを選択し、そのURLをコピー、そして `load`関数を使用します。\n",
        "\n",
        "注意: モデルからドキュメントを読み取るには、ブラウザにモデルの URL を入力してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06CWkBV5v3gr"
      },
      "outputs": [],
      "source": [
        "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "yamnet_model = hub.load(yamnet_model_handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmrPJ0GHw9rr"
      },
      "source": [
        "モデルが読み込まれたら、[YAMNet の基本的な使用に関するチュートリアル](https://www.tensorflow.org/hub/tutorials/yamnet)に従って、推論を実行するサンプル WAV ファイルをダウンロードします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5i6xktEq00P"
      },
      "outputs": [],
      "source": [
        "testing_wav_file_name = tf.keras.utils.get_file('miaow_16k.wav',\n",
        "                                                'https://storage.googleapis.com/audioset/miaow_16k.wav',\n",
        "                                                cache_dir='./',\n",
        "                                                cache_subdir='test_data')\n",
        "\n",
        "print(testing_wav_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBm9y9iV2U_-"
      },
      "source": [
        "音声ファイルの読み込む関数が必要です。この関数は、後でトレーニングデータを操作する際にも使用します。（音声ファイルとラベルの読み取りに関する詳細は、[単純な音声の認識](https://www.tensorflow.org/tutorials/audio/simple_audio#reading_audio_files_and_their_labels)をご覧ください。）\n",
        "\n",
        "注意: `load_wav_16k_mono` から返される `wav_data` はすでに `[-1.0, 1.0]` の値域に正規化されています（詳細は、[TF Hub にある YAMNet のドキュメント](https://tfhub.dev/google/yamnet/1)をご覧ください）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwc9Wrdg2EtY"
      },
      "outputs": [],
      "source": [
        "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
        "\n",
        "@tf.function\n",
        "def load_wav_16k_mono(filename):\n",
        "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(\n",
        "          file_contents,\n",
        "          desired_channels=1)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRqpjkwB0Jjw"
      },
      "outputs": [],
      "source": [
        "testing_wav_data = load_wav_16k_mono(testing_wav_file_name)\n",
        "\n",
        "_ = plt.plot(testing_wav_data)\n",
        "\n",
        "# Play the audio file.\n",
        "display.Audio(testing_wav_data, rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z6rqlEz20YB"
      },
      "source": [
        "### クラスマッピングのロード\n",
        "\n",
        "読み込むクラス名は YAMNet が認識できるものであることが重要です。マッピングファイルは CSV 形式で `yamnet_model.class_map_path()` にあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gyj23e_3Mgr"
      },
      "outputs": [],
      "source": [
        "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\n",
        "class_names =list(pd.read_csv(class_map_path)['display_name'])\n",
        "\n",
        "for name in class_names[:20]:\n",
        "  print(name)\n",
        "print('...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xbycDnT40u0"
      },
      "source": [
        "### 推論の実行\n",
        "\n",
        "YAMNet は、フレームレベルのクラススコア（フレームごとに 521 個のスコア）を提供します。クリップレベルでの予測を決定するために、スコアをフレーム全体でクラスごとに集計することができます（平均または最大集計などを使用します）。これは、`scores_np.mean(axis=0)` によって以下のように行われます。最後に、クリップレベルで最高スコアのクラスを見つけるには、521 個の集計スコアの最大値を取得します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT0otp-A4Y3u"
      },
      "outputs": [],
      "source": [
        "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n",
        "class_scores = tf.reduce_mean(scores, axis=0)\n",
        "top_class = tf.math.argmax(class_scores)\n",
        "inferred_class = class_names[top_class]\n",
        "\n",
        "print(f'The main sound is: {inferred_class}')\n",
        "print(f'The embeddings shape: {embeddings.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBaLNg5H5IWa"
      },
      "source": [
        "注意: モデルは動物の声や音を正しく推論しました。このチュートリアルでの目標は、モデルの特定のクラスの精度を上げることです。また、モデルがフレームごとに 1 つの埋め込み（計 13 個の埋め込み）を生成したことにも注意してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmthELBg1A2-"
      },
      "source": [
        "## ESC-50 dataset\n",
        "\n",
        "[ESC-50 データセット](https://github.com/karolpiczak/ESC-50#repository-content)（[Piczak, 2015](https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf)）は、5 秒の長さの環境音声データが 2,000 個含まれるラベル付きのコレクションです。データセットは 50 個のクラスと、クラス当たり 40 個の Example で構成されています。\n",
        "\n",
        "データセットをダウンロードして抽出します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWobqK8JmZOU"
      },
      "outputs": [],
      "source": [
        "_ = tf.keras.utils.get_file('esc-50.zip',\n",
        "                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcruxiuX1cO5"
      },
      "source": [
        "### データの観察\n",
        "\n",
        "<br>各ファイルのメタデータは次のcsvファイルで指定されています。 `./datasets/ESC-50-master/meta/esc50.csv`<br>\n",
        "\n",
        "また、すべてのオーディオファイルは次のディレクトリにあります。<br>`.datasets/ESC-50-master/audio/`\n",
        "\n",
        "マッピングを使用して pandas `DataFrame` を作成し、それを使用してデータをよりわかりやすく表示します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwmLygPrMAbH"
      },
      "outputs": [],
      "source": [
        "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
        "base_data_path = './datasets/ESC-50-master/audio/'\n",
        "\n",
        "pd_data = pd.read_csv(esc50_csv)\n",
        "pd_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d4rHBEQ2QAU"
      },
      "source": [
        "### データのフィルタリング\n",
        "\n",
        "データが `DataFrame` に格納されたので、変換を適用しましょう。\n",
        "\n",
        "- 行をフィルタリングして、選択したクラス（`dog` と `cat`）のみを使用します。他のクラスを使用する場合は、ここで選択してください。\n",
        "- 後での読み込み作業を簡単に行えるように、ファイル名をフルパスに変更します。\n",
        "- ターゲットを特定の範囲内に変更します。この例では、`dog` は `0` の位置のままですが、`cat` は元の `5` の値から `1` に変わります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFnEoQjgs14I"
      },
      "outputs": [],
      "source": [
        "my_classes = ['dog', 'cat']\n",
        "map_class_to_id = {'dog':0, 'cat':1}\n",
        "\n",
        "filtered_pd = pd_data[pd_data.category.isin(my_classes)]\n",
        "\n",
        "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
        "filtered_pd = filtered_pd.assign(target=class_id)\n",
        "\n",
        "full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))\n",
        "filtered_pd = filtered_pd.assign(filename=full_path)\n",
        "\n",
        "filtered_pd.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkDcBS-aJdCz"
      },
      "source": [
        "### オーディオファイルのロードとエンベディングの取得\n",
        "\n",
        "ここでは、`load_wav_16k_mono` を適用して、モデルに使用する WAV データを準備します。\n",
        "\n",
        "WAV データから埋め込みを抽出すると、形状 `(N, 1024)` の配列が得られます。`N` は、YAMNet が検出したフレーム数です（音声の 0.48 秒あたり 1 フレーム）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKDT5RomaDKO"
      },
      "source": [
        "このモデルは角フレームを 1 つの入力として使用するため、1 行当たり 1つのフレームを持つ新しい列を作成する必要があります。また、新しい行を正しく反映させるために、ラベルと `fold` 列を拡張する必要もあります。\n",
        "\n",
        "拡張された `fold` 列には元の値が保持されます。分割を行う際に異なる Split に同じ音声が含まれてしまう可能性があり、検証とテストのステップの効果が低くなってしまうため、フレームを混ぜることはできません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5Rq3_PyKLtU"
      },
      "outputs": [],
      "source": [
        "filenames = filtered_pd['filename']\n",
        "targets = filtered_pd['target']\n",
        "folds = filtered_pd['fold']\n",
        "\n",
        "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
        "main_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsEfovDVAHGY"
      },
      "outputs": [],
      "source": [
        "def load_wav_for_map(filename, label, fold):\n",
        "  return load_wav_16k_mono(filename), label, fold\n",
        "\n",
        "main_ds = main_ds.map(load_wav_for_map)\n",
        "main_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0tG8DBNAHcE"
      },
      "outputs": [],
      "source": [
        "# applies the embedding extraction model to a wav data\n",
        "def extract_embedding(wav_data, label, fold):\n",
        "  ''' run YAMNet to extract embedding from the wav data '''\n",
        "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
        "  num_embeddings = tf.shape(embeddings)[0]\n",
        "  return (embeddings,\n",
        "            tf.repeat(label, num_embeddings),\n",
        "            tf.repeat(fold, num_embeddings))\n",
        "\n",
        "# extract embedding\n",
        "main_ds = main_ds.map(extract_embedding).unbatch()\n",
        "main_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdfPIeD0Qedk"
      },
      "source": [
        "### データの分割\n",
        "\n",
        "`fold` 列を使って、データセットをテストセット、検証セット、テストセットに分割します。\n",
        "\n",
        "ESC-50 は、同じ元のソースが必ず同じ `fold` に含まれるように、5 つの均一なサイズの相互検証 `fold` に構成されます。詳細は、『[ESC: Dataset for Environmental Sound Classification](https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf)』論文をご覧ください。\n",
        "\n",
        "最後のステップでは、データセットから `fold` 列を削除します。この列は、トレーニング中に使用されません。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZYvlFiVsffC"
      },
      "outputs": [],
      "source": [
        "cached_ds = main_ds.cache()\n",
        "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)\n",
        "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)\n",
        "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)\n",
        "\n",
        "# remove the folds column now that it's not needed anymore\n",
        "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
        "\n",
        "train_ds = train_ds.map(remove_fold_column)\n",
        "val_ds = val_ds.map(remove_fold_column)\n",
        "test_ds = test_ds.map(remove_fold_column)\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5PaMwvtcAIe"
      },
      "source": [
        "## モデルの作成\n",
        "\n",
        "ここまでで、ほとんどの作業を終えました！次は、1 つの非表示レイヤーと 2 つの出力でサウンドから犬と猫を識別する非常に単純な [Sequential](https://www.tensorflow.org/guide/keras/sequential_model) モデルを定義します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYCE0Fr1GpN3"
      },
      "outputs": [],
      "source": [
        "my_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
        "                          name='input_embedding'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(my_classes))\n",
        "], name='my_model')\n",
        "\n",
        "my_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1qgH35HY0SE"
      },
      "outputs": [],
      "source": [
        "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                            patience=3,\n",
        "                                            restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3sj84eOZ3pk"
      },
      "outputs": [],
      "source": [
        "history = my_model.fit(train_ds,\n",
        "                       epochs=20,\n",
        "                       validation_data=val_ds,\n",
        "                       callbacks=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbraYKYpdoE"
      },
      "source": [
        "テストデータに対して `evaluate` メソッドを実行し、過学習がないことを確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4Nh5nec3Sky"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = my_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cid-qIrIpqHS"
      },
      "source": [
        "チェック完了です！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCKZonrJcXab"
      },
      "source": [
        "## モデルのテスト\n",
        "\n",
        "次に、先程例として視聴したデータに、YAMNetを適用して取得したエンベディングを用いて、モデルを試してみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79AFpA3_ctCF"
      },
      "outputs": [],
      "source": [
        "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n",
        "result = my_model(embeddings).numpy()\n",
        "\n",
        "inferred_class = my_classes[result.mean(axis=0).argmax()]\n",
        "print(f'The main sound is: {inferred_class}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2yleeev645r"
      },
      "source": [
        "## WAV ファイルを入力として直接取れつ形式でモデルを保存する\n",
        "\n",
        "現状、モデルにエンベディングを入力として与えると、モデルは機能します。\n",
        "\n",
        "ただし、実世界のシナリオでは、音声データを直接入力として使用したいものです。\n",
        "\n",
        "そのようにするには、YAMNet とここで作成したモデルを合わせて、他のアプリケーションにエクスポートできる単一のモデルにします。\n",
        "\n",
        "モデルの結果を使いやすくするために、最終レイヤーを `reduce_mean` 演算にします。このモデルをサービングに使用する場合（これについては、チュートリアルの後の方で説明します）、最終レイヤーの名前が必要になります。これを定義しない場合、TensorFlow はインクリメンタルで名前を自動的に定義するため、モデルをトレーニングするたびに名前が変化し、テストが困難になります。生の TensorFlow 演算を使用する際にレイヤーに名前を付けることはできません。この問題に対処するには、`reduce_mean` を適用するカスタムレイヤーを作成し、`'classifier'` と名付けます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUVCI2Suunpw"
      },
      "outputs": [],
      "source": [
        "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, axis=0, **kwargs):\n",
        "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.math.reduce_mean(input, axis=self.axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE_Npm0nzlwc"
      },
      "outputs": [],
      "source": [
        "saved_model_path = './dogs_and_cats_yamnet'\n",
        "\n",
        "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
        "embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
        "                                            trainable=False, name='yamnet')\n",
        "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
        "serving_outputs = my_model(embeddings_output)\n",
        "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
        "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
        "serving_model.save(saved_model_path, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-0bY5FMme1C"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(serving_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btHQDN9mqxM_"
      },
      "source": [
        "保存したモデルをロードして、期待どおりに機能することを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkYVpJS72WWB"
      },
      "outputs": [],
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BkmvvNzq49l"
      },
      "source": [
        "さて、最後のテストです。サウンドデータに対して、モデルは正しい結果を返すでしょうか？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeXtD5HO28y-"
      },
      "outputs": [],
      "source": [
        "reloaded_results = reloaded_model(testing_wav_data)\n",
        "cat_or_dog = my_classes[tf.math.argmax(reloaded_results)]\n",
        "print(f'The main sound is: {cat_or_dog}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRrOcBYTUgwn"
      },
      "source": [
        "新しいモデルをサービング設定で試したい場合は、「serving_default」シグネチャを使用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycC8zzDSUG2s"
      },
      "outputs": [],
      "source": [
        "serving_results = reloaded_model.signatures['serving_default'](testing_wav_data)\n",
        "cat_or_dog = my_classes[tf.math.argmax(serving_results['classifier'])]\n",
        "print(f'The main sound is: {cat_or_dog}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da7blblCHs8c"
      },
      "source": [
        "## （任意）付加的ないくつかのテスト\n",
        "\n",
        "モデルの準備が完了しました。\n",
        "\n",
        "テストデータセットのYAMNetと比較してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDf5MASIIN1z"
      },
      "outputs": [],
      "source": [
        "test_pd = filtered_pd.loc[filtered_pd['fold'] == 5]\n",
        "row = test_pd.sample(1)\n",
        "filename = row['filename'].item()\n",
        "print(filename)\n",
        "waveform = load_wav_16k_mono(filename)\n",
        "print(f'Waveform values: {waveform}')\n",
        "_ = plt.plot(waveform)\n",
        "\n",
        "display.Audio(waveform, rate=16000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYUzFxYJIcE1"
      },
      "outputs": [],
      "source": [
        "# Run the model, check the output.\n",
        "scores, embeddings, spectrogram = yamnet_model(waveform)\n",
        "class_scores = tf.reduce_mean(scores, axis=0)\n",
        "top_class = tf.math.argmax(class_scores)\n",
        "inferred_class = class_names[top_class]\n",
        "top_score = class_scores[top_class]\n",
        "print(f'[YAMNet] The main sound is: {inferred_class} ({top_score})')\n",
        "\n",
        "reloaded_results = reloaded_model(waveform)\n",
        "your_top_class = tf.math.argmax(reloaded_results)\n",
        "your_inferred_class = my_classes[your_top_class]\n",
        "class_probabilities = tf.nn.softmax(reloaded_results, axis=-1)\n",
        "your_top_score = class_probabilities[your_top_class]\n",
        "print(f'[Your model] The main sound is: {your_inferred_class} ({your_top_score})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8Tsym8Rq-0V"
      },
      "source": [
        "## 次のステップ\n",
        "\n",
        "犬と猫のサウンドを分類するモデルを作成しました。同じ考え方で別のデータセットを使用すると、鳥の鳴き声に基づく[鳥の音響識別器](https://www.kaggle.com/c/birdclef-2021/)を構築するといったことが可能になります。\n",
        "\n",
        "ソーシャルメディアで皆さんのプロジェクトを TensorFlow チームに知らせてください！\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transfer_learning_audio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
