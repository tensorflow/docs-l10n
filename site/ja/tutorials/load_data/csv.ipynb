{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DweYe9FcbMK_"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AVV2e0XKbJeX"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtoed20cRJJ"
      },
      "source": [
        "# CSV データを読み込む"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ap_W4aQcgNT"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/csv\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colabで実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHubでソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-3Xbt0FfGfs"
      },
      "source": [
        "このチュートリアルでは、TensorFlow で CSV データを使用する方法の例を示します。\n",
        "\n",
        "これには2つの主要な部分があります。\n",
        "\n",
        "1. **Loading the data off disk**\n",
        "2. **Pre-processing it into a form suitable for training.**\n",
        "\n",
        "このチュートリアルでは、読み込みに焦点を当て、前処理の簡単な例をいくつか実演します。前処理レイヤーの使用方法の詳細については、[前処理レイヤーの使用](https://www.tensorflow.org/guide/keras/preprocessing_layers)ガイドと [Keras 前処理レイヤーを使用した構造化データの分類](../structured_data/preprocessing_layers.ipynb)チュートリアルを参照してください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZhJYbJxHNGJ"
      },
      "source": [
        "## インメモリデータ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny5TEgcmHjVx"
      },
      "source": [
        "小さな CSV データセットの場合、TensorFlow モデルをトレーニングする最も簡単な方法は、pandas データフレームまたは NumPy 配列としてメモリを読み込むことです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgpBOuU8PGFf"
      },
      "source": [
        "比較的単純な例は、[アワビデータセット](https://archive.ics.uci.edu/ml/datasets/abalone)です。\n",
        "\n",
        "- データセットが小さい。\n",
        "- すべての入力特徴量は、制限された範囲の浮動小数点値です。\n",
        "\n",
        "データを[ pandas `DataFrame` ](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)にダウンロードする方法は次のとおりです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZVExo9DKoNz"
      },
      "outputs": [],
      "source": [
        "abalone_train = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
        "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
        "\n",
        "abalone_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP22mdyPQ1_t"
      },
      "source": [
        "データセットには、海のカタツムリの一種である[アワビ](https://en.wikipedia.org/wiki/Abalone)の一連の測定値が含まれています。\n",
        "\n",
        "![an abalone shell](https://tensorflow.org/images/abalone_shell.jpg)\n",
        "\n",
        "[「アワビ貝殻」](https://www.flickr.com/photos/thenickster/16641048623/) (作成者：[Nicki Dugan Pogue](https://www.flickr.com/photos/thenickster/)、CC BY-SA 2.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlfGrk_9N-wf"
      },
      "source": [
        "このデータセットの名目上のタスクは、他の測定値から年齢を予測することなので、トレーニング用に特徴とラベルを分離します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udOnDJOxNi7p"
      },
      "outputs": [],
      "source": [
        "abalone_features = abalone_train.copy()\n",
        "abalone_labels = abalone_features.pop('Age')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seK9n71-UBfT"
      },
      "source": [
        "このデータセットでは、すべての特徴を同じように扱います。特徴を単一の NumPy アレイにパックします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp3N5McbUMwb"
      },
      "outputs": [],
      "source": [
        "abalone_features = np.array(abalone_features)\n",
        "abalone_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1yFOxLOdxh"
      },
      "source": [
        "次に、回帰モデルで年齢を予測します。入力テンソルは 1 つしかないため、ここでは `keras.Sequential` モデルで十分です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8zzNrZqOmfB"
      },
      "outputs": [],
      "source": [
        "abalone_model = tf.keras.Sequential([\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
        "                      optimizer = tf.keras.optimizers.Adam())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6IWeP78O2wE"
      },
      "source": [
        "モデルをトレーニングするには、特徴とラベルを`Model.fit`に渡します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZdpCD92SN3Z"
      },
      "outputs": [],
      "source": [
        "abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GapLOj1OOTQH"
      },
      "source": [
        "CSV データを使用してモデルをトレーニングする最も基本的な方法を見てきました。次に、前処理を適用して数値列を正規化する方法を学習します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B87Rd1SOUv02"
      },
      "source": [
        "## 基本的な前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrB2Jd-U0Vt"
      },
      "source": [
        "モデルへの入力を正規化することをお勧めします。Keras 前処理レイヤーは、この正規化をモデルに組み込むための便利な方法を提供します。\n",
        "\n",
        "`tf.keras.layers.Normalization` レイヤーは、各列の平均と分散を事前に計算し、これらを使用してデータを正規化します。\n",
        "\n",
        "まず、レイヤーを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2WQpDU5VRk7"
      },
      "outputs": [],
      "source": [
        "normalize = layers.Normalization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGgEZE-7Vpt6"
      },
      "source": [
        "次に、`Normalization.adapt()` メソッドを使用して、正規化レイヤーをデータに適合させます。\n",
        "\n",
        "注意: トレーニングデータは、`PreprocessingLayer.adapt` メソッドでのみ使用してください。検証データやテストデータは使用しないでください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WgOPIiOVpLg"
      },
      "outputs": [],
      "source": [
        "normalize.adapt(abalone_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE6vh0byV7cE"
      },
      "source": [
        "次に、モデルで正規化レイヤーを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quPcZ9dTWA9A"
      },
      "outputs": [],
      "source": [
        "norm_abalone_model = tf.keras.Sequential([\n",
        "  normalize,\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "norm_abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
        "                           optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "norm_abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuqj601Qw0Ml"
      },
      "source": [
        "## 混合データ型\n",
        "\n",
        "The \"Titanic\" dataset contains information about the passengers on the Titanic. The nominal task on this dataset is to predict who survived.\n",
        "\n",
        "![The Titanic](images/csv/Titanic.jpg)\n",
        "\n",
        "Image [from Wikimedia](https://commons.wikimedia.org/wiki/File:RMS_Titanic_3.jpg)\n",
        "\n",
        "The raw data can easily be loaded as a Pandas `DataFrame`, but is not immediately usable as input to a TensorFlow model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS-dBMpuYMnz"
      },
      "outputs": [],
      "source": [
        "titanic = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8rCGIK1ZzKx"
      },
      "outputs": [],
      "source": [
        "titanic_features = titanic.copy()\n",
        "titanic_labels = titanic_features.pop('survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urHOwpCDYtcI"
      },
      "source": [
        "データ型と範囲が異なるため、特徴を NumPy 配列に単純にスタックして、`tf.keras.Sequential` モデルに渡すことはできません。各列は個別に処理する必要があります。\n",
        "\n",
        "1 つのオプションとして、データをオフラインで前処理して（任意のツールを使用して）、カテゴリカル列を数値列に変換してから、処理された出力を TensorFlow モデルに渡すことができます。このアプローチの欠点は、モデルを保存してエクスポートすると、前処理が一緒に保存されないことです。Keras 前処理レイヤーはモデルの一部であるため、この問題を回避できます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bta4Sx0Zau5v"
      },
      "source": [
        "この例では、[Keras Functional API](https://www.tensorflow.org/guide/keras/functional) を使用して前処理ロジックを実装するモデルを構築します。[サブクラス化](https://www.tensorflow.org/guide/keras/custom_layers_and_models)によって行うこともできます。\n",
        "\n",
        "Functional API は、「シンボリック」テンソルで動作します。 通常の「eager」テンソルには値があります。 対照的に、これらの「シンボリック」テンソルはそうではありません。代わりに、実行されている演算を追跡し、後で実行できる計算の表現を構築します。 簡単な例を次に示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "730F16_97D-3"
      },
      "outputs": [],
      "source": [
        "# Create a symbolic input\n",
        "input = tf.keras.Input(shape=(), dtype=tf.float32)\n",
        "\n",
        "# Perform a calculation using the input\n",
        "result = 2*input + 1\n",
        "\n",
        "# the result doesn't have a value\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtcNXWB18kMJ"
      },
      "outputs": [],
      "source": [
        "calc = tf.keras.Model(inputs=input, outputs=result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUGQOUqZ8sa-"
      },
      "outputs": [],
      "source": [
        "print(calc(1).numpy())\n",
        "print(calc(2).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNS9lT7f6_U2"
      },
      "source": [
        "前処理モデルを構築するには、まず、CSV 列の名前とデータ型に一致する一連のシンボリック `keras.Input` オブジェクトを構築します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WODe_1da3yw"
      },
      "outputs": [],
      "source": [
        "inputs = {}\n",
        "\n",
        "for name, column in titanic_features.items():\n",
        "  dtype = column.dtype\n",
        "  if dtype == object:\n",
        "    dtype = tf.string\n",
        "  else:\n",
        "    dtype = tf.float32\n",
        "\n",
        "  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
        "\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaheJFmymq8l"
      },
      "source": [
        "前処理ロジックの最初のステップは、数値入力を連結して、正規化レイヤーを介して実行することです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPRC_E6rkp8D"
      },
      "outputs": [],
      "source": [
        "numeric_inputs = {name:input for name,input in inputs.items()\n",
        "                  if input.dtype==tf.float32}\n",
        "\n",
        "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
        "norm = layers.Normalization()\n",
        "norm.adapt(np.array(titanic[numeric_inputs.keys()]))\n",
        "all_numeric_inputs = norm(x)\n",
        "\n",
        "all_numeric_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JoR45Uj712l"
      },
      "source": [
        "シンボリック前処理の結果をすべて収集して、後で連結します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7jIJw5XntdN"
      },
      "outputs": [],
      "source": [
        "preprocessed_inputs = [all_numeric_inputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Hryylyosfm"
      },
      "source": [
        "文字列入力の場合は、`tf.keras.layers.StringLookup` 関数を使用して、文字列から語彙の整数インデックスにマップします。次に、`tf.keras.layers.CategoryEncoding` を使用して、インデックスをモデルに適した `float32` データに変換します。\n",
        "\n",
        "`tf.keras.layers.CategoryEncoding` レイヤーのデフォルト設定は、入力ごとにワンホットベクターを作成します。`tf.keras.layers.Embedding` も機能します。 このトピックの詳細については、[前処理レイヤーガイド](https://www.tensorflow.org/guide/keras/preprocessing_layers)および [Keras 前処理レイヤーを使用して構造化データを分類する](../structured_data/preprocessing_layers.ipynb)チュートリアルを参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79fi1Cgan2YV"
      },
      "outputs": [],
      "source": [
        "for name, input in inputs.items():\n",
        "  if input.dtype == tf.float32:\n",
        "    continue\n",
        "  \n",
        "  lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))\n",
        "  one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
        "\n",
        "  x = lookup(input)\n",
        "  x = one_hot(x)\n",
        "  preprocessed_inputs.append(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnhv0T7itnc7"
      },
      "source": [
        "収集された `inputs` と `processed_inputs` を使用すると、前処理されたすべての入力を連結して、前処理を処理するモデルを構築できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJRzUTe8ukXc"
      },
      "outputs": [],
      "source": [
        "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
        "\n",
        "titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
        "\n",
        "tf.keras.utils.plot_model(model = titanic_preprocessing , rankdir=\"LR\", dpi=72, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNHxrNW8vdda"
      },
      "source": [
        "この`model`には、入力の前処理のみが含まれています。実行して、データに対して何が行われるかを確認できます。 Keras モデルは、Pandas <code>DataFrames</code>を自動的に変換しません。これは、1 つのテンソルに変換する必要があるのか、テンソルのディクショナリに変換する必要があるのかが明確でないためです。ここでは、テンソルのディクショナリに変換します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YjdYyMEacwQ"
      },
      "outputs": [],
      "source": [
        "titanic_features_dict = {name: np.array(value) \n",
        "                         for name, value in titanic_features.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nKJYoPByada"
      },
      "source": [
        "最初のトレーニングサンプルをスライスしてこの前処理モデルに渡すと、数値特徴と文字列のワンホットがすべて連結されていることがわかります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjnmU8PSv8T3"
      },
      "outputs": [],
      "source": [
        "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
        "titanic_preprocessing(features_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkBf4LvmzMDp"
      },
      "source": [
        "次に、この上にモデルを構築します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coIPtGaCzUV7"
      },
      "outputs": [],
      "source": [
        "def titanic_model(preprocessing_head, inputs):\n",
        "  body = tf.keras.Sequential([\n",
        "    layers.Dense(64),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  preprocessed_inputs = preprocessing_head(inputs)\n",
        "  result = body(preprocessed_inputs)\n",
        "  model = tf.keras.Model(inputs, result)\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                optimizer=tf.keras.optimizers.Adam())\n",
        "  return model\n",
        "\n",
        "titanic_model = titanic_model(titanic_preprocessing, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK5uBQQF2KbZ"
      },
      "source": [
        "モデルをトレーニングする際に、特徴のディクショナリを`x`、ラベルを`y`として渡します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1gVfwJ61ejz"
      },
      "outputs": [],
      "source": [
        "titanic_model.fit(x=titanic_features_dict, y=titanic_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxgJarZk3bfH"
      },
      "source": [
        "前処理はモデルの一部であるため、モデルを保存して別の場所で再読み込みしても、同じ結果を得ることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay-8ymNA2ZCh"
      },
      "outputs": [],
      "source": [
        "titanic_model.save('test')\n",
        "reloaded = tf.keras.models.load_model('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm6jMTpD20lK"
      },
      "outputs": [],
      "source": [
        "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
        "\n",
        "before = titanic_model(features_dict)\n",
        "after = reloaded(features_dict)\n",
        "assert (before-after)<1e-3\n",
        "print(before)\n",
        "print(after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VsPlxIRZpXf"
      },
      "source": [
        "## tf.data を使用する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyVDCwGzR5HW"
      },
      "source": [
        "前のセクションでは、モデルのトレーニングの際に、モデルに組み込まれているデータのシャッフルとバッチ処理に依存しました。\n",
        "\n",
        "入力データパイプラインをさらに制御する必要がある場合、またはメモリに簡単に収まらないデータを使用する必要がある場合は、`tf.data`を使用します。\n",
        "\n",
        "詳細については、<a><code>tf.data</code>: TensorFlow 入力パイプラインをビルドする</a>ガイドをご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP5Y1jM2Sor0"
      },
      "source": [
        "### インメモリデータ\n",
        "\n",
        "`tf.data` を CSV データに適用する最初の例として、次のコードで前のセクションの特徴のディクショナリを手動でスライスします。各インデックスは、特徴ごとにそのインデックスを取得します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8wE-MVuVu7_"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def slices(features):\n",
        "  for i in itertools.count():\n",
        "    # For each feature take index `i`\n",
        "    example = {name:values[i] for name, values in features.items()}\n",
        "    yield example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ3RTbS9YEal"
      },
      "source": [
        "実行して、最初のサンプルを出力します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwq8XK88WwFk"
      },
      "outputs": [],
      "source": [
        "for example in slices(titanic_features_dict):\n",
        "  for name, value in example.items():\n",
        "    print(f\"{name:19s}: {value}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvp8Dct6YOIE"
      },
      "source": [
        "メモリデータローダーの最も基本的な`tf.data.Dataset`は、`Dataset.from_tensor_slices`コンストラクタです。これにより、TensorFlowで上記の`slices`関数の一般化されたバージョンを実装する`tf.data.Dataset`が返されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gEJthslYxeV"
      },
      "outputs": [],
      "source": [
        "features_ds = tf.data.Dataset.from_tensor_slices(titanic_features_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZC0rTpMZMZK"
      },
      "source": [
        "他の Python 反復可能と同様に、`tf.data.Dataset`を反復できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOHbiefaY4ag"
      },
      "outputs": [],
      "source": [
        "for example in features_ds:\n",
        "  for name, value in example.items():\n",
        "    print(f\"{name:19s}: {value}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwcFoVJWZY5F"
      },
      "source": [
        "`from_tensor_slices`関数は、ネストされたディクショナリーまたはタプルの任意の構造を処理できます。次のコードは、`(features_dict, labels)`ペアのデータセットを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIHGBy76Zcrx"
      },
      "outputs": [],
      "source": [
        "titanic_ds = tf.data.Dataset.from_tensor_slices((titanic_features_dict, titanic_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQwxitt8c2GK"
      },
      "source": [
        "この`Dataset`を使用してモデルをトレーニングするには、少なくとも`shuffle`と`batch`のデータが必要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbJcbldhddeC"
      },
      "outputs": [],
      "source": [
        "titanic_batches = titanic_ds.shuffle(len(titanic_labels)).batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4FRqhRFuoJx"
      },
      "source": [
        "`features`と`labels`を`Model.fit`に渡す代わりに、データセットを渡します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yXkNPumdBtB"
      },
      "outputs": [],
      "source": [
        "titanic_model.fit(titanic_batches, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXuibiv9exT7"
      },
      "source": [
        "### 単一のファイルから\n",
        "\n",
        "これまでのところ、このチュートリアルはインメモリデータを扱ってきました。`tf.data`は、データパイプラインを構築するための非常にスケーラブルなツールキットであり、CSV ファイルの読み込みを処理するためのいくつかの関数を提供します。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncf5t6tgL5ZI"
      },
      "outputs": [],
      "source": [
        "titanic_file_path = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4N-plO4tDXd"
      },
      "source": [
        "次に、ファイルから CSV データを読み取り、`tf.data.Dataset`を作成します。\n",
        "\n",
        "（完全なドキュメントについては、`tf.data.experimental.make_csv_dataset` を参照してください。)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIbUscB9sqha"
      },
      "outputs": [],
      "source": [
        "titanic_csv_ds = tf.data.experimental.make_csv_dataset(\n",
        "    titanic_file_path,\n",
        "    batch_size=5, # Artificially small to make examples easier to show.\n",
        "    label_name='survived',\n",
        "    num_epochs=1,\n",
        "    ignore_errors=True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf3v3BKgy4AG"
      },
      "source": [
        "この関数には以下のような多くの便利な機能が含まれているため、データを簡単に操作できます。\n",
        "\n",
        "- 列ヘッダーをディクショナリキーとして使用する。\n",
        "- それぞれの列の型を自動的に決定する。\n",
        "\n",
        "注意: `tf.data.experimental.make_csv_dataset` で `num_epochs` 引数を設定してください。<br>そうしないと、`tf.data.Dataset` のデフォルトの動作は無限ループです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4oMO9MIxgTG"
      },
      "outputs": [],
      "source": [
        "for batch, label in titanic_csv_ds.take(1):\n",
        "  for key, value in batch.items():\n",
        "    print(f\"{key:20s}: {value}\")\n",
        "  print()\n",
        "  print(f\"{'label':20s}: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-TgA6o2Ja6U"
      },
      "source": [
        "注意: 上記のセルを 2 回実行すると、異なる結果が生成されます。`tf.data.experimental.make_csv_dataset` のデフォルト設定には、`shuffle_buffer_size=1000` が含まれます。これは、この小さなデータセットには十分ですが、実際のデータセットには適切ではない場合があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6uviU_KCCWD"
      },
      "source": [
        "また、その場でデータを解凍することもできます。 これは、[都市州間高速道路交通データセット](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume)を含む gzip 圧縮された CSV ファイルです。\n",
        "\n",
        "![交通渋滞](images/csv/traffic.jpg)\n",
        "\n",
        "画像出典：[Wikimedia](https://commons.wikimedia.org/wiki/File:Trafficjam.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT7oZI2E46Q8"
      },
      "outputs": [],
      "source": [
        "traffic_volume_csv_gz = tf.keras.utils.get_file(\n",
        "    'Metro_Interstate_Traffic_Volume.csv.gz', \n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\",\n",
        "    cache_dir='.', cache_subdir='traffic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-IOsFHbCw0i"
      },
      "source": [
        "圧縮ファイルから直接読み取るように`compression_type`引数を設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar0MPEVJ5NeA"
      },
      "outputs": [],
      "source": [
        "traffic_volume_csv_gz_ds = tf.data.experimental.make_csv_dataset(\n",
        "    traffic_volume_csv_gz,\n",
        "    batch_size=256,\n",
        "    label_name='traffic_volume',\n",
        "    num_epochs=1,\n",
        "    compression_type=\"GZIP\")\n",
        "\n",
        "for batch, label in traffic_volume_csv_gz_ds.take(1):\n",
        "  for key, value in batch.items():\n",
        "    print(f\"{key:20s}: {value[:5]}\")\n",
        "  print()\n",
        "  print(f\"{'label':20s}: {label[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p12Y6tGq8D6M"
      },
      "source": [
        "注意: `tf.data` パイプラインでこれらの日時文字列を解析する必要がある場合は、`tfa.text.parse_time` を使用できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrAXzYGP3l0"
      },
      "source": [
        "### キャッシング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN2dL_LRP83r"
      },
      "source": [
        "CSV データの解析にはオーバーヘッドがあります。小さなモデルの場合、これがトレーニングのボトルネックになる可能性があります。\n",
        "\n",
        "ユースケースによっては、`Dataset.cache` または `tf.data.Dataset.snapshot` を使用して、CSV データが最初のエポックでのみ解析されるようにすることをお勧めします。\n",
        "\n",
        "`cache` メソッドと `snapshot` メソッドの主な違いは、`cache` ファイルは、それらを作成した TensorFlow プロセスでのみ使用できることですが、`snapshot` ファイルは他のプロセスで読み取ることができます。\n",
        "\n",
        "たとえば、`traffic_volume_csv_gz_ds` を 20 回繰り返すと、キャッシュなしで最大15秒、キャッシュありで最大 2 秒かかります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk38Sw4MO4eh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i, (batch, label) in enumerate(traffic_volume_csv_gz_ds.repeat(20)):\n",
        "  if i % 40 == 0:\n",
        "    print('.', end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN3HtDONh5TX"
      },
      "source": [
        "注意: `Dataset.cache` は、最初のエポックからのデータを保存し、順番に再生します。したがって、`cache` を使用すると、パイプラインの早い段階でシャッフルが無効になります。以下では、`Dataset.shuffle` は、`Dataset.cache` の後に追加されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5Jj72MrPbnh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "caching = traffic_volume_csv_gz_ds.cache().shuffle(1000)\n",
        "\n",
        "for i, (batch, label) in enumerate(caching.shuffle(1000).repeat(20)):\n",
        "  if i % 40 == 0:\n",
        "    print('.', end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN7uUBjmgNZ9"
      },
      "source": [
        "注意: `tf.data.Dataset.snapshot` ファイルは、使用中のデータセットの*一時*ストレージ用です。これは、長期保存用の形式*ではありません*。このファイル形式は内部の詳細と見なされ、TensorFlow バージョン間で保証されません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHGD1E8ktUvW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "snapshotting = traffic_volume_csv_gz_ds.snapshot('titanic.tfsnap').shuffle(1000)\n",
        "\n",
        "for i, (batch, label) in enumerate(snapshotting.shuffle(1000).repeat(20)):\n",
        "  if i % 40 == 0:\n",
        "    print('.', end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUSSegnMCGRz"
      },
      "source": [
        "CSV ファイルの読み込みによってデータの読み込みが遅くなり、`Dataset.cache` と `tf.data.Dataset.snapshot` がユースケースに不十分な場合は、データをより合理化された形式に再度エンコードすることを検討してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0iGXv9pC5kr"
      },
      "source": [
        "### 複数のファイル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FFzHQrCDH4w"
      },
      "source": [
        "このセクションのこれまでのすべての例は、`tf.data`なしで簡単に実行できます。 `tf.data`が単純化できるのは、ファイルの収集を処理するときだけです。\n",
        "\n",
        "たとえば、[文字フォント画像](https://archive.ics.uci.edu/ml/datasets/Character+Font+Images)データセットは、フォントごとに1つずつの CSV ファイルのコレクションとして配布されます。\n",
        "\n",
        "![フォント](images/csv/fonts.jpg)\n",
        "\n",
        "画像出典：<a href=\"https://pixabay.com/users/wilhei-883152/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=705667\">Pixabay</a>、<a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=705667\">Willi Heidelbach</a>\n",
        "\n",
        "データセットをダウンロードし、そのファイルを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmVknMdJh5ks"
      },
      "outputs": [],
      "source": [
        "fonts_zip = tf.keras.utils.get_file(\n",
        "    'fonts.zip',  \"https://archive.ics.uci.edu/ml/machine-learning-databases/00417/fonts.zip\",\n",
        "    cache_dir='.', cache_subdir='fonts',\n",
        "    extract=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsDlMCnyi55e"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "font_csvs =  sorted(str(p) for p in pathlib.Path('fonts').glob(\"*.csv\"))\n",
        "\n",
        "font_csvs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRAEJx9ROAGl"
      },
      "outputs": [],
      "source": [
        "len(font_csvs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Udrw9iG-FS"
      },
      "source": [
        "多数のファイルを処理する場合、glob スタイルの `file_pattern` を `tf.data.experimental.make_csv_dataset` 関数に渡すことができます。ファイルの順序は、反復ごとにシャッフルされます。\n",
        "\n",
        "`num_parallel_reads`引数を使用して、並列に読み取り、共にインターリーブされるファイルの数を設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TSUNdT6iG58"
      },
      "outputs": [],
      "source": [
        "fonts_ds = tf.data.experimental.make_csv_dataset(\n",
        "    file_pattern = \"fonts/*.csv\",\n",
        "    batch_size=10, num_epochs=1,\n",
        "    num_parallel_reads=20,\n",
        "    shuffle_buffer_size=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMoexinLHYFa"
      },
      "source": [
        "これらの CSV ファイルでは、画像が 1 行にフラット化されています。列名の形式は `r{row}c{column}` です。以下は最初のバッチです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmFvBWxxi3pq"
      },
      "outputs": [],
      "source": [
        "for features in fonts_ds.take(1):\n",
        "  for i, (name, value) in enumerate(features.items()):\n",
        "    if i>15:\n",
        "      break\n",
        "    print(f\"{name:20s}: {value}\")\n",
        "print('...')\n",
        "print(f\"[total: {len(features)} features]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrC3sKdeOhb5"
      },
      "source": [
        "#### オプション：パッキングフィールド\n",
        "\n",
        "以上のように、各ピクセルを別々の列で操作することは望ましくありません。このデータセットを使用する前に、必ずピクセルをイメージテンソルにパックしてください。\n",
        "\n",
        "次に、列名を解析して各例の画像を作成するコードを示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hct5EMEWNyfH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def make_images(features):\n",
        "  image = [None]*400\n",
        "  new_feats = {}\n",
        "\n",
        "  for name, value in features.items():\n",
        "    match = re.match('r(\\d+)c(\\d+)', name)\n",
        "    if match:\n",
        "      image[int(match.group(1))*20+int(match.group(2))] = value\n",
        "    else:\n",
        "      new_feats[name] = value\n",
        "\n",
        "  image = tf.stack(image, axis=0)\n",
        "  image = tf.reshape(image, [20, 20, -1])\n",
        "  new_feats['image'] = image\n",
        "\n",
        "  return new_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qy8utAwARP"
      },
      "source": [
        "その関数をデータセット内の各バッチに適用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJnnfIW9baE4"
      },
      "outputs": [],
      "source": [
        "fonts_image_ds = fonts_ds.map(make_images)\n",
        "\n",
        "for features in fonts_image_ds.take(1):\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ThqrthGwHSm"
      },
      "source": [
        "結果の画像をプロットします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5dcey31T_tk"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,6), dpi=120)\n",
        "\n",
        "for n in range(9):\n",
        "  plt.subplot(3,3,n+1)\n",
        "  plt.imshow(features['image'][..., n])\n",
        "  plt.title(chr(features['m_label'][n]))\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-nNR0Nncdd1"
      },
      "source": [
        "## 下位レベルの関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jiGZeUijJNd"
      },
      "source": [
        "このチュートリアルでは、CSV データを読み取るための最高レベルのユーティリティに焦点を当ててきました。ユースケースが基本的なパターンに適合しない場合に上級ユーザーに役立つ API が他の 2 つあります。\n",
        "\n",
        "- `tf.io.decode_csv`: テキストの行を CSV 列テンソルのリストに解析するための関数。\n",
        "- `tf.data.experimental.CsvDataset`: 下位レベルの CSV データセットコンストラクタ。\n",
        "\n",
        "このセクションでは、`tf.data.experimental.make_csv_dataset` により提供される機能を再作成して、この低レベルの機能をどのように使用できるかを示します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL_ixywomOHW"
      },
      "source": [
        "### `tf.io.decode_csv`\n",
        "\n",
        "この関数は、文字列または文字列のリストを列のリストにデコードします。\n",
        "\n",
        "`tf.data.experimental.make_csv_dataset` とは異なり、この関数は列のデータ型を推測しようとしません。列ごとの正しい型の値を含む `record_defaults` のリストを提供することにより、列の型を指定します。\n",
        "\n",
        "次のように <code>tf.io.decode_csv</code> を使用してタイタニックデータを<strong>文字列として</strong>読み取ります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1D2C-qdlqeW"
      },
      "outputs": [],
      "source": [
        "text = pathlib.Path(titanic_file_path).read_text()\n",
        "lines = text.split('\\n')[1:-1]\n",
        "\n",
        "all_strings = [str()]*10\n",
        "all_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W4UeJYyHPx5"
      },
      "outputs": [],
      "source": [
        "features = tf.io.decode_csv(lines, record_defaults=all_strings) \n",
        "\n",
        "for f in features:\n",
        "  print(f\"type: {f.dtype.name}, shape: {f.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8TaHSQFoQL4"
      },
      "source": [
        "それらを実際のタイプで解析するには、対応する型の`record_defaults`のリストを作成します。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzUjR59yoUe1"
      },
      "outputs": [],
      "source": [
        "print(lines[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sPTunxwoeWU"
      },
      "outputs": [],
      "source": [
        "titanic_types = [int(), str(), float(), int(), int(), float(), str(), str(), str(), str()]\n",
        "titanic_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3NlViCzoB7F"
      },
      "outputs": [],
      "source": [
        "features = tf.io.decode_csv(lines, record_defaults=titanic_types) \n",
        "\n",
        "for f in features:\n",
        "  print(f\"type: {f.dtype.name}, shape: {f.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-LkTUTnpn2P"
      },
      "source": [
        "注意: CSV テキストの個々の行を呼び出すよりも、行の大きなバッチで `tf.io.decode_csv` を呼び出す方が効率的です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp1UItJmqGqw"
      },
      "source": [
        "### `tf.data.experimental.CsvDataset`\n",
        "\n",
        "`tf.data.experimental.CsvDataset` クラスは、`tf.data.experimental.make_csv_dataset` 関数の便利な機能（列ヘッダーの解析、列の型推論、自動シャッフル、ファイルのインターリーブ）なしで、最小限の CSV <code>Dataset</code> インターフェースを提供します。\n",
        "\n",
        "このコンストラクタは、`tf.io.decode_csv` と同じ方法で `record_defaults` を使用します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OzZLp3krP-t"
      },
      "outputs": [],
      "source": [
        "simple_titanic = tf.data.experimental.CsvDataset(titanic_file_path, record_defaults=titanic_types, header=True)\n",
        "\n",
        "for example in simple_titanic.take(1):\n",
        "  print([e.numpy() for e in example])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HBmfI-Ks7dw"
      },
      "source": [
        "上記のコードは基本的に次と同等です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5O5d69Yq7gG"
      },
      "outputs": [],
      "source": [
        "def decode_titanic_line(line):\n",
        "  return tf.io.decode_csv(line, titanic_types)\n",
        "\n",
        "manual_titanic = (\n",
        "    # Load the lines of text\n",
        "    tf.data.TextLineDataset(titanic_file_path)\n",
        "    # Skip the header row.\n",
        "    .skip(1)\n",
        "    # Decode the line.\n",
        "    .map(decode_titanic_line)\n",
        ")\n",
        "\n",
        "for example in manual_titanic.take(1):\n",
        "  print([e.numpy() for e in example])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R3ralsnt2AC"
      },
      "source": [
        "#### 複数のファイル\n",
        "\n",
        "`tf.data.experimental.CsvDataset` を使用してフォントデータセットを解析するには、最初に `record_defaults` の列の型を決定する必要があります。まず、1 つのファイルの最初の行を調べます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tlFOTjCvAI5"
      },
      "outputs": [],
      "source": [
        "font_line = pathlib.Path(font_csvs[0]).read_text().splitlines()[1]\n",
        "print(font_line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etyGu8K_ySRz"
      },
      "source": [
        "最初の 2 つのフィールドのみが文字列で、残りは int または float です。コンマを数えることで特徴の総数を取得できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crgZZn0BzkSB"
      },
      "outputs": [],
      "source": [
        "num_font_features = font_line.count(',')+1\n",
        "font_column_types = [str(), str()] + [float()]*(num_font_features-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeK2Pw540RNj"
      },
      "source": [
        "`tf.data.experimental.CsvDataset` コンストラクタは、入力ファイルのリストを取得できますが、それらを順番に読み取ります。CSV のリストの最初のファイルは `AGENCY.csv` です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SvL5Uvl0r0N"
      },
      "outputs": [],
      "source": [
        "font_csvs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfAX3G8Xywy6"
      },
      "source": [
        "したがって、ファイルのリストを `CsvDataset` に渡すと、`AGENCY.csv` のレコードが最初に読み取られます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtr1E66VmBqj"
      },
      "outputs": [],
      "source": [
        "simple_font_ds = tf.data.experimental.CsvDataset(\n",
        "    font_csvs, \n",
        "    record_defaults=font_column_types, \n",
        "    header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k750Mgq4yt_o"
      },
      "outputs": [],
      "source": [
        "for row in simple_font_ds.take(10):\n",
        "  print(row[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiqWKQV21FrE"
      },
      "source": [
        "複数のファイルをインターリーブするには、`Dataset.interleave`を使用します。\n",
        "\n",
        "CSV ファイル名を含む初期データセットは次のとおりです。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9dS3SNb23W8"
      },
      "outputs": [],
      "source": [
        "font_files = tf.data.Dataset.list_files(\"fonts/*.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNiLHMXpzHy5"
      },
      "source": [
        "これにより、各エポックのファイル名がシャッフルされます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNd-TYyNzIgg"
      },
      "outputs": [],
      "source": [
        "print('Epoch 1:')\n",
        "for f in list(font_files)[:5]:\n",
        "  print(\"    \", f.numpy())\n",
        "print('    ...')\n",
        "print()\n",
        "\n",
        "print('Epoch 2:')\n",
        "for f in list(font_files)[:5]:\n",
        "  print(\"    \", f.numpy())\n",
        "print('    ...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0QB1PtU3WAN"
      },
      "source": [
        "`interleave`メソッドは、親`Dataset`の各要素に対して子`Dataset`を作成する`map_func`を取ります。\n",
        "\n",
        "ここでは、ファイルのデータセットの各要素から `tf.data.experimental.CsvDataset` を作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWp4rH0Q4uPh"
      },
      "outputs": [],
      "source": [
        "def make_font_csv_ds(path):\n",
        "  return tf.data.experimental.CsvDataset(\n",
        "    path, \n",
        "    record_defaults=font_column_types, \n",
        "    header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxRGdLMB5nRF"
      },
      "source": [
        "インターリーブによって返される `Dataset` は、子 `Dataset` の数を循環することによって要素を返します。以下でデータセットが `cycle_length=3` で 3 つのフォントファイルをどのように循環するかに注目してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OePMNF_x1_Cc"
      },
      "outputs": [],
      "source": [
        "font_rows = font_files.interleave(make_font_csv_ds,\n",
        "                                  cycle_length=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UORIGWLy54-E"
      },
      "outputs": [],
      "source": [
        "fonts_dict = {'font_name':[], 'character':[]}\n",
        "\n",
        "for row in font_rows.take(10):\n",
        "  fonts_dict['font_name'].append(row[0].numpy().decode())\n",
        "  fonts_dict['character'].append(chr(row[2].numpy()))\n",
        "\n",
        "pd.DataFrame(fonts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkKZa_HX8zAm"
      },
      "source": [
        "#### 性能\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BtGHraUApdJ"
      },
      "source": [
        "前述のとおり、文字列のバッチで実行する場合、`tf.io.decode_csv` の方が効率的です。\n",
        "\n",
        "大きなバッチサイズを使用する場合は、これを利用して CSV の読み込みパフォーマンスを向上させることができます（ただし、最初に [caching](#caching) を試してください）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d35zWMH7MDL1"
      },
      "source": [
        "組み込みローダー 20 では、2048 サンプルのバッチは約 17 秒かかります。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieUVAPryjpJS"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=2048\n",
        "fonts_ds = tf.data.experimental.make_csv_dataset(\n",
        "    file_pattern = \"fonts/*.csv\",\n",
        "    batch_size=BATCH_SIZE, num_epochs=1,\n",
        "    num_parallel_reads=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUC2KW4LkQIz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i,batch in enumerate(fonts_ds.take(20)):\n",
        "  print('.',end='')\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lhnh6rZEDS2"
      },
      "source": [
        "**テキスト行のバッチ**を`decode_csv`に渡すと、約 5 秒で高速に実行されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XbPZV1okVF9"
      },
      "outputs": [],
      "source": [
        "fonts_files = tf.data.Dataset.list_files(\"fonts/*.csv\")\n",
        "fonts_lines = fonts_files.interleave(\n",
        "    lambda fname:tf.data.TextLineDataset(fname).skip(1), \n",
        "    cycle_length=100).batch(BATCH_SIZE)\n",
        "\n",
        "fonts_fast = fonts_lines.map(lambda x: tf.io.decode_csv(x, record_defaults=font_column_types))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te9C2km-qO8W"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i,batch in enumerate(fonts_fast.take(20)):\n",
        "  print('.',end='')\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aebC1plsMeOi"
      },
      "source": [
        "大きなバッチを使用して CSV パフォーマンスを向上させる別の例については、[過剰適合および過少学習チュートリアル](../keras/overfit_and_underfit.ipynb)を参照してください。\n",
        "\n",
        "このようなアプローチは機能する可能性がありますが、`Dataset.cache` や `tf.data.Dataset.snapshot` などの他のオプションやデータをより合理化された形式に再エンコードすることを検討してください。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "csv.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
