{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DweYe9FcbMK_"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AVV2e0XKbJeX"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZfSvVcDo6GQ"
      },
      "source": [
        "# tf.data を使ったテキストの読み込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giK0nMbZFnoR"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a>   </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a>   </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwlfPb11GH8J"
      },
      "source": [
        "このチュートリアルでは、テキストを読み込んで前処理する 2 つの方法を紹介します。\n",
        "\n",
        "- まず、Keras ユーティリティと前処理レイヤーを使用します。これには、データを `tf.data.Dataset` に変換するための `tf.keras.utils.text_dataset_from_directory` とデータを標準化、トークン化、およびベクトル化するための `tf.keras.layers.TextVectorization` が含まれます。TensorFlow を初めて使用する場合は、これらから始める必要があります。\n",
        "- 次に、`tf.data.TextLineDataset` などの低レベルのユーティリティを使用してテキストファイルを読み込み、`text.UnicodeScriptTokenizer` や `text.case_fold_utf8` などの [TensorFlow Text](https://www.tensorflow.org/text)  APIを使用して、よりきめ細かい制御のためにデータを前処理します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa6IKWvADqH7"
      },
      "outputs": [],
      "source": [
        "!pip install \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import pathlib\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az-d_K5_HQ5k"
      },
      "source": [
        "## 例 1: StackOverflow の質問のタグを予測する\n",
        "\n",
        "最初の例として、StackOverflow からプログラミングの質問のデータセットをダウンロードします。それぞれの質問 (「ディクショナリを値で並べ替えるにはどうすればよいですか？」) は、1 つのタグ (<code>Python</code>、`CSharp`、`JavaScript`、または`Java`) でラベルされています。このタスクでは、質問のタグを予測するモデルを開発します。これは、マルチクラス分類の例です。マルチクラス分類は、重要で広く適用できる機械学習の問題です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjC3yLa5IjP7"
      },
      "source": [
        "### データセットをダウンロードして調査する\n",
        "\n",
        "まず、`tf.keras.utils.get_file` を使用して Stack Overflow データセットをダウンロードし、ディレクトリの構造を調べます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ELgzA6SHTuV"
      },
      "outputs": [],
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "\n",
        "dataset_dir = utils.get_file(\n",
        "    origin=data_url,\n",
        "    untar=True,\n",
        "    cache_dir='stack_overflow',\n",
        "    cache_subdir='')\n",
        "\n",
        "dataset_dir = pathlib.Path(dataset_dir).parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIrPl5fUH2gb"
      },
      "outputs": [],
      "source": [
        "list(dataset_dir.iterdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEoV7YByJoWQ"
      },
      "outputs": [],
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "list(train_dir.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mxAN17MhEh0"
      },
      "source": [
        "`train/csharp`、`train/java`、`train/python` および `train/javascript` ディレクトリには、多くのテキストファイルが含まれています。それぞれが Stack Overflow の質問です。\n",
        "\n",
        "サンプルファイルを出力してデータを調べます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go1vTSGdJu08"
      },
      "outputs": [],
      "source": [
        "sample_file = train_dir/'python/1755.txt'\n",
        "\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deWBTkpJiO7D"
      },
      "source": [
        "### データセットを読み込む\n",
        "\n",
        "次に、データをディスクから読み込み、トレーニングに適した形式に準備します。これを行うには、`tf.keras.utils.text_dataset_from_directory` ユーティリティを使用して、ラベル付きの `tf.data.Dataset` を作成します。これは、入力パイプラインを構築するための強力なツールのコレクションです。`tf.data` を始めて使用する場合は、[tf.data: TensorFlow 入力パイプラインを構築する](../../guide/data.ipynb)を参照してください。\n",
        "\n",
        "`tf.keras.utils.text_dataset_from_directory` API は、次のようなディレクトリ構造を想定しています。\n",
        "\n",
        "```\n",
        "train/\n",
        "...csharp/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...java/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...javascript/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...python/\n",
        "......1.txt\n",
        "......2.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyl6JTAjlbQV"
      },
      "source": [
        "機械学習実験を実行するときは、データセットを[トレーニング](https://developers.google.com/machine-learning/glossary#training_set)、[検証](https://developers.google.com/machine-learning/glossary#validation_set)、および、[テスト](https://developers.google.com/machine-learning/glossary#test-set)の 3 つに分割することをお勧めします。\n",
        "\n",
        "Stack Overflow データセットは、すでにトレーニングセットとテストセットに分割されていますが、検証セットはありません。\n",
        "\n",
        "`tf.keras.utils.text_dataset_from_directory` を使用し、`validation_split` を `0.2` (20%) に設定し、トレーニングデータを 80:20 に分割して検証セットを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqyliMw8N-az"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMI_gPLfloD7"
      },
      "source": [
        "前のセル出力が示すように、トレーニングフォルダには 8,000 の例があり、そのうち 80％ (6,400) をトレーニングに使用します。`tf.data.Dataset` を `Model.fit` に直接渡すことで、モデルをトレーニングできます。詳細は、後ほど見ていきます。\n",
        "\n",
        "まず、データセットを反復処理し、いくつかの例を出力して、データを確認します。\n",
        "\n",
        "Note: To increase the difficulty of the classification problem, the dataset author replaced occurrences of the words *Python*, *CSharp*, *JavaScript*, or *Java* in the programming question with the word *blank*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JMTyZ6Glt_C"
      },
      "outputs": [],
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(10):\n",
        "    print(\"Question: \", text_batch.numpy()[i])\n",
        "    print(\"Label:\", label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCZGl4Q5l2sS"
      },
      "source": [
        "ラベルは、`0`、`1`、`2` または `3` です。これらのどれがどの文字列ラベルに対応するかを確認するには、データセットの `class_names` プロパティを確認します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIpCS7YjmGkj"
      },
      "outputs": [],
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "  print(\"Label\", i, \"corresponds to\", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUsdn-37qol9"
      },
      "source": [
        "次に、`tf.keras.utils.text_dataset_from_directory` を使って検証およびテスト用データセットを作成します。トレーニング用セットの残りの 1,600 件のレビューを検証に使用します。\n",
        "\n",
        "注意:  `tf.keras.utils.text_dataset_from_directory` の `validation_split` および `subset` 引数を使用する場合は、必ずランダムシードを指定するか、`shuffle=False`を渡して、検証とトレーニング分割に重複がないようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7m6sCWJQuYt"
      },
      "outputs": [],
      "source": [
        "# Create a validation set.\n",
        "raw_val_ds = utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXMZc7fMQwKE"
      },
      "outputs": [],
      "source": [
        "test_dir = dataset_dir/'test'\n",
        "\n",
        "# Create a test set.\n",
        "raw_test_ds = utils.text_dataset_from_directory(\n",
        "    test_dir,\n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdt-ATrGRGDL"
      },
      "source": [
        "### トレーニング用データセットを準備する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6fRti45Rlj8"
      },
      "source": [
        "次に、`tf.keras.layers.TextVectorization` レイヤーを使用して、データを標準化、トークン化、およびベクトル化します。\n",
        "\n",
        "- 標準化とは、テキストを前処理することを指します。通常、句読点や HTML 要素を削除して、データセットを簡素化します。\n",
        "- トークン化とは、文字列をトークンに分割することです（たとえば、空白で分割することにより、文を個々の単語に分割します）。\n",
        "- ベクトル化とは、トークンを数値に変換して、ニューラルネットワークに入力できるようにすることです。\n",
        "\n",
        "これらのタスクはすべて、このレイヤーで実行できます。これらの詳細については、`tf.keras.layers.TextVectorization` API ドキュメントを参照してください。\n",
        "\n",
        "注意点 :\n",
        "\n",
        "- デフォルトの標準化では、テキストが小文字に変換され、句読点が削除されます (`standardize='lower_and_strip_punctuation'`)。\n",
        "- デフォルトのトークナイザーは空白で分割されます (`split='whitespace'`)。\n",
        "- デフォルトのベクトル化モードは `int` です (`output_mode='int'`)。これは整数インデックスを出力します（トークンごとに1つ）。このモードは、語順を考慮したモデルを構築するために使用できます。`binary` などの他のモードを使用して、[bag-of-word](https://developers.google.com/machine-learning/glossary#bag-of-words) モデルを構築することもできます。\n",
        "\n",
        "`TextVectorization` を使用した標準化、トークン化、およびベクトル化について詳しくみるために、2 つのモデルを作成します。\n",
        "\n",
        "- まず、`'binary'` ベクトル化モードを使用して、bag-of-words モデルを構築します。\n",
        "- 次に、1D ConvNet で `'int'` モードを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voaC43rZR0jc"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "binary_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDPFxuf2Hfz"
      },
      "source": [
        "`'int'` モードの場合、最大語彙サイズに加えて、明示的な最大シーケンス長 (`MAX_SEQUENCE_LENGTH`) を設定する必要があります。これにより、レイヤーはシーケンスを正確に `output_sequence_length` 値にパディングまたは切り捨てます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWsY01Zl2aRe"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "int_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts6h9b5atD-Y"
      },
      "source": [
        "次に、`TextVectorization.adapt` を呼び出して、前処理レイヤーの状態をデータセットに適合させます。これにより、モデルは文字列から整数へのインデックスを作成します。\n",
        "\n",
        "注意: `TextVectorization.adapt` を呼び出すときは、トレーニング用データのみを使用することが重要です (テスト用セットを使用すると情報が漏洩します)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTXsdDEqSf9e"
      },
      "outputs": [],
      "source": [
        "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
        "train_text = raw_train_ds.map(lambda text, labels: text)\n",
        "binary_vectorize_layer.adapt(train_text)\n",
        "int_vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKVO6Jg7Sls0"
      },
      "source": [
        "これらのレイヤーを使用してデータを前処理した結果を出力します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RngfPyArSsvM"
      },
      "outputs": [],
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return binary_vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1W54wf0LhQ0"
      },
      "outputs": [],
      "source": [
        "def int_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return int_vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi_sElMiSmXe"
      },
      "outputs": [],
      "source": [
        "# Retrieve a batch (of 32 reviews and labels) from the dataset.\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_question, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Question\", first_question)\n",
        "print(\"Label\", first_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGukZoYv2v3v"
      },
      "outputs": [],
      "source": [
        "print(\"'binary' vectorized question:\",\n",
        "      binary_vectorize_text(first_question, first_label)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu07FsIw2yH5"
      },
      "outputs": [],
      "source": [
        "print(\"'int' vectorized question:\",\n",
        "      int_vectorize_text(first_question, first_label)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgjeF9PdS7tN"
      },
      "source": [
        "上に示したように、`TextVectorization` の `'binary'` モードは、入力に少なくとも 1 回存在するトークンを示す配列を返しますが、`'int'` モードでは、各トークンが整数に置き換えられるため、トークンの順序が保持されます。\n",
        "\n",
        "レイヤーで `TextVectorization.get_vocabulary` を呼び出すことにより、各整数が対応するトークン (文字列) を検索できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpBnTZilS8wt"
      },
      "outputs": [],
      "source": [
        "print(\"1289 ---> \", int_vectorize_layer.get_vocabulary()[1289])\n",
        "print(\"313 ---> \", int_vectorize_layer.get_vocabulary()[313])\n",
        "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kHgPE_YwHvp"
      },
      "source": [
        "モデルをトレーニングする準備がほぼ整いました。\n",
        "\n",
        "最後の前処理ステップとして、トレーニング、検証、およびデータセットのテストのために前に作成した `TextVectorization` レイヤーを適用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46LeHmnD55wJ"
      },
      "outputs": [],
      "source": [
        "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
        "\n",
        "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
        "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
        "int_test_ds = raw_test_ds.map(int_vectorize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHuAF8hYfP5Z"
      },
      "source": [
        "### パフォーマンスのためにデータセットを構成する\n",
        "\n",
        "以下は、データを読み込むときに I/O がブロックされないようにするために使用する必要がある 2 つの重要な方法です。\n",
        "\n",
        "- `Dataset.cache` はデータをディスクから読み込んだ後、データをメモリに保持します。これにより、モデルのトレーニング中にデータセットがボトルネックになることを回避できます。データセットが大きすぎてメモリに収まらない場合は、この方法を使用して、パフォーマンスの高いオンディスクキャッシュを作成することもできます。これは、多くの小さなファイルを読み込むより効率的です。\n",
        "- `Dataset.prefetch` はトレーニング中にデータの前処理とモデルの実行をオーバーラップさせます。\n",
        "\n",
        "以上の 2 つの方法とデータをディスクにキャッシュする方法についての詳細は、<a>データパフォーマンスガイド</a>の <em>プリフェッチ</em>を参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PabA9DFIfSz7"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def configure_dataset(dataset):\n",
        "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8GcJLvb3JH0"
      },
      "outputs": [],
      "source": [
        "binary_train_ds = configure_dataset(binary_train_ds)\n",
        "binary_val_ds = configure_dataset(binary_val_ds)\n",
        "binary_test_ds = configure_dataset(binary_test_ds)\n",
        "\n",
        "int_train_ds = configure_dataset(int_train_ds)\n",
        "int_val_ds = configure_dataset(int_val_ds)\n",
        "int_test_ds = configure_dataset(int_test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYGb7z_bfpGm"
      },
      "source": [
        "### モデルをトレーニングする\n",
        "\n",
        "ニューラルネットワークを作成します。\n",
        "\n",
        "`'binary'` のベクトル化されたデータの場合、単純な bag-of-words 線形モデルを定義し、それを構成してトレーニングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q8iAU-VMzaN"
      },
      "outputs": [],
      "source": [
        "binary_model = tf.keras.Sequential([layers.Dense(4)])\n",
        "\n",
        "binary_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = binary_model.fit(\n",
        "    binary_train_ds, validation_data=binary_val_ds, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwidD-SwNIkz"
      },
      "source": [
        "次に、`'int'` ベクトル化レイヤーを使用して、1D ConvNet を構築します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ztw2XH_LbVz"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, num_labels):\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
        "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
        "      layers.GlobalMaxPooling1D(),\n",
        "      layers.Dense(num_labels)\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9rG1cFRL31Z"
      },
      "outputs": [],
      "source": [
        "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n",
        "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n",
        "int_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3J9Eeuv97zE"
      },
      "source": [
        "2 つのモデルを比較します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8ViDXw99v_u"
      },
      "outputs": [],
      "source": [
        "print(\"Linear model on binary vectorized data:\")\n",
        "print(binary_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9BOeoCwborD"
      },
      "outputs": [],
      "source": [
        "print(\"ConvNet model on int vectorized data:\")\n",
        "print(int_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYYW9tUdCtTy"
      },
      "source": [
        "テストデータで両方のモデルを評価します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dTc4nZqf7fK"
      },
      "outputs": [],
      "source": [
        "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
        "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
        "\n",
        "print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))\n",
        "print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9dhj8Hey9DS"
      },
      "source": [
        "注意: このサンプルデータセットは、かなり単純な分類問題を表しています。より複雑なデータセットと問題は、前処理戦略とモデルアーキテクチャに微妙ながら重要な違いをもたらします。さまざまなアプローチを比較するために、さまざまなハイパーパラメータとエポックを試してみてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GaXTsIgP-3"
      },
      "source": [
        "### モデルをエクスポートする\n",
        "\n",
        "上記のコードでは、モデルにテキストをフィードする前に、`tf.keras.layers.TextVectorization` レイヤーをデータセットに適用しました。モデルで生の文字列を処理できるようにする場合 (たとえば、展開を簡素化するため)、モデル内に `TextVectorization` レイヤーを含めることができます。\n",
        "\n",
        "これを行うには、トレーニングしたばかりの重みを使用して新しいモデルを作成できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bRe3KX8gRCX"
      },
      "outputs": [],
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [binary_vectorize_layer, binary_model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(binary_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2eqTVBP4DUN"
      },
      "source": [
        "これで、モデルは生の文字列を入力として受け取り、`Model.predict` を使用して各ラベルのスコアを予測できます。最大スコアのラベルを見つける関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU53uRXz45iO"
      },
      "outputs": [],
      "source": [
        "def get_string_labels(predicted_scores_batch):\n",
        "  predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
        "  predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n",
        "  return predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqnWc7Nn5eou"
      },
      "source": [
        "### 新しいデータで推論を実行する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOR2MupW1_zS"
      },
      "outputs": [],
      "source": [
        "inputs = [\n",
        "    \"how do I extract keys from a dict into a list?\",  # 'python'\n",
        "    \"debug public static void main(string[] args) {...}\",  # 'java'\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QDVfii_4slI"
      },
      "source": [
        "モデル内にテキスト前処理ロジックを含めると、モデルを本番環境にエクスポートして展開を簡素化し、[トレーニング/テストスキュー](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)の可能性を減らすことができます。\n",
        "\n",
        "`tf.keras.layers.TextVectorization` を適用する場所を選択する際に性能の違いに留意する必要があります。モデルの外部で使用すると、GPU でトレーニングするときに非同期 CPU 処理とデータのバッファリングを行うことができます。したがって、GPU でモデルをトレーニングしている場合は、モデルの開発中に最高のパフォーマンスを得るためにこのオプションを使用し、デプロイの準備ができたらモデル内に `TextVectorization` レイヤーを含めるように切り替えることをお勧めします。\n",
        "\n",
        "モデルの保存の詳細については、[モデルの保存と読み込み](../keras/save_and_load.ipynb)チュートリアルをご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4cvuFzavTRy"
      },
      "source": [
        "## 例 2: イーリアスの翻訳者を予測する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOlJ22508RIe"
      },
      "source": [
        "以下に、`tf.data.TextLineDataset` を使用してテキストファイルから例を読み込み、[TensorFlow Text](https://www.tensorflow.org/text) を使用してデータを前処理する例を示します。この例では、ホーマーのイーリアスの 3 つの異なる英語翻訳を使用し、与えられた 1 行のテキストから翻訳者を識別するようにモデルをトレーニングします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pCgKbOSk7kU"
      },
      "source": [
        "### データセットをダウンロードして調査する\n",
        "\n",
        "3 つのテキストの翻訳者は次のとおりです。\n",
        "\n",
        "- [ウィリアム・クーパー](https://en.wikipedia.org/wiki/William_Cowper) — [テキスト](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n",
        "- [エドワード、ダービー伯爵](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby) — [テキスト](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n",
        "- [サミュエル・バトラー](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29) — [テキスト](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n",
        "\n",
        "このチュートリアルで使われているテキストファイルは、ヘッダ、フッタ、行番号、章のタイトルの削除など、いくつかの典型的な前処理が行われています。\n",
        "\n",
        "前処理後のファイルをローカルにダウンロードします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YlKQthEYlFw"
      },
      "outputs": [],
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "  text_dir = utils.get_file(name, origin=DIRECTORY_URL + name)\n",
        "\n",
        "parent_dir = pathlib.Path(text_dir).parent\n",
        "list(parent_dir.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8PHK5J_cXE5"
      },
      "source": [
        "### データセットを読み込む\n",
        "\n",
        "以前は、`tf.keras.utils.text_dataset_from_directory` では、ファイルのすべてのコンテンツが 1 つの例として扱われていました。ここでは、`tf.data.TextLineDataset` を使用します。これは、テキストファイルから `tf.data.Dataset` を作成するように設計されています。それぞれの例は、元のファイルからの行です。`TextLineDataset` は、主に行ベースのテキストデータ  (詩やエラーログなど) に役立ちます。\n",
        "\n",
        "これらのファイルを繰り返し処理し、各ファイルを独自のデータセットに読み込みます。各例には個別にラベルを付ける必要があるため、`Dataset.map` を使用して、それぞれにラベラー関数を適用します。これにより、データセット内のすべての例が繰り返され、 (`example, label`) ペアが返されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIIWIdPXgk7I"
      },
      "outputs": [],
      "source": [
        "def labeler(example, index):\n",
        "  return example, tf.cast(index, tf.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ajx7AmZnEg3"
      },
      "outputs": [],
      "source": [
        "labeled_data_sets = []\n",
        "\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  lines_dataset = tf.data.TextLineDataset(str(parent_dir/file_name))\n",
        "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
        "  labeled_data_sets.append(labeled_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPOsVK1e9NGM"
      },
      "source": [
        "次に、`Dataset.concatenate` を使用し、これらのラベル付きデータセットを 1 つのデータセットに結合し、`Dataset.shuffle` を使用してシャッフルします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jAeYkTIi9-2"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SIZE = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd544E-Sh63L"
      },
      "outputs": [],
      "source": [
        "all_labeled_data = labeled_data_sets[0]\n",
        "for labeled_dataset in labeled_data_sets[1:]:\n",
        "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
        "\n",
        "all_labeled_data = all_labeled_data.shuffle(\n",
        "    BUFFER_SIZE, reshuffle_each_iteration=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4JEHrJXeG5k"
      },
      "source": [
        "前述の手順でいくつかの例を出力します。データセットはまだバッチ処理されていないため、`all_labeled_data` の各エントリは 1 つのデータポイントに対応します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gywKlN0xh6u5"
      },
      "outputs": [],
      "source": [
        "for text, label in all_labeled_data.take(10):\n",
        "  print(\"Sentence: \", text.numpy())\n",
        "  print(\"Label:\", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rrpU2_sfDh0"
      },
      "source": [
        "### トレーニング用データセットを準備する\n",
        "\n",
        "`tf.keras.layers.TextVectorization` を使用してテキストデータセットを前処理する代わりに、TensorFlow Text API を使用してデータを標準化およびトークン化し、語彙を作成し、`tf.lookup.StaticVocabularyTable` を使用してトークンを整数にマッピングし、モデルにフィードします。(詳細については [TensorFlow Text](https://www.tensorflow.org/text) を参照してください)。\n",
        "\n",
        "テキストを小文字に変換してトークン化する関数を定義します。\n",
        "\n",
        "- TensorFlow Text は、さまざまなトークナイザーを提供します。この例では、`text.UnicodeScriptTokenizer` を使用してデータセットをトークン化します。\n",
        "- `Dataset.map` を使用して、トークン化をデータセットに適用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4DpQW-Y12rm"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz8xEj0ugu51"
      },
      "outputs": [],
      "source": [
        "def tokenize(text, unused_label):\n",
        "  lower_case = tf_text.case_fold_utf8(text)\n",
        "  return tokenizer.tokenize(lower_case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzUrAzOq31QL"
      },
      "outputs": [],
      "source": [
        "tokenized_ds = all_labeled_data.map(tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx4Q2i8XLV7o"
      },
      "source": [
        "データセットを反復処理して、トークン化されたいくつかの例を出力します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2mkWri7LiGq"
      },
      "outputs": [],
      "source": [
        "for text_batch in tokenized_ds.take(5):\n",
        "  print(\"Tokens: \", text_batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPd4PsskJ_Xt"
      },
      "source": [
        "次に、トークンを頻度で並べ替え、上位の `VOCAB_SIZE` トークンを保持することにより、語彙を構築します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkHtbGnDh6mg"
      },
      "outputs": [],
      "source": [
        "tokenized_ds = configure_dataset(tokenized_ds)\n",
        "\n",
        "vocab_dict = collections.defaultdict(lambda: 0)\n",
        "for toks in tokenized_ds.as_numpy_iterator():\n",
        "  for tok in toks:\n",
        "    vocab_dict[tok] += 1\n",
        "\n",
        "vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "vocab = [token for token, count in vocab]\n",
        "vocab = vocab[:VOCAB_SIZE]\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size: \", vocab_size)\n",
        "print(\"First five vocab entries:\", vocab[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyKSsaNAKi17"
      },
      "source": [
        "トークンを整数に変換するには、`vocab` セットを使用して、`tf.lookup.StaticVocabularyTable` を作成します。トークンを [`2`, `vocab_size + 2`] の範囲の整数にマップします。`TextVectorization` レイヤーと同様に、`0` はパディングを示すために予約されており、`1` は語彙外 (OOV) トークンを示すために予約されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCBo2yFHD7y6"
      },
      "outputs": [],
      "source": [
        "keys = vocab\n",
        "values = range(2, len(vocab) + 2)  # Reserve `0` for padding, `1` for OOV tokens.\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(\n",
        "    keys, values, key_dtype=tf.string, value_dtype=tf.int64)\n",
        "\n",
        "num_oov_buckets = 1\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5F-EiBpOADE"
      },
      "source": [
        "最後に、トークナイザーとルックアップテーブルを使用して、データセットを標準化、トークン化、およびベクトル化する関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcIQ7LOTh6eT"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, label):\n",
        "  standardized = tf_text.case_fold_utf8(text)\n",
        "  tokenized = tokenizer.tokenize(standardized)\n",
        "  vectorized = vocab_table.lookup(tokenized)\n",
        "  return vectorized, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6S5Qyabi-vo"
      },
      "source": [
        "1 つの例でこれを試して、出力を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgxPZaxUuTbk"
      },
      "outputs": [],
      "source": [
        "example_text, example_label = next(iter(all_labeled_data))\n",
        "print(\"Sentence: \", example_text.numpy())\n",
        "vectorized_text, example_label = preprocess_text(example_text, example_label)\n",
        "print(\"Vectorized sentence: \", vectorized_text.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9qHM0v8k_Mg"
      },
      "source": [
        "次に、`Dataset.map` を使用して、データセットに対して前処理関数を実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmQVsAgJ-RM0"
      },
      "outputs": [],
      "source": [
        "all_encoded_data = all_labeled_data.map(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZToSXSm0qr"
      },
      "source": [
        "### データセットをトレーニング用セットとテスト用セットに分割する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itxIJwkrUXgv"
      },
      "source": [
        "Keras `TextVectorization` レイヤーでも、ベクトル化されたデータをバッチ処理してパディングします。バッチ内の例は同じサイズと形状である必要があるため、パディングが必要です。これらのデータセットの例はすべて同じサイズではありません。テキストの各行には、異なる数の単語があります。\n",
        "\n",
        "`tf.data.Dataset` は、データセットの分割とパディングのバッチ処理をサポートしています"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-rmbijQh6bf"
      },
      "outputs": [],
      "source": [
        "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
        "validation_data = all_encoded_data.take(VALIDATION_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTP0IwHBCn0Q"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "validation_data = validation_data.padded_batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-wmFq8uW1zS"
      },
      "source": [
        "`validation_data` および `train_data` は (`example, label`) ペアのコレクションではなく、バッチのコレクションです。各バッチは、配列として表される (*多くの例*、*多くのラベル*) のペアです。\n",
        "\n",
        "以下に示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMslWfuwoqpB"
      },
      "outputs": [],
      "source": [
        "sample_text, sample_labels = next(iter(validation_data))\n",
        "print(\"Text batch shape: \", sample_text.shape)\n",
        "print(\"Label batch shape: \", sample_labels.shape)\n",
        "print(\"First text example: \", sample_text[0])\n",
        "print(\"First label example: \", sample_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI4I6_Sa0vWu"
      },
      "source": [
        "パディングに 0 を使用し、語彙外 (OOV) トークンに 1 を使用するため、語彙のサイズが 2 つ増えました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u21LlkO8QGRX"
      },
      "outputs": [],
      "source": [
        "vocab_size += 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h44Ox11OYLP-"
      },
      "source": [
        "以前と同じように、パフォーマンスを向上させるためにデータセットを構成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpT0b_7mYRXV"
      },
      "outputs": [],
      "source": [
        "train_data = configure_dataset(train_data)\n",
        "validation_data = configure_dataset(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8SUhGFNsmRi"
      },
      "source": [
        "### モデルをトレーニングする\n",
        "\n",
        "以前と同じように、このデータセットでモデルをトレーニングできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJgI1pow2YR9"
      },
      "outputs": [],
      "source": [
        "model = create_model(vocab_size=vocab_size, num_labels=3)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data, validation_data=validation_data, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTPCYf_Jh6TH"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(validation_data)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_knIsO-r4pHb"
      },
      "source": [
        "### モデルをエクスポートする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEuMLJA_Xiwo"
      },
      "source": [
        "モデルが生の文字列を入力として受け取ることができるようにするには、カスタム前処理関数と同じ手順を実行する `TextVectorization` レイヤーを作成します。すでに語彙をトレーニングしているので、新しい語彙をトレーニングする `TextVectorization.adapt` の代わりに、`TextVectorization.set_vocabulary` を使用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ODkRXbk6aHb"
      },
      "outputs": [],
      "source": [
        "preprocess_layer = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    standardize=tf_text.case_fold_utf8,\n",
        "    split=tokenizer.tokenize,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "preprocess_layer.set_vocabulary(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-Cvd27y4qwt"
      },
      "outputs": [],
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [preprocess_layer, model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyg0B4zsc-UD"
      },
      "outputs": [],
      "source": [
        "# Create a test dataset of raw strings.\n",
        "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
        "test_ds = configure_dataset(test_ds)\n",
        "\n",
        "loss, accuracy = export_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Mm0Y9QYQwE"
      },
      "source": [
        "エンコードされた検証セットのモデルと生の検証セットのエクスポートされたモデルの損失と正確度は、予想どおり同じです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stk2BP8GE-qo"
      },
      "source": [
        "### 新しいデータで推論を実行する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w1fQGJPD2Yh"
      },
      "outputs": [],
      "source": [
        "inputs = [\n",
        "    \"Join'd to th' Ionians with their flowing robes,\",  # Label: 1\n",
        "    \"the allies, and his armour flashed about him so that he seemed to all\",  # Label: 2\n",
        "    \"And with loud clangor of his arms he fell.\",  # Label: 0\n",
        "]\n",
        "\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = tf.argmax(predicted_scores, axis=1)\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eA8TVdnA-3L"
      },
      "source": [
        "## TensorFlow Datasets (TFDS) を使用して、より多くのデータセットをダウンロードする\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QFSxfZ3Vqsn"
      },
      "source": [
        "[TensorFlow Dataset](https://www.tensorflow.org/datasets/catalog/overview) からより多くのデータセットをダウンロードできます。\n",
        "\n",
        "この例では、[IMDB 大規模映画レビューデータセット](https://www.tensorflow.org/datasets/catalog/imdb_reviews)を使用して、感情分類のモデルをトレーニングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzC65LOaVw0B"
      },
      "outputs": [],
      "source": [
        "# Training set.\n",
        "train_ds = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='train[:80%]',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKGkgPBkFh0k"
      },
      "outputs": [],
      "source": [
        "# Validation set.\n",
        "val_ds = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='train[80%:]',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQjf3YZAb5Ne"
      },
      "source": [
        "いくつかの例を出力します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq1w8MnfWt2C"
      },
      "outputs": [],
      "source": [
        "for review_batch, label_batch in val_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(\"Review: \", review_batch[i].numpy())\n",
        "    print(\"Label: \", label_batch[i].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-lVaukyb75k"
      },
      "source": [
        "これで、以前と同じようにデータを前処理してモデルをトレーニングできます。\n",
        "\n",
        "注意: これは二項分類の問題であるため、モデルには `tf.keras.losses.SparseCategoricalCrossentropy` の代わりに  `tf.keras.losses.BinaryCrossentropy` を使用します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciz2CxAsZw3Z"
      },
      "source": [
        "### トレーニング用データセットを準備する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzT_t9ihZLH4"
      },
      "outputs": [],
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
        "train_text = train_ds.map(lambda text, labels: text)\n",
        "vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz-Xrd_ZZ4tB"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycn0Itd6g5aF"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(vectorize_text)\n",
        "val_ds = val_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc11jQTlZ5lj"
      },
      "outputs": [],
      "source": [
        "# Configure datasets for performance as before.\n",
        "train_ds = configure_dataset(train_ds)\n",
        "val_ds = configure_dataset(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQzoYkaGZ82Z"
      },
      "source": [
        "### モデルを作成、構成、およびトレーニングする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9IOTLkyZ-a7"
      },
      "outputs": [],
      "source": [
        "model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=1)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLnDs5dhaBAk"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq59QpNzaDMa"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_ds, validation_data=val_ds, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCMWCEtyaEbR"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGtqLXVnaaFy"
      },
      "source": [
        "### モデルをエクスポートする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE9WZARZaZr1"
      },
      "outputs": [],
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [vectorize_layer, model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhF8tDH-afoC"
      },
      "outputs": [],
      "source": [
        "# 0 --> negative review\n",
        "# 1 --> positive review\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "]\n",
        "\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = [int(round(x[0])) for x in predicted_scores]\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1KSXDFPWiPN"
      },
      "source": [
        "## まとめ\n",
        "\n",
        "このチュートリアルでは、テキストを読み込んで前処理するいくつかの方法を示しました。次のステップとして、以下のような [TensorFlow Text](https://www.tensorflow.org/text) テキスト前処理 チュートリアルをご覧ください。\n",
        "\n",
        "- [TF テキストを使用した BERT 前処理](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)\n",
        "- [TF Text を使用したトークン化](https://www.tensorflow.org/text/guide/tokenizers)\n",
        "- [サブワードトークナイザー](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
        "\n",
        "[TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview) でも新しいデータセットを見つけることができます。また、`tf.data` の詳細については、[入力パイプラインの構築](../../guide/data.ipynb)に関するガイドをご覧ください。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
