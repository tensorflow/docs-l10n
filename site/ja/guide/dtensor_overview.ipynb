{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljvLya59ep5"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcQIa1uG86Wh"
      },
      "source": [
        "# DTensor の概念"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dWNQEum9AfY"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/dtensor_overview\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     TensorFlow.org で表示</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/guide/dtensor_overview.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colabで実行</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/guide/dtensor_overview.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHubでソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/guide/dtensor_overview.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGZuakHVlVQf"
      },
      "source": [
        "## 概要\n",
        "\n",
        "この Colab では、同期分散コンピューティングを行うための TensorFlow の拡張機能として提供されている DTensor を紹介します。\n",
        "\n",
        "DTensor は、開発者がデバイス間の分散を内部的に管理しながら、Tensor でグローバルに動作するアプリケーションを作成できるグローバルプログラミングモデルを提供します。DTensor は、*[SPMD（単一プログラム複数データ）](https://en.wikipedia.org/wiki/SPMD) expansion* と呼ばれる手順を通じて、シャーディングディレクティブに従ってプログラムとテンソルを分散します。\n",
        "\n",
        "アプリケーションとシャーディングディレクティブを分離することで、DTensor は、グローバルセマンティクスを保持しながら、単一のデバイス、複数のデバイス、または複数のクライアントにおける同一のアプリケーションの実行を可能にします。\n",
        "\n",
        "このガイドでは、分散コンピューティングの DTensor の概念と、DTensor が TensorFlow とどのように統合するかについて説明します。モデルトレーニングで DTensor を使用したデモについては、[DTensor を使った分散型トレーニング](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)チュートリアルをご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ZTDq7KngwA"
      },
      "source": [
        "## セットアップ\n",
        "\n",
        "DTensor は TensorFlow 2.9.0 リリースの一部であり、2022 年 4 月 9 日より、TensorFlow ナイトリービルドにも含まれています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKaPw8vwwZAC"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade --pre tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3pG29uZIWYO"
      },
      "source": [
        "インストールが完了したら、`tensorflow` と `tf.experimental.dtensor` をインポートします。そして、6 個の仮想 CPU を使用するように、TensorFlow を構成します。\n",
        "\n",
        "この例では vCPU を使用しますが、DTensor は CPU、GPU、または TPU デバイスでも同じように動作します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q92lo0zjwej8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.experimental import dtensor\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "def configure_virtual_cpus(ncpu):\n",
        "  phy_devices = tf.config.list_physical_devices('CPU')\n",
        "  tf.config.set_logical_device_configuration(phy_devices[0], [\n",
        "        tf.config.LogicalDeviceConfiguration(),\n",
        "    ] * ncpu)\n",
        "\n",
        "configure_virtual_cpus(6)\n",
        "DEVICES = [f'CPU:{i}' for i in range(6)]\n",
        "\n",
        "tf.config.list_logical_devices('CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-lsrxUnlsCC"
      },
      "source": [
        "## 分散テンソルの DTensor モデル\n",
        "\n",
        "DTensor は、`dtensor.Mesh` と `dtensor.Layout` の 2 つの概念を導入します。これらはテンソルのシャーディングをトポロジー的に関連するデバイス間でモデル化する抽象です。\n",
        "\n",
        "- `Mesh` は、コンピュテーションのデバイスリストを定義します。\n",
        "- `Layout` は、`Mesh` でテンソル次元をシャーディングする方法を定義します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjiHaH0ql9yo"
      },
      "source": [
        "### Mesh\n",
        "\n",
        "`Mesh` は、一連のデバイスの論理的な直行トポロジーを表現します。直行グリッドの各次元は**メッシュ次元**と呼ばれ、名前で参照されます。同じ `Mesh` 内のメッシュの名前は一意である必要があります。\n",
        "\n",
        "メッシュ次元の名前は `Layout` によって参照され、`tf.Tensor` の各軸に沿ったシャーディングの動作を説明します。これについては、`Layout` に関する後方のセクションでさらに詳しく説明します。\n",
        "\n",
        "`Mesh` は、デバイスの多次元配列として考えることができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J6cOieEbaUw"
      },
      "source": [
        "1 次元 `Mesh` では、すべてのデバイスが単一のメッシュ次元でリストを形成します。次の例では、`dtensor.create_mesh` を使用して、6 CPU デバイスから 6 デバイスのサイズを持つメッシュ次元 `'x'` のメッシュを作成します。\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_mesh_1d.png\" class=\"no-filter\" alt=\"6 CPU による 1 次元メッシュ\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLH5fgdBmA58"
      },
      "outputs": [],
      "source": [
        "mesh_1d = dtensor.create_mesh([('x', 6)], devices=DEVICES)\n",
        "print(mesh_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSZwaUwnEgXB"
      },
      "source": [
        "`Mesh` は多次元でもあります。次の例では、6 CPU デバイスで `3x2` のメッシュを形成します。`'x'` 次元メッシュは 3 デバイスのサイズ、`'y'` 次元メッシュは 2 デバイスのサイズです。\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_mesh_2d.png\" class=\"no-filter\" alt=\"6 CPU による 2 次元メッシュ\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op6TmKUQE-sZ"
      },
      "outputs": [],
      "source": [
        "mesh_2d = dtensor.create_mesh([('x', 3), ('y', 2)], devices=DEVICES)\n",
        "print(mesh_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deAqdrDPFn2f"
      },
      "source": [
        "### Layout\n",
        "\n",
        "**`Layout`** は、テンソルが `Mesh` でどのように分散されるか、またはシャーディングされるかを指定します。\n",
        "\n",
        "注意: `Mesh` と `Layout` を混同しないために、このガイドでは、*次元*と言った場合は常に `Mesh` に、*軸*と言った場合は常に `Tensor` と `Layout` に関連付けています。\n",
        "\n",
        "`Layout` の階数は、`Layout` が適用されている `Tensor` の階数と同じです。`Tensor` の各軸では、`Layout` がテンソルをシャーディングするメッシュ次元を指定しているか、字句を \"シャーディングなし\" として指定する場合があります。テンソルはシャーディングされていない任意のメッシュ次元で複製されます。\n",
        "\n",
        "`Layout` の階数と `Mesh` の次元数が一致している必要はありません。`Layout` の `unsharded` の軸がメッシュ次元に関連する必要も、`unsharded` メッシュ次元が `layout` 軸に関連している必要もありません。\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_components_diag.png\" alt=\"Diagram of dtensor components.\" class=\"\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px_bF1c-bQ7e"
      },
      "source": [
        "前のセクションで作成した `Mesh` の `Layout` の例をいくつか分析してみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqzCNlWAbm-c"
      },
      "source": [
        "`[(\"x\", 6)]` などの 1 次元メッシュ（前のセクションの `mesh_1d`）では、`Layout([\"unsharded\", \"unsharded\"], mesh_1d)` は、6 個のデバイスで複製された 2 階数のテンソルのレイアウトです。<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_replicated.png\" class=\"\" alt=\"1 階数のメッシュで複製されたテンソル\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a3EnmZag6x1"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywRJwuLDt2Qq"
      },
      "source": [
        "同じテンソルとメッシュを使用すると、レイアウト `Layout(['unsharded', 'x'])` は、6 個のデバイスでテンソルの 2 番目の軸をシャーディングします。\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_rank1.png\" alt=\"A tensor sharded across a rank-1 mesh\" class=\"\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BgqL0jUvV5a"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgciDNmK76l9"
      },
      "source": [
        "`[(\"x\", 3), (\"y\", 2)]` などの 2 次元の 3x2 メッシュ（前のセクションの `mesh_2d`）とした場合、`Layout([\"y\", \"x\"], mesh_2d)` は 2 階数 `Tensor` のレイアウトで、最初の軸はメッシュ次元 `\"y\"` で、2 番目の軸はメッシュ次元 `\"x\"` でシャーディングされます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyp_qOSyvieo"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_rank2.png\" alt=\"A tensorr with it's first axis sharded across mesh dimension 'y' and it's second axis sharded across mesh dimension 'x'\" class=\"\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8OrehEuhPbS"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout(['y', 'x'], mesh_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kyg0V3ehMNJ"
      },
      "source": [
        "同じ `mesh_2d` において、レイアウト `Layout([\"x\", dtensor.UNSHARDED], mesh_2d)` は、`\"y\"` で複製される 2 階数 `Tensor` のレイアウトで、最初の軸はメッシュ次元 `x` でシャーディングされます。\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_hybrid.png\" alt=\"A tensor replicated across mesh-dimension y, with it's first axis sharded across mesh dimension 'x'\" class=\"\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkWe6mVl7uRb"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout([\"x\", dtensor.UNSHARDED], mesh_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTalu6M-ISYb"
      },
      "source": [
        "### シングルクライアントとマルチクライアントのアプリケーション\n",
        "\n",
        "DTensor は、シングルクライアントとマルチクライアントの両方のアプリケーションをサポートしています。Colab の Python カーネルはシングルクライアントアプリケーションの例で、Python プロセスが 1 つしかありません。\n",
        "\n",
        "マルチクライアント DTensor アプリケーションでは、複数の Python プロセスが一貫性のあるアプリケーションとして集合的に実行します。マルチクライアント DTensor アプリケーションの `Mesh` の直交グリッドは、現在のクライアントにローカルで接続されているか、別のクライアントにリモートで接続されているかに関係なく、デバイス全体に広がります。`Mesh` が使用する一連の全デバイスは、*グローバルデバイスリスト*と呼ばれます。\n",
        "\n",
        "マルチクライアント DTensor アプリケーションでの `Mesh` の作成は、すべての参加クライアントが同一の*グローバルデバイスリスト*を使う集合的な演算で、`Mesh` の作成はグローバルなバリアとして機能します。\n",
        "\n",
        "`Mesh` を作成中、各クライアントは*ローカルデバイスリスト*と期待される*グローバアルデバイスリスト*を提供し、DTensor はそれら両方のリストが一貫していることを検証します。マルチクライアントメッシュの作成と*グローバルデバイスリスト*の詳細については、`dtensor.create_mesh` と `dtensor.create_distributed_mesh` の API ドキュメントをご覧ください。\n",
        "\n",
        "シングルクライアントは、クライアントが 1 つしかないマルチクライアントの特殊ケースとして考え得られます。シングルクライアントアプリケーションの場合、*グローバルデバイスリスト*は*ローカルデバイスリスト*と同一です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_F7DWkXkB4w"
      },
      "source": [
        "## シャーディングされたテンソルとしての DTensor\n",
        "\n",
        "では、`DTensor` を使ってコーディングを始めましょう。ヘルパー関数の `dtensor_from_array` は、`tf.Tensor` のように見えるものから DTensor を作成する方法を説明しています。この関数は 2 つのステップを実行します。\n",
        "\n",
        "- テンソルをメッシュ上のすべてのデバイスに複製する\n",
        "- 引数でリクエストされているレイアウトに従って、コピーをシャーディングする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6aws-b8dN9L"
      },
      "outputs": [],
      "source": [
        "def dtensor_from_array(arr, layout, shape=None, dtype=None):\n",
        "  \"\"\"Convert a DTensor from something that looks like an array or Tensor.\n",
        "\n",
        "  This function is convenient for quick doodling DTensors from a known,\n",
        "  unsharded data object in a single-client environment. This is not the\n",
        "  most efficient way of creating a DTensor, but it will do for this\n",
        "  tutorial.\n",
        "  \"\"\"\n",
        "  if shape is not None or dtype is not None:\n",
        "    arr = tf.constant(arr, shape=shape, dtype=dtype)\n",
        "\n",
        "  # replicate the input to the mesh\n",
        "  a = dtensor.copy_to_mesh(arr,\n",
        "          layout=dtensor.Layout.replicated(layout.mesh, rank=layout.rank))\n",
        "  # shard the copy to the desirable layout\n",
        "  return dtensor.relayout(a, layout=layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3o6IysrlGMu"
      },
      "source": [
        "### DTensor の構造\n",
        "\n",
        "DTensor は `tf.Tensor` オブジェクトですが、シャーディングの振る舞いを定義する `Layout` アノテーションで拡張されています。DTensor は以下の内容で構成されています。\n",
        "\n",
        "- テンソルのグローバルな形状と dtype などを含むグローバルテンソルメタデータ\n",
        "- `Tensor` が属する `Mesh` と、`Tensor` がその `Mesh` にどのようにシャーディングされるかを定義する `Layout`\n",
        "- `Mesh` 内のローカルデバイスあたり 1 つの項目を持つ**コンポーネントテンソル**のリスト\n",
        "\n",
        "`dtensor_from_array` を使用すると、最初の DTensor である `my_first_dtensor` を作成し、その内容を調べることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQu_nScGUvYH"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
        "layout = dtensor.Layout([dtensor.UNSHARDED], mesh)\n",
        "\n",
        "my_first_dtensor = dtensor_from_array([0, 1], layout)\n",
        "\n",
        "# Examine the dtensor content\n",
        "print(my_first_dtensor)\n",
        "print(\"global shape:\", my_first_dtensor.shape)\n",
        "print(\"dtype:\", my_first_dtensor.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8LQy1nqmvFy"
      },
      "source": [
        "#### レイアウトと `fetch_layout`\n",
        "\n",
        "DTensor のレイアウトは、`tf.Tensor` の通常の属性ではありません。代わりに DTensor は DTensor のレイアウトにアクセスするための関数 `dtensor.fetch_layout` を提供します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCSFyaAjmzGu"
      },
      "outputs": [],
      "source": [
        "print(dtensor.fetch_layout(my_first_dtensor))\n",
        "assert layout == dtensor.fetch_layout(my_first_dtensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7i3l2lmatm"
      },
      "source": [
        "#### コンポーネントテンソル、`pack` と `unpack`\n",
        "\n",
        "DTensor は**コンポーネントテンソル**のリストで構成されます。`Mesh` 内のデバイスのコンポーネントテンソルは、そのデバイスに格納されているグローバル DTensor を表現する `Tensor` オブジェクトです。\n",
        "\n",
        "DTensor は `dtensor.unpack` を使ってコンポーネントテンソルにアンパックできます。`dtensor.unpack` を使用すれば、DTensor のコンポーネントを調べて、それらが `Mesh` のすべてのデバイス上にあることを確認できます。\n",
        "\n",
        "グローバルビューのコンポーネントテンソルの位置は、互いに重なり合っていることに注意してください。たとえば、完全に複製されたレイアウトの場合、すべてのコンポーネントはグローバルテンソルの同一のレプリカになっています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGbjqVAOnXMk"
      },
      "outputs": [],
      "source": [
        "for component_tensor in dtensor.unpack(my_first_dtensor):\n",
        "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tqIQM52k788"
      },
      "source": [
        "示されているとおり、`my_first_dtensor` は、すべての 6 個のデバイスに複製されている `[0, 1]` のテンソルです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6By3k-CGn3yv"
      },
      "source": [
        "`dtensor.unpack` の反対の演算は `dtensor.pack` です。コンポーネントテンソルは DTensor にパックし直すことができます。\n",
        "\n",
        "コンポーネントには同じ階数と dtype がある必要があります。つまり、これが、戻される DTensor の階数と dtype になります。ただし、`dtensor.unpack` の入力として、コンポーネントテンソルのデバイスの配置に関する厳格な要件はありません。関数は、コンポーネントテンソルを自動的に対応するそれぞれのデバイスにコピーします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lT-6qQwxOgf"
      },
      "outputs": [],
      "source": [
        "packed_dtensor = dtensor.pack(\n",
        "    [[0, 1], [0, 1], [0, 1],\n",
        "     [0, 1], [0, 1], [0, 1]],\n",
        "     layout=layout\n",
        ")\n",
        "print(packed_dtensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvS3autrpK2U"
      },
      "source": [
        "### DTensor をメッシュにシャーディングする\n",
        "\n",
        "ここまで、`my_first_dtensor` を操作してきました。これは、1 次元 `Mesh` に完全に複製された 1 階数 DTensor です。\n",
        "\n",
        "次は、2 次元 `Mesh` にシャーディングされた DTensor を作成して検査します。次の例では、6 個の CPU デバイス上の 3x2 `Mesh` でこの操作を行います。メッシュ次元 `'x'` のサイズは 3 デバイス、メッシュ次元 `'y'` のサイズは 2 デバイスです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWb9Ae0VJ-Rc"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndSeQSFWKQk9"
      },
      "source": [
        "#### 2 次元メッシュで完全にシャーディングされた 2 階数 Tensor\n",
        "\n",
        "3x2 の 2 階数 DTensor を作成し、最初の軸を `'x'` メッシュ次元に沿って、2 番目の軸を `'y'` メッシュ次元に沿ってシャーディングします。\n",
        "\n",
        "- テンソルの形状は、すべてのシャーディングされた軸に沿ってメッシュ次元と同じであるため、各デバイスは DTensor の1 つの要素を受け取ります。\n",
        "- コンポーネントテンソルの階数は、必ずグローバル形状の階数と同じです。DTensor はコンポーネントテンソルとグローバル DTensor の関係を特定するための情報を保持する単純な方法として、この手法を採用しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax_ZHouJp1MX"
      },
      "outputs": [],
      "source": [
        "fully_sharded_dtensor = dtensor_from_array(\n",
        "    tf.reshape(tf.range(6), (3, 2)),\n",
        "    layout=dtensor.Layout([\"x\", \"y\"], mesh))\n",
        "\n",
        "for raw_component in dtensor.unpack(fully_sharded_dtensor):\n",
        "  print(\"Device:\", raw_component.device, \",\", raw_component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhsLC-NgrC2p"
      },
      "source": [
        "#### 2 次元メッシュで完全に複製された 2 階数 Tensor\n",
        "\n",
        "比較するために、同じ 2 次元メッシュに完全に複製された 3x2 の 2 階数 DTensor を作成します。\n",
        "\n",
        "- DTensor は完全に複製されているため、各デバイスは 3x2 DTensor の完全レプリカを受け取ります。\n",
        "- コンポーネントテンソルの階数はグローバル形状の階数と同じです。この場合、コンポーネントテンソルの形状はいずれにしてもグローバル形状と同じであるため、特に難しい事ではありません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmyC6H6Ec90P"
      },
      "outputs": [],
      "source": [
        "fully_replicated_dtensor = dtensor_from_array(\n",
        "    tf.reshape(tf.range(6), (3, 2)),\n",
        "    layout=dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh))\n",
        "# Or, layout=tensor.Layout.fully_replicated(mesh, rank=2)\n",
        "\n",
        "for component_tensor in dtensor.unpack(fully_replicated_dtensor):\n",
        "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWoyv_oHMzk1"
      },
      "source": [
        "#### 2 次元メッシュのハイブリッド 2 階数 Tensor\n",
        "\n",
        "完全シャーディングと完全複製の合間の場合はどうでしょうか。\n",
        "\n",
        "DTensor では、`Layout` をハイブリッドにすることができます。ある軸でシャーディングされ、他の軸で複製されたレイアウトです。\n",
        "\n",
        "たとえば、同じ 3x2 の 2 階数 DTensor を以下のようにシャーディングできます。\n",
        "\n",
        "- 1 つ目の軸を `'x'` 次元メッシュに沿ってシャーディング\n",
        "- 2 つ目の軸を `'y'` 次元メッシュに沿って複製\n",
        "\n",
        "このシャーディングスキームは、2 つ目の軸のシャーディング仕様を `'y'` から `dtensor.UNSHARDED` に置き換え、2 番目の軸にそって複製する意図を示すだけで実現できます。レイアウトオブジェクトは `Layout(['x', dtensor.UNSHARDED], mesh)` のようになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DygnbkQ1Lu42"
      },
      "outputs": [],
      "source": [
        "hybrid_sharded_dtensor = dtensor_from_array(\n",
        "    tf.reshape(tf.range(6), (3, 2)),\n",
        "    layout=dtensor.Layout(['x', dtensor.UNSHARDED], mesh))\n",
        "\n",
        "for component_tensor in dtensor.unpack(hybrid_sharded_dtensor):\n",
        "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7FtZ9kQRZgE"
      },
      "source": [
        "作成した DTensor のコンポーネントテンソルを検査し、これらが実際にスキームに従ってシャーディングされていることを確認できます。この様子をチャートで示すとわかりやすいでしょう。\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_hybrid_mesh.png\" alt=\"A 3x2 hybrid mesh with 6 CPUs\" class=\"\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auAkA38XjL-q"
      },
      "source": [
        "#### Tensor.numpy() とシャーディングされた DTensor\n",
        "\n",
        "シャーディングされた DTensor に `.numpy()` を呼び出すとエラーが発生することに注意してください。エラーが発生する理由は、複数のコンピューティングデバイスのデータが、返される NumPy 配列をサポートするホスト CPU デバイスに意図せずに収集されないようにするためです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNdwmnL0jAXS"
      },
      "outputs": [],
      "source": [
        "print(fully_replicated_dtensor.numpy())\n",
        "\n",
        "try:\n",
        "  fully_sharded_dtensor.numpy()\n",
        "except tf.errors.UnimplementedError:\n",
        "  print(\"got an error as expected for fully_sharded_dtensor\")\n",
        "\n",
        "try:\n",
        "  hybrid_sharded_dtensor.numpy()\n",
        "except tf.errors.UnimplementedError:\n",
        "  print(\"got an error as expected for hybrid_sharded_dtensor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WcMkiagPF_6"
      },
      "source": [
        "## DTensor での TensorFlow API\n",
        "\n",
        "DTensor はプログラムのテンソルのドロップイン代替となることを目指しています。Ops ライブラリ関数の `tf.function` や `tf.GradientTape` といった、`tf.Tensor` を消費する TensorFlow Python API も DTensor と動作します。\n",
        "\n",
        "それを実現するため、それぞれの [TensorFlow Graph](https://www.tensorflow.org/guide/intro_to_graphs) に対し、DTensor は *SPMD expansion* と呼ばれる手順で相当する [SPMD](https://en.wikipedia.org/wiki/SPMD) グラフを生成して実行します。DTensor の SPMD expansion には、以下のような重要なステップがいくつか含まれます。\n",
        "\n",
        "- DTensor のシャーディング `Layout` を TensorFlow グラフに伝搬する\n",
        "- グローバル DTensor の TensorFlow Ops をコンポーネントテンソルの相当する TensorFlow Ops に書き換え、必要に応じて集合的な通信 Ops を挿入する\n",
        "- バックエンドの中立した TensorFlow Ops をバックエンド固有の TensorFlow Ops に降格する\n",
        "\n",
        "最終的に、**DTensor は Tensor のドロップイン代替**になります。\n",
        "\n",
        "注意: DTensor はまだ実験的 API であるため、DTensor プログラミングモデルの境界と制限を探索しながら克服する作業となります。\n",
        "\n",
        "DTensor の実行は 2 つの方法でトリガーされます。\n",
        "\n",
        "- `tf.matmul(a, b)` のように、Python 関数のオペランドとしての DTensor は `a` または `b` のいずれか、または両方が DTensor である場合に DTensor を介して実行します。\n",
        "- Python 関数の結果が DTensor となるようにリクエストすると（`dtensor.call_with_layout(tf.ones, layout, shape=(3, 2))` など）、tf.ones の出力が `layout` に従ってシャーディングされるようにリクエストすることになるため、DTensor を介して実行されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urKzmqAoPssT"
      },
      "source": [
        "### オペランドとしての DTensor\n",
        "\n",
        "多数の TensorFlow API 関数はオペランドとして `tf.Tensor` を取り、結果として `tf.Tensor` を返します。このような関数の場合、DTensor をオペランドとして渡すことで、DTensor を介して関数を実行する意図を示すことができます。このセクションでは、例として `tf.matmul(a, b)` を使用します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LO8ZT7iWVga"
      },
      "source": [
        "#### 完全に複製された入力と出力\n",
        "\n",
        "この場合、DTensors は完全に複製されています。`Mesh` の各デバイスで、以下のようになっています。\n",
        "\n",
        "- オペランド `a` のコンポーネントテンソルは `[[1, 2, 3], [4, 5, 6]]`（2x3）\n",
        "- オペランド `b` のコンポーネントテンソルは `[[6, 5], [4, 3], [2, 1]]`（3x2）\n",
        "- コンピュテーションは、単一の `MatMul` の `(2x3, 3x2) -> 2x2` で構成されます。\n",
        "- 結果 `c` のコンポーネントテンソルは `[[20, 14], [56,41]]`（2x2）\n",
        "\n",
        "浮動小数点 mul 演算の合計数は、`6 device * 4 result * 3 mul = 72` です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiZf2J9JNd2D"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
        "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)\n",
        "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=layout)\n",
        "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=layout)\n",
        "\n",
        "c = tf.matmul(a, b) # runs 6 identical matmuls in parallel on 6 devices\n",
        "\n",
        "# `c` is a DTensor replicated on all devices (same as `a` and `b`)\n",
        "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)\n",
        "print(\"components:\")\n",
        "for component_tensor in dtensor.unpack(c):\n",
        "  print(component_tensor.device, component_tensor.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXtR9qgKWgWV"
      },
      "source": [
        "#### 収縮した軸に沿ってオペランドをシャーディングする\n",
        "\n",
        "デバイスごとのコンピュテーションの量は、オペランド `a` と `b` をシャーディングすることで、減らすことができます。`tf.matmul` の一般的なシャーディングスキームは、収縮の軸に沿ったオペランドのシャーディングで、2 番目の軸に沿った `a` のシャーディングと 1 番目の軸に沿った `b` のシャーディングです。\n",
        "\n",
        "このスキームでシャーディングされるグローバル行列積は、同時に実行するローカル matmul と、それに続くローカル結果を集計するための一括還元によって効率的に実行可能です。これは、分散行列ドット積の[正規の方法](https://github.com/open-mpi/ompi/blob/ee87ec391f48512d3718fc7c8b13596403a09056/docs/man-openmpi/man3/MPI_Reduce.3.rst?plain=1#L265)でもあります。\n",
        "\n",
        "浮動小数点 mul 演算の合計数は、`6 devices * 4 result * 1 = 24` で、完全に複製された上記のケース（72）に対する係数 3 の還元です。係数 3 は、`3` デバイスのサイズで `x` 次元メッシュに沿って共有されるためです。\n",
        "\n",
        "順次実行される操作数の削減は、同期モデル並列処理がトレーニングを加速する主なメカニズムです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyVAUvMePbms"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "a_layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh)\n",
        "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=a_layout)\n",
        "b_layout = dtensor.Layout(['x', dtensor.UNSHARDED], mesh)\n",
        "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=b_layout)\n",
        "\n",
        "c = tf.matmul(a, b)\n",
        "# `c` is a DTensor replicated on all devices (same as `a` and `b`)\n",
        "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhD8yYgJiCEh"
      },
      "source": [
        "#### 追加シャーディング\n",
        "\n",
        "入力に追加のシャーディングを実行し、結果に適切に引き継ぐことが可能です。たとえば、最初の軸に沿ったオペランド `a` の追加のシャーディングを `'y'` 次元メッシュに適用することができます。追加のシャーディングは、結果 `c` の最初に軸に引き継がれます。\n",
        "\n",
        "浮動小数点 mul 演算の合計数は、`6 devices * 2 result * 1 = 12` で、完全に複製された上記のケース（24）に対する係数 2 の還元です。係数 2 は、`2` デバイスのサイズで `y` 次元メッシュに沿って共有されるためです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PYqe0neiOpR"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "\n",
        "a_layout = dtensor.Layout(['y', 'x'], mesh)\n",
        "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=a_layout)\n",
        "b_layout = dtensor.Layout(['x', dtensor.UNSHARDED], mesh)\n",
        "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=b_layout)\n",
        "\n",
        "c = tf.matmul(a, b)\n",
        "# The sharding of `a` on the first axis is carried to `c'\n",
        "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)\n",
        "print(\"components:\")\n",
        "for component_tensor in dtensor.unpack(c):\n",
        "  print(component_tensor.device, component_tensor.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-1NazCVmLWZ"
      },
      "source": [
        "### 出力としての DTensor\n",
        "\n",
        "オペランドを取らずに、シャーディング可能な Tensor 結果を返す Python 関数の場合はどうでしょうか。以下のような関数がこれに該当します。\n",
        "\n",
        "- `tf.ones`、`tf.zeros`、`tf.random.stateless_normal`\n",
        "\n",
        "こういった Python 関数の場合、DTensor には、DTensor で Python 関数を Eager 実行する `dtensor.call_with_layout` が備わっており、返される Tensor が要求された `Layout` を使った DTensor であることを保証します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0jo_8NPtJiO"
      },
      "outputs": [],
      "source": [
        "help(dtensor.call_with_layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-YdLvfytM7g"
      },
      "source": [
        "Eager 実行された Python 関数には通常、1 つの自明ではない TensorFlow Op のみが含まれます。\n",
        "\n",
        "`dtensor.call_with_layout` で複数の TensorFlow Op を発行する Python 関数を使用するには、関数を `tf.function` に変換する必要があります。`tf.function` の呼び出しは、単一の TensorFlow Op です。`tf.function` が呼び出されると、DTensor は `tf.function` の計算グラフを分析するときに、中間テンソルのいずれかが具体化される前にレイアウトの伝播を実行できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLrksgFjqRLS"
      },
      "source": [
        "#### 1 つの TensorFlow Op を発行する API\n",
        "\n",
        "関数が 1 つの TensorFlow Op を発行する場合、その関数に直接 `dtensor.call_with_layout` を適用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1CuKYSFtFeM"
      },
      "outputs": [],
      "source": [
        "help(tf.ones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m_EAwy-ozOh"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "ones = dtensor.call_with_layout(tf.ones, dtensor.Layout(['x', 'y'], mesh), shape=(6, 4))\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx-7Xo8Cpb8S"
      },
      "source": [
        "#### 複数の TensorFlow Op を発行する API\n",
        "\n",
        "API が複数の TensorFlow Op を発行する場合、`tf.function` を介して関数を 1 つの Op に変換します。たとえば、`tf.random.stateleess_normal` です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8BQSTRFtCih"
      },
      "outputs": [],
      "source": [
        "help(tf.random.stateless_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvP81eYopSPm"
      },
      "outputs": [],
      "source": [
        "ones = dtensor.call_with_layout(\n",
        "    tf.function(tf.random.stateless_normal),\n",
        "    dtensor.Layout(['x', 'y'], mesh),\n",
        "    shape=(6, 4),\n",
        "    seed=(1, 1))\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKoojp9ZyWzW"
      },
      "source": [
        "1 つの TensorFlow Op を発行する Python 関数を `tf.function` でラップすることができます。唯一の注意点は、Python 関数から `tf.function` を作成するための関連コストと複雑さが発生することです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbAtKrSkpOaq"
      },
      "outputs": [],
      "source": [
        "ones = dtensor.call_with_layout(\n",
        "    tf.function(tf.ones),\n",
        "    dtensor.Layout(['x', 'y'], mesh),\n",
        "    shape=(6, 4))\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-m1816JP3CE"
      },
      "source": [
        "### `tf.Variable` から `dtensor.DVariable`\n",
        "\n",
        "Tensorflow では、`tf.Variable` はミュータブルの `Tensor` 値のホルダーです。DTensor では、対応する変数のセマンティクスが `dtensor.DVariable` によって提供されます。\n",
        "\n",
        "DTensor 変数に新しい型 `Variable` が導入されたのは、変数にはレイアウトを初期値から変更できないという追加の要件があるためです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awRPuR26P0Sc"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
        "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)\n",
        "\n",
        "v = dtensor.DVariable(\n",
        "    initial_value=dtensor.call_with_layout(\n",
        "        tf.function(tf.random.stateless_normal),\n",
        "        layout=layout,\n",
        "        shape=tf.TensorShape([64, 32]),\n",
        "        seed=[1, 1],\n",
        "        dtype=tf.float32))\n",
        "\n",
        "print(v.handle)\n",
        "assert layout == dtensor.fetch_layout(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb9jn473prC_"
      },
      "source": [
        "`layout` の一致に関する要件を除けば、`Variable` は `tf.Variable` と同じように動作します。たとえば、変数を DTensor に追加できます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adxFw9wJpqQQ"
      },
      "outputs": [],
      "source": [
        "a = dtensor.call_with_layout(tf.ones, layout=layout, shape=(64, 32))\n",
        "b = v + a # add DVariable and DTensor\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxBdNHWSu-kV"
      },
      "source": [
        "また、DTensor を DVariable に代入することもできます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYwfiyw5P94U"
      },
      "outputs": [],
      "source": [
        "v.assign(a) # assign a DTensor to a DVariable\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fvSk_VUvGnj"
      },
      "source": [
        "DTensor に互換性のないレイアウトを割り当てて `DVariable` のレイアウトを変更しようとすると、エラーが発生します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pckUugYP_r-"
      },
      "outputs": [],
      "source": [
        "# variable's layout is immutable.\n",
        "another_mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "b = dtensor.call_with_layout(tf.ones,\n",
        "                     layout=dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], another_mesh),\n",
        "                     shape=(64, 32))\n",
        "try:\n",
        "  v.assign(b)\n",
        "except:\n",
        "  print(\"exception raised\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LadIcwRvR6f"
      },
      "source": [
        "## 次のステップ\n",
        "\n",
        "この Colab では、分散コンピューティングを行うための TensorFlow 拡張機能である DTensor について学習しました。チュートリアルでこれらの概念を試すには、[DTensor による分散トレーニング](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)をご覧ください。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "dtensor_overview.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
