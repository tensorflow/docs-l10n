{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e083398b477"
      },
      "source": [
        "# Working with preprocessing layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64010bd23c2e"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/preprocessing_layers\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で実行</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/keras-team/keras-io/blob/master/guides/preprocessing_layers.py\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     GitHubでソースを表示</a></td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/keras/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5387d1418bf5"
      },
      "source": [
        "## Keras preprocessing layers\n",
        "\n",
        "The Keras preprocessing layers API allows developers to build Keras-native input\n",
        "processing pipelines. These input processing pipelines can be used as independent\n",
        "preprocessing code in non-Keras workflows, combined directly with Keras models, and\n",
        "exported as part of a Keras SavedModel.\n",
        "\n",
        "With Keras preprocessing layers, you can build and export models that are truly\n",
        "end-to-end: models that accept raw images or raw structured data as input; models that\n",
        "handle feature normalization or feature value indexing on their own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "338796ee379c"
      },
      "source": [
        "## 利用可能な前処理レイヤー\n",
        "\n",
        "### 主な前処理レイヤー\n",
        "\n",
        "- `TextVectorization`レイヤー：生の文字列を、`Embedded`レイヤーまたは`Dense`レイヤーで読み取ることができるエンコードされた表現に変換します。\n",
        "- `Normalization`レイヤー：入力する特徴の特徴ごとの正規化を実行します。\n",
        "\n",
        "### 構造化されたデータ前処理レイヤー\n",
        "\n",
        "これらのレイヤーは、構造化データのエンコードとフィーチャエンジニアリング用です。\n",
        "\n",
        "- `CategoryEncoding`レイヤー：整数のカテゴリカルフィーチャをワンホット、マルチホット、または TF-IDF の密な表現に変換します。\n",
        "- `Hashing`レイヤー：「ハッシュトリック」とも呼ばれるカテゴリカルフィーチャハッシュを実行します。\n",
        "- `Discretization`レイヤー：連続する数値特徴を整数のカテゴリカルフィーチャに変換します。\n",
        "- `StringLookup`レイヤー：文字列のカテゴリ値を整数のインデックスに変換します。\n",
        "- `IntegerLookup`レイヤー：整数のカテゴリ値を整数のインデックスに変換します。\n",
        "- `CategoryCrossing`レイヤー：カテゴリカルフィーチャを共起フィーチャに結合します。 例えば、 特徴値「a」と「b」がある場合、「a と b が同時に存在する」という組み合わせの特徴を提供できます。\n",
        "\n",
        "### 画像前処理レイヤー\n",
        "\n",
        "これらのレイヤーは、画像モデルの入力を標準化するためのものです。\n",
        "\n",
        "- `Resizing`レイヤー：画像のバッチをターゲットサイズにサイズ変更します。\n",
        "- `Rescaling`レイヤー：画像のバッチの値を再スケーリングしてオフセットします（たとえば、`[0, 255]`範囲の入力から`[0, 1]`範囲の入力に移動します。\n",
        "- `CenterCrop`レイヤー：画像のバッチの場合、中央のトリミングを返します。\n",
        "\n",
        "### 画像データ増強レイヤー\n",
        "\n",
        "これらのレイヤーは、ランダムな増強変換を画像のバッチに適用します。これらはトレーニング中にのみアクティブになります。\n",
        "\n",
        "- `RandomCrop`レイヤー\n",
        "- `RandomFlip`レイヤー\n",
        "- `RandomTranslation`レイヤー\n",
        "- `RandomRotation`レイヤー\n",
        "- `RandomZoom`レイヤー\n",
        "- `RandomHeight`レイヤー\n",
        "- `RandomWidth`レイヤー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "672e84af6eb7"
      },
      "source": [
        "## The `adapt()` method\n",
        "\n",
        "Some preprocessing layers have an internal state that must be computed based on\n",
        "a sample of the training data. The list of stateful preprocessing layers is:\n",
        "\n",
        "- `TextVectorization`: holds a mapping between string tokens and integer indices\n",
        "- `Normalization`: holds the mean and standard deviation of the features\n",
        "- `StringLookup` and `IntegerLookup`: hold a mapping between input values and output\n",
        "indices.\n",
        "- `CategoryEncoding`: holds an index of input values.\n",
        "- `Discretization`: holds information about value bucket boundaries.\n",
        "\n",
        "Crucially, these layers are **non-trainable**. Their state is not set during training; it\n",
        "must be set **before training**, a step called \"adaptation\".\n",
        "\n",
        "You set the state of a preprocessing layer by exposing it to training data, via the\n",
        "`adapt()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e05c8fc4d032"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "data = np.array([[0.1, 0.2, 0.3], [0.8, 0.9, 1.0], [1.5, 1.6, 1.7],])\n",
        "layer = preprocessing.Normalization()\n",
        "layer.adapt(data)\n",
        "normalized_data = layer(data)\n",
        "\n",
        "print(\"Features mean: %.2f\" % (normalized_data.numpy().mean()))\n",
        "print(\"Features std: %.2f\" % (normalized_data.numpy().std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d43b8246b8a3"
      },
      "source": [
        "`adapt()`メソッドは、Numpy 配列または`tf.data.Dataset`オブジェクトのいずれかを取ります。`StringLookup`および`TextVectorization`の場合、文字列のリストを渡すこともできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ef94620592"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    \"ξεῖν᾽, ἦ τοι μὲν ὄνειροι ἀμήχανοι ἀκριτόμυθοι\",\n",
        "    \"γίγνοντ᾽, οὐδέ τι πάντα τελείεται ἀνθρώποισι.\",\n",
        "    \"δοιαὶ γάρ τε πύλαι ἀμενηνῶν εἰσὶν ὀνείρων:\",\n",
        "    \"αἱ μὲν γὰρ κεράεσσι τετεύχαται, αἱ δ᾽ ἐλέφαντι:\",\n",
        "    \"τῶν οἳ μέν κ᾽ ἔλθωσι διὰ πριστοῦ ἐλέφαντος,\",\n",
        "    \"οἵ ῥ᾽ ἐλεφαίρονται, ἔπε᾽ ἀκράαντα φέροντες:\",\n",
        "    \"οἱ δὲ διὰ ξεστῶν κεράων ἔλθωσι θύραζε,\",\n",
        "    \"οἵ ῥ᾽ ἔτυμα κραίνουσι, βροτῶν ὅτε κέν τις ἴδηται.\",\n",
        "]\n",
        "layer = preprocessing.TextVectorization()\n",
        "layer.adapt(data)\n",
        "vectorized_text = layer(data)\n",
        "print(vectorized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7619914dfb40"
      },
      "source": [
        "さらに、適応可能なレイヤーは、コンストラクタ引数または重みの割り当てを介して状態を直接設定するオプションを常に公開します。意図した状態値がレイヤー構築時にわかっている場合、または`adapt()`呼び出しの外で計算される場合は、レイヤーの内部計算に依存せずに設定できます。例えば、`TextVectorization`、`StringLookup`、または、`IntegerLookup`レイヤーの外部語彙ファイルがすでに存在する場合、レイヤーのコンストラクタ引数で語彙ファイルへのパスを渡すことにより、それらをルックアップテーブルに直接読み込めます。\n",
        "\n",
        "以下の例では、事前に計算された語彙を使用して`StringLookup`レイヤーをインスタンス化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76aeb9346838"
      },
      "outputs": [],
      "source": [
        "vocab = [\"a\", \"b\", \"c\", \"d\"]\n",
        "data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n",
        "layer = preprocessing.StringLookup(vocabulary=vocab)\n",
        "vectorized_data = layer(data)\n",
        "print(vectorized_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd07d2533da"
      },
      "source": [
        "## Preprocessing data before the model or inside the model\n",
        "\n",
        "There are two ways you could be using preprocessing layers:\n",
        "\n",
        "**Option 1:** Make them part of the model, like this:\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = preprocessing_layer(inputs)\n",
        "outputs = rest_of_the_model(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "```\n",
        "\n",
        "With this option, preprocessing will happen on device, synchronously with the rest of the\n",
        "model execution, meaning that it will benefit from GPU acceleration.\n",
        "If you're training on GPU, this is the best option for the `Normalization` layer, and for\n",
        "all image preprocessing and data augmentation layers.\n",
        "\n",
        "**Option 2:** apply it to your `tf.data.Dataset`, so as to obtain a dataset that yields\n",
        "batches of preprocessed data, like this:\n",
        "\n",
        "```python\n",
        "dataset = dataset.map(\n",
        "  lambda x, y: (preprocessing_layer(x), y))\n",
        "```\n",
        "\n",
        "With this option, your preprocessing will happen on CPU, asynchronously, and will be\n",
        "buffered before going into the model.\n",
        "\n",
        "This is the best option for `TextVectorization`, and all structured data preprocessing\n",
        "layers. It can also be a good option if you're training on CPU\n",
        "and you use image preprocessing layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32f6d2a104b7"
      },
      "source": [
        "## 推論時にモデル内で前処理を行うことの利点\n",
        "\n",
        "オプション 2 を選択した場合でも、後で前処理レイヤーを含む推論のみのエンドツーエンドモデルをエクスポートしたい場合があります。これを行う主な利点は、**モデルを移植可能にする**ことと、**[トレーニング/サービングスキュー](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)**を減らせることです。\n",
        "\n",
        "すべてのデータ前処理がモデルの一部である場合、他のユーザーは、各特徴がどのようにエンコードおよび正規化されるかを知らなくても、モデルを読み込んで使用できます。推論モデルは生の画像または生の構造化データを処理できるようになり、モデルのユーザーは画像のピクセル値が`[-1, +1]`、または、`[0, 1]`に正規化されていても、テキストに使用されるトークン化スキーム、カテゴリカルフィーチャに使用されるインデックススキームの詳細を認識する必要はありません。これは、モデルを TensorFlow.js などの別のランタイムにエクスポートする場合に特に有用です。JavaScript で前処理パイプラインを再実装する必要はありません。\n",
        "\n",
        "最初に前処理レイヤーを`tf.data`パイプラインに配置した場合、前処理をパッケージ化する推論モデルをエクスポートできます。前処理レイヤーとトレーニングモデルをチェーンする新しいモデルをインスタンス化するだけです。\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = preprocessing_layer(inputs)\n",
        "outputs = training_model(x)\n",
        "inference_model = keras.Model(inputs, outputs)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff9bad6ba7d5"
      },
      "source": [
        "## Quick recipes\n",
        "\n",
        "### Image data augmentation (on-device)\n",
        "\n",
        "Note that image data augmentation layers are only active during training (similarly to\n",
        "the `Dropout` layer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a621c2645ae6"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        preprocessing.RandomFlip(\"horizontal\"),\n",
        "        preprocessing.RandomRotation(0.1),\n",
        "        preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a model that includes the augmentation stage\n",
        "input_shape = (32, 32, 3)\n",
        "classes = 10\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "# Augment images\n",
        "x = data_augmentation(inputs)\n",
        "# Rescale image values to [0, 1]\n",
        "x = preprocessing.Rescaling(1.0 / 255)(x)\n",
        "# Add the rest of the model\n",
        "outputs = keras.applications.ResNet50(\n",
        "    weights=None, input_shape=input_shape, classes=classes\n",
        ")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51d369f0310f"
      },
      "source": [
        "You can see a similar setup in action in the example\n",
        "[image classification from scratch](https://keras.io/examples/vision/image_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a79a1c48b2b7"
      },
      "source": [
        "### Normalizing numerical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5a7c8f6270b"
      },
      "outputs": [],
      "source": [
        "# Load some data\n",
        "(x_train, y_train), _ = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.reshape((len(x_train), -1))\n",
        "input_shape = x_train.shape[1:]\n",
        "classes = 10\n",
        "\n",
        "# Create a Normalization layer and set its internal state using the training data\n",
        "normalizer = preprocessing.Normalization()\n",
        "normalizer.adapt(x_train)\n",
        "\n",
        "# Create a model that include the normalization layer\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = normalizer(inputs)\n",
        "outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Train the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62685d477010"
      },
      "source": [
        "### Encoding string categorical features via one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36303d995404"
      },
      "outputs": [],
      "source": [
        "# Define some toy data\n",
        "data = tf.constant([\"a\", \"b\", \"c\", \"b\", \"c\", \"a\"])\n",
        "\n",
        "# Use StringLookup to build an index of the feature values\n",
        "indexer = preprocessing.StringLookup()\n",
        "indexer.adapt(data)\n",
        "\n",
        "# Use CategoryEncoding to encode the integer indices to a one-hot vector\n",
        "encoder = preprocessing.CategoryEncoding(output_mode=\"binary\")\n",
        "encoder.adapt(indexer(data))\n",
        "\n",
        "# Convert new test data (which includes unknown feature values)\n",
        "test_data = tf.constant([\"a\", \"b\", \"c\", \"d\", \"e\", \"\"])\n",
        "encoded_data = encoder(indexer(test_data))\n",
        "print(encoded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d88c64d9b33"
      },
      "source": [
        "Note that index 0 is reserved for missing values (which you should specify as the empty\n",
        "string `\"\"`), and index 1 is reserved for out-of-vocabulary values (values that were not\n",
        "seen during `adapt()`). You can configure this by using the `mask_token` and `oov_token`\n",
        "constructor arguments  of `StringLookup`.\n",
        "\n",
        "You can see the `StringLookup` and `CategoryEncoding` layers in action in the example\n",
        "[structured data classification from scratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc8af3e290df"
      },
      "source": [
        "### Encoding integer categorical features via one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e55f069cbdf5"
      },
      "outputs": [],
      "source": [
        "# Define some toy data\n",
        "data = tf.constant([10, 20, 20, 10, 30, 0])\n",
        "\n",
        "# Use IntegerLookup to build an index of the feature values\n",
        "indexer = preprocessing.IntegerLookup()\n",
        "indexer.adapt(data)\n",
        "\n",
        "# Use CategoryEncoding to encode the integer indices to a one-hot vector\n",
        "encoder = preprocessing.CategoryEncoding(output_mode=\"binary\")\n",
        "encoder.adapt(indexer(data))\n",
        "\n",
        "# Convert new test data (which includes unknown feature values)\n",
        "test_data = tf.constant([10, 10, 20, 50, 60, 0])\n",
        "encoded_data = encoder(indexer(test_data))\n",
        "print(encoded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82830d172b7f"
      },
      "source": [
        "Note that index 0 is reserved for missing values (which you should specify as the value\n",
        "0), and index 1 is reserved for out-of-vocabulary values (values that were not seen\n",
        "during `adapt()`). You can configure this by using the `mask_value` and `oov_value`\n",
        "constructor arguments  of `IntegerLookup`.\n",
        "\n",
        "You can see the `IntegerLookup` and `CategoryEncoding` layers in action in the example\n",
        "[structured data classification from scratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fbfaa6ab3e2"
      },
      "source": [
        "### Applying the hashing trick to an integer categorical feature\n",
        "\n",
        "If you have a categorical feature that can take many different values (on the order of\n",
        "10e3 or higher), where each value only appears a few times in the data,\n",
        "it becomes impractical and ineffective to index and one-hot encode the feature values.\n",
        "Instead, it can be a good idea to apply the \"hashing trick\": hash the values to a vector\n",
        "of fixed size. This keeps the size of the feature space manageable, and removes the need\n",
        "for explicit indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2def8456f905"
      },
      "outputs": [],
      "source": [
        "# Sample data: 10,000 random integers with values between 0 and 100,000\n",
        "data = np.random.randint(0, 100000, size=(10000, 1))\n",
        "\n",
        "# Use the Hashing layer to hash the values to the range [0, 64]\n",
        "hasher = preprocessing.Hashing(num_bins=64, salt=1337)\n",
        "\n",
        "# Use the CategoryEncoding layer to one-hot encode the hashed values\n",
        "encoder = preprocessing.CategoryEncoding(max_tokens=64, output_mode=\"binary\")\n",
        "encoded_data = encoder(hasher(data))\n",
        "print(encoded_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df69b434d327"
      },
      "source": [
        "### Encoding text as a sequence of token indices\n",
        "\n",
        "This is how you should preprocess text to be passed to an `Embedding` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a689d9dcf6ab"
      },
      "outputs": [],
      "source": [
        "# Define some text data to adapt the layer\n",
        "data = tf.constant(\n",
        "    [\n",
        "        \"The Brain is wider than the Sky\",\n",
        "        \"For put them side by side\",\n",
        "        \"The one the other will contain\",\n",
        "        \"With ease and You beside\",\n",
        "    ]\n",
        ")\n",
        "# Instantiate TextVectorization with \"int\" output_mode\n",
        "text_vectorizer = preprocessing.TextVectorization(output_mode=\"int\")\n",
        "# Index the vocabulary via `adapt()`\n",
        "text_vectorizer.adapt(data)\n",
        "\n",
        "# You can retrieve the vocabulary we indexed via get_vocabulary()\n",
        "vocab = text_vectorizer.get_vocabulary()\n",
        "print(\"Vocabulary:\", vocab)\n",
        "\n",
        "# Create an Embedding + LSTM model\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = layers.Embedding(input_dim=len(vocab), output_dim=64)(x)\n",
        "outputs = layers.LSTM(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Call the model on test data (which includes unknown tokens)\n",
        "test_data = tf.constant([\"The Brain is deeper than the sea\"])\n",
        "test_output = model(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84d4f2ec0ef"
      },
      "source": [
        "<a>テキスト分類を最初から行う</a>の例では、`Embedded`モードと組み合わされて<code>TextVectorization</code>レイヤーが動作する方法を確認できます。\n",
        "\n",
        "このようなモデルをトレーニングする場合、最高のパフォーマンスを得るには、入力パイプラインの一部として`TextVectorization`レイヤーを使用する必要があることに注意してください（上記のテキスト分類の例で示すように）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28c2f2ff61fb"
      },
      "source": [
        "### Encoding text as a dense matrix of ngrams with multi-hot encoding\n",
        "\n",
        "This is how you should preprocess text to be passed to a `Dense` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b62472e32529"
      },
      "outputs": [],
      "source": [
        "# Define some text data to adapt the layer\n",
        "data = tf.constant(\n",
        "    [\n",
        "        \"The Brain is wider than the Sky\",\n",
        "        \"For put them side by side\",\n",
        "        \"The one the other will contain\",\n",
        "        \"With ease and You beside\",\n",
        "    ]\n",
        ")\n",
        "# Instantiate TextVectorization with \"binary\" output_mode (multi-hot)\n",
        "# and ngrams=2 (index all bigrams)\n",
        "text_vectorizer = preprocessing.TextVectorization(output_mode=\"binary\", ngrams=2)\n",
        "# Index the bigrams via `adapt()`\n",
        "text_vectorizer.adapt(data)\n",
        "\n",
        "print(\n",
        "    \"Encoded text:\\n\",\n",
        "    text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
        "    \"\\n\",\n",
        ")\n",
        "\n",
        "# Create a Dense model\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Call the model on test data (which includes unknown tokens)\n",
        "test_data = tf.constant([\"The Brain is deeper than the sea\"])\n",
        "test_output = model(test_data)\n",
        "\n",
        "print(\"Model output:\", test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "336a4d3426ed"
      },
      "source": [
        "### Encoding text as a dense matrix of ngrams with TF-IDF weighting\n",
        "\n",
        "This is an alternative way of preprocessing text before passing it to a `Dense` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdf16938fe7c"
      },
      "outputs": [],
      "source": [
        "# Define some text data to adapt the layer\n",
        "data = tf.constant(\n",
        "    [\n",
        "        \"The Brain is wider than the Sky\",\n",
        "        \"For put them side by side\",\n",
        "        \"The one the other will contain\",\n",
        "        \"With ease and You beside\",\n",
        "    ]\n",
        ")\n",
        "# Instantiate TextVectorization with \"tf-idf\" output_mode\n",
        "# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\n",
        "text_vectorizer = preprocessing.TextVectorization(output_mode=\"tf-idf\", ngrams=2)\n",
        "# Index the bigrams and learn the TF-IDF weights via `adapt()`\n",
        "text_vectorizer.adapt(data)\n",
        "\n",
        "print(\n",
        "    \"Encoded text:\\n\",\n",
        "    text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
        "    \"\\n\",\n",
        ")\n",
        "\n",
        "# Create a Dense model\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Call the model on test data (which includes unknown tokens)\n",
        "test_data = tf.constant([\"The Brain is deeper than the sea\"])\n",
        "test_output = model(test_data)\n",
        "print(\"Model output:\", test_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocessing_layers.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
