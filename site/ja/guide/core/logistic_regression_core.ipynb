{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhGuhbZ6M5tl"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AwOEIRJC6Une"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Core  API を使用したロジスティック回帰で二項分類を実行する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBIlTPscrIT9"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/core/logistic_regression_core\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     TensorFlow.org で表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colabで実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHubでソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauaqJ7WhIhO"
      },
      "source": [
        "このガイドでは、[TensorFlow Core 低レベル API](https://www.tensorflow.org/guide/core) を使用して、[ロジスティック回帰](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external} で[二項分類](https://developers.google.com/machine-learning/glossary#binary_classification){:.external} を実行する方法を示します。腫瘍の分類には [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external} を使用します。\n",
        "\n",
        "[ロジスティック回帰](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external} は、二項分類で最も一般的なアルゴリズムの 1 つです。ロジスティック回帰の目標は、特徴のある一連のサンプルから、各サンプルが特定のクラスに属する確率として 0 から 1 の間の値を出力することです。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nchsZfwEVtVs"
      },
      "source": [
        "## セットアップ\n",
        "\n",
        "このチュートリアルでは、CSV ファイルを [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html){:.external} に読み込むために [pandas](https://pandas.pydata.org){:.external}、データセット内のペアワイズ関係をプロットするために [seaborn](https://seaborn.pydata.org){:.external}、混同行列を計算するために [Scikit-learn](https://scikit-learn.org/){:.external}、視覚化を作成するために [matplotlib](https://matplotlib.org/){:.external} を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lZoUK6AVTos"
      },
      "outputs": [],
      "source": [
        "!pip install -q seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.metrics as sk_metrics\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Preset matplotlib figure sizes.\n",
        "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
        "\n",
        "print(tf.__version__)\n",
        "# To make the results reproducible, set the random seed value.\n",
        "tf.random.set_seed(22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9ne3FZ-On"
      },
      "source": [
        "## データを読み込む\n",
        "\n",
        "次に、[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/){:.external} から [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external} を読み込みます。 このデータセットには、腫瘍の半径、テクスチャ、凹面などのさまざまな特徴が含まれています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "outputs": [],
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
        "\n",
        "features = ['radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness',\n",
        "            'concavity', 'concave_poinits', 'symmetry', 'fractal_dimension']\n",
        "column_names = ['id', 'diagnosis']\n",
        "\n",
        "for attr in ['mean', 'ste', 'largest']:\n",
        "  for feature in features:\n",
        "    column_names.append(feature + \"_\" + attr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3VR1aTP92nV"
      },
      "source": [
        "[`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html){:.external} を使用して、データセットを pandas [DataFrame](){:.external} に読み取ります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvR2Bzb691lJ"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(url, names=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB9eq6Zq-IZ4"
      },
      "outputs": [],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Z1V6Dg-La_"
      },
      "source": [
        "最初の 5 行を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWxktwbv-KPp"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-Wn2jzVC1W"
      },
      "source": [
        "[`pandas.DataFrame.sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html){:.external}、[`pandas.DataFrame.drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html){:.external} および [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html){:.external} を使用して、データセットをトレーニングセットとテストセットに分割します。ターゲットラベルから特徴を必ず分割してください。テストセットは、新しいデータに対するモデルの一般化可能性を評価するために使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2O60B-IVG9Q"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset.sample(frac=0.75, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i06vHFv_QB24"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19JaochhaQ3m"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset.drop(train_dataset.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmHRcbAfaSag"
      },
      "outputs": [],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6JxBhBc_wwO"
      },
      "outputs": [],
      "source": [
        "# The `id` column can be dropped since each row is unique\n",
        "x_train, y_train = train_dataset.iloc[:, 2:], train_dataset.iloc[:, 1]\n",
        "x_test, y_test = test_dataset.iloc[:, 2:], test_dataset.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWuJTKEDM-f"
      },
      "source": [
        "## データを処理する\n",
        "\n",
        "このデータセットには、サンプルごとに収集された 10 の腫瘍測定値のそれぞれの平均値、標準誤差、および最大値が含まれています。 `\"diagnosis\"` ターゲット列は、悪性腫瘍を示す `'M'` と良性腫瘍の診断を示す `'B'` を持つカテゴリカル変数です。この列は、モデルのトレーニングのために数値バイナリ形式に変換する必要があります。\n",
        "\n",
        "[`pandas.Series.map`](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html){:.external} 関数は、バイナリ値をカテゴリにマッピングするのに役立ちます。\n",
        "\n",
        "また、データセットは、前処理の完了後に `tf.convert_to_tensor` 関数を使用してテンソルに変換する必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEJHhN65a2VV"
      },
      "outputs": [],
      "source": [
        "y_train, y_test = y_train.map({'B': 0, 'M': 1}), y_test.map({'B': 0, 'M': 1})\n",
        "x_train, y_train = tf.convert_to_tensor(x_train, dtype=tf.float32), tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "x_test, y_test = tf.convert_to_tensor(x_test, dtype=tf.float32), tf.convert_to_tensor(y_test, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ubs136WLNp"
      },
      "source": [
        "[`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html){:.external} を使用して、トレーニングセットからの平均ベースの特徴のいくつかのペアの同時分布を確認し、ターゲットとの関係を観察します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRKO_x8gWKv-"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(train_dataset.iloc[:, 1:6], hue = 'diagnosis', diag_kind='kde');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YOG5iKYKW_3"
      },
      "source": [
        "このペアプロットは、半径、周長、面積などの特定の特徴が高度に相関していることを示しています。これは、腫瘍半径が周長と面積の両方の計算に直接関与しているためです。さらに、悪性診断は多くの特徴でより右に歪んでいるように見えます。\n",
        "\n",
        "全体の統計も確認してください。各特徴の値の範囲が大きく異なることに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi2FzC3T21jR"
      },
      "outputs": [],
      "source": [
        "train_dataset.describe().transpose()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8pDCIFjMla8"
      },
      "source": [
        "範囲に一貫性がない場合は、各特徴の平均がゼロで分散が 1 になるようにデータを標準化することをお勧めします。このプロセスは [正則化](https://developers.google.com/machine-learning/glossary#normalization){:.external} と呼ばれます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrzKNFNjLQDl"
      },
      "outputs": [],
      "source": [
        "class Normalize(tf.Module):\n",
        "  def __init__(self, x):\n",
        "    # Initialize the mean and standard deviation for normalization\n",
        "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
        "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0))\n",
        "\n",
        "  def norm(self, x):\n",
        "    # Normalize the input\n",
        "    return (x - self.mean)/self.std\n",
        "\n",
        "  def unnorm(self, x):\n",
        "    # Unnormalize the input\n",
        "    return (x * self.std) + self.mean\n",
        "\n",
        "norm_x = Normalize(x_train)\n",
        "x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3CrycBXA2s"
      },
      "source": [
        "## ロジスティック回帰\n",
        "\n",
        "ロジスティック回帰モデルを構築する前に、従来の線形回帰との違いを理解することが重要です。\n",
        "\n",
        "### ロジスティック回帰の基礎\n",
        "\n",
        "線形回帰は、その入力の線形結合を返し、出力は無制限です。[ロジスティック回帰](https://developers.google.com/machine-learning/glossary#logistic_regression){:.external}の出力は、`(0, 1)` の範囲にあります。各例について、例が*正*クラスに属する確率を表します。\n",
        "\n",
        "ロジスティック回帰は、従来の線形回帰の連続出力 `(-∞, ∞)` を確率 `(0, 1)` にマッピングします。また、この変換は対称であるため、線形出力の符号を反転すると、元の確率の逆になります。\n",
        "\n",
        "$Y$ がクラス `1`（腫瘍が悪性）である確率を表すとします。マッピングは、線形回帰出力をクラス `0` ではなく、クラス `1` にある[対数オッズ](https://developers.google.com/machine-learning/glossary#log-odds){:.external} 比率として解釈することで実現できます。\n",
        "\n",
        "```\n",
        "$$\\ln(\\frac{Y}{1-Y}) = wX + b$$\n",
        "```\n",
        "\n",
        "$wX + b = z$ を設定すると、この方程式を $Y$ について解くことができます。\n",
        "\n",
        "```\n",
        "$$Y = \\frac{e^{z}}{1 + e^{z}} = \\frac{1}{1 + e^{-z}}$$\n",
        "```\n",
        "\n",
        "式 $\\frac{1}{1 + e^{-z}}$ は、<a>シグモイド関数</a>{:.external} $\\sigma(z)$ として知られています。したがって、ロジスティック回帰の式は $Y = \\sigma(wX + b)$ と記述することができます。\n",
        "\n",
        "線形出力 `(-∞, ∞)` を `0` と `1` の間に変換するシグモイド関数を視覚化することから始めます。シグモイド関数は `tf.math.sigmoid` で利用できます。\n",
        "\n",
        "```\n",
        "$${\\mathrm{Y}} = \\sigma({\\mathrm{X}}w + b)$$\n",
        "```\n",
        "\n",
        "これは、それぞれ以下を意味します。\n",
        "\n",
        "- $\\underset{m\\times 1}{\\mathrm{Y}}$: ターゲットベクトル\n",
        "- $\\underset{m\\times n}{\\mathrm{X}}$: 特徴行列\n",
        "- $\\underset{n\\times 1}w$: 重みベクトル\n",
        "- $b$: バイアス\n",
        "- $\\sigma$: 出力ベクトルの各要素に適用されるシグモイド関数\n",
        "\n",
        "線形出力 `(-∞, ∞)` を `0` と `1` の間に変換するシグモイド関数を視覚化することから始めます。シグモイド関数は `tf.math.sigmoid` で利用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThHaV_RmucZl"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-10, 10, 500)\n",
        "x = tf.cast(x, tf.float32)\n",
        "f = lambda x : (1/20)*x + 0.6\n",
        "plt.plot(x, tf.math.sigmoid(x))\n",
        "plt.ylim((-0.1,1.1))\n",
        "plt.title(\"Sigmoid function\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMXEhrZuKECV"
      },
      "source": [
        "### 対数損失関数\n",
        "\n",
        "[対数損失](https://developers.google.com/machine-learning/glossary#Log_Loss){:.external}（バイナリクロスエントロピー損失）は、ロジスティック回帰によるバイナリ分類問題の理想的な損失関数です。各例について、対数損失は、予測された確率と例の真の値の間の類似性を定量化します。 次の式で求めます。\n",
        "\n",
        "```\n",
        "ここでは、\n",
        "```\n",
        "\n",
        "`tf.nn.sigmoid_cross_entropy_with_logits` 関数を使用して対数損失を計算できます。この関数は、シグモイド活性化を回帰出力に自動的に適用します。\n",
        "\n",
        "- $\\hat{y}$: 予測確率のベクトル\n",
        "- $y$: 真のターゲットのベクトル\n",
        "\n",
        "`tf.nn.sigmoid_cross_entropy_with_logits` 関数を使用して対数損失を計算できます。この関数は、シグモイド活性化を回帰出力に自動的に適用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVBInnSqS36W"
      },
      "outputs": [],
      "source": [
        "def log_loss(y_pred, y):\n",
        "  # Compute the log loss function\n",
        "  ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
        "  return tf.reduce_mean(ce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_mutLj0KNUb"
      },
      "source": [
        "### 勾配降下法の更新\n",
        "\n",
        "TensorFlow Core API は、`tf.GradientTape` による自動微分をサポートしています。ここでは、ロジスティック回帰 [勾配降下法の更新](https://developers.google.com/machine-learning/glossary#gradient_descent){:.external} の背後にある数学について簡単に説明します。\n",
        "\n",
        "上記の対数損失の式で、各 $\\hat{y}_i$ は、入力に関して $\\sigma({\\mathrm{X_i}}w + b)$ と書き直すことができることを思い出してください。\n",
        "\n",
        "目標は、対数損失を最小限に抑える $w^*$ と $b^*$ を見つけることです。\n",
        "\n",
        "```\n",
        "$w$ に対して勾配 $L$ を取ると、以下が得られます。\n",
        "```\n",
        "\n",
        "$b$ に対して勾配 $L$ を取ると、以下が得られます。\n",
        "\n",
        "```\n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m}(\\sigma({\\mathrm{X}}w + b) - y)X$$\n",
        "```\n",
        "\n",
        "$b$ に対して勾配 $L$ を取ると、以下が得られます。\n",
        "\n",
        "```\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}\\sigma({\\mathrm{X_i}}w + b) - y_i$$\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTCndUecKZho"
      },
      "source": [
        "次に、ロジスティック回帰モデルを構築します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0sXM7qLlKfZ"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.built = False\n",
        "    \n",
        "  def __call__(self, x, train=True):\n",
        "    # Initialize the model parameters on the first call\n",
        "    if not self.built:\n",
        "      # Randomly generate the weights and the bias term\n",
        "      rand_w = tf.random.uniform(shape=[x.shape[-1], 1], seed=22)\n",
        "      rand_b = tf.random.uniform(shape=[], seed=22)\n",
        "      self.w = tf.Variable(rand_w)\n",
        "      self.b = tf.Variable(rand_b)\n",
        "      self.built = True\n",
        "    # Compute the model output\n",
        "    z = tf.add(tf.matmul(x, self.w), self.b)\n",
        "    z = tf.squeeze(z, axis=1)\n",
        "    if train:\n",
        "      return z\n",
        "    return tf.sigmoid(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObQu9fDnXGL"
      },
      "source": [
        "検証するには、トレーニングデータの小さなサブセットに対して、トレーニングされていないモデルが `(0, 1)` の範囲の値を出力することを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bIovC0Z4QHJ"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ2ievISyf0p"
      },
      "outputs": [],
      "source": [
        "y_pred = log_reg(x_train_norm[:5], train=False)\n",
        "y_pred.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PribnwDHUksC"
      },
      "source": [
        "次に、トレーニング中に正しい分類の割合を計算するための精度関数を記述します。予測された確率から分類を取得するには、しきい値よりも高いすべての確率がクラス `1` に属するようにしきい値を設定します。これは、構成可能なハイパーパラメータで、デフォルトで `0.5` に設定できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssnVcKg7oMe6"
      },
      "outputs": [],
      "source": [
        "def predict_class(y_pred, thresh=0.5):\n",
        "  # Return a tensor with  `1` if `y_pred` > `0.5`, and `0` otherwise\n",
        "  return tf.cast(y_pred > thresh, tf.float32)\n",
        "\n",
        "def accuracy(y_pred, y):\n",
        "  # Return the proportion of matches between `y_pred` and `y`\n",
        "  y_pred = tf.math.sigmoid(y_pred)\n",
        "  y_pred_class = predict_class(y_pred)\n",
        "  check_equal = tf.cast(y_pred_class == y,tf.float32)\n",
        "  acc_val = tf.reduce_mean(check_equal)\n",
        "  return acc_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_0KHQ25_2dF"
      },
      "source": [
        "### モデルをトレーニングする\n",
        "\n",
        "トレーニングにミニバッチを使用すると、メモリ効率と収束の高速化を実現できます。`tf.data.Dataset` API には、バッチ処理とシャッフルに役立つ関数があります。API を使用すると、単純で再利用可能な部分から複雑な入力パイプラインを構築できます。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJD7-4U0etqa"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0]).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test))\n",
        "test_dataset = test_dataset.shuffle(buffer_size=x_test.shape[0]).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLiWZZPBSDip"
      },
      "source": [
        "次に、ロジスティック回帰モデルのトレーニングループを作成します。ループは、モデルのパラメータを繰り返し更新するために、対数損失関数と入力に対するその勾配を利用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNC3D1DGsGgK"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "epochs = 200\n",
        "learning_rate = 0.01\n",
        "train_losses, test_losses = [], []\n",
        "train_accs, test_accs = [], []\n",
        "\n",
        "# Set up the training loop and begin training\n",
        "for epoch in range(epochs):\n",
        "  batch_losses_train, batch_accs_train = [], []\n",
        "  batch_losses_test, batch_accs_test = [], []\n",
        "\n",
        "  # Iterate over the training data\n",
        "  for x_batch, y_batch in train_dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred_batch = log_reg(x_batch)\n",
        "      batch_loss = log_loss(y_pred_batch, y_batch)\n",
        "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
        "    # Update the parameters with respect to the gradient calculations\n",
        "    grads = tape.gradient(batch_loss, log_reg.variables)\n",
        "    for g,v in zip(grads, log_reg.variables):\n",
        "      v.assign_sub(learning_rate * g)\n",
        "    # Keep track of batch-level training performance\n",
        "    batch_losses_train.append(batch_loss)\n",
        "    batch_accs_train.append(batch_acc)\n",
        "\n",
        "  # Iterate over the testing data\n",
        "  for x_batch, y_batch in test_dataset:\n",
        "    y_pred_batch = log_reg(x_batch)\n",
        "    batch_loss = log_loss(y_pred_batch, y_batch)\n",
        "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
        "    # Keep track of batch-level testing performance\n",
        "    batch_losses_test.append(batch_loss)\n",
        "    batch_accs_test.append(batch_acc)\n",
        "\n",
        "  # Keep track of epoch-level model performance\n",
        "  train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n",
        "  test_loss, test_acc = tf.reduce_mean(batch_losses_test), tf.reduce_mean(batch_accs_test)\n",
        "  train_losses.append(train_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accs.append(test_acc)\n",
        "  if epoch % 20 == 0:\n",
        "    print(f\"Epoch: {epoch}, Training log loss: {train_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoLiAg7fYft7"
      },
      "source": [
        "### パフォーマンス評価\n",
        "\n",
        "モデルの損失と精度の経時変化を観察します。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv3oCQPvWhr0"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(epochs), train_losses, label = \"Training loss\")\n",
        "plt.plot(range(epochs), test_losses, label = \"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Log loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Log loss vs training iterations\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2HDVGLPODIE"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(epochs), train_accs, label = \"Training accuracy\")\n",
        "plt.plot(range(epochs), test_accs, label = \"Testing accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy vs training iterations\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jonKhUzuPyfa"
      },
      "outputs": [],
      "source": [
        "print(f\"Final training log loss: {train_losses[-1]:.3f}\")\n",
        "print(f\"Final testing log Loss: {test_losses[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3DF4qyrPyke"
      },
      "outputs": [],
      "source": [
        "print(f\"Final training accuracy: {train_accs[-1]:.3f}\")\n",
        "print(f\"Final testing accuracy: {test_accs[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrj1TbOJasjA"
      },
      "source": [
        "このモデルは、トレーニングデータセットでの腫瘍の分類に関して高い精度と低い損失を示し、新しいテストデータに対しても一般化されています。さらに、全体的な精度スコアを超える洞察を提供する誤差率を調べることができます。二項分類問題の最も一般的な 2 つの誤差率は、偽陽性率（FPR）と偽陰性率（FNR）です。\n",
        "\n",
        "この問題では、FPR は、実際には良性である腫瘍のうち、悪性腫瘍と予測される割合です。逆に、FNR は、実際に悪性である腫瘍の中で良性腫瘍と予測される割合です。\n",
        "\n",
        "分類の精度を評価する [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix){:.external} を使用して混同行列を計算し、matplotlib を使用して行列を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJO7YkA8ZDMU"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(y, y_classes, typ):\n",
        "  # Compute the confusion matrix and normalize it\n",
        "  plt.figure(figsize=(10,10))\n",
        "  confusion = sk_metrics.confusion_matrix(y.numpy(), y_classes.numpy())\n",
        "  confusion_normalized = confusion / confusion.sum(axis=1, keepdims=True)\n",
        "  axis_labels = range(2)\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.4f', square=True)\n",
        "  plt.title(f\"Confusion matrix: {typ}\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "y_pred_train, y_pred_test = log_reg(x_train_norm, train=False), log_reg(x_test_norm, train=False)\n",
        "train_classes, test_classes = predict_class(y_pred_train), predict_class(y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ5DFcleiDFm"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(y_train, train_classes, 'Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtfcsAp_iCNR"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(y_test, test_classes, 'Testing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlivxaDmTnGq"
      },
      "source": [
        "誤り率の測定値を観察し、この例のコンテキストでその重要性を解釈します。癌の検出などの多くの医療検査研究では、偽陰性率を低く抑えるために偽陽性率を高くすることは完全に許容され、実際には推奨されます。悪性腫瘍の診断を見逃すリスク（偽陰性）は、良性腫瘍を悪性と誤分類するリスク（偽陽性）よりもはるかに悪いためです。\n",
        "\n",
        "FPR と FNR を制御するには、確率予測を分類する前にしきい値ハイパーパラメータを変更してみてください。しきい値を低くすると、モデルが悪性腫瘍に分類される可能性が全体的に高くなります。 これにより、必然的に偽陽性の数と FPR が増加しますが、偽陰性の数と FNR の低減にも役立ちます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADEN2rb4Nhj"
      },
      "source": [
        "## モデルを保存する\n",
        "\n",
        "まず、生データを取り込み、次の演算を実行するエクスポートモジュールを作成します。\n",
        "\n",
        "- 正則化\n",
        "- 確率予測\n",
        "- クラス予測\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KPRHCzg4ZxH"
      },
      "outputs": [],
      "source": [
        "class ExportModule(tf.Module):\n",
        "  def __init__(self, model, norm_x, class_pred):\n",
        "    # Initialize pre- and post-processing functions\n",
        "    self.model = model\n",
        "    self.norm_x = norm_x\n",
        "    self.class_pred = class_pred\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], dtype=tf.float32)])\n",
        "  def __call__(self, x):\n",
        "    # Run the `ExportModule` for new data points\n",
        "    x = self.norm_x.norm(x)\n",
        "    y = self.model(x, train=False)\n",
        "    y = self.class_pred(y)\n",
        "    return y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzRclo5-yjO"
      },
      "outputs": [],
      "source": [
        "log_reg_export = ExportModule(model=log_reg,\n",
        "                              norm_x=norm_x,\n",
        "                              class_pred=predict_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtofGIBN_qFd"
      },
      "source": [
        "モデルをその時点の状態で保存する場合は、`tf.saved_model.save` 関数を使用して保存できます。保存されたモデルを読み込んで予測を行うには、`tf.saved_model.load` 関数を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4Qum1Ts_pmF"
      },
      "outputs": [],
      "source": [
        "models = tempfile.mkdtemp()\n",
        "save_path = os.path.join(models, 'log_reg_export')\n",
        "tf.saved_model.save(log_reg_export, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KPILr1i_M_c"
      },
      "outputs": [],
      "source": [
        "log_reg_loaded = tf.saved_model.load(save_path)\n",
        "test_preds = log_reg_loaded(x_test)\n",
        "test_preds[:10].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgGQuV-yqYZH"
      },
      "source": [
        "## 結論\n",
        "\n",
        "このノートブックでは、ロジスティック回帰問題を扱うためのテクニックをいくつか紹介しました。以下は、役に立つかもしれないその他のヒントです。\n",
        "\n",
        "- [TensorFlow Core API](https://www.tensorflow.org/guide/core) を使用して、高度な設定が可能な機械学習ワークフローを構築できます。\n",
        "- 誤り率を分析することにより、全体的な精度スコアだけでなく、分類モデルのパフォーマンスについてより多くの洞察を得られます。\n",
        "- 過適合は、ロジスティック回帰モデルのもう 1 つの一般的な問題ですが、このチュートリアルでは問題になりませんでした。 詳しくは、[過学習と過少学習](../../tutorials/keras/overfit_and_underfit.ipynb)のチュートリアルを参照してください。\n",
        "\n",
        "TensorFlow Core API のその他の使用例については、[ガイド](https://www.tensorflow.org/guide/core)をご覧ください。データの読み込みと準備についてさらに学習するには、[画像データの読み込み](../../tutorials/load_data/images.ipynb)または [CSV データの読み込み](../../tutorials/load_data/csv.ipynb)に関するチュートリアルをご覧ください。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "logistic_regression_core.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
