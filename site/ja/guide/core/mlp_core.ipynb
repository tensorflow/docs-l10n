{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhGuhbZ6M5tl"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AwOEIRJC6Une"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Core API を使用した数字認識のための多層パーセプトロン"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBIlTPscrIT9"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/core/mlp_core\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/guide/core/mlp_core.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/guide/core/mlp_core.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/guide/core/mlp_core.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjAxxRpBzVYg"
      },
      "source": [
        "このノートブックでは、[TensorFlow Core 低レベル API](https://www.tensorflow.org/guide/core) を使用して、[多層パーセプトロン](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy)と [MNIST データセット](http://yann.lecun.com/exdb/mnist)を使った、手書きの数字を分類するためのエンドツーエンドの機械学習ワークフローを構築します。TensorFlow Core とその意図するユースケースの詳細については、[Core API の概要](https://www.tensorflow.org/guide/core)を参照してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHVMVIFHSzl1"
      },
      "source": [
        "## 多層パーセプトロン（MLP）の概要\n",
        "\n",
        "多層パーセプトロン（MLP）は、[マルチクラス分類](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/video-lecture)の問題に使用される一種のフィードフォワードニューラルネットワークです。MLP を構築する前に、パーセプトロン、レイヤー、活性化関数の概念を理解することが重要です。\n",
        "\n",
        "多層パーセプトロンは、パーセプトロンと呼ばれる単位で構成されています。パーセプトロンの方程式は次のとおりです。\n",
        "\n",
        "$$Z = \\vec{w}⋅\\mathrm{X} + b$$\n",
        "\n",
        "上記の記号は以下を意味します。\n",
        "\n",
        "- $Z$: パーセプトロン出力\n",
        "- $\\mathrm{X}$: 特徴量の行列\n",
        "- $\\vec{w}$: 重みベクトル\n",
        "- $b$: バイアス\n",
        "\n",
        "これらのパーセプトロンが積み重ねられると、高密度レイヤーと呼ばれる構造が形成され、それらを接続してニューラルネットワークを構築できます。高密度レイヤーの方程式はパーセプトロンの方程式に似ていますが、代わりに重み行列とバイアスベクトルを使用します。\n",
        "\n",
        "$$Z = \\mathrm{W}⋅\\mathrm{X} + \\vec{b}$$\n",
        "\n",
        "上記の記号は以下を意味します。\n",
        "\n",
        "- $Z$: 高密度レイヤー出力\n",
        "- $\\mathrm{X}$: 特徴量の行列\n",
        "- $\\mathrm{W}$: 重み行列\n",
        "- $\\vec{b}$: バイアスベクトル\n",
        "\n",
        "MLP では、複数の高密度レイヤーが接続され、1 つのレイヤーの出力は次のレイヤーの入力に完全に接続されます。高密度レイヤーの出力に非線形活性化関数を追加すると、MLP 分類器が複雑な決定境界を学習し、トレーニングに使用されていないデータに対して適切に一般化するのに役立ちます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nchsZfwEVtVs"
      },
      "source": [
        "## セットアップ\n",
        "\n",
        "まず、TensorFlow、[pandas](https://pandas.pydata.org)、[Matplotlib](https://matplotlib.org) および [seaborn](https://seaborn.pydata.org) をインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSfgqmwBagw_"
      },
      "outputs": [],
      "source": [
        "# Use seaborn for countplot.\n",
        "!pip install -q seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import tempfile\n",
        "import os\n",
        "# Preset Matplotlib figure sizes.\n",
        "matplotlib.rcParams['figure.figsize'] = [9, 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xQKvCJ85kCQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "print(tf.__version__)\n",
        "# Set random seed for reproducible results \n",
        "tf.random.set_seed(22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_72b0LCNbjx"
      },
      "source": [
        "## データを読み込む\n",
        "\n",
        "このチュートリアルでは [MNIST データセット](http://yann.lecun.com/exdb/mnist)を使用し、手書きの数字を分類できる MLP モデルを構築する方法を示します。データセットは [TensorFlow データセット](https://www.tensorflow.org/datasets/catalog/mnist)から入手できます。\n",
        "\n",
        "MNIST データセットをトレーニングセット、検証セット、およびテストセットに分割します。検証セットを使用して、トレーニング中にモデルの一般化可能性を評価し、テストセットを使用してモデルの最終的なバイアスのないパフォーマンスを推定します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uiuh0B098_3p"
      },
      "outputs": [],
      "source": [
        "train_data, val_data, test_data = tfds.load(\"mnist\", \n",
        "                                            split=['train[10000:]', 'train[0:10000]', 'test'],\n",
        "                                            batch_size=128, as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9uN3Lf6ANtn"
      },
      "source": [
        "MNIST データセットは、手書きの数字とそれに対応する真のラベルで構成されています。以下のいくつかの例を可視化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V8hSqJ7AMjQ"
      },
      "outputs": [],
      "source": [
        "x_viz, y_viz = tfds.load(\"mnist\", split=['train[:1500]'], batch_size=-1, as_supervised=True)[0]\n",
        "x_viz = tf.squeeze(x_viz, axis=3)\n",
        "\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,1+i)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x_viz[i], cmap='gray')\n",
        "    plt.title(f\"True Label: {y_viz[i]}\")\n",
        "    plt.subplots_adjust(hspace=.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRald9dSE4qS"
      },
      "source": [
        "また、トレーニングデータの数字の分布を調べて、各クラスがデータセットで適切に表現されていることを確認します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj3K4XgQE7qR"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=y_viz.numpy());\n",
        "plt.xlabel('Digits')\n",
        "plt.title(\"MNIST Digit Distribution\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Wt4bDx_BRV"
      },
      "source": [
        "## データを前処理する\n",
        "\n",
        "まず、画像を平坦化し、特徴量の行列を 2 次元に再形成します。次に、[0,255] のピクセル値が [0,1] の範囲に収まるようにデータを再スケーリングします。この手順により、入力ピクセルが同様の分布を持つようになり、トレーニングの収束に役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSyCm2V2_AvI"
      },
      "outputs": [],
      "source": [
        "def preprocess(x, y):\n",
        "  # Reshaping the data\n",
        "  x = tf.reshape(x, shape=[-1, 784])\n",
        "  # Rescaling the data\n",
        "  x = x/255\n",
        "  return x, y\n",
        "\n",
        "train_data, val_data = train_data.map(preprocess), val_data.map(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3CrycBXA2s"
      },
      "source": [
        "## MLP を構築する\n",
        "\n",
        "まず、[ReLU](https://developers.google.com/machine-learning/glossary#ReLU) と [ソフトマックス](https://developers.google.com/machine-learning/glossary#softmax)活性化関数を可視化します。両方の関数は、それぞれ `tf.nn.relu` と `tf.nn.softmax` で利用できます。 ReLU は、正の場合は入力を出力し、それ以外の場合は 0 を出力する非線形活性化関数です。\n",
        "\n",
        "$$\\text{ReLU}(X) = max(0, X)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYunzt3UyT9G"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-2, 2, 201)\n",
        "x = tf.cast(x, tf.float32)\n",
        "plt.plot(x, tf.nn.relu(x));\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('ReLU(x)')\n",
        "plt.title('ReLU activation function');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuGrM9jMwsRM"
      },
      "source": [
        "ソフトマックス活性化関数は、$m$ 実数を $m$ 結果/クラスの確率分布に変換する正規化された指数関数です。これは、ニューラルネットワークの出力からクラスの確率を予測するのに役立ちます。\n",
        "\n",
        "$$\\text{Softmax}(X) = \\frac{e^{X}}{\\sum_{i=1}^{m}e^{X_i}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVM8pvhWwuwI"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-4, 4, 201)\n",
        "x = tf.cast(x, tf.float32)\n",
        "plt.plot(x, tf.nn.softmax(x, axis=0));\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Softmax(x)')\n",
        "plt.title('Softmax activation function');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHW6Yvg2yS6H"
      },
      "source": [
        "### 高密度レイヤー\n",
        "\n",
        "高密度レイヤーのクラスを作成します。定義により、1 つのレイヤーの出力は、MLP の次のレイヤーの入力に完全に接続されます。したがって、高密度レイヤーの入力次元は、前のレイヤーの出力次元に基づいて推測でき、初期化時に事前に指定する必要はありません。活性化出力が大きくなりすぎたり小さくなりすぎたりしないように、重みも適切に初期化する必要があります。最も一般的な重みの初期化方法の 1 つは、重み行列の各要素が次の方法でサンプリングされる Xavier スキームです。\n",
        "\n",
        "$$W_{ij} \\sim \\text{Uniform}(-\\frac{\\sqrt{6}}{\\sqrt{n + m}},\\frac{\\sqrt{6}}{\\sqrt{n + m}})$$\n",
        "\n",
        "バイアスベクトルはゼロに初期化できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re1SSFyBdMrS"
      },
      "outputs": [],
      "source": [
        "def xavier_init(shape):\n",
        "  # Computes the xavier initialization values for a weight matrix\n",
        "  in_dim, out_dim = shape\n",
        "  xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(in_dim + out_dim, tf.float32))\n",
        "  weight_vals = tf.random.uniform(shape=(in_dim, out_dim), \n",
        "                                  minval=-xavier_lim, maxval=xavier_lim, seed=22)\n",
        "  return weight_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otDFX4u6e6ml"
      },
      "source": [
        "また、Xavier の初期化メソッドは `tf.keras.initializers.GlorotUniform` で実装することもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM0yJos25FG5"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(tf.Module):\n",
        "\n",
        "  def __init__(self, out_dim, weight_init=xavier_init, activation=tf.identity):\n",
        "    # Initialize the dimensions and activation functions\n",
        "    self.out_dim = out_dim\n",
        "    self.weight_init = weight_init\n",
        "    self.activation = activation\n",
        "    self.built = False\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if not self.built:\n",
        "      # Infer the input dimension based on first call\n",
        "      self.in_dim = x.shape[1]\n",
        "      # Initialize the weights and biases\n",
        "      self.w = tf.Variable(self.weight_init(shape=(self.in_dim, self.out_dim)))\n",
        "      self.b = tf.Variable(tf.zeros(shape=(self.out_dim,)))\n",
        "      self.built = True\n",
        "    # Compute the forward pass\n",
        "    z = tf.add(tf.matmul(x, self.w), self.b)\n",
        "    return self.activation(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-7MzpjgyHg6"
      },
      "source": [
        "次に、レイヤーを順次実行する MLP モデルのクラスを作成します。モデル変数は、次元の推定により、高密度レイヤー呼び出しの最初のシーケンスの後にのみ使用できることに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XisRWiCyHAb"
      },
      "outputs": [],
      "source": [
        "class MLP(tf.Module):\n",
        "\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "   \n",
        "  @tf.function\n",
        "  def __call__(self, x, preds=False): \n",
        "    # Execute the model's layers sequentially\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luXKup-43nd7"
      },
      "source": [
        "次のアーキテクチャで MLP モデルを初期化します。\n",
        "\n",
        "- フォワードパス: ReLU(784×700)×ReLU(700×500)×Softmax(500×10)\n",
        "\n",
        "ソフトマックス活性化関数は、MLP によって適用される必要はありません。これは、損失関数と予測関数で別々に計算されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmlACuki3oPi"
      },
      "outputs": [],
      "source": [
        "hidden_layer_1_size = 700\n",
        "hidden_layer_2_size = 500\n",
        "output_size = 10\n",
        "\n",
        "mlp_model = MLP([\n",
        "    DenseLayer(out_dim=hidden_layer_1_size, activation=tf.nn.relu),\n",
        "    DenseLayer(out_dim=hidden_layer_2_size, activation=tf.nn.relu),\n",
        "    DenseLayer(out_dim=output_size)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyBATDoRmDkg"
      },
      "source": [
        "### 損失関数を定義する\n",
        "\n",
        "交差エントロピー損失関数は、モデルの確率予測に従ってデータの負の対数尤度を測定するため、マルチクラス分類問題に最適です。真のクラスに割り当てられる確率が高いほど、損失は低くなります。交差エントロピー損失の式は次のとおりです。\n",
        "\n",
        "$$L = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{i=j}^{n} {y_j}^{[i]}⋅\\log(\\hat{{y_j}}^{[i]})$$\n",
        "\n",
        "ここで記号は以下を意味します。\n",
        "\n",
        "- $\\underset{n\\times m}{\\hat{y}}$: 予測されたクラス分布の行列\n",
        "- $\\underset{n\\times m}{y}$: 真のクラスのワンホットエンコードされた行列\n",
        "\n",
        "`tf.nn.sparse_softmax_cross_entropy_with_logits` 関数を使用して交差エントロピー損失を計算できます。この関数は、モデルの最後のレイヤーにソフトマックス活性化関数を適用する必要はなく、クラスラベルをホットエンコードする必要もありません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rskOYA7FVCwg"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(y_pred, y):\n",
        "  # Compute cross entropy loss with a sparse operation\n",
        "  sparse_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
        "  return tf.reduce_mean(sparse_ce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvWxED1km8jh"
      },
      "source": [
        "トレーニング中に正しい分類の割合を計算する基本的な精度関数を記述します。ソフトマックス出力からクラス予測を生成するために、最大のクラス確率に対応するインデックスを返します。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPJMWx2UgiBm"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred, y):\n",
        "  # Compute accuracy after extracting class predictions\n",
        "  class_preds = tf.argmax(tf.nn.softmax(y_pred), axis=1)\n",
        "  is_equal = tf.equal(y, class_preds)\n",
        "  return tf.reduce_mean(tf.cast(is_equal, tf.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSiNRhTOnKZr"
      },
      "source": [
        "### モデルをトレーニングする\n",
        "\n",
        "オプティマイザを使用すると、標準の勾配降下法に比べて収束が大幅に速くなる可能性があります。Adam オプティマイザは以下に実装されています。TensorFlow Core を使用したカスタムオプティマイザの設計について詳しくは、[オプティマイザ](https://www.tensorflow.org/guide/core/optimizers_core)ガイドを参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGIBDk3cAv6a"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "\n",
        "    def __init__(self, learning_rate=1e-3, beta_1=0.9, beta_2=0.999, ep=1e-7):\n",
        "      # Initialize optimizer parameters and variable slots\n",
        "      self.beta_1 = beta_1\n",
        "      self.beta_2 = beta_2\n",
        "      self.learning_rate = learning_rate\n",
        "      self.ep = ep\n",
        "      self.t = 1.\n",
        "      self.v_dvar, self.s_dvar = [], []\n",
        "      self.built = False\n",
        "      \n",
        "    def apply_gradients(self, grads, vars):\n",
        "      # Initialize variables on the first call\n",
        "      if not self.built:\n",
        "        for var in vars:\n",
        "          v = tf.Variable(tf.zeros(shape=var.shape))\n",
        "          s = tf.Variable(tf.zeros(shape=var.shape))\n",
        "          self.v_dvar.append(v)\n",
        "          self.s_dvar.append(s)\n",
        "        self.built = True\n",
        "      # Update the model variables given their gradients\n",
        "      for i, (d_var, var) in enumerate(zip(grads, vars)):\n",
        "        self.v_dvar[i].assign(self.beta_1*self.v_dvar[i] + (1-self.beta_1)*d_var)\n",
        "        self.s_dvar[i].assign(self.beta_2*self.s_dvar[i] + (1-self.beta_2)*tf.square(d_var))\n",
        "        v_dvar_bc = self.v_dvar[i]/(1-(self.beta_1**self.t))\n",
        "        s_dvar_bc = self.s_dvar[i]/(1-(self.beta_2**self.t))\n",
        "        var.assign_sub(self.learning_rate*(v_dvar_bc/(tf.sqrt(s_dvar_bc) + self.ep)))\n",
        "      self.t += 1.\n",
        "      return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osEK3rqpYfKd"
      },
      "source": [
        "次に、ミニバッチ勾配降下で MLP パラメータを更新するカスタムトレーニングループを作成します。トレーニングにミニバッチを使用すると、メモリ効率と収束のスピードが向上します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJLeY2ao1aw6"
      },
      "outputs": [],
      "source": [
        "def train_step(x_batch, y_batch, loss, acc, model, optimizer):\n",
        "  # Update the model state given a batch of data\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(x_batch)\n",
        "    batch_loss = loss(y_pred, y_batch)\n",
        "  batch_acc = acc(y_pred, y_batch)\n",
        "  grads = tape.gradient(batch_loss, model.variables)\n",
        "  optimizer.apply_gradients(grads, model.variables)\n",
        "  return batch_loss, batch_acc\n",
        "\n",
        "def val_step(x_batch, y_batch, loss, acc, model):\n",
        "  # Evaluate the model on given a batch of validation data\n",
        "  y_pred = model(x_batch)\n",
        "  batch_loss = loss(y_pred, y_batch)\n",
        "  batch_acc = acc(y_pred, y_batch)\n",
        "  return batch_loss, batch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC85kuZgmh3q"
      },
      "outputs": [],
      "source": [
        "def train_model(mlp, train_data, val_data, loss, acc, optimizer, epochs):\n",
        "  # Initialize data structures\n",
        "  train_losses, train_accs = [], []\n",
        "  val_losses, val_accs = [], []\n",
        "\n",
        "  # Format training loop and begin training\n",
        "  for epoch in range(epochs):\n",
        "    batch_losses_train, batch_accs_train = [], []\n",
        "    batch_losses_val, batch_accs_val = [], []\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for x_batch, y_batch in train_data:\n",
        "      # Compute gradients and update the model's parameters\n",
        "      batch_loss, batch_acc = train_step(x_batch, y_batch, loss, acc, mlp, optimizer)\n",
        "      # Keep track of batch-level training performance\n",
        "      batch_losses_train.append(batch_loss)\n",
        "      batch_accs_train.append(batch_acc)\n",
        "\n",
        "    # Iterate over the validation data\n",
        "    for x_batch, y_batch in val_data:\n",
        "      batch_loss, batch_acc = val_step(x_batch, y_batch, loss, acc, mlp)\n",
        "      batch_losses_val.append(batch_loss)\n",
        "      batch_accs_val.append(batch_acc)\n",
        "\n",
        "    # Keep track of epoch-level model performance\n",
        "    train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n",
        "    val_loss, val_acc = tf.reduce_mean(batch_losses_val), tf.reduce_mean(batch_accs_val)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    print(f\"Training loss: {train_loss:.3f}, Training accuracy: {train_acc:.3f}\")\n",
        "    print(f\"Validation loss: {val_loss:.3f}, Validation accuracy: {val_acc:.3f}\")\n",
        "  return train_losses, train_accs, val_losses, val_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvbfXlN5lwwB"
      },
      "source": [
        "バッチ サイズ 128 で MLP モデルを 10 エポックトレーニングします。GPU や TPU などのハードウェアアクセラレータもトレーニング時間をスピードアップするのに役立ちます。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPlT8QfxptYl"
      },
      "outputs": [],
      "source": [
        "train_losses, train_accs, val_losses, val_accs = train_model(mlp_model, train_data, val_data, \n",
        "                                                             loss=cross_entropy_loss, acc=accuracy,\n",
        "                                                             optimizer=Adam(), epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_RVmt43G12R"
      },
      "source": [
        "### パフォーマンス評価\n",
        "\n",
        "まず、トレーニング中のモデルの損失と精度を可視化するプロット関数を作成します。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXTCYVtNDjAM"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(train_metric, val_metric, metric_type):\n",
        "  # Visualize metrics vs training Epochs\n",
        "  plt.figure()\n",
        "  plt.plot(range(len(train_metric)), train_metric, label = f\"Training {metric_type}\")\n",
        "  plt.plot(range(len(val_metric)), val_metric, label = f\"Validation {metric_type}\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric_type)\n",
        "  plt.legend()\n",
        "  plt.title(f\"{metric_type} vs Training epochs\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC-qIvZbHo0G"
      },
      "outputs": [],
      "source": [
        "plot_metrics(train_losses, val_losses, \"cross entropy loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-w2xk2PIDve"
      },
      "outputs": [],
      "source": [
        "plot_metrics(train_accs, val_accs, \"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbrJJaFrD_XR"
      },
      "source": [
        "## モデルを保存して読み込む\n",
        "\n",
        "まず、生データを取り込み、次の演算を実行するエクスポートモジュールを作成します。\n",
        "\n",
        "- データの前処理\n",
        "- 確率予測\n",
        "- クラス予測"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sszfWuJJZoo"
      },
      "outputs": [],
      "source": [
        "class ExportModule(tf.Module):\n",
        "  def __init__(self, model, preprocess, class_pred):\n",
        "    # Initialize pre and postprocessing functions\n",
        "    self.model = model\n",
        "    self.preprocess = preprocess\n",
        "    self.class_pred = class_pred\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None, None, None], dtype=tf.uint8)]) \n",
        "  def __call__(self, x):\n",
        "    # Run the ExportModule for new data points\n",
        "    x = self.preprocess(x)\n",
        "    y = self.model(x)\n",
        "    y = self.class_pred(y)\n",
        "    return y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8x6gjTDVi5d"
      },
      "outputs": [],
      "source": [
        "def preprocess_test(x):\n",
        "  # The export module takes in unprocessed and unlabeled data\n",
        "  x = tf.reshape(x, shape=[-1, 784])\n",
        "  x = x/255\n",
        "  return x\n",
        "\n",
        "def class_pred_test(y):\n",
        "  # Generate class predictions from MLP output\n",
        "  return tf.argmax(tf.nn.softmax(y), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu9H5STrJzdo"
      },
      "source": [
        "次に、このエクスポートモジュールを `tf.saved_model.save` 関数で保存します。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN9pPBQTKTe3"
      },
      "outputs": [],
      "source": [
        "mlp_model_export = ExportModule(model=mlp_model,\n",
        "                                preprocess=preprocess_test,\n",
        "                                class_pred=class_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idS7rQKbKwRS"
      },
      "outputs": [],
      "source": [
        "models = tempfile.mkdtemp()\n",
        "save_path = os.path.join(models, 'mlp_model_export')\n",
        "tf.saved_model.save(mlp_model_export, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zZxO8iqBGZ-"
      },
      "source": [
        "保存されたモデルを `tf.saved_model.load` で読み込み、トレーニングに使用されていないテストデータでそのパフォーマンスを調べます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5cwBTUqxldW"
      },
      "outputs": [],
      "source": [
        "mlp_loaded = tf.saved_model.load(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmv0u6j_b5OC"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(y_pred, y):\n",
        "  # Generic accuracy function\n",
        "  is_equal = tf.equal(y_pred, y)\n",
        "  return tf.reduce_mean(tf.cast(is_equal, tf.float32))\n",
        "\n",
        "x_test, y_test = tfds.load(\"mnist\", split=['test'], batch_size=-1, as_supervised=True)[0]\n",
        "test_classes = mlp_loaded(x_test)\n",
        "test_acc = accuracy_score(test_classes, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5t9vgv_ciQ_"
      },
      "source": [
        "このモデルは、トレーニングデータセット内の手書きの数字をうまく分類し、トレーニングに使用されていないテストデータにも一般化しています。次に、モデルのクラスごとの精度を調べて、各数字のパフォーマンスが良好であることを確認します。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD8YiC1Vfeyp"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy breakdown by digit:\")\n",
        "print(\"---------------------------\")\n",
        "label_accs = {}\n",
        "for label in range(10):\n",
        "  label_ind = (y_test == label)\n",
        "  # extract predictions for specific true label\n",
        "  pred_label = test_classes[label_ind]\n",
        "  labels = y_test[label_ind]\n",
        "  # compute class-wise accuracy\n",
        "  label_accs[accuracy_score(pred_label, labels).numpy()] = label\n",
        "for key in sorted(label_accs):\n",
        "  print(f\"Digit {label_accs[key]}: {key:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcykuJFhdGb0"
      },
      "source": [
        "いくつかの数字では、他の数字よりもモデルのパフォーマンスが低くなっています。これは、多くのマルチクラス分類問題で非常に一般的です。最後の演習として、モデルの予測とそれに対応する真のラベルの混同行列をプロットして、より多くのクラス レベルの洞察を収集します。 Sklearn と seaborn には、混同行列を生成して視覚化する関数があります。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqCaqPwwh1tN"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as sk_metrics\n",
        "\n",
        "def show_confusion_matrix(test_labels, test_classes):\n",
        "  # Compute confusion matrix and normalize\n",
        "  plt.figure(figsize=(10,10))\n",
        "  confusion = sk_metrics.confusion_matrix(test_labels.numpy(), \n",
        "                                          test_classes.numpy())\n",
        "  confusion_normalized = confusion / confusion.sum(axis=1, keepdims=True)\n",
        "  axis_labels = range(10)\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.4f', square=True)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "show_confusion_matrix(y_test, test_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT-WA7GVda6d"
      },
      "source": [
        "クラスレベルの洞察は、誤分類の理由を特定し、将来のトレーニングサイクルでモデルのパフォーマンスを向上させるのに役立ちます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFLfEH4ManbW"
      },
      "source": [
        "## まとめ\n",
        "\n",
        "このノートブックでは、[MLP](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax) を使用してマルチクラス分類の問題を処理するためのいくつかの手法を紹介しました。以下に役立つヒントをいくつか紹介します。\n",
        "\n",
        "- [TensorFlow Core API](https://www.tensorflow.org/guide/core) を使用して、高度な設定が可能な機械学習ワークフローを構築できます。\n",
        "- 初期化スキームは、トレーニング時にモデルパラメータが大きくなりすぎたり小さくなりすぎたりするのを防ぐのに役立ちます。\n",
        "- 過学習は、ニューラルネットワークのもう 1 つの一般的な問題ですが、このチュートリアルでは問題になりませんでした。詳しくは、[過学習と過少学習](overfit_and_underfit.ipynb)のチュートリアルを参照してください。\n",
        "\n",
        "TensorFlow Core API のその他の使用例については、[ガイド](https://www.tensorflow.org/guide/core)をご覧ください。データの読み込みと準備についてさらに学習するには、[画像データの読み込み](https://www.tensorflow.org/tutorials/load_data/images)または [CSV データの読み込み](https://www.tensorflow.org/tutorials/load_data/csv)に関するチュートリアルをご覧ください。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FhGuhbZ6M5tl"
      ],
      "name": "mlp_core.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
