{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XX46cTrh6iD"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sKrlWr6Kh-mF"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hST65kOHXpiL"
      },
      "source": [
        "# TensorFlow Lite Model Maker を使用した音声分野での転移学習\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/audio_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/lite/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/lite/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub で表示</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/lite/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a>\n",
        "</td>\n",
        "  <td><a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub モデルを見る</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB5k6xNKJ5Xe"
      },
      "source": [
        "この colab ノートブックでは、[TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker) を使用して、カスタム音声分類モデルをトレーニングする方法について説明します。\n",
        "\n",
        "Model Maker ライブラリは、転移学習によって、カスタムデータセットを使用した TensorFlow Lite モデルのトレーニングプロセスを簡素化します。TensorFlow Lite モデルと独自のカスタムデータセットを維持すると、必要なトレーニングデータと時間の量が減ります。\n",
        "\n",
        "これは、[音声モデルをカスタマイズして、Android でデプロイする Codelab](https://codelabs.developers.google.com/codelabs/tflite-audio-classification-custom-model-android) の一部です。\n",
        "\n",
        "カスタム Birds データセットを使用します。スマートフォンで使用できる TFLite モデル、ブラウザでの推論に使用できる TensorFlow.JS モデル、サービスに使用できる SavedModel バージョンをエクスポートします。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeZZ_cSsZfPx"
      },
      "source": [
        "## 依存関係のインストール\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbMc4vHjaYdQ"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install tflite-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2ck_Ghdcgt9"
      },
      "source": [
        "## TensorFlow、Model Maker、およびその他のライブラリのインポート\n",
        "\n",
        "必要な依存関係のうち、TensorFlow と Model Maker を使用します。その他は、音声の操作、再生、視覚化用です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwUA9u4oWoCR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tflite_model_maker as mm\n",
        "from tflite_model_maker import audio_classifier\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import glob\n",
        "import random\n",
        "\n",
        "from IPython.display import Audio, Image\n",
        "from scipy.io import wavfile\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Model Maker Version: {mm.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIfm2TxKZAuA"
      },
      "source": [
        "## Birds データセット\n",
        "\n",
        "Birds データセットは、次の 5 種類の鳥の鳴き声の教育コレクションです。\n",
        "\n",
        "- ムナジロモリミソサザイ\n",
        "- イエスズメ\n",
        "- イスカ\n",
        "- クリガシラジアリドリ\n",
        "- ミヤマオナガカマドドリ\n",
        "\n",
        "元の音声は、世界中の鳥の鳴き声を共有する Web サイトである [Xeno-canto](https://www.xeno-canto.org/) から取得されました。\n",
        "\n",
        "まず、データをダウンロードします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upNRfilkNSmr"
      },
      "outputs": [],
      "source": [
        "birds_dataset_folder = tf.keras.utils.get_file('birds_dataset.zip',\n",
        "                                                'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset.zip',\n",
        "                                                cache_dir='./',\n",
        "                                                cache_subdir='dataset',\n",
        "                                                extract=True)\n",
        "                                                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441bbzZ5d6oq"
      },
      "source": [
        "## データの観察\n",
        "\n",
        "音声はすでにトレーニングフォルダとテストフォルダに分割されています。分割されたフォルダには、1 つのフォルダに 1 つの種類の鳥の鳴き声が格納されています。名前には、`bird_code` が使用されています。\n",
        "\n",
        "音声はすべてモノラルで、サンプリングレートは 16kHz です。\n",
        "\n",
        "各ファイルの詳細については、`metadata.csv` ファイルをお読みください。ファイルの作成者、ライセンス、詳細情報が記載されています。このチュートリアルでは読む必要はありません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayd7UqCfQQFU"
      },
      "outputs": [],
      "source": [
        "# @title [Run this] Util functions and data structures.\n",
        "\n",
        "data_dir = './dataset/small_birds_dataset'\n",
        "\n",
        "bird_code_to_name = {\n",
        "  'wbwwre1': 'White-breasted Wood-Wren',\n",
        "  'houspa': 'House Sparrow',\n",
        "  'redcro': 'Red Crossbill',  \n",
        "  'chcant2': 'Chestnut-crowned Antpitta',\n",
        "  'azaspi1': \"Azara's Spinetail\",   \n",
        "}\n",
        "\n",
        "birds_images = {\n",
        "  'wbwwre1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg/640px-Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg', # \tAlejandro Bayer Tamayo from Armenia, Colombia \n",
        "  'houspa': 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/House_Sparrow%2C_England_-_May_09.jpg/571px-House_Sparrow%2C_England_-_May_09.jpg', # \tDiliff\n",
        "  'redcro': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Red_Crossbills_%28Male%29.jpg/640px-Red_Crossbills_%28Male%29.jpg', #  Elaine R. Wilson, www.naturespicsonline.com\n",
        "  'chcant2': 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Chestnut-crowned_antpitta_%2846933264335%29.jpg/640px-Chestnut-crowned_antpitta_%2846933264335%29.jpg', # \tMike's Birds from Riverside, CA, US\n",
        "  'azaspi1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Synallaxis_azarae_76608368.jpg/640px-Synallaxis_azarae_76608368.jpg', # https://www.inaturalist.org/photos/76608368\n",
        "}\n",
        "\n",
        "test_files = os.path.abspath(os.path.join(data_dir, 'test/*/*.wav'))\n",
        "\n",
        "def get_random_audio_file():\n",
        "  test_list = glob.glob(test_files)\n",
        "  random_audio_path = random.choice(test_list)\n",
        "  return random_audio_path\n",
        "\n",
        "\n",
        "def show_bird_data(audio_path):\n",
        "  sample_rate, audio_data = wavfile.read(audio_path, 'rb')\n",
        "\n",
        "  bird_code = audio_path.split('/')[-2]\n",
        "  print(f'Bird name: {bird_code_to_name[bird_code]}')\n",
        "  print(f'Bird code: {bird_code}')\n",
        "  display(Image(birds_images[bird_code]))\n",
        "\n",
        "  plttitle = f'{bird_code_to_name[bird_code]} ({bird_code})'\n",
        "  plt.title(plttitle)\n",
        "  plt.plot(audio_data)\n",
        "  display(Audio(audio_data, rate=sample_rate))\n",
        "\n",
        "print('functions and data structures created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrv0uD7aXYl4"
      },
      "source": [
        "### 一部の音声の再生\n",
        "\n",
        "データの理解を深めるために、テストフォルダのランダム音声ファイルを聴いてみます。\n",
        "\n",
        "注意: このノートブックの後半では、この音声に対して推論を実行し、テストを行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEeMZh-VQy97"
      },
      "outputs": [],
      "source": [
        "random_audio = get_random_audio_file()\n",
        "show_bird_data(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQj1Mf7YZELS"
      },
      "source": [
        "## モデルのトレーニング\n",
        "\n",
        "音声で Model Maker を使用するときには、モデル仕様から始める必要があります。これは基本モデルであり、新しいモデルは情報を抽出して、新しいクラスを学習します。また、サンプリングレート、チャネル数などのモデル仕様パラメータに準拠するようにデータセットを変換する方法にも影響します。\n",
        "\n",
        "[YAMNet](https://tfhub.dev/google/yamnet/1) は AudioSet データセットでトレーニングされた音声イベント分類器であり、AudioSet オントロジーから音声イベントを予測します。\n",
        "\n",
        "入力は、16kHz、1 チャネルであると想定されています。\n",
        "\n",
        "自分でリサンプリングする必要はありません。Model Maker で実行されます。\n",
        "\n",
        "- `frame_length`: 各トレーニングサンプルの長さを決定します。この場合は EXPECTED_WAVEFORM_LENGTH * 3s です。\n",
        "\n",
        "- `frame_steps`: トレーニングサンプルがどの程度離れているのかを決定します。この場合、サンプルは、(i-1)th サンプルの後、EXPECTED_WAVEFORM_LENGTH * 6s に開始します。\n",
        "\n",
        "これらの値を設定する理由は、現実的なデータセットにおける一部の制限事項を回避するためです。\n",
        "\n",
        "たとえば、Birds データセットでは、鳥が常に鳴いているわけではありません。鳥は鳴き、休んで、もう一度鳴きます。間にはノイズがあります。長いフレームでは、鳴き声を取り込むことができますが、あまり長く設定すると、トレーニングのサンプル数が減ってしまいます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUcxtfHXY7XS"
      },
      "outputs": [],
      "source": [
        "spec = audio_classifier.YamNetSpec(\n",
        "    keep_yamnet_and_custom_heads=True,\n",
        "    frame_step=3 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,\n",
        "    frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF185yZ_M7zu"
      },
      "source": [
        "## データの読み込み\n",
        "\n",
        "Model Maker には、フォルダからデータを読み込み、モデル仕様に合った形式にするための API があります。\n",
        "\n",
        "トレーニングとテストの分割はフォルダに基づいています。検証データセットは、トレーニング分割の 20% として作成されます。\n",
        "\n",
        "注意: `cache=True` はトレーニングを後から高速化するために重要ですが、データを格納するために必要な RAM の量が多くなります。Birds データセットは 300 MB しかないので問題にはなりませんが、独自のデータを使用する場合には、注意が必要です。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX0RqETqZgzo"
      },
      "outputs": [],
      "source": [
        "train_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, os.path.join(data_dir, 'train'), cache=True)\n",
        "train_data, validation_data = train_data.split(0.8)\n",
        "test_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, os.path.join(data_dir, 'test'), cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMghju-Rts2"
      },
      "source": [
        "## モデルをトレーニングする\n",
        "\n",
        "audio_classifier には、モデルを作成し、すでにトレーニングを開始している [`create`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/create) メソッドがあります。\n",
        "\n",
        "さまざまなパラメータをカスタマイズできます。詳細については、ドキュメントをお読みください。\n",
        "\n",
        "今回は初めてなので、既定の構成をすべて使用し、100 エポックでトレーニングします。\n",
        "\n",
        "注意: 最初のエポックは、キャッシュの作成時点であるため、すべてのエポックの中で一番長いエポックです。その後、各エポックに約 1 秒かかります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r6Awvl4ZkIv"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "print('Training the model')\n",
        "model = audio_classifier.create(\n",
        "    train_data,\n",
        "    spec,\n",
        "    validation_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXMEHZkAxJTl"
      },
      "source": [
        "精度は高いように見えますが、テストデータに対して評価ステップを実行し、シードが設定されていないデータに対してもモデルの結果が良好であることを検証することが重要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDoQACMrZnOx"
      },
      "outputs": [],
      "source": [
        "print('Evaluating the model')\n",
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QRRAM39aOxS"
      },
      "source": [
        "## モデルの理解\n",
        "\n",
        "分類器をトレーニングする際、[混同行列](https://en.wikipedia.org/wiki/Confusion_matrix)を見ると役に立ちます。混同行列では、分類器がテストデータどどのように実行しているかを詳しく知ることができます。\n",
        "\n",
        "Model Maker では、すでに混同行列が作成されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqB3c0368iH3"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion, test_labels):\n",
        "  \"\"\"Compute confusion matrix and normalize.\"\"\"\n",
        "  confusion_normalized = confusion.astype(\"float\") / confusion.sum(axis=1)\n",
        "  axis_labels = test_labels\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.2f', square=True)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "confusion_matrix = model.confusion_matrix(test_data)\n",
        "show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gr1s7juBy7H"
      },
      "source": [
        "## モデルのテスト [任意]\n",
        "\n",
        "テストデータセットからサンプル音声に対してモデルを試し、結果を確認できます。\n",
        "\n",
        "まず、サービングモデルを取得します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmlmTl42Bq_u"
      },
      "outputs": [],
      "source": [
        "serving_model = model.create_serving_model()\n",
        "\n",
        "print(f'Model\\'s input shape and type: {serving_model.inputs}')\n",
        "print(f'Model\\'s output shape and type: {serving_model.outputs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQsZFO2mrYhx"
      },
      "source": [
        "前に読み込んだランダム音声に戻ります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dv5ViK0reXc"
      },
      "outputs": [],
      "source": [
        "# if you want to try another file just uncoment the line below\n",
        "random_audio = get_random_audio_file()\n",
        "show_bird_data(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uixOfKSUj_9m"
      },
      "source": [
        "作成されたモデルには、固定入力ウィンドウがあります。\n",
        "\n",
        "特定の音声ファイルでは、想定されたサイズのデータのウィンドウに分割する必要があります。最後のウィンドウは、ゼロ埋めにする必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAvGKQL0lNty"
      },
      "outputs": [],
      "source": [
        "sample_rate, audio_data = wavfile.read(random_audio, 'rb')\n",
        "\n",
        "audio_data = np.array(audio_data) / tf.int16.max\n",
        "input_size = serving_model.input_shape[1]\n",
        "\n",
        "splitted_audio_data = tf.signal.frame(audio_data, input_size, input_size, pad_end=True, pad_value=0)\n",
        "\n",
        "print(f'Test audio path: {random_audio}')\n",
        "print(f'Original size of the audio data: {len(audio_data)}')\n",
        "print(f'Number of windows for inference: {len(splitted_audio_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLxKd0eFkMcR"
      },
      "source": [
        "すべての分割された音声をループし、それぞれに対してモデルを適用します。\n",
        "\n",
        "トレーニングしたモデルには 2 つの出力があります。元の YAMNet の出力と、トレーニングした出力です。実際の環境は鳥の鳴き声よりももっと複雑であるため、この点は重要です。YAMNet の出力を使用して、関連しない音声を除外することができます。たとえば、鳥の鳴き声のユースケースでは、YAMNet で鳥か動物かが分類されない場合、モデルの出力で関連しない分類が実行されている可能性を示していると考えられます。\n",
        "\n",
        "次の出力はいずれも表示され、関係を理解しやすくなっています。モデルにおけるほとんどの誤りは、YAMNet の予測が分野 (例: 鳥) に関連しないときに発生します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-8fJLrxGwYT"
      },
      "outputs": [],
      "source": [
        "print(random_audio)\n",
        "\n",
        "results = []\n",
        "print('Result of the window ith:  your model class -> score,  (spec class -> score)')\n",
        "for i, data in enumerate(splitted_audio_data):\n",
        "  yamnet_output, inference = serving_model(data)\n",
        "  results.append(inference[0].numpy())\n",
        "  result_index = tf.argmax(inference[0])\n",
        "  spec_result_index = tf.argmax(yamnet_output[0])\n",
        "  t = spec._yamnet_labels()[spec_result_index]\n",
        "  result_str = f'Result of the window {i}: ' \\\n",
        "  f'\\t{test_data.index_to_label[result_index]} -> {inference[0][result_index].numpy():.3f}, ' \\\n",
        "  f'\\t({spec._yamnet_labels()[spec_result_index]} -> {yamnet_output[0][spec_result_index]:.3f})'\n",
        "  print(result_str)\n",
        "\n",
        "\n",
        "results_np = np.array(results)\n",
        "mean_results = results_np.mean(axis=0)\n",
        "result_index = mean_results.argmax()\n",
        "print(f'Mean result: {test_data.index_to_label[result_index]} -> {mean_results[result_index]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yASrikBgZ9ZO"
      },
      "source": [
        "## モデルのエクスポート\n",
        "\n",
        "最後のステップでは、埋め込まれたデバイスまたはブラウザで使用されるモデルをエクスポートします。\n",
        "\n",
        "`export` メソッドでは、両方の形式がエクスポートされます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw_ehPxAdQlz"
      },
      "outputs": [],
      "source": [
        "models_path = './birds_models'\n",
        "print(f'Exporing the TFLite model to {models_path}')\n",
        "\n",
        "model.export(models_path, tflite_filename='my_birds_model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjZRKmurA3y_"
      },
      "source": [
        "Python 環境でサービスを提供したり、使用したりするための SavedModel バージョンをエクスポートすることもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veBwppOsA-kn"
      },
      "outputs": [],
      "source": [
        "model.export(models_path, export_format=[mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xr0idac6xfi"
      },
      "source": [
        "## 次のステップ\n",
        "\n",
        "完了しました。\n",
        "\n",
        "これで、[TFLite AudioClassifier Task API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier) を使用して、モバイルデバイスに新しいモデルをデプロイできます。\n",
        "\n",
        "異なるクラスが設定された独自のデータでも同じプロセスを試すことができます。[音声分類のための Model Maker](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier) のドキュメントを参照してください。\n",
        "\n",
        "エンドツーエンドリファレンスアプリでも学習できます。[Android](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android/) 版、[iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios) 版"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "audio_classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
