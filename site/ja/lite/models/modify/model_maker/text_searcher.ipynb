{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# TensorFlow Lite Model Maker によるテキスト検索"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5Y7snSuG51"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/text_searcher\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/lite/models/modify/model_maker/text_searcher.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/lite/models/modify/model_maker/text_searcher.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a> </td>\n",
        "  <td> ノートブックをダウンロード</td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub モデルを参照</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2sdIlXEVPZR"
      },
      "source": [
        "この colab ノートブックでは、[TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker) ライブラリを使用して、TFLite Searcher モデルを作成する方法について説明します。テキスト検索モデルを使用すると、アプリでセマンティック検索またはスマートリプライを構築できます。この種類のモデルでは、テキストクエリを取得して、Web ページのデータベースなどのテキストデータセットにおける最も関連性が高いエントリを検索できます。このモデルは、URL、ページタイトル、他のテキスト入力識別子などの指定されたメタデータを含む、データセットの最小距離スコアエントリのリストを返します。構築した後は、[Task Library Searcher API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/text_searcher) を使用してデバイス (例: Android) にデプロイし、数行のコードだけで推論を実行できます。\n",
        "\n",
        "このチュートリアルでは、CNN/DailyMail データセットをインスタンスとして利用し、TFLite 検索モデルを作成します。互換性がある入力カンマ区切り値 (CSV) 形式で独自のデータセットを試すことができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_aZYF85VaVK"
      },
      "source": [
        "## 拡張最近傍を使用したテキスト検索"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3q-gvm3cI8"
      },
      "source": [
        "このチュートリアルでは、[GitHub repo](https://github.com/abisee/cnn-dailymail) から生成された、公開 CNN/DailyMail 非匿名化集約データセットを使用します。このデータセットには、30 万件以上の新しい記事が含まれており、検索モデルを構築するための良質のデータセットになります。また、テキストクエリのモデル推論中に、さまざまな関連ニュースを返します。\n",
        "\n",
        "この例のテキスト検索モデルでは、[ScaNN](https://github.com/google-research/google-research/tree/master/scann) (Scalable Nearest Neighbors: 拡張最近傍) インデックスファイルを使用します。このファイルは、定義済みのデータベースから類似した項目を検索できます。ScaNN では、最先端のパフォーマンスが実現され、大規模なベクトル類似性検索を効率的に実行できます。\n",
        "\n",
        "この colab では、このデータセットのハイライトと URL が使用され、モデルを作成します。\n",
        "\n",
        "1. ハイライトは、埋め込み特徴量ベクトルを生成するためのテキストであり、検索で使用されます。\n",
        "2. URL は、関連するハイライトを検索した後に、ユーザーに表示される返された結果です。\n",
        "\n",
        "このチュートリアルでは、このデータを CSV ファイルに保存し、その CSV ファイルを使用してモデルを構築します。次に、データセットの例をいくつか示します。\n",
        "\n",
        "ハイライト | URL\n",
        "--- | ---\n",
        "Hawaiian Airlines again lands at No. 1 in on-time performance. The Airline Quality Rankings Report looks at the 14 largest U.S. airlines. ExpressJet <br> and American Airlines had the worst on-time performance. Virgin America had the best baggage  handling; Southwest had lowest complaint rate. | http://www.cnn.com/2013/04/08/travel/airline-quality-report\n",
        "European football's governing body reveals list of countries bidding to host 2020 finals. The 60th anniversary edition of the finals will be hosted by 13 <br> countries. Thirty-two countries are considering bids to host 2020 matches. UEFA will announce host cities on September 25. | http://edition.cnn.com:80/2013/09/20/sport/football/football-euro-2020-bid-countries/index.html?\n",
        "Once octopus-hunter Dylan Mayer has now also signed a petition of 5,000 divers banning their hunt at Seacrest Park. Decision by Washington <br> Department of Fish and Wildlife could take months. | http://www.dailymail.co.uk:80/news/article-2238423/Dylan-Mayer-Washington-considers-ban-Octopus-hunting-diver-caught-ate-Puget-Sound.html?\n",
        "Galaxy was observed 420 million years after the Big Bang. found by NASA’s Hubble Space Telescope, Spitzer Space Telescope, and one of nature’s <br> own natural 'zoom lenses' in space. | http://www.dailymail.co.uk/sciencetech/article-2233883/The-furthest-object-seen-Record-breaking-image-shows-galaxy-13-3-BILLION-light-years-Earth.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## 設定\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvAObmTqglq"
      },
      "source": [
        "まず、[GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) の Model Maker パッケージなどの必要なパッケージをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install -q tflite-model-maker\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "必要なパッケージをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "from tflite_model_maker import searcher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veelLw_VT6uQ"
      },
      "source": [
        "### データセットを準備する\n",
        "\n",
        "このチュートリアルでは、[GitHub repo](https://github.com/abisee/cnn-dailymail) の CNN / Daily Mail 集約データセットを使用します。\n",
        "\n",
        "まず、CNN と Daily Mail のテキストと URL をダウンロードして解凍します。Google Drive からダウンロードできなかった場合は、数分間待ってからもう一度ダウンロードするか、手動でダウンロードして colab にアップロードしてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P3zxue1T6Iy"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\n",
        "!gdown https://drive.google.com/uc?id=0BwmD_VLjROrfM1BxdkxVaTY2bWs\n",
        "\n",
        "!wget -O all_train.txt https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt\n",
        "!tar xzf cnn_stories.tgz\n",
        "!tar xzf dailymail_stories.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoOWzTU7ViPM"
      },
      "source": [
        "次に、`tflite_model_maker` ライブラリにアップロードできる CSV ファイルにデータを保存します。コードは、[`tensorflow_datasets`](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/summarization/cnn_dailymail.py) でこのデータを読み込むために使用されるロジックに基づいています。`tensorflow_dataset` には、この colab で使用されている URL が含まれていないため、直接使用することはできません。\n",
        "\n",
        "データセット全体では、データを埋め込み特徴量ベクトルに処理するのに時間がかかるため、既定では、CNN および Daily Mail データセットの最初の 5% のみが既定で選択されます。この比率を調整するか、検索対象の CNN および Daily Mail データセットの 50% の記事が含まれている構築済みの TFLite [モデル](https://storage.googleapis.com/download.tensorflow.org/models/tflite_support/searcher/text_to_image_blogpost/cnn_daily_text_searcher.tflite) で試すことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bA4PsR6NVU69"
      },
      "outputs": [],
      "source": [
        "#@title Save the highlights and urls to the CSV file\n",
        "#@markdown Load the highlights from the stories of CNN / Daily Mail, map urls with highlights, and save them to the CSV file.\n",
        "\n",
        "CNN_FRACTION = 0.05 #@param {type:\"number\"}\n",
        "DAILYMAIL_FRACTION = 0.05 #@param {type:\"number\"}\n",
        "\n",
        "import csv\n",
        "import hashlib\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "dm_single_close_quote = u\"\\u2019\"  # unicode\n",
        "dm_double_close_quote = u\"\\u201d\"\n",
        "END_TOKENS = [\n",
        "    \".\", \"!\", \"?\", \"...\", \"'\", \"`\", '\"', dm_single_close_quote,\n",
        "    dm_double_close_quote, \")\"\n",
        "]  # acceptable ways to end a sentence\n",
        "\n",
        "\n",
        "def read_file(file_path):\n",
        "  \"\"\"Reads lines in the file.\"\"\"\n",
        "  lines = []\n",
        "  with tf.io.gfile.GFile(file_path, \"r\") as f:\n",
        "    for line in f:\n",
        "      lines.append(line.strip())\n",
        "  return lines\n",
        "\n",
        "\n",
        "def url_hash(url):\n",
        "  \"\"\"Gets the hash value of the url.\"\"\"\n",
        "  h = hashlib.sha1()\n",
        "  url = url.encode(\"utf-8\")\n",
        "  h.update(url)\n",
        "  return h.hexdigest()\n",
        "\n",
        "\n",
        "def get_url_hashes_dict(urls_path):\n",
        "  \"\"\"Gets hashes dict that maps the hash value to the original url in file.\"\"\"\n",
        "  urls = read_file(urls_path)\n",
        "  return {url_hash(url): url[url.find(\"id_/\") + 4:] for url in urls}\n",
        "\n",
        "\n",
        "def find_files(folder, url_dict):\n",
        "  \"\"\"Finds files corresponding to the urls in the folder.\"\"\"\n",
        "  all_files = tf.io.gfile.listdir(folder)\n",
        "  ret_files = []\n",
        "  for file in all_files:\n",
        "    # Gets the file name without extension.\n",
        "    filename = os.path.splitext(os.path.basename(file))[0]\n",
        "    if filename in url_dict:\n",
        "      ret_files.append(os.path.join(folder, file))\n",
        "  return ret_files\n",
        "\n",
        "\n",
        "def fix_missing_period(line):\n",
        "  \"\"\"Adds a period to a line that is missing a period.\"\"\"\n",
        "  if \"@highlight\" in line:\n",
        "    return line\n",
        "  if not line:\n",
        "    return line\n",
        "  if line[-1] in END_TOKENS:\n",
        "    return line\n",
        "  return line + \".\"\n",
        "\n",
        "\n",
        "def get_highlights(story_file):\n",
        "  \"\"\"Gets highlights from a story file path.\"\"\"\n",
        "  lines = read_file(story_file)\n",
        "\n",
        "  # Put periods on the ends of lines that are missing them\n",
        "  # (this is a problem in the dataset because many image captions don't end in\n",
        "  # periods; consequently they end up in the body of the article as run-on\n",
        "  # sentences)\n",
        "  lines = [fix_missing_period(line) for line in lines]\n",
        "\n",
        "  # Separate out article and abstract sentences\n",
        "  highlight_list = []\n",
        "  next_is_highlight = False\n",
        "  for line in lines:\n",
        "    if not line:\n",
        "      continue  # empty line\n",
        "    elif line.startswith(\"@highlight\"):\n",
        "      next_is_highlight = True\n",
        "    elif next_is_highlight:\n",
        "      highlight_list.append(line)\n",
        "\n",
        "  # Make highlights into a single string.\n",
        "  highlights = \"\\n\".join(highlight_list)\n",
        "\n",
        "  return highlights\n",
        "\n",
        "url_hashes_dict = get_url_hashes_dict(\"all_train.txt\")\n",
        "cnn_files = find_files(\"cnn/stories\", url_hashes_dict)\n",
        "dailymail_files = find_files(\"dailymail/stories\", url_hashes_dict)\n",
        "\n",
        "# The size to be selected.\n",
        "cnn_size = int(CNN_FRACTION * len(cnn_files))\n",
        "dailymail_size = int(DAILYMAIL_FRACTION * len(dailymail_files))\n",
        "print(\"CNN size: %d\"%cnn_size)\n",
        "print(\"Daily Mail size: %d\"%dailymail_size)\n",
        "\n",
        "with open(\"cnn_dailymail.csv\", \"w\") as csvfile:\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=[\"highlights\", \"urls\"])\n",
        "  writer.writeheader()\n",
        "\n",
        "  for file in cnn_files[:cnn_size] + dailymail_files[:dailymail_size]:\n",
        "    highlights = get_highlights(file)\n",
        "    # Gets the filename which is the hash value of the url.\n",
        "    filename = os.path.splitext(os.path.basename(file))[0]\n",
        "    url = url_hashes_dict[filename]\n",
        "    writer.writerow({\"highlights\": highlights, \"urls\": url})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xushUyZXqP59"
      },
      "source": [
        "## テキスト検索モデルの構築"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn61LJ9QbOPi"
      },
      "source": [
        "テキスト検索モデルを作成するには、データセットを読み込み、そのデータを使用してモデルを作成し、TFLite モデルをエクスポートします。\n",
        "\n",
        "### ステップ 1. データセットの読み込み\n",
        "\n",
        "Model Maker では、CSV 形式のテキストデータセットと、各テキスト文字列 (この例の URL など) の対応するメタデータが取り込まれます。ユーザーが指定した埋め込みモデルを使用して、テキスト文字列が特徴量ベクトルに埋め込まれます。\n",
        "\n",
        "このデモでは、[Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder-lite/2) を使用して検索モデルを構築します。これは、すでに [colab](https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/examples/colab/on_device_text_to_image_search_tflite.ipynb) から再トレーニングされた最先端の文章埋め込みモデルです。このモデルは、オンデバイス推論パフォーマンスのために最適化され、クエリ文字列を埋め込む時間はわずか 6 ミリ秒です (Pixel 6 で測定)。あるいは、小さいサイズながらも各埋め込み時間が 38 ミリ秒である、[この](https://tfhub.dev/google/lite-model/universal-sentence-encoder-qa-ondevice/1?lite-format=tflite)量子化バージョンを使用することもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ymHbk0wjHHZ"
      },
      "outputs": [],
      "source": [
        "!wget -O universal_sentence_encoder.tflite https://storage.googleapis.com/download.tensorflow.org/models/tflite_support/searcher/text_to_image_blogpost/text_embedder.tflite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knJwmJHxkFbx"
      },
      "source": [
        "`searcher.TextDataLoader` インスタンスを作成し、`data_loader.load_from_csv` メソッドを使用して、データセットを読み込みます。このステップでは、各テキストの埋め込み特徴量ベクトルが 1 つずつ生成されるため、最長で 10 分間かかります。独自の CSV ファイルをアップロードし、それを読み込んで、カスタマイズされたモデルを構築することもできます。\n",
        "\n",
        "CSV ファイルでテキスト列とメタデータ列の名前を指定します。\n",
        "\n",
        "- テキストは、埋め込み特徴量ベクトルを生成するために使用されます。\n",
        "- メタデータは、特定のテキストを検索するときに表示されるコンテンツです。\n",
        "\n",
        "次に、上記で生成された CNN-DailyMail CSV ファイルの最初の 4 行を示します。\n",
        "\n",
        "ハイライト | URL\n",
        "--- | ---\n",
        "Syrian official: Obama climbed to the top of the tree, doesn't know how to get down. Obama sends a letter to the heads of the House and Senate. Obama <br> to seek congressional approval on military action against Syria. Aim is to determine whether CW were used, not by whom, says U.N. spokesman. | http://www.cnn.com/2013/08/31/world/meast/syria-civil-war/\n",
        "Usain Bolt wins third gold of world championship. Anchors Jamaica to 4x100m relay victory. Eighth gold at the championships for Bolt. Jamaica double <br> up in women's 4x100m relay. | http://edition.cnn.com/2013/08/18/sport/athletics-bolt-jamaica-gold\n",
        "The employee in agency's Kansas City office is among hundreds of \"virtual\" workers. The employee's travel to and from the mainland U.S. last year cost <br> more than $24,000. The telecommuting program, like all GSA practices, is under review. | http://www.cnn.com:80/2012/08/23/politics/gsa-hawaii-teleworking\n",
        "NEW: A Canadian doctor says she was part of a team examining Harry Burkhart in 2010. NEW: Diagnosis: \"autism, severe anxiety, post-traumatic stress <br> disorder and depression\" Burkhart is also suspected in a German arson probe, officials say. Prosecutors believe the German national set a string of fires <br> in Los Angeles. | http://edition.cnn.com:80/2012/01/05/justice/california-arson/index.html?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtdZ-JDwMimd"
      },
      "outputs": [],
      "source": [
        "data_loader = searcher.TextDataLoader.create(\"universal_sentence_encoder.tflite\", l2_normalize=True)\n",
        "data_loader.load_from_csv(\"cnn_dailymail.csv\", text_column=\"highlights\", metadata_column=\"urls\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVN5bkSFiZdV"
      },
      "source": [
        "画像のユースケースでは、`searcher.ImageDataLoader` インスタンスを作成し、`data_loader.load_from_folder` を使用して、フォルダから画像を読み込むことができます。TFLite 埋め込みモデルでは、`searcher.ImageDataLoader` インスタンスを作成する必要があります。これは、クエリを特徴量ベクトルにエンコードするために使用され、TFLite 検索モデルでエクスポートされるためです。\n",
        "\n",
        "```python\n",
        "data_loader = searcher.ImageDataLoader.create(\"mobilenet_v2_035_96_embedder_with_metadata.tflite\")\n",
        "data_loader.load_from_folder(\"food/\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "###ステップ 2. 検索モデルの作成\n",
        "\n",
        "- ScaNN オプションを構成します。詳細については、[api ドキュメント](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/searcher/ScaNNOptions)を参照してください。\n",
        "- データと ScaNN オプションから検索モデルを作成します。ScaNN アルゴリズムの詳細については、[in-depth examination](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) を参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwlYdTcg63xy"
      },
      "outputs": [],
      "source": [
        "scann_options = searcher.ScaNNOptions(\n",
        "      distance_measure=\"dot_product\",\n",
        "      tree=searcher.Tree(num_leaves=140, num_leaves_to_search=4),\n",
        "      score_ah=searcher.ScoreAH(dimensions_per_block=1, anisotropic_quantization_threshold=0.2))\n",
        "model = searcher.Searcher.create_from_data(data_loader, scann_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lluAWms3soFm"
      },
      "source": [
        "上記の例では、次のオプションを定義します。\n",
        "\n",
        "- `distance_measure`: \"dot_product\" を使用して、2 つの埋め込みベクトル間の距離を測定します。実際には、**負**のドット積値を計算するため、「小さいほど近い」という概念が守られます。\n",
        "\n",
        "- `tree`: データセットは、140 のパーティション (おおよそデータサイズの平方根) に分割され、そのうちの 4 つが検索されます。これはデータセットの約 3% です。\n",
        "\n",
        "- `score_ah`: 浮動小数点数埋め込みを同じ次元の int8 値に量子化し。スペースを削減します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "###ステップ 3. TFLite モデルのエクスポート\n",
        "\n",
        "次に、TFLite Searcher モデルをエクスポートできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm_UULdW7A9T"
      },
      "outputs": [],
      "source": [
        "model.export(\n",
        "      export_filename=\"searcher.tflite\",\n",
        "      userinfo=\"\",\n",
        "      export_format=searcher.ExportFormat.TFLITE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me6_RwPZqNhX"
      },
      "source": [
        "## クエリでの TFLite モデルのテスト\n",
        "\n",
        "カスタムクエリテキストを使用して、エクスポート済みの TFLite モデルをテストできます。検索モデルを使用してテキストを照会するには、次のように、モデルを初期化して、テキストフレーズで検索を実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkXtipXKqXp4"
      },
      "outputs": [],
      "source": [
        "from tflite_support.task import text\n",
        "\n",
        "# Initializes a TextSearcher object.\n",
        "searcher = text.TextSearcher.create_from_file(\"searcher.tflite\")\n",
        "\n",
        "# Searches the input query.\n",
        "results = searcher.search(\"The Airline Quality Rankings Report looks at the 14 largest U.S. airlines.\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzI0tPM2rFlc"
      },
      "source": [
        "モデルをさまざまなプラットフォームに統合する方法については、[Task Library ドキュメント](https://www.tensorflow.org/lite/inference_with_metadata/task_library/text_searcher)を参照してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS4u77W5gnzQ"
      },
      "source": [
        "# その他の資料\n",
        "\n",
        "詳細については、次のドキュメントを参照してください。\n",
        "\n",
        "- TensorFlow Lite Model Maker の[ガイド](https://www.tensorflow.org/lite/models/modify/model_maker)と [API リファレンス](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker)\n",
        "\n",
        "- タスクライブラリ: デプロイ用の [TextSearcher](https://www.tensorflow.org/lite/inference_with_metadata/task_library/text_searcher)\n",
        "\n",
        "- エンドツーエンドリファレンスアプリ: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/text_searcher/android) 版\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "text_searcher.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
