{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XX46cTrh6iD"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sKrlWr6Kh-mF"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hST65kOHXpiL"
      },
      "source": [
        "# TensorFlow Lite Model Maker による音声認識モデルの再トレーニング\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nShlCXGkbRVA"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/speech_recognition\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub で表示</a>\n",
        "</td>\n",
        "  <td> ノートブックをダウンロード</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB5k6xNKJ5Xe"
      },
      "source": [
        "この colab ノートブックでは、[TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker) を使用して、1 秒間の音声サンプルから発話された単語と短いフレーズを分類できる、音声認識モデルをトレーニングする方法について説明します。Model Maker ライブラリは転移学習を使用して、新しいデータセットで既存の TensorFlow モデルを再トレーニングします。これにより、トレーニングに必要なサンプルデータと時間が少なくなります。\n",
        "\n",
        "既定では、このノートブックは、[音声認識データセット](https://www.tensorflow.org/datasets/catalog/speech_commands) (\"up\"、\"down\"、\"left\"、\"right\" など) を使用して、モデル ([TFJS Speech Command Recognizer](https://github.com/tensorflow/tfjs-models/tree/master/speech-commands#speech-command-recognizer) の BrowserFft) を再トレーニングします。次に、モバイルデバイスまたは組み込みシステム (Raspberry Pi など) で実行できる TFLite モデルがエクスポートされます。トレーニング済みのモデルも TensorFlow SavedModel としてエクスポートされます。\n",
        "\n",
        "また、このノートブックは、ZIP ファイルで Colab にアップロードされる、WAV ファイルのカスタムデータセットを許可するように設計されています。各クラスのサンプルが多いほど、精度が上がります。転移学習プロセスでは、トレーニング済みのモデルから特徴埋め込みを使用するため、各クラスのサンプル数が 20 ～ 30 件しかなくても、かなり精度が高いモデルを実現できます。\n",
        "\n",
        "**注意:** トレーニングするモデルは、1 秒間のサンプルの音声認識向けに最適化されています。異なる種類の音楽の検出のように、より一般的な音声分類を実行する場合は、[この Colab に従い、音声分類器を再トレーニング](https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb)することをお勧めします。\n",
        "\n",
        "既定の音声データセットでこのノートブックを実行する場合は、Colab ツールバーで **[Runtime] &gt; [Run all]** をクリックして、すべて実行できます。ただし、独自のデータセットを使用する場合は、 [データセットの準備](#scrollTo=cBsSAeYLkc1Z)に進み、その手順に従ってください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeZZ_cSsZfPx"
      },
      "source": [
        "### 必要なパッケージのインポート\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MelHQlE7FVue"
      },
      "source": [
        "音声の操作、再生、視覚化を行うには、TensorFlow、TFLite Model Maker、一部のモジュールが必要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbMc4vHjaYdQ"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install tflite-model-maker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwUA9u4oWoCR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import tflite_model_maker as mm\n",
        "from tflite_model_maker import audio_classifier\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Model Maker Version: {mm.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBsSAeYLkc1Z"
      },
      "source": [
        "## データセットを準備する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTTSXJxJq2Bz"
      },
      "source": [
        "既定の音声データセットでトレーニングするには、次のコードすべてをそのまま実行します。\n",
        "\n",
        "ただし、独自の音声データセットでトレーニングする場合は、次の手順に従います。\n",
        "\n",
        "**注意:** トレーニングするモデルでは、44.1 kHz で約 1 秒間の音声が入力データとして想定されています。Model Maker によって、自動リサンプリングが実行され、データセットがトレーニングされます。このため、44.1 kHz 以外のサンプリングレートの場合、データセットをリサンプリングする必要はありません。ただし、音声サンプルが 1 秒を超える場合は、複数の 1 秒間のチャンクに分割され、最後のチャンクが 1 秒未満の場合は、その最後のチャンクが破棄されます。\n",
        "\n",
        "1. データセットの各サンプルは **約 1 秒間の WAV ファイル形式**です。すべての WAV ファイルを含んだ ZIP ファイルを作成します。この ZIP ファイルでは、各分類が別個のサブフォルダに整理されています。たとえば、音声コマンド「yes」の各サンプルは、「yes」サブフォルダに格納されます。クラスが 1 つしかない場合でも、クラス名がディレクトリ名になっているサブディレクトリにそのサンプルを保存する必要があります。(このスクリプトでは、データセットがトレーニング/検証/テストセットに分割されていない状態で、自動的に分割が実行されることを想定しています。)\n",
        "2. 左側のパネルで **[Files]** タブをクリックし、ZIP ファイルをドラッグアンドドロップしてアップロードします。\n",
        "3. 次のドロップダウンオプションを使用して、**`use_custom_dataset`** を True に設定します。\n",
        "4. [カスタム音声データセットの準備](#scrollTo=EobYerLQkiF1)に進み、ZIP ファイル名とデータセットディレクトリ名を指定します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AK9o98X7qyhU"
      },
      "outputs": [],
      "source": [
        "use_custom_dataset = False #@param [\"False\", \"True\"] {type:\"raw\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2sNXbYVHjjy"
      },
      "source": [
        "### 背景ノイズデータセットの生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBVClNMwHtMD"
      },
      "source": [
        "既定の音声データセットか、カスタムデータセットのどちらを使用している場合でも、モデルで音声とその他のノイズ (無音を含む) を識別できるように、良質の背景ノイズのセットを用意する必要があります。\n",
        "\n",
        "次の背景サンプルは、1 分間以上の WAV ファイルで提供されているため、小さい 1 秒間のサンプルに分割し、テストデータセット用に一部を予約できるようにする必要があります。いくつかの異なるサンプルソースを組み合わせ、背景ノイズと無音の包括的なセットを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvJd9VfmHu29"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.get_file('speech_commands_v0.01.tar.gz',\n",
        "                        'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='dataset-speech',\n",
        "                        extract=True)\n",
        "tf.keras.utils.get_file('background_audio.zip',\n",
        "                        'https://storage.googleapis.com/download.tensorflow.org/models/tflite/sound_classification/background_audio.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='dataset-background',\n",
        "                        extract=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CAVFc3woB3_"
      },
      "source": [
        "**注意:** 新しいバージョンが利用可能ですが、ダウンロードサイズが小さい v0.01 音声コマンドデータセットを使用します。v0.01 には 30 コマンドが含まれていますが、v0.02 ではさらに 5 コマンド (\"backward\"、\"forward\"、\"follow\"、\"learn\"、\"visual\") が追加されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgwWNifGL-3b"
      },
      "outputs": [],
      "source": [
        "# Create a list of all the background wav files\n",
        "files = glob.glob(os.path.join('./dataset-speech/_background_noise_', '*.wav'))\n",
        "files = files + glob.glob(os.path.join('./dataset-background', '*.wav'))\n",
        "\n",
        "background_dir = './background'\n",
        "os.makedirs(background_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all files and split each into several one-second wav files\n",
        "for file in files:\n",
        "  filename = os.path.basename(os.path.normpath(file))\n",
        "  print('Splitting', filename)\n",
        "  name = os.path.splitext(filename)[0]\n",
        "  rate = librosa.get_samplerate(file)\n",
        "  length = round(librosa.get_duration(filename=file))\n",
        "  for i in range(length - 1):\n",
        "    start = i * rate\n",
        "    stop = (i * rate) + rate\n",
        "    data, _ = sf.read(file, start=start, stop=stop)\n",
        "    sf.write(os.path.join(background_dir, name + str(i) + '.wav'), data, rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVlvVq-SkeeO"
      },
      "source": [
        "### 音声コマンドデータセットの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_q22T5UHbJG"
      },
      "source": [
        "音声コマンドデータセットをダウンロードしたので、モデルのクラス数を減らす必要があります。\n",
        "\n",
        "このデータセットには、30 以上の音声コマンド分類が含まれ、そのほとんどに 2,000 サンプル以上が含まれています。ここでは転移学習を使用しているため、それほど多くのサンプルは必要ありません。そこで、次のコードで次の処理を実行します。\n",
        "\n",
        "- 使用する分類を指定し、それ以外を削除する。\n",
        "- トレーニング用に各クラスにつき 150 サンプルだけ保持する (転移学習が少ないデータセットでも機能することを証明し、トレーニング時間を減らすため)。\n",
        "- テストデータセット用に別のディレクトリを作成し、後から簡単に推論を実行できるようにする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUSRpw2nOp8p"
      },
      "outputs": [],
      "source": [
        "if not use_custom_dataset:\n",
        "  commands = [ \"up\", \"down\", \"left\", \"right\", \"go\", \"stop\", \"on\", \"off\", \"background\"]\n",
        "  dataset_dir = './dataset-speech'\n",
        "  test_dir = './dataset-test'\n",
        "\n",
        "  # Move the processed background samples\n",
        "  shutil.move(background_dir, os.path.join(dataset_dir, 'background'))   \n",
        "\n",
        "  # Delete all directories that are not in our commands list\n",
        "  dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
        "  for dir in dirs:\n",
        "    name = os.path.basename(os.path.normpath(dir))\n",
        "    if name not in commands:\n",
        "      shutil.rmtree(dir)\n",
        "\n",
        "  # Count is per class\n",
        "  sample_count = 150\n",
        "  test_data_ratio = 0.2\n",
        "  test_count = round(sample_count * test_data_ratio)\n",
        "\n",
        "  # Loop through child directories (each class of wav files)\n",
        "  dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
        "  for dir in dirs:\n",
        "    files = glob.glob(os.path.join(dir, '*.wav'))\n",
        "    random.seed(42)\n",
        "    random.shuffle(files)\n",
        "    # Move test samples:\n",
        "    for file in files[sample_count:sample_count + test_count]:\n",
        "      class_dir = os.path.basename(os.path.normpath(dir))\n",
        "      os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)\n",
        "      os.rename(file, os.path.join(test_dir, class_dir, os.path.basename(file)))\n",
        "    # Delete remaining samples\n",
        "    for file in files[sample_count + test_count:]:\n",
        "      os.remove(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EobYerLQkiF1"
      },
      "source": [
        "### カスタムデータセットの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3xTvDP3knMd"
      },
      "source": [
        "独自の音声データセットでモデルをトレーニングする場合は、ZIP の WAV ファイルとしてサンプルをアップロード ([上記を参照](#scrollTo=cBsSAeYLkc1Z)) し、次の変数を修正してデータセットを指定する必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77PsQAKA4Arx"
      },
      "outputs": [],
      "source": [
        "if use_custom_dataset:\n",
        "  # Specify the ZIP file you uploaded:\n",
        "  !unzip YOUR-FILENAME.zip\n",
        "  # Specify the unzipped path to your custom dataset\n",
        "  # (this path contains all the subfolders with classification names):\n",
        "  dataset_dir = './YOUR-DIRNAME'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwp6EQqvttgf"
      },
      "source": [
        "上記のファイル名とパスを変更した後、カスタムデータセットでモデルをトレーニングできます。Colab ツールバーで **[Runtime] &gt; [Run all]** を選択し、ノートブック全体を実行します。\n",
        "\n",
        "次のコードは、新しい背景ノイズサンプルをデータセットに統合し、すべてのサンプルの部分を分離して、テストセットを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMQ6cpw_B9e_"
      },
      "outputs": [],
      "source": [
        "def move_background_dataset(dataset_dir):\n",
        "  dest_dir = os.path.join(dataset_dir, 'background')\n",
        "  if os.path.exists(dest_dir):\n",
        "    files = glob.glob(os.path.join(background_dir, '*.wav'))\n",
        "    for file in files:\n",
        "      shutil.move(file, dest_dir)\n",
        "  else:\n",
        "    shutil.move(background_dir, dest_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45iru8OdliG3"
      },
      "outputs": [],
      "source": [
        "if use_custom_dataset:\n",
        "  # Move background samples into custom dataset\n",
        "  move_background_dataset(dataset_dir)\n",
        "\n",
        "  # Now we separate some of the files that we'll use for testing:\n",
        "  test_dir = './dataset-test'\n",
        "  test_data_ratio = 0.2\n",
        "  dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
        "  for dir in dirs:\n",
        "    files = glob.glob(os.path.join(dir, '*.wav'))\n",
        "    test_count = round(len(files) * test_data_ratio)\n",
        "    random.seed(42)\n",
        "    random.shuffle(files)\n",
        "    # Move test samples:\n",
        "    for file in files[:test_count]:\n",
        "      class_dir = os.path.basename(os.path.normpath(dir))\n",
        "      os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)\n",
        "      os.rename(file, os.path.join(test_dir, class_dir, os.path.basename(file)))\n",
        "    print('Moved', test_count, 'images from', class_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPa1dfEoagz"
      },
      "source": [
        "### サンプルの再生"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jecBYREgMk6"
      },
      "source": [
        "データセットが正しいことを確認するために、テストセットからランダムサンプルを再生します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLC3ayJsoeNw"
      },
      "outputs": [],
      "source": [
        "def get_random_audio_file(samples_dir):\n",
        "  files = os.path.abspath(os.path.join(samples_dir, '*/*.wav'))\n",
        "  files_list = glob.glob(files)\n",
        "  random_audio_path = random.choice(files_list)\n",
        "  return random_audio_path\n",
        "\n",
        "def show_sample(audio_path):\n",
        "  audio_data, sample_rate = sf.read(audio_path)\n",
        "  class_name = os.path.basename(os.path.dirname(audio_path))\n",
        "  print(f'Class: {class_name}')\n",
        "  print(f'File: {audio_path}')\n",
        "  print(f'Sample rate: {sample_rate}')\n",
        "  print(f'Sample length: {len(audio_data)}')\n",
        "\n",
        "  plt.title(class_name)\n",
        "  plt.plot(audio_data)\n",
        "  display(Audio(audio_data, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "todbtEWFy0mj"
      },
      "outputs": [],
      "source": [
        "random_audio = get_random_audio_file(test_dir)\n",
        "show_sample(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-jRIWcQv7xt"
      },
      "source": [
        "## モデルを定義する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQj1Mf7YZELS"
      },
      "source": [
        "Model Maker を使用してモデルを再トレーニングするときには、モデル仕様の定義から始める必要があります。仕様では、新しいモデルが、新しいクラスの学習を開始するために特徴埋め込みを抽出する基本モデルが定義されます。この音声認識器の仕様は、トレーニング済みの [TFJS の BrowserFft モデル](https://github.com/tensorflow/tfjs-models/tree/master/speech-commands#speech-command-recognizer)に基づいています。\n",
        "\n",
        "モデルでは、44.1 kHz、1 秒以下の音声サンプルが入力として想定されています。正確なサンプルの長さは 44034 フレームです。\n",
        "\n",
        "トレーニングデータセットでは、リサンプリングを実行する必要がありません。Model Maker で実行されます。ただし、後から推論を実行するときには、入力が想定された形式と一致していることを確認してください。\n",
        "\n",
        "ここで必要な手順は、[`BrowserFftSpec`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec) を初期化するだけです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUcxtfHXY7XS"
      },
      "outputs": [],
      "source": [
        "spec = audio_classifier.BrowserFftSpec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maTOoRvAwI9l"
      },
      "source": [
        "## データセットの読み込み "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UASCEHoVwQ1q"
      },
      "source": [
        "モデル仕様に従って、データセットを読み込む必要があります。Model Maker には、[`DataLoader`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/DataLoader) API があります。これにより、フォルダからデータセットが読み込まれ、モデル仕様の想定された形式であることが保証されます。\n",
        "\n",
        "一部のファイルはすでに個別のディレクトリに移動され、予約されています。これで、後から簡単に推論を実行できます。ここでは、各分割 (トレーニングセット、検証セット、テストセット) の `DataLoader` を作成します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAhAfHwiw2_F"
      },
      "source": [
        "#### 音声コマンドデータセットの読み込み\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX0RqETqZgzo"
      },
      "outputs": [],
      "source": [
        "if not use_custom_dataset:\n",
        "  train_data_ratio = 0.8\n",
        "  train_data = audio_classifier.DataLoader.from_folder(\n",
        "      spec, dataset_dir, cache=True)\n",
        "  train_data, validation_data = train_data.split(train_data_ratio)\n",
        "  test_data = audio_classifier.DataLoader.from_folder(\n",
        "      spec, test_dir, cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OWQ_O9_t-C-"
      },
      "source": [
        "#### カスタムデータセットの読み込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPiwaJwMt7yo"
      },
      "source": [
        "**注意:** 特に、データセットをリサンプリングする必要があるときには、トレーニングを高速にするために、`cache=True` に設定することが重要です。ただし、データを格納するために必要な RAM の量が増えます。非常に大きいカスタムデータセットを使用する場合は、キャッシュが RAM 容量を超える可能性があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "e86Ej-ZmuCzy"
      },
      "outputs": [],
      "source": [
        "if use_custom_dataset:\n",
        "  train_data_ratio = 0.8\n",
        "  train_data = audio_classifier.DataLoader.from_folder(\n",
        "      spec, dataset_dir, cache=True)\n",
        "  train_data, validation_data = train_data.split(train_data_ratio)\n",
        "  test_data = audio_classifier.DataLoader.from_folder(\n",
        "      spec, test_dir, cache=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh1P_zfzwbfE"
      },
      "source": [
        "## モデルのトレーニング\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMghju-Rts2"
      },
      "source": [
        "Model Maker [`create()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/create) 関数を使用して、モデル仕様とトレーニングデータセットに基づいてモデルを作成し、トレーニングを開始します。\n",
        "\n",
        "カスタムデータセットを使用している場合は、必要に応じて、トレーニングセットのサンプル数に合わせてバッチサイズを変更できます。\n",
        "\n",
        "**注意:** 最初のエポックではキャッシュを作成する必要があるため、最初のエポックは時間がかかります。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYaZvaOPgLUC"
      },
      "outputs": [],
      "source": [
        "# If your dataset has fewer than 100 samples per class,\n",
        "# you might want to try a smaller batch size\n",
        "batch_size = 25\n",
        "epochs = 25\n",
        "model = audio_classifier.create(train_data, spec, validation_data, batch_size, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtLuRA2xweZA"
      },
      "source": [
        "## モデルパフォーマンスの確認"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXMEHZkAxJTl"
      },
      "source": [
        "上記のトレーニング出力で精度/損失に問題がないように思われても、モデルがまだ確認していないテストデータを使用して、モデルを実行することが重要です。これは `evaluate()` メソッドで実行できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_4MGpzhWVhr"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqvpAnqsVExO"
      },
      "source": [
        "### 混同行列の表示"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QRRAM39aOxS"
      },
      "source": [
        "このような分類モデルをトレーニングするときには、[混同行列](https://en.wikipedia.org/wiki/Confusion_matrix)の調査が役に立ちます。混同行列では、テストデータの各分類に対する分類器のパフォーマンスを視覚的に詳細に表示できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqB3c0368iH3"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion, test_labels):\n",
        "  \"\"\"Compute confusion matrix and normalize.\"\"\"\n",
        "  confusion_normalized = confusion.astype(\"float\") / confusion.sum(axis=1)\n",
        "  sns.set(rc = {'figure.figsize':(6,6)})\n",
        "  sns.heatmap(\n",
        "      confusion_normalized, xticklabels=test_labels, yticklabels=test_labels,\n",
        "      cmap='Blues', annot=True, fmt='.2f', square=True, cbar=False)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "confusion_matrix = model.confusion_matrix(test_data)\n",
        "show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yASrikBgZ9ZO"
      },
      "source": [
        "## モデルをエクスポートする\n",
        "\n",
        "最後のステップでは、モバイル/組み込みデバイスで実行できるようにモデルを TensorFlow Lite 形式にエクスポートし、別の場所で実行できるように [SavedModel 形式](https://www.tensorflow.org/guide/saved_model)にエクスポートします。\n",
        "\n",
        "Model Maker から `.tflite`ファイルをエクスポートすると、推論中に、後から役立つさまざまな詳細情報を説明する[モデルメタデータ](https://www.tensorflow.org/lite/inference_with_metadata/overview)が含まれます。分類ラベルファイルのコピーも含まれるため、別の `labels.txt` ファイルは必要ありません (次のセクションでは、このメタデータを使用して、推論を実行する方法を示します)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gEf59NfGWjq"
      },
      "outputs": [],
      "source": [
        "TFLITE_FILENAME = 'browserfft-speech.tflite'\n",
        "SAVE_PATH = './models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw_ehPxAdQlz"
      },
      "outputs": [],
      "source": [
        "print(f'Exporing the model to {SAVE_PATH}')\n",
        "model.export(SAVE_PATH, tflite_filename=TFLITE_FILENAME)\n",
        "model.export(SAVE_PATH, export_format=[mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIC1ddGq6xQX"
      },
      "source": [
        "## TF Lite モデルを使用した推論の実行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xr0idac6xfi"
      },
      "source": [
        "これで、TFLite モデルをデプロイし、サポートされている[推論ライブラリ](https://www.tensorflow.org/lite/guide/inference)または新しい [TFLite AudioClassifier Task API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier) を使用して実行できます。次のコードは、Python で `.tflite` モデルを使用して、推論を実行する方法を示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR5zV53YbCIQ"
      },
      "outputs": [],
      "source": [
        "# This library provides the TFLite metadata API\n",
        "! pip install -q tflite_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AC7PRyiayU5"
      },
      "outputs": [],
      "source": [
        "from tflite_support import metadata\n",
        "import json\n",
        "\n",
        "def get_labels(model):\n",
        "  \"\"\"Returns a list of labels, extracted from the model metadata.\"\"\"\n",
        "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
        "  labels_file = displayer.get_packed_associated_file_list()[0]\n",
        "  labels = displayer.get_associated_file_buffer(labels_file).decode()\n",
        "  return [line for line in labels.split('\\n')]\n",
        "\n",
        "def get_input_sample_rate(model):\n",
        "  \"\"\"Returns the model's expected sample rate, from the model metadata.\"\"\"\n",
        "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
        "  metadata_json = json.loads(displayer.get_metadata_json())\n",
        "  input_tensor_metadata = metadata_json['subgraph_metadata'][0][\n",
        "          'input_tensor_metadata'][0]\n",
        "  input_content_props = input_tensor_metadata['content']['content_properties']\n",
        "  return input_content_props['sample_rate']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC7TEvQ9o4mu"
      },
      "source": [
        "実際のサンプルでのモデルのパフォーマンスを観察するには、次のコードブロックを繰り返し実行します。毎回、新しいテストサンプルが取得され、そのサンプルで推論が実行されます。次の音声サンプルを聴くことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loU6PleipSPf"
      },
      "outputs": [],
      "source": [
        "# Get a WAV file for inference and list of labels from the model\n",
        "tflite_file = os.path.join(SAVE_PATH, TFLITE_FILENAME)\n",
        "labels = get_labels(tflite_file)\n",
        "random_audio = get_random_audio_file(test_dir)\n",
        "\n",
        "# Ensure the audio sample fits the model input\n",
        "interpreter = tf.lite.Interpreter(tflite_file)\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_size = input_details[0]['shape'][1]\n",
        "sample_rate = get_input_sample_rate(tflite_file)\n",
        "audio_data, _ = librosa.load(random_audio, sr=sample_rate)\n",
        "if len(audio_data) < input_size:\n",
        "  audio_data.resize(input_size)\n",
        "audio_data = np.expand_dims(audio_data[:input_size], axis=0)\n",
        "\n",
        "# Run inference\n",
        "interpreter.allocate_tensors()\n",
        "interpreter.set_tensor(input_details[0]['index'], audio_data)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# Display prediction and ground truth\n",
        "top_index = np.argmax(output_data[0])\n",
        "label = labels[top_index]\n",
        "score = output_data[0][top_index]\n",
        "print('---prediction---')\n",
        "print(f'Class: {label}\\nScore: {score}')\n",
        "print('----truth----')\n",
        "show_sample(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtmfoJW6G2fd"
      },
      "source": [
        "## TF Lite モデルのダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zLDeiQ_z2Vj"
      },
      "source": [
        "これで、TF Lite モデルをモバイルデバイスまたは組み込みデバイスにデプロイできます。ラベルファイルをダウンロードする必要はありません。上記の推論の例のように、ラベルは `.tflite` ファイルメタデータから取得できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNuQoqtjG4zu"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download(tflite_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iERuGZz4z6rB"
      },
      "source": [
        "[Android](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android/) および [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios) では、TFLite 音声モデルを使用して推論を実行する、エンドツーエンドのサンプルアプリを確認できます。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "speech_recognition.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
