{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2J3nB-ZrRv1"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Probability Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9qDhTJmprPnm"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfPtIQ3DdZ8r"
      },
      "source": [
        "# 一般化線形モデル\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/probability/examples/Generalized_Linear_Models\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/probability/examples/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/probability/examples/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/probability/examples/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOfH1_F9YsOG"
      },
      "source": [
        "このノートブックでは、実際の例を使用して一般化線形モデルを見ていきます。TensorFlow Probability で GLM を効率的に適合させるために 2 つのアルゴリズム (フィッシャーのスコア法、疎なデータの場合はスパースな座標近接勾配降下法) を使用して、この例を 2 つの異なる方法で解決します。近似係数を真の係数と比較し、座標近接勾配降下法の場合は、R の同様の `glmnet` アルゴリズムの出力と比較します。最後に、GLM のいくつかの重要なプロパティの数学的詳細と導出について説明します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsfQ6vLb5I0"
      },
      "source": [
        "# 背景情報"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdMX-QKagFnY"
      },
      "source": [
        "一般化線形モデル (GLM) は、変換 (リンク関数) にラップされ、指数型分布族に含まれた応答分布がある線形モデル ($\\eta = x^\\top \\beta$) です。リンク機能と応答分布の選択は非常に柔軟なので、GLM には優れた表現度があります。明確な表記法で一般化線形モデルを作成するためのすべての定義と結果の順次表示を含む完全な詳細は、以下の「一般化線形モデルの詳細の導出」を参照してください。要約は以下のとおりです。\n",
        "\n",
        "GLM では、応答変数 $Y$ の予測分布は、観測された予測変数 $x$ のベクトルに関連付けられます。分布の形式は次のとおりです。\n",
        "\n",
        "$$ \\begin{align*} p(y , |, x) &amp;= m(y, \\phi) \\exp\\left(\\frac{\\theta, T(y) - A(\\theta)}{\\phi}\\right) \\ \\theta &amp;:= h(\\eta) \\ \\eta &amp;:= x^\\top \\beta \\end{align*} $$\n",
        "\n",
        "ここでは、$\\beta$ はパラメータ (「重み」)、$\\phi$ は分散（「バリアンス」）を表すハイパーパラメータであり、$m$、$h$、$T$、$A$ はユーザーにより指定されたモデルの族により特徴付けられます。\n",
        "\n",
        "$Y$ の平均は、**線形応答** $\\eta$ と (逆) リンク関数の合成によって $x$ に依存します。つまり、次のようになります。\n",
        "\n",
        "$$ \\mu := g^{-1}(\\eta) $$\n",
        "\n",
        "$g$ はいわゆる**リンク関数**です。TFP では、リンク関数とモデルの族は、`tfp.glm.ExponentialFamily` サブクラスにより共に指定されます。例は次のとおりです。\n",
        "\n",
        "- `tfp.glm.Normal`、「線形回帰」\n",
        "- `tfp.glm.Bernoulli`、「ロジスティック回帰」\n",
        "- `tfp.glm.Poisson`、「ポアソン回帰」\n",
        "- `tfp.glm.BernoulliNormalCDF`、プロビット回帰」。\n",
        "\n",
        "`tfp.Distribution` はすでに第一級市民であるため、TFP では、リンク関数ではなく `Y` の分布に従ってモデルの族に名前を付けます。`tfp.glm.ExponentialFamily`サブクラス名に 2 番目の単語が含まれている場合、これは[非正規リンク関数](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function)を示します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oGScpRnqH_b"
      },
      "source": [
        "GLM には、最尤推定量の効率的な実装を可能にするいくつかの注目すべき特性があります。これらのプロパティの中で最も重要なのは、対数尤度 $\\ell$ の勾配の簡単な式です。フィッシャー情報行列の場合、これは、同じ予測子の下での応答のリサンプリングの下での負の対数尤度のヘッセ行列の期待値です。\n",
        "\n",
        "$$ \\begin{align*} \\nabla_\\beta, \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{y}) &amp;= \\mathbf{x}^\\top ,\\text{diag}\\left(\\frac{ {\\textbf{Mean}_T}'(\\mathbf{x} \\beta) }{ {\\textbf{Var}*T}(\\mathbf{x} \\beta) }\\right) \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}*T}(\\mathbf{x} \\beta)\\right) \\ \\mathbb{E}*{Y_i \\sim \\text{GLM} | x_i} \\left[ \\nabla*\\beta^2, \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{Y}) \\right] &amp;= -\\mathbf{x}^\\top ,\\text{diag}\\left( \\frac{ \\phi, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)^2 }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta) }\\right), \\mathbf{x} \\end{align*} $$\n",
        "\n",
        "ここで、$\\mathbf{x}$ は、$i$ 番目の行が $i$ 番目のデータサンプルの予測ベクトルである行列で、$\\mathbf{y}$ は、$i$ 番目の座標が $i$ 番目のデータサンプルで観測された応答であるベクトルです。ここで (大まかに言えば)、${\\text{Mean}_T}(\\eta) := \\mathbb{E}[T(Y),|,\\eta]$ および ${\\text{Var}_T}(\\eta) := \\text{Var}[T(Y),|,\\eta]$ であり、太字はこれらの関数のベクトル化を示します。これらの期待と分散がどのような分布を超えているかについての完全な詳細は、以下の「一般化線形モデルの詳細の導出」を参照してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNDwfwBObKl"
      },
      "source": [
        "# 例\n",
        "\n",
        "このセクションでは、TensorFlow Probability に組み込まれている 2 つの GLM フィッティングアルゴリズム、フィッシャースコアリング (`tfp.glm.fit`) および座標近接勾配降下 (`tfp.glm.fit_sparse`) について簡単に説明します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4phryMfsP4Sn"
      },
      "source": [
        "## 合成データセット\n",
        "\n",
        "トレーニングデータセットを読み込むふりをしてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA2Rf9PPgMAD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEVnTz2hh9RN"
      },
      "outputs": [],
      "source": [
        "def make_dataset(n, d, link, scale=1., dtype=np.float32):\n",
        "  model_coefficients = tfd.Uniform(\n",
        "      low=-1., high=np.array(1, dtype)).sample(d, seed=42)\n",
        "  radius = np.sqrt(2.)\n",
        "  model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n",
        "  mask = tf.random.shuffle(tf.range(d)) < int(0.5 * d)\n",
        "  model_coefficients = tf.where(\n",
        "      mask, model_coefficients, np.array(0., dtype))\n",
        "  model_matrix = tfd.Normal(\n",
        "      loc=0., scale=np.array(1, dtype)).sample([n, d], seed=43)\n",
        "  scale = tf.convert_to_tensor(scale, dtype)\n",
        "  linear_response = tf.linalg.matvec(model_matrix, model_coefficients)\n",
        "  \n",
        "  if link == 'linear':\n",
        "    response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n",
        "  elif link == 'probit':\n",
        "    response = tf.cast(\n",
        "        tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n",
        "                   dtype)\n",
        "  elif link == 'logit':\n",
        "    response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n",
        "  else:\n",
        "    raise ValueError('unrecognized true link: {}'.format(link))\n",
        "  return model_matrix, response, model_coefficients, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Fk5XZKbvi4"
      },
      "source": [
        "### 注意: ローカルランタイムに接続します。\n",
        "\n",
        "このノートブックでは、ローカルファイルを使用して Python カーネルと R カーネルの間でデータを共有します。この共有を有効にするには、ローカルファイルの読み取りと書き込みの権限がある同じマシンのランタイムを使用してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EAQjTrZJqKx"
      },
      "outputs": [],
      "source": [
        "x, y, model_coefficients_true, _ = [t.numpy() for t in make_dataset(\n",
        "    n=int(1e5), d=100, link='probit')]\n",
        "\n",
        "DATA_DIR = '/tmp/glm_example'\n",
        "tf.io.gfile.makedirs(DATA_DIR)\n",
        "with tf.io.gfile.GFile('{}/x.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, x, delimiter=',')\n",
        "with tf.io.gfile.GFile('{}/y.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, y.astype(np.int32) + 1, delimiter=',', fmt='%d')\n",
        "with tf.io.gfile.GFile(\n",
        "    '{}/model_coefficients_true.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, model_coefficients_true, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P5I-aJdN6GZ"
      },
      "source": [
        "## L1 正則化なし"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN6HfiH3bAb0"
      },
      "source": [
        "関数 `tfp.glm.fit` は、フィッシャースコアリングを実装します。これは、引数として以下を取ります。\n",
        "\n",
        "- `model_matrix` = $\\mathbf{x}$\n",
        "- `response` = $\\mathbf{y}$\n",
        "- `model` = callable which, given argument $\\boldsymbol{\\eta}$, returns the triple $\\left( {\\textbf{Mean}_T}(\\boldsymbol{\\eta}), {\\textbf{Var}_T}(\\boldsymbol{\\eta}), {\\textbf{Mean}_T}'(\\boldsymbol{\\eta}) \\right)$.\n",
        "\n",
        "`model` を `tfp.glm.ExponentialFamily` クラスのインスタンスにすることをお勧めします。いくつかの事前に作成された実装が利用可能なので、ほとんどの一般的な GLM では、カスタムコードは必要ありません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXkxVBSmesjn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_converged: True\n",
            "    num_iter: 6\n",
            "    accuracy: 0.75241\n",
            "    deviance: -0.992436110973\n",
            "||w0-w1||_2 / (1+||w0||_2): 0.0231555201462\n"
          ]
        }
      ],
      "source": [
        "@tf.function(autograph=False)\n",
        "def fit_model():\n",
        "  model_coefficients, linear_response, is_converged, num_iter = tfp.glm.fit(\n",
        "      model_matrix=x, response=y, model=tfp.glm.BernoulliNormalCDF())\n",
        "  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(y, linear_response)\n",
        "  return (model_coefficients, linear_response, is_converged, num_iter,\n",
        "          log_likelihood)\n",
        " \n",
        "[model_coefficients, linear_response, is_converged, num_iter,\n",
        " log_likelihood] = [t.numpy() for t in fit_model()]\n",
        "\n",
        "print(('is_converged: {}\\n'\n",
        "       '    num_iter: {}\\n'\n",
        "       '    accuracy: {}\\n'\n",
        "       '    deviance: {}\\n'\n",
        "       '||w0-w1||_2 / (1+||w0||_2): {}'\n",
        "      ).format(\n",
        "    is_converged,\n",
        "    num_iter,\n",
        "    np.mean((linear_response > 0.) == y),\n",
        "    2. * np.mean(log_likelihood),\n",
        "    np.linalg.norm(model_coefficients_true - model_coefficients, ord=2) /\n",
        "        (1. + np.linalg.norm(model_coefficients_true, ord=2))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6qexoHAJzEF"
      },
      "source": [
        "### 数学的詳細\n",
        "\n",
        "フィッシャースコアリングは、最尤推定値を見つけるための修正されたニュートン法です。\n",
        "\n",
        "$$ \\hat\\beta := \\underset{\\beta}{\\text{arg max}}\\ \\ \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}). $$\n",
        "\n",
        "対数尤度の勾配 0 を検索する普通のニュートン法は、更新ルールに従います。\n",
        "\n",
        "## $$ \\beta^{(t+1)}_{\\text{Newton}} := \\beta^{(t)}\n",
        "\n",
        "\\alpha \\left( \\nabla^2_\\beta, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}) \\right)*{\\beta = \\beta^{(t)}}^{-1} \\left( \\nabla*\\beta, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}) \\right)_{\\beta = \\beta^{(t)}} $$\n",
        "\n",
        "ここで、$\\alpha \\in (0, 1]$ は、ステップサイズを制御するために使用される学習率です。\n",
        "\n",
        "フィッシャースコアリングでは、ヘッセ行列を負のフィッシャー情報行列に置き換えます。\n",
        "\n",
        "## $$ \\begin{align*} \\beta^{(t+1)} &amp;:= \\beta^{(t)}\n",
        "\n",
        "\\alpha, \\mathbb{E}*{ Y_i \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta^{(t)}), \\phi) } \\left[ \\left( \\nabla^2_\\beta, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{Y}) \\right)*{\\beta = \\beta^{(t)}} \\right]^{-1} \\left( \\nabla*\\beta, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}) \\right)_{\\beta = \\beta^{(t)}} \\[3mm] \\end{align*} $$\n",
        "\n",
        "[ここで $\\mathbf{Y} = (Y_i)_{i=1}^{n}$ はランダムですが、$\\mathbf{y}$ は依然として観測された応答のベクトルであることに注意してください。]\n",
        "\n",
        "以下の「GLM パラメータのデータへの適合」の式により、これは次のように簡略化されます。\n",
        "\n",
        "$$ \\begin{align*} \\beta^{(t+1)} &amp;= \\beta^{(t)} + \\alpha \\left( \\mathbf{x}^\\top \\text{diag}\\left( \\frac{ \\phi, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t)})^2 }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t)}) }\\right), \\mathbf{x} \\right)^{-1} \\left( \\mathbf{x}^\\top \\text{diag}\\left(\\frac{ {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t)}) }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t)}) }\\right) \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta^{(t)})\\right) \\right). \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076quM7tN8_1"
      },
      "source": [
        "## L1 正則化あり"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnP3jeZOk7Y5"
      },
      "source": [
        "`tfp.glm.fit_sparse` は、[Yuan, Ho and Lin 2012](#1) のアルゴリズムに基づいて、スパースデータセットにより適した GLM フィッターを実装します。機能は次のとおりです。\n",
        "\n",
        "- L1 正則化\n",
        "- 行列反転なし\n",
        "- 勾配とヘッセ行列の評価はほとんどない。\n",
        "\n",
        "まず、コードの使用例を示します。アルゴリズムの詳細については、以下の「`tfp.glm.fit_sparse`のアルゴリズムの詳細」で詳しく説明しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Oky1X4ijfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_converged: True\n",
            "    num_iter: 1\n",
            "\n",
            "Coefficients:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learned</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.216240</td>\n",
              "      <td>0.220758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.043702</td>\n",
              "      <td>0.063950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.145379</td>\n",
              "      <td>-0.153256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.024382</td>\n",
              "      <td>0.046572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.242985</td>\n",
              "      <td>-0.242609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.106168</td>\n",
              "      <td>-0.123367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.039745</td>\n",
              "      <td>-0.067560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.217717</td>\n",
              "      <td>-0.222169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.016553</td>\n",
              "      <td>-0.041692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.018959</td>\n",
              "      <td>0.049624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.057686</td>\n",
              "      <td>-0.078299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.234406</td>\n",
              "      <td>-0.240482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.232209</td>\n",
              "      <td>0.225448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.144485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>-0.178534</td>\n",
              "      <td>-0.186722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.218493</td>\n",
              "      <td>0.229656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.195579</td>\n",
              "      <td>0.200442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.031153</td>\n",
              "      <td>0.050457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.229065</td>\n",
              "      <td>0.231451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.006512</td>\n",
              "      <td>-0.039516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-0.107947</td>\n",
              "      <td>-0.119896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.149419</td>\n",
              "      <td>0.171693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.047955</td>\n",
              "      <td>0.063434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.083171</td>\n",
              "      <td>-0.107145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.084615</td>\n",
              "      <td>0.101221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-0.168431</td>\n",
              "      <td>-0.175473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.138411</td>\n",
              "      <td>0.152623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.061161</td>\n",
              "      <td>0.081945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-0.083348</td>\n",
              "      <td>-0.104929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.141154</td>\n",
              "      <td>-0.153871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Learned      True\n",
              "0   0.216240  0.220758\n",
              "1   0.000000  0.000000\n",
              "2   0.000000  0.000000\n",
              "3   0.000000  0.000000\n",
              "4   0.000000  0.000000\n",
              "5   0.043702  0.063950\n",
              "6  -0.145379 -0.153256\n",
              "7   0.000000  0.000000\n",
              "8   0.000000  0.000000\n",
              "9   0.000000  0.000000\n",
              "10  0.000000  0.000000\n",
              "11  0.000000  0.000000\n",
              "12  0.000000  0.000000\n",
              "13  0.024382  0.046572\n",
              "14 -0.242985 -0.242609\n",
              "15 -0.106168 -0.123367\n",
              "16  0.000000  0.000000\n",
              "17 -0.039745 -0.067560\n",
              "18 -0.217717 -0.222169\n",
              "19  0.000000  0.000000\n",
              "20  0.000000  0.000000\n",
              "21 -0.016553 -0.041692\n",
              "22  0.018959  0.049624\n",
              "23 -0.057686 -0.078299\n",
              "24  0.003642  0.035682\n",
              "25  0.000000  0.000000\n",
              "26  0.000000  0.000000\n",
              "27 -0.234406 -0.240482\n",
              "28  0.000000  0.000000\n",
              "29  0.232209  0.225448\n",
              "..       ...       ...\n",
              "70  0.000000  0.000000\n",
              "71  0.130166  0.144485\n",
              "72  0.000000  0.000000\n",
              "73  0.000000  0.000000\n",
              "74  0.000000  0.000000\n",
              "75 -0.178534 -0.186722\n",
              "76  0.000000  0.000000\n",
              "77  0.218493  0.229656\n",
              "78  0.000000  0.000000\n",
              "79  0.000000  0.000000\n",
              "80  0.195579  0.200442\n",
              "81  0.000000  0.000000\n",
              "82  0.000000  0.000000\n",
              "83  0.031153  0.050457\n",
              "84  0.229065  0.231451\n",
              "85 -0.006512 -0.039516\n",
              "86 -0.107947 -0.119896\n",
              "87  0.000000  0.000000\n",
              "88  0.149419  0.171693\n",
              "89  0.000000  0.000000\n",
              "90  0.047955  0.063434\n",
              "91  0.000000  0.003592\n",
              "92 -0.083171 -0.107145\n",
              "93  0.084615  0.101221\n",
              "94 -0.168431 -0.175473\n",
              "95  0.138411  0.152623\n",
              "96  0.000000  0.000000\n",
              "97  0.061161  0.081945\n",
              "98 -0.083348 -0.104929\n",
              "99 -0.141154 -0.153871\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = tfp.glm.Bernoulli()\n",
        "model_coefficients_start = tf.zeros(x.shape[-1], np.float32)\n",
        "@tf.function(autograph=False)\n",
        "def fit_model():\n",
        "  return tfp.glm.fit_sparse(\n",
        "    model_matrix=tf.convert_to_tensor(x),\n",
        "    response=tf.convert_to_tensor(y),\n",
        "    model=model,\n",
        "    model_coefficients_start=model_coefficients_start,\n",
        "    l1_regularizer=800.,\n",
        "    l2_regularizer=None,\n",
        "    maximum_iterations=10,\n",
        "    maximum_full_sweeps_per_iteration=10,\n",
        "    tolerance=1e-6,\n",
        "    learning_rate=None)\n",
        "\n",
        "model_coefficients, is_converged, num_iter = [t.numpy() for t in fit_model()]\n",
        "coefs_comparison = pd.DataFrame({\n",
        "  'Learned': model_coefficients,\n",
        "  'True': model_coefficients_true,\n",
        "})\n",
        "  \n",
        "print(('is_converged: {}\\n'\n",
        "       '    num_iter: {}\\n\\n'\n",
        "       'Coefficients:').format(\n",
        "    is_converged,\n",
        "    num_iter))\n",
        "coefs_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrJC2J1YbR5L"
      },
      "source": [
        "学習された係数は、真の係数と同じスパースパターンを持っていることに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ7SzrPZMpke"
      },
      "outputs": [],
      "source": [
        "# Save the learned coefficients to a file.\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_prox.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, model_coefficients, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW9NgB1Zisqh"
      },
      "source": [
        "### R の `glmnet` と比較する\n",
        "\n",
        "座標近接勾配降下法の出力を、同様のアルゴリズムを使用する R の `glmnet` の出力と比較します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aptz7SWwkd5v"
      },
      "source": [
        "#### 注：このセクションを実行するには、R colab ランタイムに切り替える必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS1H3n53h9qc"
      },
      "outputs": [],
      "source": [
        "suppressMessages({\n",
        "  library('glmnet')\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X6zKSaxie7I"
      },
      "outputs": [],
      "source": [
        "data_dir <- '/tmp/glm_example'\n",
        "x <- as.matrix(read.csv(paste(data_dir, '/x.csv', sep=''),\n",
        "                        header=FALSE))\n",
        "y <- as.matrix(read.csv(paste(data_dir, '/y.csv', sep=''),\n",
        "                        header=FALSE, colClasses='integer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb31LbhRjsSz"
      },
      "outputs": [],
      "source": [
        "fit <- glmnet(\n",
        "x = x,\n",
        "y = y,\n",
        "family = \"binomial\",  # Logistic regression\n",
        "alpha = 1,  # corresponds to l1_weight = 1, l2_weight = 0\n",
        "standardize = FALSE,\n",
        "intercept = FALSE,\n",
        "thresh = 1e-30,\n",
        "type.logistic = \"Newton\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTN4RKQbhlCm"
      },
      "outputs": [],
      "source": [
        "write.csv(as.matrix(coef(fit, 0.008)),\n",
        "          paste(data_dir, '/model_coefficients_glmnet.csv', sep=''),\n",
        "          row.names=FALSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsrEKgUGjGjf"
      },
      "source": [
        "#### R、TFP、および真の係数を比較します（注意: Python カーネルに戻る）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCOlGo_4i2sb"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/tmp/glm_example'\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_glmnet.csv'.format(DATA_DIR),\n",
        "                       'r') as f:\n",
        "  model_coefficients_glmnet = np.loadtxt(f,\n",
        "                                   skiprows=2  # Skip column name and intercept\n",
        "                               )\n",
        "\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_prox.csv'.format(DATA_DIR),\n",
        "                       'r') as f:\n",
        "  model_coefficients_prox = np.loadtxt(f)\n",
        "\n",
        "with tf.io.gfile.GFile(\n",
        "    '{}/model_coefficients_true.csv'.format(DATA_DIR), 'r') as f:\n",
        "  model_coefficients_true = np.loadtxt(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l-SZ85lnKg5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R</th>\n",
              "      <th>TFP</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.281080</td>\n",
              "      <td>0.216240</td>\n",
              "      <td>0.220758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.056625</td>\n",
              "      <td>0.043702</td>\n",
              "      <td>0.063950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.188771</td>\n",
              "      <td>-0.145379</td>\n",
              "      <td>-0.153256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.030112</td>\n",
              "      <td>0.024382</td>\n",
              "      <td>0.046572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.316488</td>\n",
              "      <td>-0.242985</td>\n",
              "      <td>-0.242609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.139214</td>\n",
              "      <td>-0.106168</td>\n",
              "      <td>-0.123367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.050239</td>\n",
              "      <td>-0.039745</td>\n",
              "      <td>-0.067560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.283372</td>\n",
              "      <td>-0.217717</td>\n",
              "      <td>-0.222169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.021815</td>\n",
              "      <td>-0.016553</td>\n",
              "      <td>-0.041692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.024070</td>\n",
              "      <td>0.018959</td>\n",
              "      <td>0.049624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.074039</td>\n",
              "      <td>-0.057686</td>\n",
              "      <td>-0.078299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.005321</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.304958</td>\n",
              "      <td>-0.234406</td>\n",
              "      <td>-0.240482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301562</td>\n",
              "      <td>0.232209</td>\n",
              "      <td>0.225448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.169291</td>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.144485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>-0.231294</td>\n",
              "      <td>-0.178534</td>\n",
              "      <td>-0.186722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.284215</td>\n",
              "      <td>0.218493</td>\n",
              "      <td>0.229656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.254524</td>\n",
              "      <td>0.195579</td>\n",
              "      <td>0.200442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.040716</td>\n",
              "      <td>0.031153</td>\n",
              "      <td>0.050457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.297475</td>\n",
              "      <td>0.229065</td>\n",
              "      <td>0.231451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.008569</td>\n",
              "      <td>-0.006512</td>\n",
              "      <td>-0.039516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-0.141028</td>\n",
              "      <td>-0.107947</td>\n",
              "      <td>-0.119896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.194130</td>\n",
              "      <td>0.149419</td>\n",
              "      <td>0.171693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.062601</td>\n",
              "      <td>0.047955</td>\n",
              "      <td>0.063434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.107693</td>\n",
              "      <td>-0.083171</td>\n",
              "      <td>-0.107145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.109381</td>\n",
              "      <td>0.084615</td>\n",
              "      <td>0.101221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-0.218831</td>\n",
              "      <td>-0.168431</td>\n",
              "      <td>-0.175473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.180662</td>\n",
              "      <td>0.138411</td>\n",
              "      <td>0.152623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.078815</td>\n",
              "      <td>0.061161</td>\n",
              "      <td>0.081945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-0.108332</td>\n",
              "      <td>-0.083348</td>\n",
              "      <td>-0.104929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.183284</td>\n",
              "      <td>-0.141154</td>\n",
              "      <td>-0.153871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           R       TFP      True\n",
              "0   0.281080  0.216240  0.220758\n",
              "1   0.000000  0.000000  0.000000\n",
              "2   0.000000  0.000000  0.000000\n",
              "3   0.000000  0.000000  0.000000\n",
              "4   0.000000  0.000000  0.000000\n",
              "5   0.056625  0.043702  0.063950\n",
              "6  -0.188771 -0.145379 -0.153256\n",
              "7   0.000000  0.000000  0.000000\n",
              "8   0.000000  0.000000  0.000000\n",
              "9   0.000000  0.000000  0.000000\n",
              "10  0.000000  0.000000  0.000000\n",
              "11  0.000000  0.000000  0.000000\n",
              "12  0.000000  0.000000  0.000000\n",
              "13  0.030112  0.024382  0.046572\n",
              "14 -0.316488 -0.242985 -0.242609\n",
              "15 -0.139214 -0.106168 -0.123367\n",
              "16  0.000000  0.000000  0.000000\n",
              "17 -0.050239 -0.039745 -0.067560\n",
              "18 -0.283372 -0.217717 -0.222169\n",
              "19  0.000000  0.000000  0.000000\n",
              "20  0.000000  0.000000  0.000000\n",
              "21 -0.021815 -0.016553 -0.041692\n",
              "22  0.024070  0.018959  0.049624\n",
              "23 -0.074039 -0.057686 -0.078299\n",
              "24  0.005321  0.003642  0.035682\n",
              "25  0.000000  0.000000  0.000000\n",
              "26  0.000000  0.000000  0.000000\n",
              "27 -0.304958 -0.234406 -0.240482\n",
              "28  0.000000  0.000000  0.000000\n",
              "29  0.301562  0.232209  0.225448\n",
              "..       ...       ...       ...\n",
              "70  0.000000  0.000000  0.000000\n",
              "71  0.169291  0.130166  0.144485\n",
              "72  0.000000  0.000000  0.000000\n",
              "73  0.000000  0.000000  0.000000\n",
              "74  0.000000  0.000000  0.000000\n",
              "75 -0.231294 -0.178534 -0.186722\n",
              "76  0.000000  0.000000  0.000000\n",
              "77  0.284215  0.218493  0.229656\n",
              "78  0.000000  0.000000  0.000000\n",
              "79  0.000000  0.000000  0.000000\n",
              "80  0.254524  0.195579  0.200442\n",
              "81  0.000000  0.000000  0.000000\n",
              "82  0.000000  0.000000  0.000000\n",
              "83  0.040716  0.031153  0.050457\n",
              "84  0.297475  0.229065  0.231451\n",
              "85 -0.008569 -0.006512 -0.039516\n",
              "86 -0.141028 -0.107947 -0.119896\n",
              "87  0.000000  0.000000  0.000000\n",
              "88  0.194130  0.149419  0.171693\n",
              "89  0.000000  0.000000  0.000000\n",
              "90  0.062601  0.047955  0.063434\n",
              "91  0.000000  0.000000  0.003592\n",
              "92 -0.107693 -0.083171 -0.107145\n",
              "93  0.109381  0.084615  0.101221\n",
              "94 -0.218831 -0.168431 -0.175473\n",
              "95  0.180662  0.138411  0.152623\n",
              "96  0.000000  0.000000  0.000000\n",
              "97  0.078815  0.061161  0.081945\n",
              "98 -0.108332 -0.083348 -0.104929\n",
              "99 -0.183284 -0.141154 -0.153871\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coefs_comparison = pd.DataFrame({\n",
        "    'TFP': model_coefficients_prox,\n",
        "    'R': model_coefficients_glmnet,\n",
        "    'True': model_coefficients_true,\n",
        "})\n",
        "coefs_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfv0GVXqY74Y"
      },
      "source": [
        "# `tfp.glm.fit_sparse` のアルゴリズムの詳細\n",
        "\n",
        "ニュートン法に対する 3 つの変更のシーケンスとしてアルゴリズムを提示します。それぞれにおいて、$\\beta$ の更新ルールは、対数尤度の勾配とヘッセ行列を近似するベクトル $s$ と行列 $H$ に基づいています。ステップ $t$ で、変更する座標 $j^{(t)}$ を選択し、更新ルールに従って $\\beta$ を更新します。\n",
        "\n",
        "## $$ \\begin{align*} u^{(t)} &amp;:= \\frac{ \\left( s^{(t)} \\right)*{j^{(t)}} }{ \\left( H^{(t)} \\right)*{j^{(t)},, j^{(t)}} } \\[3mm] \\beta^{(t+1)} &amp;:= \\beta^{(t)}\n",
        "\n",
        "\\alpha, u^{(t)} ,\\text{onehot}(j^{(t)}) \\end{align*} $$\n",
        "\n",
        "この更新は、学習率が $\\alpha$ のニュートンのようなステップです。最後の部分 (L1 正則化) を除いて、以下の変更は、$s$ と $s$ の更新方法のみが異なります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH7C1xBWUV7_"
      },
      "source": [
        "## 座標的ニュートン法で始める\n",
        "\n",
        "座標的ニュートン法では、$s$ と $H$ 対数尤度の真の勾配とヘッセ行列に設定します。\n",
        "\n",
        "$$ \\begin{align*} s^{(t)}*{\\text{vanilla}} &amp;:= \\left( \\nabla*\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)*{\\beta = \\beta^{(t)}} \\ H^{(t)}*{\\text{vanilla}} &amp;:= \\left( \\nabla^2_\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)_{\\beta = \\beta^{(t)}} \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rJZD6iyUl0v"
      },
      "source": [
        "## 勾配とヘッセ行列の評価はほとんどない\n",
        "\n",
        "対数尤度の勾配とヘッセ行列の計算は高コストになることが多いため、次のように概算することがすすめられます。\n",
        "\n",
        "- 通常、ヘッセ行列を局所定数として近似し、(近似) ヘッセ行列を使用して勾配を 1 次に近似します。\n",
        "\n",
        "$$ \\begin{align*} H_{\\text{approx}}^{(t+1)} &amp;:= H^{(t)} \\ s_{\\text{approx}}^{(t+1)} &amp;:= s^{(t)} + H^{(t)} \\left( \\beta^{(t+1)} - \\beta^{(t)} \\right) \\end{align*} $$\n",
        "\n",
        "- 場合によっては、上記のように「通常の」更新ステップを実行し、$ s ^ {(t + 1)} $ を正確な勾配に設定し、$ H ^ {(t + 1)} $ を$ \\ beta ^ {(t + 1)} $で評価される対数尤度の正確なヘッセ行列に設定します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvvyaVnUqIQ"
      },
      "source": [
        "## 負のフィッシャー情報をヘシアンに置き換える\n",
        "\n",
        "通常の更新ステップのコストをさらに削減するために、正確なヘッセ行列ではなく、負のフィッシャー情報行列 (以下の「GLM パラメーターのデータへの適合」の式を使用して効率的に計算できます）に $H$ を設定します。\n",
        "\n",
        "$$ \\begin{align*} H_{\\text{Fisher}}^{(t+1)} &amp;:= \\mathbb{E}*{Y_i \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta^{(t+1)}), \\phi)} \\left[ \\left( \\nabla_\\beta^2, \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{Y}) \\right)_{\\beta = \\beta^{(t+1)}} \\right] \\ &amp;= -\\mathbf{x}^\\top ,\\text{diag}\\left( \\frac{ \\phi, {\\textbf{Mean}*T}'(\\mathbf{x} \\beta^{(t+1)})^2 }{ {\\textbf{Var}*T}(\\mathbf{x} \\beta^{(t+1)}) }\\right), \\mathbf{x} \\ s*{\\text{Fisher}}^{(t+1)} &amp;:= s*{\\text{vanilla}}^{(t+1)} \\ &amp;= \\left( \\mathbf{x}^\\top ,\\text{diag}\\left(\\frac{ {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t+1)}) }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t+1)}) }\\right) \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta^{(t+1)})\\right) \\right) \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTH07xYpWGcR"
      },
      "source": [
        "## 近接勾配降下による L1 正則化\n",
        "\n",
        "L1 正則化を組み込むために、更新ルール\n",
        "\n",
        "## $$ \\beta^{(t+1)} := \\beta^{(t)}\n",
        "\n",
        "\\alpha, u^{(t)} ,\\text{onehot}(j^{(t)}) $$\n",
        "\n",
        "をより一般的な更新ルールにを置き換えます\n",
        "\n",
        "$$ \\begin{align*} \\gamma^{(t)} &amp;:= -\\frac{\\alpha, r_{\\text<l>}}{\\left(H^{(t)}\\right)<em data-md-type=\"emphasis\">{j^{(t)},, j^{(t)}}} \\[2mm] \\left(\\beta</em>{\\text{reg}}^{(t+1)}\\right)_j &amp;:= \\begin{cases} \\beta^{(t+1)}_j &amp;\\text{if } j \\neq j^{(t)} \\ \\text{SoftThreshold} \\left( \\beta^{(t)}_j - \\alpha, u^{(t)} ,\\ \\gamma^{(t)} \\right) &amp;\\text{if } j = j^{(t)} \\end{cases} \\end{align*} $$</l>\n",
        "\n",
        "$r_{\\text<l>} &gt; 0$ は指定された定数( L1 正則化係数) であり、$\\text{SoftThreshold}$ は以下により定義されるソフトしきい値演算子です。</l>\n",
        "\n",
        "$$ \\text{SoftThreshold}(\\beta, \\gamma) := \\begin{cases} \\beta + \\gamma &amp;\\text{if } \\beta &lt; -\\gamma \\ 0 &amp;\\text{if } -\\gamma \\leq \\beta \\leq \\gamma \\ \\beta - \\gamma &amp;\\text{if } \\beta &gt; \\gamma. \\end{cases} $$\n",
        "\n",
        "この更新ルールには、以下で説明する 2 つのプロパティがあります。\n",
        "\n",
        "1. 限定的なケース $r_{\\text<l>} \\to 0$ (つまり、L1 正則化なし) では、この更新ルールは元の更新ルールと同じです。</l>\n",
        "\n",
        "2. この更新ルールは、不動点が L1 正則化最小化問題の解である近接演算子を適用するものとして解釈できます。\n",
        "\n",
        "$$ \\underset{\\beta - \\beta^{(t)} \\in \\text{span}{ \\text{onehot}(j^{(t)}) }}{\\text{arg min}} \\left( -\\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y})\n",
        "\n",
        "- r_{\\text<l>} \\left\\lVert \\beta \\right\\rVert_1 \\right). $$</l>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSs7_osNPLVt"
      },
      "source": [
        "### 縮退ケース $r_{\\text<l>} = 0$ は、元の更新ルールを復元します</l>\n",
        "\n",
        "(1) を確認するには、$r_{\\text<l>} = 0$ の場合、$\\gamma^{(t)} = 0$であるため、</l>\n",
        "\n",
        "$$ \\begin{align*} \\left(\\beta_{\\text{reg}}^{(t+1)}\\right)*{j^{(t)}} &amp;= \\text{SoftThreshold} \\left( \\beta^{(t)}*{j^{(t)}} - \\alpha, u^{(t)} ,\\ 0 \\right) \\ &amp;= \\beta^{(t)}_{j^{(t)}} - \\alpha, u^{(t)}. \\end{align*} $$\n",
        "\n",
        "よって以下が成り立ちます。\n",
        "\n",
        "$$ \\begin{align*} \\beta_{\\text{reg}}^{(t+1)} &amp;= \\beta^{(t)} - \\alpha, u^{(t)} ,\\text{onehot}(j^{(t)}) \\ &amp;= \\beta^{(t+1)}. \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiHy_0NIPT5f"
      },
      "source": [
        "### 不動点が正則化された MLE である近接演算子\n",
        "\n",
        "(2) を確認するには、最初に  ([Wikipedia](#3) を参照) $\\gamma &gt; 0$ の場合、更新ルール\n",
        "\n",
        "$$ \\left(\\beta_{\\text{exact-prox}, \\gamma}^{(t+1)}\\right)*{j^{(t)}} := \\text{prox}*{\\gamma \\lVert \\cdot \\rVert_1} \\left( \\beta^{(t)}*{j^{(t)}} + \\frac{\\gamma}{r*{\\text<l>}} \\left( \\left( \\nabla_\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)<em data-md-type=\"emphasis\">{\\beta = \\beta^{(t)}} \\right)</em>{j^{(t)}} \\right) $$</l>\n",
        "\n",
        "が(2) を満たすことに注意してください。ここで、$\\text{prox}$ は近接演算子です ([Yu](#4) を参照してください。この演算子は、$\\mathsf{P}$で表されています)。上記の方程式の右辺は[ここで](#2)計算されます。\n",
        "\n",
        "# $$ \\left(\\beta_{\\text{exact-prox}, \\gamma}^{(t+1)}\\right)_{j^{(t)}}\n",
        "\n",
        "\\text{SoftThreshold} \\left( \\beta^{(t)}*{j^{(t)}} + \\frac{\\gamma}{r*{\\text<l>}} \\left( \\left( \\nabla_\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)<em data-md-type=\"emphasis\">{\\beta = \\beta^{(t)}} \\right)</em>{j^{(t)}} ,\\ \\gamma \\right). $$</l>\n",
        "\n",
        "特に、$\\gamma = \\gamma^{(t)} = -\\frac{\\alpha, r_{\\text<l>}}{\\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}}}$<br>は更新ルールを取得します（負の対数尤度が 凸 である限り、$\\gamma^{(t)} &gt; 0$ であることに注意してください）。</l>\n",
        "\n",
        "# $$ \\left(\\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\\right)_{j^{(t)}}\n",
        "\n",
        "\\text{SoftThreshold} \\left( \\beta^{(t)}*{j^{(t)}} - \\alpha \\frac{ \\left( \\left( \\nabla*\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)*{\\beta = \\beta^{(t)}} \\right)*{j^{(t)}} }{ \\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}} } ,\\ \\gamma^{(t)} \\right). $$\n",
        "\n",
        "次に、正確な勾配 $\\left( \\nabla_\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)_{\\beta = \\beta^{(t)}}$ を近似値 $s^{(t)}$ に置き換え、以下を取得します。\n",
        "\n",
        "\\begin{align*} \\left(\\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\\right)*{j^{(t)}} &amp;\\approx \\text{SoftThreshold} \\left( \\beta^{(t)}*{j^{(t)}} - \\alpha \\frac{ \\left(s^{(t)}\\right)*{j^{(t)}} }{ \\left(H^{(t)}\\right)*{j^{(t)}, j^{(t)}} } ,\\ \\gamma^{(t)} \\right) \\ &amp;= \\text{SoftThreshold} \\left( \\beta^{(t)}_{j^{(t)}} - \\alpha, u^{(t)} ,\\ \\gamma^{(t)} \\right). \\end{align*}\n",
        "\n",
        "よって以下が成り立ちます。\n",
        "\n",
        "$$ \\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)} \\approx \\beta_{\\text{reg}}^{(t+1)}. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7YOOrmI8j0L"
      },
      "source": [
        "# 一般化線形モデルの詳細の導出\n",
        "\n",
        "このセクションでは、前のセクションで使用された GLM に関する詳細を説明し、結果を導き出します。次に、TensorFlow の `gradients` を使用して、対数尤度とフィッシャー情報の勾配について導出された式を数値的に検証します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkHZyhuAIW-p"
      },
      "source": [
        "## スコアとフィッシャー情報"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbyYy0bE8pOK"
      },
      "source": [
        "パラメータベクトル $\\theta$ によってパラメータ化され、確率密度 $\\left{p(\\cdot | \\theta)\\right}_{\\theta \\in \\mathcal{T}}$ を持つ確率分布族をみてみましょう。パラメータベクトル $\\theta_0$ での結果 $y$ の**スコア**は、 $y$  ($\\theta_0$で評価) の対数尤度の勾配として定義されます。\n",
        "\n",
        "$$ \\text{score}(y, \\theta_0) := \\left[\\nabla_\\theta, \\log p(y | \\theta)\\right]_{\\theta=\\theta_0}. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYGaMPIx8uOc"
      },
      "source": [
        "### 主張: スコアの期待値はゼロである\n",
        "\n",
        "穏やかな規則性の条件下では (積分の下で微分を渡すことを許可する)\n",
        "\n",
        "$$ \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\text{score}(Y, \\theta_0)\\right] = 0. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3H-wNmJ800R"
      },
      "source": [
        "#### 証明\n",
        "\n",
        "まず、\n",
        "\n",
        "$$ \\begin{align*} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\text{score}(Y, \\theta_0)\\right] &amp;:=\\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\left(\\nabla_\\theta \\log p(Y|\\theta)\\right)*{\\theta=\\theta_0}\\right] \\ &amp;\\stackrel{\\text{(1)}}{=} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\frac{\\left(\\nabla_\\theta p(Y|\\theta)\\right)*{\\theta=\\theta_0}}{p(Y|\\theta=\\theta_0)}\\right] \\ &amp;\\stackrel{\\text{(2)}}{=} \\int*{\\mathcal{Y}} \\left[\\frac{\\left(\\nabla_\\theta p(y|\\theta)\\right)*{\\theta=\\theta_0}}{p(y|\\theta=\\theta_0)}\\right] p(y | \\theta=\\theta_0), dy \\ &amp;= \\int*{\\mathcal{Y}} \\left(\\nabla_\\theta p(y|\\theta)\\right)*{\\theta=\\theta_0}, dy \\ &amp;\\stackrel{\\text{(3)}}{=} \\left[\\nabla*\\theta \\left(\\int_{\\mathcal{Y}} p(y|\\theta), dy\\right) \\right]*{\\theta=\\theta_0} \\ &amp;\\stackrel{\\text{(4)}}{=} \\left[\\nabla*\\theta, 1 \\right]_{\\theta=\\theta_0} \\ &amp;= 0, \\end{align*} $$\n",
        "\n",
        "ここでは次を使用しました。 (1) 微分の連鎖律、(2) 期待値の定義、(3) 積分記号の下で微分を渡す（規則性条件を使用）、(4) 確率密度の積分は 1。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1DPVOI9OT2"
      },
      "source": [
        "### 主張 (フィッシャー情報): スコアの分散は、対数尤度の負の予想ヘッセ行列に等しい\n",
        "\n",
        "穏やかな規則性の条件下では (積分の下で微分を渡すことを許可する)\n",
        "\n",
        "# $$ \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\text{score}(Y, \\theta_0) \\text{score}(Y, \\theta_0)^\\top \\right]\n",
        "\n",
        "-\\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\left(\\nabla*\\theta^2 \\log p(Y | \\theta)\\right)_{\\theta=\\theta_0} \\right] $$\n",
        "\n",
        "ここで、$\\nabla_\\theta^2 F$ はヘッセ行列を示し、その $(i, j)$ エントリは $\\frac{\\partial^2 F}{\\partial \\theta_i \\partial \\theta_j}$ です。\n",
        "\n",
        "この方程式の左辺は、パラメータベクトル $\\theta_0$ の族 $\\left{p(\\cdot | \\theta)\\right}_{\\theta \\in \\mathcal{T}}$ の**フィッシャー情報量**と呼ばれます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF-ac0Bk-HmR"
      },
      "source": [
        "#### 主張の証明\n",
        "\n",
        "まず、\n",
        "\n",
        "## $$ \\begin{align*} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\left(\\nabla*\\theta^2 \\log p(Y | \\theta)\\right)*{\\theta=\\theta_0} \\right] &amp;\\stackrel{\\text{(1)}}{=} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\left(\\nabla_\\theta^\\top \\frac{ \\nabla_\\theta p(Y | \\theta) }{ p(Y|\\theta) }\\right)*{\\theta=\\theta_0} \\right] \\ &amp;\\stackrel{\\text{(2)}}{=} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\frac{ \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) }\n",
        "\n",
        "## \\left(\\frac{ \\left(\\nabla_\\theta, p(Y|\\theta)\\right)*{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) }\\right) \\left(\\frac{ \\left(\\nabla*\\theta, p(Y|\\theta)\\right)*{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) }\\right)^\\top \\right] \\ &amp;\\stackrel{\\text{(3)}}{=} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\frac{ \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) }\n",
        "\n",
        "\\text{score}(Y, \\theta_0) ,\\text{score}(Y, \\theta_0)^\\top \\right], \\end{align*} $$\n",
        "\n",
        "ここでは (1) 微分のための連鎖律、(2) 微分のための商の法則、(3) 再び連鎖律を (逆に) 使用しました。\n",
        "\n",
        "証明を完了するには、以下を示すだけで十分です\n",
        "\n",
        "$$ \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\frac{ \\left(\\nabla^2*\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) } \\right] \\stackrel{\\text{?}}{=} 0. $$\n",
        "\n",
        "そのために、積分記号の下で微分を 2 回渡します。\n",
        "\n",
        "$$ \\begin{align*} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\frac{ \\left(\\nabla^2*\\theta p(Y | \\theta)\\right)*{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) } \\right] &amp;= \\int*{\\mathcal{Y}} \\left[ \\frac{ \\left(\\nabla^2_\\theta p(y | \\theta)\\right)*{\\theta=\\theta_0} }{ p(y|\\theta=\\theta_0) } \\right] , p(y | \\theta=\\theta_0), dy \\ &amp;= \\int*{\\mathcal{Y}} \\left(\\nabla^2_\\theta p(y | \\theta)\\right)*{\\theta=\\theta_0} , dy \\ &amp;= \\left[ \\nabla*\\theta^2 \\left( \\int_{\\mathcal{Y}} p(y | \\theta) , dy \\right) \\right]*{\\theta=\\theta_0} \\ &amp;= \\left[ \\nabla*\\theta^2 , 1 \\right]_{\\theta=\\theta_0} \\ &amp;= 0. \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAIJfX7IX_lP"
      },
      "source": [
        "### 対数分配関数の導関数についての補題\n",
        "\n",
        "$a$、$b$ および $c$ がスカラー値関数の場合、$c$ は 2 回微分可能であり、分布族 $\\left{p(\\cdot | \\theta)\\right}_{\\theta \\in \\mathcal{T}}$ は以下によって定義されます。\n",
        "\n",
        "$$ p(y|\\theta) = a(y) \\exp\\left(b(y), \\theta - c(\\theta)\\right) $$\n",
        "\n",
        "$y$ に関する積分の下で、$\\theta$ に関する微分を渡すことを可能にする穏やかな規則性条件を満たします。\n",
        "\n",
        "$$ \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right] = c'(\\theta_0) $$\n",
        "\n",
        "および\n",
        "\n",
        "$$ \\text{Var}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right] = c''(\\theta_0). $$\n",
        "\n",
        "(ここで $'$ は微分を表すので、 $c'$ と $c''$ は $c$ の 1 次および 2 次導関数です。)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYBH-KwpfWhr"
      },
      "source": [
        "#### 証明\n",
        "\n",
        "この分布族の場合、$\\text{score}(y, \\theta_0) = b(y) - c'(\\theta_0)$ です。最初の方程式は、$\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\text{score}(y, \\theta_0) \\right] = 0$ に従います。 次に、\n",
        "\n",
        "$$ \\begin{align*} \\text{Var}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right] &amp;= \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\left(b(Y) - c'(\\theta_0)\\right)^2 \\right] \\ &amp;= \\text{the one entry of } \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\text{score}(y, \\theta_0) \\text{score}(y, \\theta_0)^\\top \\right] \\ &amp;= \\text{the one entry of } -\\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\left(\\nabla_\\theta^2 \\log p(\\cdot | \\theta)\\right)*{\\theta=\\theta_0} \\right] \\ &amp;= -\\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ -c''(\\theta_0) \\right] \\ &amp;= c''(\\theta_0). \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYpWUvvKcX-e"
      },
      "source": [
        "## 過分散指数型分布族\n",
        "\n",
        "(スカラー) **過分散指数型分布族**は、密度が次の形式をとる分布の族です。\n",
        "\n",
        "$$ p_{\\text{OEF}(m,  T)}(y, |, \\theta, \\phi) = m(y, \\phi) \\exp\\left(\\frac{\\theta, T(y) - A(\\theta)}{\\phi}\\right), $$\n",
        "\n",
        "ここで、$m$ と $T$ は既知のスカラー値関数であり、$\\theta$ と $\\phi$ はスカラーパラメータです。\n",
        "\n",
        "*[$A$ は過剰決定であることに注意してください。任意の $\\phi_0$ では、関数 $A$ はすべての $\\theta$ に対して $\\int p_{\\text{OEF}(m,  T)}(y\\ |\\ \\theta, \\phi=\\phi_0), dy = 1$ の制約によって完全に決定されます。$\\phi_0$ の異なる値によって生成される $A$ はすべて同じである必要があり、これにより関数 $m$ と $T$ に制約が課せられます。]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgpoijwPf7TV"
      },
      "source": [
        "### 十分統計量の平均と分散\n",
        "\n",
        "「対数分配関数の導関数についての補題」と同じ条件で、次のようになります\n",
        "\n",
        "# $$ \\mathbb{E}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[ T(Y) \\right]\n",
        "\n",
        "A'(\\theta) $$\n",
        "\n",
        "および\n",
        "\n",
        "# $$ \\text{Var}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[ T(Y) \\right]\n",
        "\n",
        "\\phi A''(\\theta). $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyf51flphGOK"
      },
      "source": [
        "#### 証明\n",
        "\n",
        "「対数分配関数の導関数についての補題」により、次が成り立ちます。\n",
        "\n",
        "# $$ \\mathbb{E}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[ \\frac{T(Y)}{\\phi} \\right]\n",
        "\n",
        "\\frac{A'(\\theta)}{\\phi} $$\n",
        "\n",
        "および\n",
        "\n",
        "# $$ \\text{Var}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[ \\frac{T(Y)}{\\phi} \\right]\n",
        "\n",
        "\\frac{A''(\\theta)}{\\phi}. $$\n",
        "\n",
        "結果は、期待値が線形 ($\\mathbb{E}[aX] = a\\mathbb{E}[X]$) および、分散が 2 次均一 ($\\text{Var}[aX] = a^2 ,\\text{Var}[X]$)であるということから成り立ちます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYOnAZv9d4XH"
      },
      "source": [
        "## 一般化線形モデル\n",
        "\n",
        "一般化線形モデルでは、応答変数 $Y$ の予測分布は、観測された予測子 $x$ のベクトルに関連付けられます。分布は過分散指数型分布族のメンバーであり、パラメータ $\\theta$ は $h(\\eta)$ に置き換えられます。ここで、$h$ は既知の関数、$\\eta := x^\\top \\beta$ は**線形応答**であり、$\\beta$ は学習するパラメータ (回帰係数) のベクトルです。一般的に、分散パラメータ $\\phi$ も学習できますが、ここでは以下のように $\\phi$ を既知として扱います。\n",
        "\n",
        "$$ Y \\sim p_{\\text{OEF}(m, T)}(\\cdot, |, \\theta = h(\\eta), \\phi) $$\n",
        "\n",
        "ここで、モデル構造は、分布 $p_{\\text{OEF}(m, T)}$ と線形応答をパラメーターに変換する関数 $h$ により特徴付けられます。\n",
        "\n",
        "従来、線形応答 $\\eta$ から平均 $\\mu := \\mathbb{E}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot, |, \\theta = h(\\eta), \\phi)}\\left[ Y\\right]$ へのマッピングは、以下のように示されます。\n",
        "\n",
        "$$ \\mu = g^{-1}(\\eta). $$\n",
        "\n",
        "このマッピングは 1 対 1 である必要があり、その逆関数 $g$ は、この GLM の**リンク関数**と呼ばれます。通常、GLM は、そのリンク関数とその分布族で名前を付けて説明します。例: 「ベルヌーイ分布とロジットリンク関数を使用した GLM」(ロジスティック回帰モデルとも呼ばれます)。GLM を完全に特性化するには、関数 $h$ も指定する必要があります。$h$ がアイデンティティの場合、$g$ は**正規リンク関数**になります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-mrWHH2-wtv"
      },
      "source": [
        "### 主張：十分統計量の観点から $h'$ を表現する\n",
        "\n",
        "以下を定義します\n",
        "\n",
        "$$ {\\text{Mean}*T}(\\eta) := \\mathbb{E}*{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(\\eta), \\phi)} \\left[ T(Y) \\right] $$\n",
        "\n",
        "および\n",
        "\n",
        "$$ {\\text{Var}*T}(\\eta) := \\text{Var}*{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(\\eta), \\phi)} \\left[ T(Y) \\right]. $$\n",
        "\n",
        "すると、以下が成り立ちます。\n",
        "\n",
        "$$ h'(\\eta) = \\frac{\\phi, {\\text{Mean}_T}'(\\eta)}{{\\text{Var}_T}(\\eta)}. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z36iGKlf_-3F"
      },
      "source": [
        "#### 証明\n",
        "\n",
        "「十分統計量の平均と分散」により、以下が成り立ちます。\n",
        "\n",
        "$$ {\\text{Mean}_T}(\\eta) = A'(h(\\eta)). $$\n",
        "\n",
        "連鎖律で微分すると、$$ {\\text{Mean}_T}'(\\eta) = A''(h(\\eta)), h'(\\eta), $$ が得られます。\n",
        "\n",
        "「十分統計量の平均と分散」により、以下が成り立ちます。\n",
        "\n",
        "$$ \\cdots = \\frac{1}{\\phi} {\\text{Var}_T}(\\eta)\\ h'(\\eta). $$\n",
        "\n",
        "結論は次のとおりです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8LV_QHPx-wV"
      },
      "source": [
        "## GLM パラメータのデータへの適合\n",
        "\n",
        "上記で導出されたプロパティは、GLM パラメータ $\\beta$ をデータセットに適合させるのに非常に役立ちます。フィッシャースコアリングなどの準ニュートン法は、対数尤度の勾配とフィッシャー情報に依存します。これは、以下に示すように GLM に対して特に効率的に計算できます。\n",
        "\n",
        "予測ベクトル $x_i$ と関連するスカラー応答  $y_i$ を観測したとします。行列形式では、予測子 $\\mathbf{x}$ と応答 $\\mathbf{y}$ を観測しました。ここで、$\\mathbf{x}$ は、 $i$ 番目の行が $x_i^\\top$ である行列であり、$\\mathbf{y}$ は、$i$ 番目の要素が$y_i$であるベクトルです。パラメータ $\\beta$ の対数尤度は次のようになります。\n",
        "\n",
        "$$ \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{N} \\log p_{\\text{OEF}(m, T)}(y_i, |, \\theta = h(x_i^\\top \\beta), \\phi). $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aghNxiO_HFW1"
      },
      "source": [
        "### 単一のデータサンプルの場合\n",
        "\n",
        "表記を簡単にするために、最初に単一のデータポイント $N=1$ の場合を検討します。次に、加法性によって一般的なケースに拡張します。\n",
        "\n",
        "#### 勾配\n",
        "\n",
        "まず、\n",
        "\n",
        "$$ \\begin{align*} \\ell(\\beta, ;, x, y) &amp;= \\log p_{\\text{OEF}(m, T)}(y, |, \\theta = h(x^\\top \\beta), \\phi) \\ &amp;= \\log m(y, \\phi) + \\frac{\\theta, T(y) - A(\\theta)}{\\phi}, \\quad\\text{where}\\  \\theta = h(x^\\top \\beta). \\end{align*} $$\n",
        "\n",
        "したがって、連鎖律により以下が成り立ちます。\n",
        "\n",
        "$$ \\nabla_\\beta \\ell(\\beta, ; , x, y) = \\frac{T(y) - A'(\\theta)}{\\phi}, h'(x^\\top \\beta), x. $$\n",
        "\n",
        "一方、「十分統計量の平均と分散」により、$A'(\\theta) = {\\text{Mean}_T}(x^\\top \\beta)$ が得られます。したがって、「主張: 十分統計量の観点から $h'$ を表現する」ことにより、次が成り立ちます。\n",
        "\n",
        "$$ \\cdots = \\left(T(y) - {\\text{Mean}_T}(x^\\top \\beta)\\right) \\frac{{\\text{Mean}_T}'(x^\\top \\beta)}{{\\text{Var}_T}(x^\\top \\beta)} ,x. $$\n",
        "\n",
        "#### ヘッセ行列\n",
        "\n",
        "積の法則により 2 回微分することにより、以下が得られます。\n",
        "\n",
        "$$ \\begin{align*} \\nabla_\\beta^2 \\ell(\\beta, ;, x, y) &amp;= \\left[ -A''(h(x^\\top \\beta)), h'(x^\\top \\beta) \\right] h'(x^\\top \\beta), x x^\\top + \\left[ T(y) - A'(h(x^\\top \\beta)) \\right] h''(x^\\top \\beta), xx^\\top ] \\ &amp;= \\left( -{\\text{Mean}_T}'(x^\\top \\beta), h'(x^\\top \\beta) + \\left[T(y) - A'(h(x^\\top \\beta))\\right] \\right), x x^\\top. \\end{align*} $$\n",
        "\n",
        "#### フィッシャー情報\n",
        "\n",
        "「十分統計量の平均と分散」により、以下が成り立ちます。\n",
        "\n",
        "$$ \\mathbb{E}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x^\\top \\beta), \\phi)} \\left[ T(y) - A'(h(x^\\top \\beta)) \\right] = 0. $$\n",
        "\n",
        "よって以下が成り立ちます。\n",
        "\n",
        "$$ \\begin{align*} \\mathbb{E}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x^\\top \\beta), \\phi)} \\left[ \\nabla_\\beta^2 \\ell(\\beta, ;, x, y) \\right] &amp;= -{\\text{Mean}_T}'(x^\\top \\beta), h'(x^\\top \\beta) x x^\\top \\ &amp;= -\\frac{\\phi, {\\text{Mean}_T}'(x^\\top \\beta)^2}{{\\text{Var}_T}(x^\\top \\beta)}, x x^\\top. \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrA1A583HOng"
      },
      "source": [
        "### 複数のデータサンプルの場合\n",
        "\n",
        "次に、$N=1$ を一般的なケースに拡張します。$\\boldsymbol{\\eta} := \\mathbf{x} \\beta$ が、$i$ 番目の座標が $i$ 番目のデータサンプルからの線形応答であるベクトルを表すとします。$\\mathbf{T}$ (resp. ${\\textbf{Mean}_T}$, resp. ${\\textbf{Var}_T}$) がスカラー値関数 $T$ (resp. ${\\text{Mean}_T}$, resp. ${\\text{Var}_T}$)を各座標に適用するブロードキャスト (ベクトル化) 関数を示すとすると、以下が成り立ちます。\n",
        "\n",
        "$$ \\begin{align*} \\nabla_\\beta \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{y}) &amp;= \\sum_{i=1}^{N} \\nabla_\\beta \\ell(\\beta, ;, x_i, y_i) \\ &amp;= \\sum_{i=1}^{N} \\left(T(y) - {\\text{Mean}_T}(x_i^\\top \\beta)\\right) \\frac{{\\text{Mean}_T}'(x_i^\\top \\beta)}{{\\text{Var}_T}(x_i^\\top \\beta)} , x_i \\ &amp;= \\mathbf{x}^\\top ,\\text{diag}\\left(\\frac{ {\\textbf{Mean}_T}'(\\mathbf{x} \\beta) }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta) }\\right) \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta)\\right) \\ \\end{align*} $$\n",
        "\n",
        "および\n",
        "\n",
        "$$ \\begin{align*} \\mathbb{E}*{Y_i \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta), \\phi)} \\left[ \\nabla_\\beta^2 \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{Y}) \\right] &amp;= \\sum_{i=1}^{N} \\mathbb{E}*{Y_i \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta), \\phi)} \\left[ \\nabla_\\beta^2 \\ell(\\beta, ;, x_i, Y_i) \\right] \\ &amp;= \\sum_{i=1}^{N} -\\frac{\\phi, {\\text{Mean}_T}'(x_i^\\top \\beta)^2}{{\\text{Var}_T}(x_i^\\top \\beta)}, x_i x_i^\\top \\ &amp;= -\\mathbf{x}^\\top ,\\text{diag}\\left( \\frac{ \\phi, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)^2 }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta) }\\right), \\mathbf{x}, \\end{align*} $$\n",
        "\n",
        "ここで、分数は要素ごとの除算を示します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUrOmdt395hZ"
      },
      "source": [
        "## 数式を数値的に検証する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVp59IBW-TK6"
      },
      "source": [
        "次に、`tf.gradients` を使用して対数尤度の勾配について上記の式を数値的に検証し、`tf.hessians` を使用してモンテカルロ推定でフィッシャー情報の式を検証します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM-HDPdPepE-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coordinatewise relative error between naively computed gradients and formula-based gradients (should be zero):\n",
            "[[2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]]\n",
            "\n",
            "Coordinatewise relative error between average of naively computed Hessian and formula-based FIM (should approach zero as num_trials -> infinity):\n",
            "[[0.00072369 0.00072369 0.00072369]\n",
            " [0.00072369 0.00072369 0.00072369]\n",
            " [0.00072369 0.00072369 0.00072369]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def VerifyGradientAndFIM():\n",
        "  model = tfp.glm.BernoulliNormalCDF()\n",
        "  model_matrix = np.array([[1., 5, -2],\n",
        "                           [8, -1, 8]])\n",
        "\n",
        "  def _naive_grad_and_hessian_loss_fn(x, response):\n",
        "    # Computes gradient and Hessian of negative log likelihood using autodiff.\n",
        "    predicted_linear_response = tf.linalg.matvec(model_matrix, x)\n",
        "    log_probs = model.log_prob(response, predicted_linear_response)\n",
        "    grad_loss = tf.gradients(-log_probs, [x])[0]\n",
        "    hessian_loss = tf.hessians(-log_probs, [x])[0]\n",
        "    return [grad_loss, hessian_loss]\n",
        "\n",
        "  def _grad_neg_log_likelihood_and_fim_fn(x, response):\n",
        "    # Computes gradient of negative log likelihood and Fisher information matrix\n",
        "    # using the formulas above.\n",
        "    predicted_linear_response = tf.linalg.matvec(model_matrix, x)\n",
        "    mean, variance, grad_mean = model(predicted_linear_response)\n",
        "\n",
        "    v = (response - mean) * grad_mean / variance\n",
        "    grad_log_likelihood = tf.linalg.matvec(model_matrix, v, adjoint_a=True)\n",
        "    w = grad_mean**2 / variance\n",
        "\n",
        "    fisher_info = tf.linalg.matmul(\n",
        "        model_matrix,\n",
        "        w[..., tf.newaxis] * model_matrix,\n",
        "        adjoint_a=True)\n",
        "    return [-grad_log_likelihood, fisher_info]\n",
        "\n",
        "  @tf.function(autograph=False)\n",
        "  def compute_grad_hessian_estimates():\n",
        "    # Monte Carlo estimate of E[Hessian(-LogLikelihood)], where the expectation is\n",
        "    # as written in \"Claim (Fisher information)\" above.\n",
        "    num_trials = 20\n",
        "    trial_outputs = []\n",
        "    np.random.seed(10)\n",
        "    model_coefficients_ = np.random.random(size=(model_matrix.shape[1],))\n",
        "    model_coefficients = tf.convert_to_tensor(model_coefficients_)\n",
        "    for _ in range(num_trials):\n",
        "      # Sample from the distribution of `model`\n",
        "      response = np.random.binomial(\n",
        "          1,\n",
        "          scipy.stats.norm().cdf(np.matmul(model_matrix, model_coefficients_))\n",
        "      ).astype(np.float64)\n",
        "      trial_outputs.append(\n",
        "          list(_naive_grad_and_hessian_loss_fn(model_coefficients, response)) +\n",
        "          list(\n",
        "              _grad_neg_log_likelihood_and_fim_fn(model_coefficients, response))\n",
        "      )\n",
        "\n",
        "    naive_grads = tf.stack(\n",
        "        list(naive_grad for [naive_grad, _, _, _] in trial_outputs), axis=0)\n",
        "    fancy_grads = tf.stack(\n",
        "        list(fancy_grad for [_, _, fancy_grad, _] in trial_outputs), axis=0)\n",
        "\n",
        "    average_hess = tf.reduce_mean(tf.stack(\n",
        "        list(hess for [_, hess, _, _] in trial_outputs), axis=0), axis=0)\n",
        "    [_, _, _, fisher_info] = trial_outputs[0]\n",
        "    return naive_grads, fancy_grads, average_hess, fisher_info\n",
        "  \n",
        "  naive_grads, fancy_grads, average_hess, fisher_info = [\n",
        "      t.numpy() for t in compute_grad_hessian_estimates()]\n",
        "\n",
        "  print(\"Coordinatewise relative error between naively computed gradients and\"\n",
        "        \" formula-based gradients (should be zero):\\n{}\\n\".format(\n",
        "            (naive_grads - fancy_grads) / naive_grads))\n",
        "\n",
        "  print(\"Coordinatewise relative error between average of naively computed\"\n",
        "        \" Hessian and formula-based FIM (should approach zero as num_trials\"\n",
        "        \" -> infinity):\\n{}\\n\".format(\n",
        "                (average_hess - fisher_info) / average_hess))\n",
        "    \n",
        "VerifyGradientAndFIM()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAiNubQ-WDHN"
      },
      "source": [
        "# 参照文献\n",
        "\n",
        "<a name=\"1\"></a>[1]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for L1-regularized Logistic Regression. *Journal of Machine Learning Research*, 13, 2012. http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf\n",
        "\n",
        "<a name=\"2\"></a>[2]: skd. Derivation of Soft Thresholding Operator.  2018. https://math.stackexchange.com/q/511106\n",
        "\n",
        "<a name=\"3\"></a>[3]: Wikipedia Contributors. Proximal gradient methods for learning. *Wikipedia, The Free Encyclopedia*, 2018. https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n",
        "\n",
        "<a name=\"4\"></a>[4]: Yao-Liang Yu. The Proximity Operator. https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Generalized_Linear_Models.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
