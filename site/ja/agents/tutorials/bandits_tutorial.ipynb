{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "##### Copyright 2020 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqslkUeyEJFg"
      },
      "source": [
        "# TF-Agents における多腕バンディット問題のチュートリアル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MimUC9NrYFaS"
      },
      "source": [
        "### はじめに\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/bandits_tutorial\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> TensorFlow.org で表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/agents/tutorials/bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/agents/tutorials/bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/agents/tutorials/bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "### セットアップ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "以下の依存関係をインストールしていない場合は、実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7gLdUS6b2EG"
      },
      "source": [
        "### インポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oCS94Z83Jo2"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "nest = tf.nest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcIob6rYqien"
      },
      "source": [
        "# はじめに\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdnTJrzaeft3"
      },
      "source": [
        "タ腕バンディット問題（MAB）は、エージェントがある環境の状態を観察した後に何らかのアクションを取ることで、その環境における報酬を得るという強化学習の特殊なケースです。一般的な RL と MAB の主な違いは、MAB では、エージェントが環境の次の状態に影響を与えないことです。したがって、エージェントは状態遷移をモデル化したり、過去のアクションに対する報酬を与えたり、より多くの報酬を得るための「予測」を行いません。\n",
        "\n",
        "ほかの RL 分野と同様に、MAB *エージェント*の目標は、できる限り多くの報酬を収集する*ポリシー*を見つけ出すことです。ただし、十分に調べなかった場合により適したアクションを見逃す可能性があるため、最高の報酬を約束するアクションを常に使用しようとするのは間違いです。これが MAB で解決しなければならない主な問題であり、通常、*探索と知識利用のジレンマ*と呼ばれています。\n",
        "\n",
        "MAB のバンディット環境、ポリシー、およびエージェントは、[tf_agents/bandits](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits) のサブディレクトリにあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPzsBCTperx3"
      },
      "source": [
        "# 環境"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LOXW8i320Cp"
      },
      "source": [
        "TF-Agents では、環境クラスは現在の状態（**観測**または**コンテキスト**）に関する情報を提供し、アクションを入力として受け取って状態遷移を実行し、報酬を出力する役割があります。このクラスは、エピソードが終了したときに新しいエピソードが開始されるよう、リセットも行います。これは、状態のラベルがエピソードの「最後」となった時に `reset` 関数を呼び出して行われます。\n",
        "\n",
        "詳細については、「[TF-Agents 環境のチュートリアル](https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb)」をご覧ください。\n",
        "\n",
        "前述のとおり、MAB は、アクションが次の観測に影響を与えないという点で一般的な RL と異なりますが、もう一つの違いは、バンディットには、前の時間ステップから独立して新しい観測でステップが開始するたびに「エピソード」がないところにあります。\n",
        "\n",
        "観測が確実に独立しており、RL エピソードの概念を中傷かするために、`PyEnvironment` と `TFEnvironment` のサブクラスである [BanditPyEnvironment](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/environments/bandit_py_environment.py) と [BanditTFEnvironment](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/environments/bandit_tf_environment.py) を導入します。これらのクラスは、ユーザーが実装したままにする 2 つのプライベートメンバー関数を公開します。\n",
        "\n",
        "```python\n",
        "@abc.abstractmethod\n",
        "def _observe(self):\n",
        "```\n",
        "\n",
        "と\n",
        "\n",
        "```python\n",
        "@abc.abstractmethod\n",
        "def _apply_action(self, action):\n",
        "```\n",
        "\n",
        "`_observe` 関数は観測を戻します。すると、ポリシーがこの観測に基づくアクションを選択します。`_apply_action` は、アクションを入力として受け取り、対応する報酬を戻します。これらのプライベートメンバー関数はそれぞれ、`reset` 関数と `step` 関数によって呼び出されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTaG2ZapQvHX"
      },
      "outputs": [],
      "source": [
        "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, observation_spec, action_spec):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    super(BanditPyEnvironment, self).__init__()\n",
        "\n",
        "  # Helper functions.\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _empty_observation(self):\n",
        "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
        "                                 self.observation_spec())\n",
        "\n",
        "  # These two functions below should not be overridden by subclasses.\n",
        "  def _reset(self):\n",
        "    \"\"\"Returns a time step containing an observation.\"\"\"\n",
        "    return ts.restart(self._observe(), batch_size=self.batch_size)\n",
        "\n",
        "  def _step(self, action):\n",
        "    \"\"\"Returns a time step containing the reward for the action taken.\"\"\"\n",
        "    reward = self._apply_action(action)\n",
        "    return ts.termination(self._observe(), reward)\n",
        "\n",
        "  # These two functions below are to be implemented in subclasses.\n",
        "  @abc.abstractmethod\n",
        "  def _observe(self):\n",
        "    \"\"\"Returns an observation.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _apply_action(self, action):\n",
        "    \"\"\"Applies `action` to the Environment and returns the corresponding reward.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVtLk28xVo0j"
      },
      "source": [
        "上記の中間抽象クラスは `PyEnvironment` の `_reset` 関数と `_step` 関数を実装し、サブクラスが実装する抽象関数の `_observe` と `_apply_action` を公開します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQbI-6PdtSJn"
      },
      "source": [
        "## 単純な環境クラスの例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qspwAx0tS6l"
      },
      "source": [
        "以下のクラスは、観測が -2 から 2 のランダム整数で、3 つの可能なアクション (0, 1, 2) があり、報酬がこのアクションと観測の積である非常に単純な環境を提供します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV6DhsSi227-"
      },
      "outputs": [],
      "source": [
        "class SimplePyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
        "    super(SimplePyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return action * self._observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipEQgYDIf55t"
      },
      "source": [
        "次に、この環境を使用して観測を取得し、アクションに対する報酬を受け取ります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo_uwSz2gAKX"
      },
      "outputs": [],
      "source": [
        "environment = SimplePyEnvironment()\n",
        "observation = environment.reset().observation\n",
        "print(\"observation: %d\" % observation)\n",
        "\n",
        "action = 2 #@param\n",
        "\n",
        "print(\"action: %d\" % action)\n",
        "reward = environment.step(action).reward\n",
        "print(\"reward: %f\" % reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuVYHI8aDgCx"
      },
      "source": [
        "## TF Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP46VwLTDnOR"
      },
      "source": [
        "`BanditTFEnvironment` をサブクラス化するか、RL 環境と同様に `BanditPyEnvironment` を定義して `TFPyEnvironment` でラップすることでバンディット環境を定義できます。単純さを維持するために、このチュートリアルでは、後者を使用することにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPPpwSi3EtWz"
      },
      "outputs": [],
      "source": [
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S9fhxF9GUaT"
      },
      "source": [
        "# ポリシー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbTt5jnuGlYj"
      },
      "source": [
        "バンディット問題における*ポリシー*は、RL 問題と同様に機能し、観測を入力としてアクション（またはアクションの分布）を提供します。\n",
        "\n",
        "詳細については、「[TF-Agents ポリシーのチュートリアル](https://github.com/tensorflow/agents/blob/master/docs/tutorials/3_policies_tutorial.ipynb)」をご覧ください。\n",
        "\n",
        "環境と同様に、ポリシーの構築には 2 つの方法があります。1 つは、`PyPolicy` を作成して `TFPyPolicy` でラップする方法で、もう 1 つは、`TFPolicy` を直接作成する方法です。ここでは、直接作成する方法を使用します。\n",
        "\n",
        "この例は非常に単純であるため、最適なポリシーを手動で作成できます。アクションは観測の表示にのみ依存しており、負の場合は 0、正の場合は 2 となります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpMZlplNK5ND"
      },
      "outputs": [],
      "source": [
        "class SignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "    super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return ()\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
        "    action = observation_sign + 1\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAM7hb4LVQ70"
      },
      "source": [
        "次に、環境に観測をリクエストし、ポリシーを呼び出してポリシーを選択すると、環境が報酬を出力します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0_5vMDCVZWT"
      },
      "outputs": [],
      "source": [
        "sign_policy = SignPolicy()\n",
        "\n",
        "current_time_step = tf_environment.reset()\n",
        "print('Observation:')\n",
        "print (current_time_step.observation)\n",
        "action = sign_policy.action(current_time_step).action\n",
        "print('Action:')\n",
        "print (action)\n",
        "reward = tf_environment.step(action).reward\n",
        "print('Reward:')\n",
        "print(reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AExuQ7u0-PF6"
      },
      "source": [
        "バンディット環境の実装方法によって、ステップを取るたびに、選択したアクションに対する報酬が得られるだけでなく、次の観測も得られます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiB935of-wVv"
      },
      "outputs": [],
      "source": [
        "step = tf_environment.reset()\n",
        "action = 1\n",
        "next_step = tf_environment.step(action)\n",
        "reward = next_step.reward\n",
        "next_observation = next_step.observation\n",
        "print(\"Reward: \")\n",
        "print(reward)\n",
        "print(\"Next observation:\")\n",
        "print(next_observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFnqVHfeANZP"
      },
      "source": [
        "# エージェント"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pDK_faXAPSA"
      },
      "source": [
        "バンディット環境とバンディットポリシーを準備したので、バンディットエージェントを定義することにしましょう。バンディットエージェントは、トレーニングサンプルに基づいてポリシーの変更を行います。\n",
        "\n",
        "バンディットエージェントの API は RL のエージェントと同じですが、`_initialize` メソッドと `_train` メソッドを実装し、`policy` と `collect_policy` を定義する必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVCb-vPJOayG"
      },
      "source": [
        "## より複雑な環境"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ksv7i7zPGSa"
      },
      "source": [
        "バンディットエージェントを記述する前に、少し理解しにくい環境を用意する必要があります。もう少し面白くするために、次の環境は、`reward = observation * action` または `reward = -observation * action` のいずれかを必ず与えるようにしましょう。どちらが与えられるかは、環境か初期化するときに決定されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fte7-Mr8O0QR"
      },
      "outputs": [],
      "source": [
        "class TwoWayPyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
        "\n",
        "    # Flipping the sign with probability 1/2.\n",
        "    self._reward_sign = 2 * np.random.randint(2) - 1\n",
        "    print(\"reward sign:\")\n",
        "    print(self._reward_sign)\n",
        "\n",
        "    super(TwoWayPyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return self._reward_sign * action * self._observation[0]\n",
        "\n",
        "two_way_tf_environment = tf_py_environment.TFPyEnvironment(TwoWayPyEnvironment())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zb4jWpQUA75"
      },
      "source": [
        "## より複雑なポリシー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz2rEEA1USJu"
      },
      "source": [
        "より複雑なかんきょうには、より複雑なポリシーが伴います。基盤の環境の動作を検出するポリシーが必要です。ポリシーが処理する必要のある状況は 3 つあります。\n",
        "\n",
        "1. エージェントが、実行している環境のバージョンを検出していない場合\n",
        "2. エージェントが、実行している環境の元のバージョンを検出した場合\n",
        "3. エージェントが、実行している環境の反転バージョンを検出した場合\n",
        "\n",
        "`_situation` という `tf_variable` を、`[0, 2]` の値にエンコーディングされた情報を格納するように定義し、ポリシーが適宜に動作するようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srm2jsGHVM8N"
      },
      "outputs": [],
      "source": [
        "class TwoWaySignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self, situation):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "    self._situation = situation\n",
        "    super(TwoWaySignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                           action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return [self._situation]\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    sign = tf.cast(tf.sign(time_step.observation[0, 0]), dtype=tf.int32)\n",
        "    def case_unknown_fn():\n",
        "      # Choose 1 so that we get information on the sign.\n",
        "      return tf.constant(1, shape=(1,))\n",
        "\n",
        "    # Choose 0 or 2, depending on the situation and the sign of the observation.\n",
        "    def case_normal_fn():\n",
        "      return tf.constant(sign + 1, shape=(1,))\n",
        "    def case_flipped_fn():\n",
        "      return tf.constant(1 - sign, shape=(1,))\n",
        "\n",
        "    cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
        "             (tf.equal(self._situation, 1), case_normal_fn),\n",
        "             (tf.equal(self._situation, 2), case_flipped_fn)]\n",
        "    action = tf.case(cases, exclusive=True)\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6PPdRQQbE3Q"
      },
      "source": [
        "## エージェント"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO8HpL0tUP32"
      },
      "source": [
        "では、環境のサインを検出して、ポリシーを適切に設定するエージェントを定義することにしましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f-0W0cMbS_z"
      },
      "outputs": [],
      "source": [
        "class SignAgent(tf_agent.TFAgent):\n",
        "  def __init__(self):\n",
        "    self._situation = tf.Variable(0, dtype=tf.int32)\n",
        "    policy = TwoWaySignPolicy(self._situation)\n",
        "    time_step_spec = policy.time_step_spec\n",
        "    action_spec = policy.action_spec\n",
        "    super(SignAgent, self).__init__(time_step_spec=time_step_spec,\n",
        "                                    action_spec=action_spec,\n",
        "                                    policy=policy,\n",
        "                                    collect_policy=policy,\n",
        "                                    train_sequence_length=None)\n",
        "\n",
        "  def _initialize(self):\n",
        "    return tf.compat.v1.variables_initializer(self.variables)\n",
        "\n",
        "  def _train(self, experience, weights=None):\n",
        "    observation = experience.observation\n",
        "    action = experience.action\n",
        "    reward = experience.reward\n",
        "\n",
        "    # We only need to change the value of the situation variable if it is\n",
        "    # unknown (0) right now, and we can infer the situation only if the\n",
        "    # observation is not 0.\n",
        "    needs_action = tf.logical_and(tf.equal(self._situation, 0),\n",
        "                                  tf.not_equal(reward, 0))\n",
        "\n",
        "\n",
        "    def new_situation_fn():\n",
        "      \"\"\"This returns either 1 or 2, depending on the signs.\"\"\"\n",
        "      return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(action[0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
        "\n",
        "    new_situation = tf.cond(needs_action,\n",
        "                            new_situation_fn,\n",
        "                            lambda: self._situation)\n",
        "    new_situation = tf.cast(new_situation, tf.int32)\n",
        "    tf.compat.v1.assign(self._situation, new_situation)\n",
        "    return tf_agent.LossInfo((), ())\n",
        "\n",
        "sign_agent = SignAgent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyclF0ZZpW-f"
      },
      "source": [
        "上記のコードでは、エージェントがポリシーを定義し、エージェントとポリシーが変数 `situation` を共有しています。\n",
        "\n",
        "また、`_train` 関数のパラメータ `experience` はトラジェクトリです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NlF228LGoiR"
      },
      "source": [
        "# トラジェクトリ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GbBDi1iGsnN"
      },
      "source": [
        "TF-Agents では、`trajectories` は名前付きのタプルであり、前のステップで取得されたサンプルを含みます。これらのサンプルはエージェントによってポリシーのトレーニングと更新に使用されます。RL では、トラジェクトリには現在の状態、次の状態、そして現在のエピソードが終了したかどうかに関する情報が含まれている必要があります。バンディットの世界では、これらの情報は不要であるため、ヘルパー関数をセットアップしてトラジェクトリを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdSG1nv-HUJq"
      },
      "outputs": [],
      "source": [
        "# We need to add another dimension here because the agent expects the\n",
        "# trajectory of shape [batch_size, time, ...], but in this tutorial we assume\n",
        "# that both batch size and time are 1. Hence all the expand_dims.\n",
        "\n",
        "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
        "  return trajectory.Trajectory(observation=tf.expand_dims(initial_step.observation, 0),\n",
        "                               action=tf.expand_dims(action_step.action, 0),\n",
        "                               policy_info=action_step.info,\n",
        "                               reward=tf.expand_dims(final_step.reward, 0),\n",
        "                               discount=tf.expand_dims(final_step.discount, 0),\n",
        "                               step_type=tf.expand_dims(initial_step.step_type, 0),\n",
        "                               next_step_type=tf.expand_dims(final_step.step_type, 0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFEJ8kbI_e6Q"
      },
      "source": [
        "# エージェントのトレーニング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gh-41og_hDB"
      },
      "source": [
        "これで、バンディットエージェントをトレーニングするためのピースがすべて用意できました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPx43dZgoyKg"
      },
      "outputs": [],
      "source": [
        "step = two_way_tf_environment.reset()\n",
        "for _ in range(10):\n",
        "  action_step = sign_agent.collect_policy.action(step)\n",
        "  next_step = two_way_tf_environment.step(action_step.action)\n",
        "  experience = trajectory_for_bandit(step, action_step, next_step)\n",
        "  print(experience)\n",
        "  sign_agent.train(experience)\n",
        "  step = next_step\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iVSNiYdy4U4"
      },
      "source": [
        "出力から、2 つ目のステップ（最初のステップで観測が 0 でなければ）の後に、ポリシーが正しい方法でアクションを選択しており、したがって収集された報酬が常に非負であることがわかります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCKyKEjOlOPE"
      },
      "source": [
        "# 実際の文脈付きバンディットの例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecnQwUpmllar"
      },
      "source": [
        "このチュートリアルの残りでは、TF-Agents Bandits ライブラリの事前実装済みの[環境](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/environments/)と[エージェント](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/agents/)を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEnXUwd-nZKl"
      },
      "outputs": [],
      "source": [
        "# Imports for example.\n",
        "from tf_agents.bandits.agents import lin_ucb_agent\n",
        "from tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\n",
        "from tf_agents.bandits.metrics import tf_metrics\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37oy70dUmmie"
      },
      "source": [
        "## 線形ペイオフ関数を使った定常確率的環境"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euPPd8x1m7iG"
      },
      "source": [
        "この例で使用する環境は、[StationaryStochasticPyEnvironment](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/environments/stationary_stochastic_py_environment.py) です。この環境は、観測（コンテキスト）を提供する（非常にノイズの多い）関数をパラメータとして取り、アームごとに、与えられた観測に基づいて報酬を計算する（やはりノイズの多い）関数を取ります。このチュートリアルの例では、d 次元の立方体から均一にコンテキストをサンプリングすると、報酬関数はコンテキストの線形関数で、一部はガウスノイズです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVa0hmQrpe6w"
      },
      "outputs": [],
      "source": [
        "batch_size = 2 # @param\n",
        "arm0_param = [-3, 0, 1, -2] # @param\n",
        "arm1_param = [1, -2, 3, 0] # @param\n",
        "arm2_param = [0, 0, 1, 1] # @param\n",
        "def context_sampling_fn(batch_size):\n",
        "  \"\"\"Contexts from [-10, 10]^4.\"\"\"\n",
        "  def _context_sampling_fn():\n",
        "    return np.random.randint(-10, 10, [batch_size, 4]).astype(np.float32)\n",
        "  return _context_sampling_fn\n",
        "\n",
        "class LinearNormalReward(object):\n",
        "  \"\"\"A class that acts as linear reward function when called.\"\"\"\n",
        "  def __init__(self, theta, sigma):\n",
        "    self.theta = theta\n",
        "    self.sigma = sigma\n",
        "  def __call__(self, x):\n",
        "    mu = np.dot(x, self.theta)\n",
        "    return np.random.normal(mu, self.sigma)\n",
        "\n",
        "arm0_reward_fn = LinearNormalReward(arm0_param, 1)\n",
        "arm1_reward_fn = LinearNormalReward(arm1_param, 1)\n",
        "arm2_reward_fn = LinearNormalReward(arm2_param, 1)\n",
        "\n",
        "environment = tf_py_environment.TFPyEnvironment(\n",
        "    sspe.StationaryStochasticPyEnvironment(\n",
        "        context_sampling_fn(batch_size),\n",
        "        [arm0_reward_fn, arm1_reward_fn, arm2_reward_fn],\n",
        "        batch_size=batch_size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haID-SPgsLyY"
      },
      "source": [
        "## LinUCB エージェント"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "298-1Q0bsQmR"
      },
      "source": [
        "以下のエージェントは、[LinUCB](http://rob.schapire.net/papers/www10.pdf) アルゴリズムを実装します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4XmGgIusj-K"
      },
      "outputs": [],
      "source": [
        "observation_spec = tensor_spec.TensorSpec([4], tf.float32)\n",
        "time_step_spec = ts.time_step_spec(observation_spec)\n",
        "action_spec = tensor_spec.BoundedTensorSpec(\n",
        "    dtype=tf.int32, shape=(), minimum=0, maximum=2)\n",
        "\n",
        "agent = lin_ucb_agent.LinearUCBAgent(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eua_aC7Rt78G"
      },
      "source": [
        "## Regret 基準"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBJDiJvEt-xC"
      },
      "source": [
        "バンディットで最も重要な基準は *regret* です。エージェントが収集した報酬と、環境の報酬関数にアクセスできる予測ポリシーの期待報酬の差として計算されます。そのため、[RegretMetric](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/metrics/tf_metrics.py) には、特定の観測があった場合に、達成可能な最大の期待報酬を計算する *baseline_reward_fn* 関数が必要です。このチュートリアルの例では、この環境に定義した報酬関数に相当するノイズのない関数の最大値を取る必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX7MiFhNu3_L"
      },
      "outputs": [],
      "source": [
        "def compute_optimal_reward(observation):\n",
        "  expected_reward_for_arms = [\n",
        "      tf.linalg.matvec(observation, tf.cast(arm0_param, dtype=tf.float32)),\n",
        "      tf.linalg.matvec(observation, tf.cast(arm1_param, dtype=tf.float32)),\n",
        "      tf.linalg.matvec(observation, tf.cast(arm2_param, dtype=tf.float32))]\n",
        "  optimal_action_reward = tf.reduce_max(expected_reward_for_arms, axis=0)\n",
        "  return optimal_action_reward\n",
        "\n",
        "regret_metric = tf_metrics.RegretMetric(compute_optimal_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRWz-Qeb13JC"
      },
      "source": [
        "## トレーニング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khdKjTs516Pg"
      },
      "source": [
        "上記で説明した環境、ポリシー、およびエージェントの要素をすべてを組み合わせましょう。*ドライバー*を使用して、環境でポリシーを実行してトレーニングデータを出力し、そのデータでエージェントをトレーニングします。\n",
        "\n",
        "必要なステップ数を共に指定するパラメータが 2 つあることに注意してください。`num_iterations` はトレーナーループを実行する回数を指定し、ドライバーはイテレーションごとに `steps_per_loop` ステップを実行します。これらのパラメータを維持するのは、主に、イテレーションごとに実行される演算と、ステップごとにドライバーが行う演算があるためです。たとえば、エージェントの `train` 関数はイテレーションにつき一度しか呼び出されません。ここでは、トレーニングの頻度を高めると、ポリシーが「より新しく」なるのに対し、より大きなバッチでトレーニングすると時間の効率がよくなるというトレードオフがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ggn45g62DWx"
      },
      "outputs": [],
      "source": [
        "num_iterations = 90 # @param\n",
        "steps_per_loop = 1 # @param\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.policy.trajectory_spec,\n",
        "    batch_size=batch_size,\n",
        "    max_length=steps_per_loop)\n",
        "\n",
        "observers = [replay_buffer.add_batch, regret_metric]\n",
        "\n",
        "driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    env=environment,\n",
        "    policy=agent.collect_policy,\n",
        "    num_steps=steps_per_loop * batch_size,\n",
        "    observers=observers)\n",
        "\n",
        "regret_values = []\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  driver.run()\n",
        "  loss_info = agent.train(replay_buffer.gather_all())\n",
        "  replay_buffer.clear()\n",
        "  regret_values.append(regret_metric.result())\n",
        "\n",
        "plt.plot(regret_values)\n",
        "plt.ylabel('Average Regret')\n",
        "plt.xlabel('Number of Iterations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2diHS5IzLuo"
      },
      "source": [
        "最後のコードスニペットを実行したら、生成されるプロットから、エージェントのトレーニングが増えて、特定の観測が与えられる場合にポリシーが適切なアクションを選択する確率が高まる過程で、平均 Regret が下降しているのが示されます（そうであることを願います）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qLMnOL00-2V"
      },
      "source": [
        "# 今後の内容"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOiRWZbf1Drs"
      },
      "source": [
        "さらに機能例を確認する場合は、[bandits/agents/examples](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/agents/examples) ディレクトリをご覧ください。さまざまなエージェントと環境用にすぐに実行できる例が掲載されています。\n",
        "\n",
        "TF-Agents ライブラリは、アームごとの特徴量でタ腕バンディットを処理することもできます。それについては、アームごとのバンディット問題の[チュートリアル](https://github.com/tensorflow/agents/tree/master/docs/tutorials/per_arm_bandits_tutorial.ipynb)をご覧ください。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bandits_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
