{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "**Copyright 2021 The TF-Agents Authors.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "# Actor-Learner APIを使用したSAC minitaur\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/7_SAC_minitaur_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/agents/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a> </td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/agents/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/agents/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a>   </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOUOQOrFs3zn"
      },
      "source": [
        "## 前書き"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "ここでは、[Minitaur](https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/minitaur.py) 環境で [Soft Actor Critic](https://arxiv.org/abs/1812.05905) エージェントをトレーニングする方法を紹介します。\n",
        "\n",
        "[DQN Colab](https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb) にすでに精通されている場合は、馴染みやすいと思います。主な違いは次のとおりです。\n",
        "\n",
        "- エージェントを DQN から SAC に変更します。\n",
        "- Minitaur（CartPole よりもはるかに複雑な環境）環境でのトレーニング。Minitaur 環境は、四足歩行ロボットが前進するようにトレーニングすることを目的としています。\n",
        "- 分散型強化学習のための TF-Agent Actor-Learner API を使用します。\n",
        "\n",
        "API は、経験再生バッファーと変数コンテナ（パラメータサーバー）を使用した分散データ収集と、複数のデバイスにわたる分散トレーニングの両方をサポートしています。API は非常にシンプルでモジュール化されています。再生バッファーと可変コンテナには [Reverb](https://deepmind.com/research/open-source/Reverb)、GPU と TPU での分散トレーニングには [TF DistributionStrategy API](https://www.tensorflow.org/guide/distributed_training) が使用されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vUQms4DAY5A"
      },
      "source": [
        "以下の依存関係をインストールしていない場合は、実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fskoLlB-AZ9j"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install matplotlib\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pybullet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## セットアップ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNV5wyH3dyMl"
      },
      "source": [
        "まず、必要なさまざまなツールをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import reverb\n",
        "import tempfile\n",
        "import PIL.Image\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.environments import suite_pybullet\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.train import actor\n",
        "from tf_agents.train import learner\n",
        "from tf_agents.train import triggers\n",
        "from tf_agents.train.utils import spec_utils\n",
        "from tf_agents.train.utils import strategy_utils\n",
        "from tf_agents.train.utils import train_utils\n",
        "\n",
        "tempdir = tempfile.gettempdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## ハイパーパラメータ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "outputs": [],
      "source": [
        "env_name = \"MinitaurBulletEnv-v0\" # @param {type:\"string\"}\n",
        "\n",
        "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
        "# 1e5 is just so this doesn't take too long (1 hr)\n",
        "num_iterations = 100000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256 # @param {type:\"integer\"}\n",
        "\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "actor_fc_layer_params = (256, 256)\n",
        "critic_joint_fc_layer_params = (256, 256)\n",
        "\n",
        "log_interval = 5000 # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
        "eval_interval = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "policy_save_interval = 5000 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## 環境変数\n",
        "\n",
        "RL の環境は、解決しようとしているタスクまたは問題を表しています。標準環境は、`suites`を使用して TF-Agent で簡単に作成できます。OpenAI Gym、Atari、DM Control などのソースから環境を読み込むためのさまざまな`suites`が用意されています。これには、文字列の環境名が与えられます。\n",
        "\n",
        "では、PybulletスイートからMinituar環境を読み込みましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [],
      "source": [
        "env = suite_pybullet.load(env_name)\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY179d1xlmoM"
      },
      "source": [
        "この環境での目標は、エージェントがMinitaurロボットを制御し、可能な限り速く前進させられるようにポリシーをトレーニングすることです。エピソードには1000ステップあり、リターンはエピソード全体の報酬の合計になります。\n",
        "\n",
        "環境が`観察`として提供する情報を見てみましょう。ポリシーは観察を使用して`行動`を生成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "outputs": [],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg5ysVTnctIm"
      },
      "source": [
        "観測はかなり複雑で、すべてのモーターの角度、速度、トルクを表す 28 個の値を受け取ります。それに対して環境は行動に関する `[-1, 1]` 間の値を 8 個受け取ることを期待します。これらの値は望ましいモーター角度です。\n",
        "\n",
        "通常、2つの環境を作成します。1つ目はトレーニング中にデータを収集するため、もう2つ目は評価のための環境です。環境は純粋なpythonで記述され、Actor Learner APIが直接使用するnumpy配列を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "outputs": [],
      "source": [
        "collect_env = suite_pybullet.load(env_name)\n",
        "eval_env = suite_pybullet.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da-z2yF66FR9"
      },
      "source": [
        "## 分散ストラテジー\n",
        "\n",
        "DistributionStrategy APIでは、データの並列処理を使用して、複数のGPUやTPUなどの複数のデバイス間でトレーニングステップの計算を実行できます。トレーニングステップは以下のとおりです。\n",
        "\n",
        "- トレーニングデータのバッチを受けとる\n",
        "- デバイス間で分割する\n",
        "- 前進ステップを計算する\n",
        "- 損失のMEANを集計して計算する\n",
        "- 後退ステップを計算し、勾配変数の更新を実行する\n",
        "\n",
        "TF-Agent Learner API と DistributionStrategy API を使用すると、以下のトレーニングロジックを変更せずに、トレーニングステップの実行を GPU（MirroredStrategy を使用）からTPU（TPUStrategyを使用）に簡単に切り替えることができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGREYZCaDB1h"
      },
      "source": [
        "### GPUを有効にする\n",
        "\n",
        "GPU で実行する場合は、まずノートブックで GPU を有効にする必要があります。\n",
        "\n",
        "- ［編集］→［ノートブック設定］に移動します\n",
        "- ［ハードウェアアクセラレータ］ドロップダウンから GPU を選択します"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZuvwDV66Mn1"
      },
      "source": [
        "### ストラテジーの選択\n",
        "\n",
        "`strategy_utils`を使用してストラテジーを生成します。 内部的に、パラメータを渡します。\n",
        "\n",
        "- `use_gpu = False`は`tf.distribute.get_strategy()`を返します。 この場合、CPU を使用します。\n",
        "- `use_gpu = True`は`tf.distribute.MirroredStrategy()`を返します。この場合、1 台のマシンのTensorFlowにより認識されるすべての GPU を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff5ZZRZI15ds"
      },
      "outputs": [],
      "source": [
        "use_gpu = True #@param {type:\"boolean\"}\n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMn5FTs5kHvt"
      },
      "source": [
        "以下に示すように、すべての変数とエージェントは`strategy.scope()`の下に作成する必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## エージェント\n",
        "\n",
        "SAC エージェントを作成するには、まず、トレーニングするネットワークを作成する必要があります。SAC は actor-critic エージェントなので、2 つのネットワークを必要とします。\n",
        "\n",
        "critic は、`Q(s,a)` の値を推定します。つまり、入力として観測と行動を受け取り、状態に対して行動がどのくらい効果的であるかを推定します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(collect_env))\n",
        "\n",
        "with strategy.scope():\n",
        "  critic_net = critic_network.CriticNetwork(\n",
        "        (observation_spec, action_spec),\n",
        "        observation_fc_layer_params=None,\n",
        "        action_fc_layer_params=None,\n",
        "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        last_kernel_initializer='glorot_uniform')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYy4AH4V7Ph4"
      },
      "source": [
        "このcriticを使用して、`actor`ネットワークをトレーニングします。これにより、与えられた観察に対する行動を生成できます。\n",
        "\n",
        "`ActorNetwork`は、tanh-squashed [MultivariateNormalDiag](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag) 分布のパラメータを予測します。行動を生成する必要がある場合は常に、その時点の観測を条件としてこの分布がサンプリングされます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB5Y3Oub4u7f"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=actor_fc_layer_params,\n",
        "      continuous_projection_net=(\n",
        "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "これらのネットワークを使用して、エージェントをインスタンス化できます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = sac_agent.SacAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        actor_network=actor_net,\n",
        "        critic_network=critic_net,\n",
        "        actor_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=actor_learning_rate),\n",
        "        critic_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=critic_learning_rate),\n",
        "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=alpha_learning_rate),\n",
        "        target_update_tau=target_update_tau,\n",
        "        target_update_period=target_update_period,\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\n",
        "        gamma=gamma,\n",
        "        reward_scale_factor=reward_scale_factor,\n",
        "        train_step_counter=train_step)\n",
        "\n",
        "  tf_agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## 再生バッファ\n",
        "\n",
        "環境から収集されたデータを追跡するためには [Reverb](https://deepmind.com/research/open-source/Reverb) を使用します。Reverb は、Deepmind により開発された効率的かつ拡張可能で使いやすい再生システムです。これは、Actor により収集され、トレーニング中に Learner により消費される経験データを格納します。\n",
        "\n",
        "このチュートリアルでは、`max_size`よりも重要ではありませんが、非同期収集とトレーニングを使用する分散設定では、2〜1000 の samples_per_insert を使用して、`rate_limiters.SampleToInsertRatio`を試すことをお勧めします。以下に例を示します。\n",
        "\n",
        "```\n",
        "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRNvAnkO7JK2"
      },
      "source": [
        "再生バッファは、格納されるテンソルを記述する仕様を使用して構築されます。これは、`tf_agent.collect_data_spec`を使用してエージェントから取得できます。\n",
        "\n",
        "SAC エージェントは損失を計算するためにその時点と次の両方の観測を必要とするため、`sequence_length=2`を設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVLUxyUo7HQR"
      },
      "outputs": [],
      "source": [
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2,\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "次に、Reverb再生バッファーからTensorFlowデータセットを生成します。これをLearnerに渡して、トレーニングの体験をサンプリングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba7bilizt_qW"
      },
      "outputs": [],
      "source": [
        "dataset = reverb_replay.as_dataset(\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
        "experience_dataset_fn = lambda: dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## ポリシー\n",
        "\n",
        "TF-Agent では、ポリシーは RL のポリシーの標準的な概念を表します。`time_step`が指定されると、行動または行動の分布が生成されます。主なメソッドは`policy_step = policy.step(time_step)`で、`policy_step`は、名前付きのタプル`PolicyStep(action, state, info)`です。`policy_step.action`は、環境に適用される`action`で、`state`は、ステートフル（RNN）ポリシーの状態を表し、`info`には行動のログ確率などの補助情報が含まれる場合があります。\n",
        "\n",
        "エージェントには 2 つのポリシーが含まれています。\n",
        "\n",
        "- `agent.policy` — 評価と導入に使用される主なポリシー。\n",
        "- `agent.collect_policy` — データ収集に使用される補助的なポリシー。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq7JE8IwFe0E"
      },
      "outputs": [],
      "source": [
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_eval_policy, use_tf_function=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_A4rZveEQzW"
      },
      "outputs": [],
      "source": [
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_collect_policy, use_tf_function=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azkJZ8oaF8uc"
      },
      "source": [
        "ポリシーはエージェントとは無関係に作成できます。たとえば、`tf_agents.policies.random_tf_policy`を使用して、各<code>time_step</code>の行動をランダムに選択するポリシーを作成できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "outputs": [],
      "source": [
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  collect_env.time_step_spec(), collect_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1LMqw60Kuso"
      },
      "source": [
        "## アクター\n",
        "\n",
        "Actorは、ポリシーと環境の間の相互作用を管理します。\n",
        "\n",
        "- Actorコンポーネントには、環境のインスタンス（`py_environment`として）とポリシー変数のコピーが含まれています。\n",
        "- ポリシー変数のローカル値が指定されると、各Actorワーカーは、一連のデータ収集ステップを実行します。\n",
        "- 変数の更新は、`actor.run()`を呼び出す前に、トレーニングスクリプトの変数コンテナクライアントインスタンスを使用して明示的に行われます。\n",
        "- 観測された経験は、各データ収集ステップで再生バッファーに書き込まれます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjE59ct9fU7W"
      },
      "source": [
        "Actor がデータ収集ステップを実行すると、状態、行動、報酬）のTrajectoryをオブザーバーに渡し、オブザーバーはそれらをキャッシュして Reverb 再生システムに書き込みます。\n",
        "\n",
        "`stride_length=1`なので、フレーム [(t0,t1) (t1,t2) (t2,t3), ...] の Trajectory を保存します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbyGmdiNfNDc"
      },
      "outputs": [],
      "source": [
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yaVVC22fOcF"
      },
      "source": [
        "ランダムなポリシーで Actor を作成し、再生バッファーをシードする経験を収集します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGq3SY0kKwsa"
      },
      "outputs": [],
      "source": [
        "initial_collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "initial_collect_actor.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pkg-0vZP_Ya"
      },
      "source": [
        "収集ポリシーを使用して Actor をインスタンス化し、トレーニング中にさらに経験を収集します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6ooXyk0FZ5j"
      },
      "outputs": [],
      "source": [
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR9CZ-jfPN2T"
      },
      "source": [
        "トレーニング中にポリシーを評価するために使用される Actor を作成します。後でメトリックを記録するために`actor.eval_metrics(num_eval_episodes)`を渡します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHY2BT5lFhgL"
      },
      "outputs": [],
      "source": [
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=os.path.join(tempdir, 'eval'),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6eBGSYiOf83"
      },
      "source": [
        "## 学習者\n",
        "\n",
        "Learner コンポーネントにはエージェントが含まれており、再生バッファーからの経験データを使用して、ポリシー変数への勾配ステップの更新を実行します。1 つ以上のトレーニングステップの後、Learner は新しい一連の変数値を変数コンテナにプッシュできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi37YicSFTfF"
      },
      "outputs": [],
      "source": [
        "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
        "\n",
        "# Triggers to save the agent's policy checkpoints.\n",
        "learning_triggers = [\n",
        "    triggers.PolicySavedModelTrigger(\n",
        "        saved_model_dir,\n",
        "        tf_agent,\n",
        "        train_step,\n",
        "        interval=policy_save_interval),\n",
        "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
        "]\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  tempdir,\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  triggers=learning_triggers,\n",
        "  strategy=strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## 指標と評価\n",
        "\n",
        "上記の`actor.eval_metrics`でevalアクターをインスタンス化しました。これにより、ポリシー評価中に最も一般的に使用されるメトリックが作成されます。\n",
        "\n",
        "- 平均リターン。 リターンは、エピソードの環境でポリシーを実行しているときに得られる報酬の合計であり、通常、これをいくつかのエピソードで平均します。\n",
        "- エピソードの長さの平均。\n",
        "\n",
        "Actorを実行して、これらのメトリクスを生成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83iMSHUC71RG"
      },
      "outputs": [],
      "source": [
        "def get_eval_metrics():\n",
        "  eval_actor.run()\n",
        "  results = {}\n",
        "  for metric in eval_actor.metrics:\n",
        "    results[metric.name] = metric.result()\n",
        "  return results\n",
        "\n",
        "metrics = get_eval_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnOMvX_eZvOW"
      },
      "outputs": [],
      "source": [
        "def log_eval_metrics(step, metrics):\n",
        "  eval_results = (', ').join(\n",
        "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "  print('step = {0}: {1}'.format(step, eval_results))\n",
        "\n",
        "log_eval_metrics(0, metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWWURm_rXG-f"
      },
      "source": [
        "さまざまなメトリクスの他の標準実装については、[metrics module](https://github.com/tensorflow/agents/blob/master/tf_agents/metrics/tf_metrics.py)をご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## エージェントのトレーニング\n",
        "\n",
        "トレーニングループには、環境からのデータ収集とエージェントのネットワークの最適化の両方が含まれます。途中で、エージェントのポリシーを時々評価して、状況を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Training.\n",
        "  collect_actor.run()\n",
        "  loss_info = agent_learner.run(iterations=1)\n",
        "\n",
        "  # Evaluating.\n",
        "  step = agent_learner.train_step_numpy\n",
        "\n",
        "  if eval_interval and step % eval_interval == 0:\n",
        "    metrics = get_eval_metrics()\n",
        "    log_eval_metrics(step, metrics)\n",
        "    returns.append(metrics[\"AverageReturn\"])\n",
        "\n",
        "  if log_interval and step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## 可視化\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### プロット\n",
        "\n",
        "エージェントのパフォーマンスを確認するために、平均利得とグローバルステップをプロットできます。`Minitaur`では、報酬関数は、Minitaur が 1000 ステップで歩く距離に基づいており、エネルギー消費にペナルティを課します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXKzyGt72HS8"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### ビデオ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "各ステップで環境をレンダリングすると、エージェントのパフォーマンスを可視化できます。その前に、この Colab に動画を埋め込む関数を作成しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "次のコードは、いくつかのエピソードに渡るエージェントのポリシーを可視化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSgaQN1nXT-h"
      },
      "outputs": [],
      "source": [
        "num_episodes = 3\n",
        "video_filename = 'sac_minitaur.mp4'\n",
        "with imageio.get_writer(video_filename, fps=60) as video:\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = eval_env.reset()\n",
        "    video.append_data(eval_env.render())\n",
        "    while not time_step.is_last():\n",
        "      action_step = eval_actor.policy.action(time_step)\n",
        "      time_step = eval_env.step(action_step.action)\n",
        "      video.append_data(eval_env.render())\n",
        "\n",
        "embed_mp4(video_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "7_SAC_minitaur_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
