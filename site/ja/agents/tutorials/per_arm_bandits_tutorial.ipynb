{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPjtEgqN4SjA"
      },
      "source": [
        "##### Copyright 2021 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6AZJOyCA4NpL"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdBl31Dqwomt"
      },
      "source": [
        "# アーム単位の特徴量を使った多腕バンディットのチュートリアル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vvG61d35bG"
      },
      "source": [
        "### はじめに\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/per_arm_bandits_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/per_arm_bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/per_arm_bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/per_arm_bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a>   </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddRJNIiEwu9O"
      },
      "source": [
        "このチュートリアルは、特徴別（ジャンル、リリース年など）にまとめられた映画のリストなど、アクション（アーム）に独自の特徴量のある文脈的バンディット問題に TF-Agents ライブラリを使用する手順を示すガイドです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6mUk-hZa3pB"
      },
      "source": [
        "### 前提条件\n",
        "\n",
        "TF-Agents のバンディットライブラリについてある程度の知識があること、特にこのチュートリアルの前に「[TF-Agents におけるバンディットのチュートリアル](https://github.com/tensorflow/agents/tree/master/docs/tutorials/bandits_tutorial.ipynb)」を実行したことを前提としています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kscmAIA5xtJW"
      },
      "source": [
        "## アームの特徴量を使用した多腕バンディット\n",
        "\n",
        "「古典的」な多腕バンディットの設定では、エージェントは時間ステップごとのコンテキストベクトル（観測）を受け取り、累積報酬を最大化するために、番号付きのアクション（アーム）の有限セットから選択する必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDkno4bQ1vPE"
      },
      "source": [
        "では、エージェントがユーザーに、次に見る英があを推奨するというシナリオについて考えてみましょう。決定を行うたびに、エージェントはコンテキストとしてユーザーに関する情報（鑑賞履歴、好みのジャンルなど）と選択する映画のリストを受け取ります。\n",
        "\n",
        "コンテキストとしてのユーザー情報を使ってこの問題を定式化すると、アームは `movie_1, movie_2, ..., movie_K` となりますが、このアプローチには以下のような複数の欠点があります。\n",
        "\n",
        "- アクション数はすべてシステム内の映画に限られ、新しい映画を追加するのは面倒である。\n",
        "- エージェントは映画ごとのモデルを学習する必要がある。\n",
        "- 映画同士の類似性が考慮されていない。\n",
        "\n",
        "映画に番号を付ける代わりに、より直感的に、ジャンル、長さ、キャスト、レーティング、年などを含む一連の特徴量で映画を表現することができます。このアプローチには、以下のようにさまざまなメリットがあります。\n",
        "\n",
        "- 映画の一般化。\n",
        "- エージェントは、ユーザーと映画の特徴量で報酬をモデル化する報酬関数を 1 つ学習するだけでよい。\n",
        "- システム上の映画の追加と削除を簡単に行える。\n",
        "\n",
        "この新しい設定では、アクション数は時間ステップごとに異なることが可能です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMXxIHXNRP5_"
      },
      "source": [
        "## TF-Agents におけるアーム単位のバンディット\n",
        "\n",
        "TF-Agents Bandit スイートは、アーム単位のケースにも使用できるように開発されています。アーム単位の環境が用意されており、ポリシーとエージェントのほとんどもアーム単位のモードで動作可能です。\n",
        "\n",
        "コーディング例に進む前に、必要なインポートを行いましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl5_CCIWSFvn"
      },
      "source": [
        "### インストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxiNIm5XSIIp"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDKNu5JTSDmf"
      },
      "source": [
        "### インポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbQXsoeKR2ui"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.bandits.agents import lin_ucb_agent\n",
        "from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
        "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "nest = tf.nest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4cVyq3JMM7Z"
      },
      "source": [
        "### パラメータ -- 自由にいじってみましょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfna8xm0MSCn"
      },
      "outputs": [],
      "source": [
        "# The dimension of the global features.\n",
        "GLOBAL_DIM = 40  #@param {type:\"integer\"}\n",
        "# The elements of the global feature will be integers in [-GLOBAL_BOUND, GLOBAL_BOUND).\n",
        "GLOBAL_BOUND = 10  #@param {type:\"integer\"}\n",
        "# The dimension of the per-arm features.\n",
        "PER_ARM_DIM = 50  #@param {type:\"integer\"}\n",
        "# The elements of the PER-ARM feature will be integers in [-PER_ARM_BOUND, PER_ARM_BOUND).\n",
        "PER_ARM_BOUND = 6  #@param {type:\"integer\"}\n",
        "# The variance of the Gaussian distribution that generates the rewards.\n",
        "VARIANCE = 100.0  #@param {type: \"number\"}\n",
        "# The elements of the linear reward parameter will be integers in [-PARAM_BOUND, PARAM_BOUND).\n",
        "PARAM_BOUND = 10  #@param {type: \"integer\"}\n",
        "\n",
        "NUM_ACTIONS = 70  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 20  #@param {type:\"integer\"}\n",
        "\n",
        "# Parameter for linear reward function acting on the\n",
        "# concatenation of global and per-arm features.\n",
        "reward_param = list(np.random.randint(\n",
        "      -PARAM_BOUND, PARAM_BOUND, [GLOBAL_DIM + PER_ARM_DIM]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-mEz1HvRIBC"
      },
      "source": [
        "### 単純なアーム単位の環境\n",
        "\n",
        "他の[チュートリアル](https://github.com/tensorflow/agents/tree/master/docs/tutorials/bandits_tutorial.ipynb)で説明した静止した確定的環境には、アーム単位の相当物があります。\n",
        "\n",
        "アーム単位の環境を初期化するには、以下を生成する関数を定義する必要があります。\n",
        "\n",
        "- *グローバル特徴量とアーム単位の特徴量*: これらの関数には入力パラメータがなく、呼び出されると単一（グローバルまたはアーム単位）の特徴量ベクトルを生成します。\n",
        "- *報酬*: この関数はパラメータとしてグローバルとアーム単位の特徴量ベクトルの連結を取り、報酬を生成します。基本的に、これが、エージェントが「推測」する必要のある関数です。アーム単位のケースでは、報酬関数はすべてのアームで同一であることに注意してください。これが、エージェントがアームごとに個別に報酬関数を推定する必要のある古典的なバンディットのケースとの根本的な違いです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfTa5Y4ZYjhO"
      },
      "outputs": [],
      "source": [
        "def global_context_sampling_fn():\n",
        "  \"\"\"This function generates a single global observation vector.\"\"\"\n",
        "  return np.random.randint(\n",
        "      -GLOBAL_BOUND, GLOBAL_BOUND, [GLOBAL_DIM]).astype(np.float32)\n",
        "\n",
        "def per_arm_context_sampling_fn():\n",
        "  \"\"\"\"This function generates a single per-arm observation vector.\"\"\"\n",
        "  return np.random.randint(\n",
        "      -PER_ARM_BOUND, PER_ARM_BOUND, [PER_ARM_DIM]).astype(np.float32)\n",
        "\n",
        "def linear_normal_reward_fn(x):\n",
        "  \"\"\"This function generates a reward from the concatenated global and per-arm observations.\"\"\"\n",
        "  mu = np.dot(x, reward_param)\n",
        "  return np.random.normal(mu, VARIANCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2vpRPhheTo5"
      },
      "source": [
        "これで、環境を初期化できるようになりました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-yikfQQi9l"
      },
      "outputs": [],
      "source": [
        "per_arm_py_env = p_a_env.StationaryStochasticPerArmPyEnvironment(\n",
        "    global_context_sampling_fn,\n",
        "    per_arm_context_sampling_fn,\n",
        "    NUM_ACTIONS,\n",
        "    linear_normal_reward_fn,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "per_arm_tf_env = tf_py_environment.TFPyEnvironment(per_arm_py_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIzFl8HiAIxg"
      },
      "source": [
        "以下で、この環境が生成するものを確認できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8ZVqMU5AOzs"
      },
      "outputs": [],
      "source": [
        "print('observation spec: ', per_arm_tf_env.observation_spec())\n",
        "print('\\nAn observation: ', per_arm_tf_env.reset().observation)\n",
        "\n",
        "action = tf.zeros(BATCH_SIZE, dtype=tf.int32)\n",
        "time_step = per_arm_tf_env.step(action)\n",
        "print('\\nRewards after taking an action: ', time_step.reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIlCGssRAzIr"
      },
      "source": [
        "観測仕様は、以下の 2 つの要素を持つディクショナリであることがわかります。\n",
        "\n",
        "- キー `'global'` を持つ要素: これはグローバルコンテキストの部分で、形状はパラメータ `GLOBAL_DIM` に一致します。\n",
        "- キー `'per_arm'` を持つ要素: これがアーム単位のコンテキストで、その形状は `[NUM_ACTIONS, PER_ARM_DIM]` です。この部分は、実行ステップのすべてのアームのアーム特徴量のプレースホルダーです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTpWbNVeS6ci"
      },
      "source": [
        "### LinUCB エージェント"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q83tgZR9LLUx"
      },
      "source": [
        "LinUCB エージェントは、同盟のバンディットアルゴリズムを実行します。これは、線形報酬関数のパラメータを推定する一方で、その推定周囲の信頼楕円も維持します。エージェントは、パラメータがその信頼楕円内にあるものと仮定して、推定される最も高い期待報酬を持つアームを選択します。\n",
        "\n",
        "エージェントを作成するには、観測とアクション仕様に関する知識が必要です。エージェントを定義する場合、ブール型パラメータ `accepts_per_arm_features` を `True` に設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqzA9Zi0Q2No"
      },
      "outputs": [],
      "source": [
        "observation_spec = per_arm_tf_env.observation_spec()\n",
        "time_step_spec = ts.time_step_spec(observation_spec)\n",
        "action_spec = tensor_spec.BoundedTensorSpec(\n",
        "    dtype=tf.int32, shape=(), minimum=0, maximum=NUM_ACTIONS - 1)\n",
        "\n",
        "agent = lin_ucb_agent.LinearUCBAgent(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec,\n",
        "                                     accepts_per_arm_features=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaQlv1lpC-oc"
      },
      "source": [
        "### トレーニングデータのフロー\n",
        "\n",
        "このセクションでは、アーム単位の特徴がポリシーからトレーニングにどのように移行するかについての仕組みを簡単に説明します。 自由に次のセクション（後悔指標を定義する）に進み、興味があれば後でこのセクションに戻ることもできます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUSy2IFK5NpU"
      },
      "source": [
        "まず、エージェントのデータ仕様を確認しましょう。エージェントの `training_data_spec` 属性は、トレーニングデータに必要な要素とトレーニングデータを指定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQRZd43o5M0j"
      },
      "outputs": [],
      "source": [
        "print('training data spec: ', agent.training_data_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyEFAHUg-m9V"
      },
      "source": [
        "仕様の `observation` の部分を詳しく見てみると、アーム単位の特徴量が含まれないことがわかります！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTBR1vgG-2AM"
      },
      "outputs": [],
      "source": [
        "print('observation spec in training: ', agent.training_data_spec.observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDtmUgeJA_DN"
      },
      "source": [
        "アーム単位の特徴量はどうなったのでしょうか。この疑問に答えるには、まず、LinUCB エージェントがトレーニングする際に、**すべての**アームのアーム単位の特徴量は不要であり、**選択された**アームのみのアーム単位の特徴量が必要であることに注意しましょう。したがって、形状 `[BATCH_SIZE, NUM_ACTIONS, PER_ARM_DIM]` のテンソルをドロップするのが合理的です。特にアクション数が多い場合には非常に無駄であるためです。\n",
        "\n",
        "とは言え、それでも選択されたアームのアーム単位の特徴量はどこかに存在しなければなりません！これを確認するために、LinUCB ポリシーが、選択されたアームの特徴量をトレーニングデータの `policy_info` フィールド内に格納していることを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0fHvLm0Cpq9"
      },
      "outputs": [],
      "source": [
        "print('chosen arm features: ', agent.training_data_spec.policy_info.chosen_arm_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4psGJKGIDVgN"
      },
      "source": [
        "この形状から、`chosen_arm_features` には 1 つのアームの特徴量ベクトルのみが含まれており、それが選択されたアームであることがわかります。`policy_info` と `chosen_arm_features` は、トレーニングデータの仕様を調べたところからわかるように、トレーニングデータの一部であるため、トレーニング時に利用できることに注意してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ancYxxEHc-6Q"
      },
      "source": [
        "### 後悔指標を定義する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j2RTrBfzVJQ"
      },
      "source": [
        "トレーニングループを開始する前に、エージェントの後悔の計算を支援するユーティリティ関数を定義します。これらの関数は、アクション（アームの特徴量によって指定される）とエージェントが表示できない線形パラメータのセットを基に、最適な期待報酬を判定する上で役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1_kJAOS6VPo"
      },
      "outputs": [],
      "source": [
        "def _all_rewards(observation, hidden_param):\n",
        "  \"\"\"Outputs rewards for all actions, given an observation.\"\"\"\n",
        "  hidden_param = tf.cast(hidden_param, dtype=tf.float32)\n",
        "  global_obs = observation['global']\n",
        "  per_arm_obs = observation['per_arm']\n",
        "  num_actions = tf.shape(per_arm_obs)[1]\n",
        "  tiled_global = tf.tile(\n",
        "      tf.expand_dims(global_obs, axis=1), [1, num_actions, 1])\n",
        "  concatenated = tf.concat([tiled_global, per_arm_obs], axis=-1)\n",
        "  rewards = tf.linalg.matvec(concatenated, hidden_param)\n",
        "  return rewards\n",
        "\n",
        "def optimal_reward(observation):\n",
        "  \"\"\"Outputs the maximum expected reward for every element in the batch.\"\"\"\n",
        "  return tf.reduce_max(_all_rewards(observation, reward_param), axis=1)\n",
        "\n",
        "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i06WDbjrUSog"
      },
      "source": [
        "これで、バンディットトレーニングループを開始する準備が整いました。以下のドライバーはポリシーを使ってアクションの選択を処理し、選択されたアクションの報酬を再生バッファに格納し、事前定義済みの後悔指標を計算し、エージェントのトレーニングステップを実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2Iimtmkzs0-"
      },
      "outputs": [],
      "source": [
        "num_iterations = 20 # @param\n",
        "steps_per_loop = 1 # @param\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.policy.trajectory_spec,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_length=steps_per_loop)\n",
        "\n",
        "observers = [replay_buffer.add_batch, regret_metric]\n",
        "\n",
        "driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    env=per_arm_tf_env,\n",
        "    policy=agent.collect_policy,\n",
        "    num_steps=steps_per_loop * BATCH_SIZE,\n",
        "    observers=observers)\n",
        "\n",
        "regret_values = []\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  driver.run()\n",
        "  loss_info = agent.train(replay_buffer.gather_all())\n",
        "  replay_buffer.clear()\n",
        "  regret_values.append(regret_metric.result())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG5VMgSlUqYS"
      },
      "source": [
        "では、結果を見てみましょう。すべてを正しく行ったのであれば、エージェントは線形報酬関数をうまく推定できるため、ポリシーは最適な報酬に近い期待報酬を持つアクションを選択できます。これは、上記で定義した後悔指標で示されており、この指標は 0 に向かって低下します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4DOYwhMSUVh"
      },
      "outputs": [],
      "source": [
        "plt.plot(regret_values)\n",
        "plt.title('Regret of LinUCB on the Linear per-arm environment')\n",
        "plt.xlabel('Number of Iterations')\n",
        "_ = plt.ylabel('Average Regret')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ZgHgOx5Ojq"
      },
      "source": [
        "### 今後の内容\n",
        "\n",
        "上記の例は、[ニューラル Epsilon-Greedy エージェント](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/agents/neural_epsilon_greedy_agent.py)など、他のエージェントからも選択できるコードベースに[実装](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/agents/examples/v2/train_eval_per_arm_stationary_linear.py)されています。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "t7ZgHgOx5Ojq"
      ],
      "name": "per_arm_bandits_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
