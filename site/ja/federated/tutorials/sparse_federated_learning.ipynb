{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a930wM_fqUNH"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Federated Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VxVUPYkahDa6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naLrzWx6FbQQ"
      },
      "source": [
        "# `federated_select` とスパースな集約によるクライアント効率の高い大規模なモデルの連合学習\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jM4S9YFXamd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/sparse_federated_learning\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     TensorFlow.org で表示</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/federated/tutorials/sparse_federated_learning.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Google Colab で実行</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/federated/tutorials/sparse_federated_learning.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     GitHubでソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/federated/tutorials/sparse_federated_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saL2ruE25Cx1"
      },
      "source": [
        "このチュートリアルでは、TFF で `tff.federated_select` とスパースな集約を使用して、非常に大規模なモデルをトレーニングする方法を実演します。各クライアントデバイスはモデルのごく一部のみをダウンロードおよび更新します。このチュートリアルは自己完結型ですが、ここで使用されるいくつかのテクニックの基本的な説明については、<a href=\"https://www.tensorflow.org/federated/tutorials/federated_select\" data-md-type=\"link\">`tff.federated_select` チュートリアル</a>と[カスタム連合学習アルゴリズムチュートリアル](https://www.tensorflow.org/federated/tutorials/building_your_own_federated_learning_algorithm)を参照してください。\n",
        "\n",
        "このチュートリアルでは、マルチラベル分類のロジスティック回帰を検討し、bag-of-words の特徴量表現に基づいて、どの「タグ」がテキスト文字列に関連付けられているかを予測します。重要なのは、通信とクライアント側の計算コストは固定定数 (`MAX_TOKENS_SELECTED_PER_CLIENT`) によって制御され、全体の語彙サイズ（実際の設定では非常に大きくなる可能性があります）に合わせて*スケーリングされない*ということです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg3OmMVWp_9D"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89APlnIQCvmp"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from collections.abc import Callable\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92wW8RDBvE9F"
      },
      "source": [
        "各クライアントは、最大でこの数の一意のトークンのモデルの重みの行を `federated_select` します。これにより、クライアントのローカルモデルのサイズと、実行されるサーバー-&gt;クライアント (`federated_select`) およびクライアント-&gt;サーバー `(federated_aggregate`) 通信の量が制限されます。\n",
        "\n",
        "これを 1（各クライアントからのすべてのトークンが選択されていないことを確認）または大きな値に設定しても、このチュートリアルは正しく実行されますが、モデルの収束が影響を受ける可能性があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkDumr_6bDtY"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS_SELECTED_PER_CLIENT = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th2PI1FpvaSL"
      },
      "source": [
        "また、さまざまな型の定数をいくつか定義します。このコラボの場合、**トークン**は、データセットを解析した後の特定の単語の整数識別子です。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyQ9xmIyvZFO"
      },
      "outputs": [],
      "source": [
        "# There are some constraints on types\n",
        "# here that will require some explicit type conversions:\n",
        "#    - `tff.federated_select` requires int32\n",
        "#    - `tf.SparseTensor` requires int64 indices.\n",
        "TOKEN_DTYPE = tf.int64\n",
        "SELECT_KEY_DTYPE = tf.int32\n",
        "\n",
        "# Type for counts of token occurences.\n",
        "TOKEN_COUNT_DTYPE = tf.int32\n",
        "\n",
        "# A sparse feature vector can be thought of as a map\n",
        "# from TOKEN_DTYPE to FEATURE_DTYPE. \n",
        "# Our features are {0, 1} indicators, so we could potentially\n",
        "# use tf.int8 as an optimization.\n",
        "FEATURE_DTYPE = tf.int32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ej2eY4-zk_a"
      },
      "source": [
        "# 問題の設定: データセットとモデル\n",
        "\n",
        "このチュートリアルでは、簡単に実験できるように小規模なトイデータセットを作成します。ただし、データセットの形式は [Federated StackOverflow](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data) と互換性があり、[前処理](https://github.com/google-research/federated/blob/0a558bac8a724fc38175ff4f0ce46c7af3d24be2/utils/datasets/stackoverflow_tag_prediction.py)と[モデルアーキテクチャ](https://github.com/google-research/federated/blob/49a43456aa5eaee3e1749855eed89c0087983541/utils/models/stackoverflow_lr_models.py)は、[*適応型連合最適化*](https://arxiv.org/abs/2003.00295) の StackOverflow タグ予測問題から採用されています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ofeMf1phYY"
      },
      "source": [
        "## データセットの解析と前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM9Fae0PXSfa"
      },
      "outputs": [],
      "source": [
        "NUM_OOV_BUCKETS = 1\n",
        "\n",
        "BatchType = collections.namedtuple('BatchType', ['tokens', 'tags'])\n",
        "\n",
        "def build_to_ids_fn(word_vocab: list[str],\n",
        "                    tag_vocab: list[str]) -> Callable[[tf.Tensor], tf.Tensor]:\n",
        "  \"\"\"Constructs a function mapping examples to sequences of token indices.\"\"\"\n",
        "  word_table_values = np.arange(len(word_vocab), dtype=np.int64)\n",
        "  word_table = tf.lookup.StaticVocabularyTable(\n",
        "      tf.lookup.KeyValueTensorInitializer(word_vocab, word_table_values),\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS)\n",
        "\n",
        "  tag_table_values = np.arange(len(tag_vocab), dtype=np.int64)\n",
        "  tag_table = tf.lookup.StaticVocabularyTable(\n",
        "      tf.lookup.KeyValueTensorInitializer(tag_vocab, tag_table_values),\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS)\n",
        "\n",
        "  def to_ids(example):\n",
        "    \"\"\"Converts a Stack Overflow example to a bag-of-words/tags format.\"\"\"\n",
        "    sentence = tf.strings.join([example['tokens'], example['title']],\n",
        "                               separator=' ')\n",
        "\n",
        "    # We represent that label (output tags) densely.\n",
        "    raw_tags = example['tags']\n",
        "    tags = tf.strings.split(raw_tags, sep='|')\n",
        "    tags = tag_table.lookup(tags)\n",
        "    tags, _ = tf.unique(tags)\n",
        "    tags = tf.one_hot(tags, len(tag_vocab) + NUM_OOV_BUCKETS)\n",
        "    tags = tf.reduce_max(tags, axis=0)\n",
        "\n",
        "    # We represent the features as a SparseTensor of {0, 1}s.\n",
        "    words = tf.strings.split(sentence)\n",
        "    tokens = word_table.lookup(words)\n",
        "    tokens, _ = tf.unique(tokens)\n",
        "    # Note:  We could choose to use the word counts as the feature vector\n",
        "    # instead of just {0, 1} values (see tf.unique_with_counts).\n",
        "    tokens = tf.reshape(tokens, shape=(tf.size(tokens), 1))\n",
        "    tokens_st = tf.SparseTensor(\n",
        "        tokens,\n",
        "        tf.ones(tf.size(tokens), dtype=FEATURE_DTYPE),\n",
        "        dense_shape=(len(word_vocab) + NUM_OOV_BUCKETS,))\n",
        "    tokens_st = tf.sparse.reorder(tokens_st)\n",
        "\n",
        "    return BatchType(tokens_st, tags)\n",
        "\n",
        "  return to_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv9NMKgbYki9"
      },
      "outputs": [],
      "source": [
        "def build_preprocess_fn(word_vocab, tag_vocab):\n",
        "\n",
        "  @tf.function\n",
        "  def preprocess_fn(dataset):\n",
        "    to_ids = build_to_ids_fn(word_vocab, tag_vocab)\n",
        "    # We *don't* shuffle in order to make this colab deterministic for\n",
        "    # easier testing and reproducibility.\n",
        "    # But real-world training should use `.shuffle()`.\n",
        "    return dataset.map(to_ids, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return preprocess_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxji1x30tZCi"
      },
      "source": [
        "## 小規模なトイデータセット\n",
        "\n",
        "12 語のグローバル語彙と 3 つのクライアントを使用して、小規模なトイデータセットを作成します。この小規模なサンプルは、エッジケースのテスト（たとえば、`MAX_TOKENS_SELECTED_PER_CLIENT = 6` 未満の個別のトークンを持つ 2 つのクライアントと、それ以上のトークンを持つ 1 つのクライアント）およびコードの開発に役立ちます。\n",
        "\n",
        "ただし、このアプローチの実際のユースケースでは、数千万以上のグローバル語彙であり、各クライアントに数千の異なるトークンが表示される可能性があります。データの形式が同じであるため、より現実的なテストベッドの問題への拡張（`tff.simulation.datasets.stackoverflow.load_data()`データセット）は簡単です。\n",
        "\n",
        "まず、単語とタグの語彙を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gtatbx4tdPo"
      },
      "outputs": [],
      "source": [
        "# Features\n",
        "FRUIT_WORDS = ['apple', 'orange', 'pear', 'kiwi']\n",
        "VEGETABLE_WORDS = ['carrot', 'broccoli', 'arugula', 'peas']\n",
        "FISH_WORDS = ['trout', 'tuna', 'cod', 'salmon']\n",
        "WORD_VOCAB = FRUIT_WORDS + VEGETABLE_WORDS + FISH_WORDS\n",
        "\n",
        "# Labels\n",
        "TAG_VOCAB = ['FRUIT', 'VEGETABLE', 'FISH']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQLblngP7gmJ"
      },
      "source": [
        "ここで、小規模なローカルデータセットをもつ 3 つのクライアントを作成します。このチュートリアルを colab で実行している場合は、以下で開発した関数の出力を解釈/確認するために、「タブ内のセルのミラーリング」機能を使用してこのセルとその出力を固定すると便利な場合があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr74BLPM1Mxa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word vocab\n",
            " 0 apple\n",
            " 1 orange\n",
            " 2 pear\n",
            " 3 kiwi\n",
            " 4 carrot\n",
            " 5 broccoli\n",
            " 6 arugula\n",
            " 7 peas\n",
            " 8 trout\n",
            " 9 tuna\n",
            "10 cod\n",
            "11 salmon\n",
            "\n",
            "Tag vocab\n",
            " 0 FRUIT\n",
            " 1 VEGETABLE\n",
            " 2 FISH\n"
          ]
        }
      ],
      "source": [
        "preprocess_fn = build_preprocess_fn(WORD_VOCAB, TAG_VOCAB)\n",
        "\n",
        "\n",
        "def make_dataset(raw):\n",
        "  d = tf.data.Dataset.from_tensor_slices(\n",
        "      # Matches the StackOverflow formatting\n",
        "      collections.OrderedDict(\n",
        "          tokens=tf.constant([t[0] for t in raw]),\n",
        "          tags=tf.constant([t[1] for t in raw]),\n",
        "          title=['' for _ in raw]))\n",
        "  d = preprocess_fn(d)\n",
        "  return d\n",
        "\n",
        "\n",
        "# 4 distinct tokens\n",
        "CLIENT1_DATASET = make_dataset([\n",
        "    ('apple orange apple orange', 'FRUIT'),\n",
        "    ('carrot trout', 'VEGETABLE|FISH'),\n",
        "    ('orange apple', 'FRUIT'),\n",
        "    ('orange', 'ORANGE|CITRUS')  # 2 OOV tag\n",
        "])\n",
        "\n",
        "# 6 distinct tokens\n",
        "CLIENT2_DATASET = make_dataset([\n",
        "    ('pear cod', 'FRUIT|FISH'),\n",
        "    ('arugula peas', 'VEGETABLE'),\n",
        "    ('kiwi pear', 'FRUIT'),\n",
        "    ('sturgeon', 'FISH'),  # OOV word\n",
        "    ('sturgeon bass', 'FISH')  # 2 OOV words\n",
        "])\n",
        "\n",
        "# A client with all possible words & tags (13 distinct tokens).\n",
        "# With MAX_TOKENS_SELECTED_PER_CLIENT = 6, we won't download the model\n",
        "# slices for all tokens that occur on this client.\n",
        "CLIENT3_DATASET = make_dataset([\n",
        "    (' '.join(WORD_VOCAB + ['oovword']), '|'.join(TAG_VOCAB)),\n",
        "    # Mathe the OOV token and 'salmon' occur in the largest number\n",
        "    # of examples on this client:\n",
        "    ('salmon oovword', 'FISH|OOVTAG')\n",
        "])\n",
        "\n",
        "print('Word vocab')\n",
        "for i, word in enumerate(WORD_VOCAB):\n",
        "  print(f'{i:2d} {word}')\n",
        "\n",
        "print('\\nTag vocab')\n",
        "for i, tag in enumerate(TAG_VOCAB):\n",
        "  print(f'{i:2d} {tag}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH3T6M0f8Bh2"
      },
      "source": [
        "入力フィーチャ（トークン/単語）とラベル（ポストタグ）の生の数の定数を定義します。OOV トークン/タグを追加するため、実際の入出力スペースは `NUM_OOV_BUCKETS = 1` 大きくなります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBULpAepxloO"
      },
      "outputs": [],
      "source": [
        "NUM_WORDS = len(WORD_VOCAB) \n",
        "NUM_TAGS = len(TAG_VOCAB)\n",
        "\n",
        "WORD_VOCAB_SIZE = NUM_WORDS + NUM_OOV_BUCKETS\n",
        "TAG_VOCAB_SIZE = NUM_TAGS + NUM_OOV_BUCKETS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zF6NF_f8PQR"
      },
      "source": [
        "データセットのバッチバージョンと個々のバッチを作成します。これは、コードのテストに役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uxRQAYdJISP"
      },
      "outputs": [],
      "source": [
        "batched_dataset1 = CLIENT1_DATASET.batch(2)\n",
        "batched_dataset2 = CLIENT2_DATASET.batch(3)\n",
        "batched_dataset3 = CLIENT3_DATASET.batch(2)\n",
        "\n",
        "batch1 = next(iter(batched_dataset1))\n",
        "batch2 = next(iter(batched_dataset2))\n",
        "batch3 = next(iter(batched_dataset3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBcObjtlaq8q"
      },
      "source": [
        "## スパース入力でモデルを定義する\n",
        "\n",
        "タグごとに単純な独立ロジスティック回帰モデルを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTtIkxCKao2Y"
      },
      "outputs": [],
      "source": [
        "def create_logistic_model(word_vocab_size: int, vocab_tags_size: int):\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(word_vocab_size,), sparse=True),\n",
        "      tf.keras.layers.Dense(\n",
        "          vocab_tags_size,\n",
        "          activation='sigmoid',\n",
        "          kernel_initializer=tf.keras.initializers.zeros,\n",
        "          # For simplicity, don't use a bias vector; this means the model\n",
        "          # is a single tensor, and we only need sparse aggregation of\n",
        "          # the per-token slices of the model. Generalizing to also handle\n",
        "          # other model weights that are fully updated \n",
        "          # (non-dense broadcast and aggregate) would be a good exercise.\n",
        "          use_bias=False),\n",
        "  ])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCMScjuWn9t-"
      },
      "source": [
        "まず、予測を行って、それが機能することを確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGQOnvbfa3w4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]]\n"
          ]
        }
      ],
      "source": [
        "model = create_logistic_model(WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\n",
        "p = model.predict(batch1.tokens)\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOT2veElT-ij"
      },
      "source": [
        "そして、いくつかの簡単な集中トレーニングを実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn3lB84WgRgV"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy())\n",
        "model.train_on_batch(batch1.tokens, batch1.tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1aahpVomC4G"
      },
      "source": [
        "# 連合計算のビルディングブロック\n",
        "\n",
        "[連合平均化](https://arxiv.org/abs/1602.05629)アルゴリズムの単純なバージョンを実装しますが、重要な違いは、各デバイスはモデルの関連するサブセットのみをダウンロードし、そのサブセットへの更新のみを提供するということです。\n",
        "\n",
        "`MAX_TOKENS_SELECTED_PER_CLIENT` の省略形として `M` を使用します。上位レベルでは、1 ラウンドのトレーニングには次の手順が含まれます。\n",
        "\n",
        "1. 参加している各クライアントは、ローカルデータセットをスキャンし、入力文字列を解析して、正しいトークン（int インデックス）にマッピングします。これには、グローバル（大規模）ディクショナリへのアクセスが必要です（これは、[機能ハッシュ](https://en.wikipedia.org/wiki/Feature_hashing)手法を使用して回避できる可能性があります）。次に、各トークンが発生する回数をスパースにカウントします。`U` の一意のトークンがデバイスで発生する場合、トレーニングする `num_actual_tokens = min(U, M)` の最も頻繁なトークンを選択します。\n",
        "\n",
        "2. クライアントは `federated_select` を使用して、サーバーから `num_actual_tokens` で選択されたトークンのモデル係数を取得します。各モデルスライスは形状 `(TAG_VOCAB_SIZE, )` のテンソルであるため、クライアントに送信されるデータの合計は最大でサイズ `TAG_VOCAB_SIZE * M` になります（以下の注意事項を参照）。\n",
        "\n",
        "3. クライアントは、マッピング `global_token -> local_token` を作成します。ローカルトークン（int index）は、選択されたトークンのリスト内のグローバルトークンのインデックスです。\n",
        "\n",
        "4. クライアントは、範囲 `[0, num_actual_tokens)` から最大 `M` トークンの係数のみを持つグローバルモデルの「小規模な」バージョンを使用します。`global -> local` マッピングは、選択したモデルスライスからこのモデルの密なパラメータを初期化するために使用されます。\n",
        "\n",
        "5. クライアントは、`global -> local` マッピングで前処理されたデータに対して SGD を使用してローカルモデルをトレーニングします。\n",
        "\n",
        "6. クライアントは、`local -> global` マッピングを使用して行にインデックスを付けることにより、ローカルモデルのパラメータを `IndexedSlices` 更新に変換します。サーバーは、スパースな集約を使用してこれらの更新を集約します。\n",
        "\n",
        "7. サーバーは、上記の集約の（密な）結果を取得し、それを参加しているクライアントの数で除算し、結果の平均更新をグローバルモデルに適用します。\n",
        "\n",
        "このセクションでは、これらのステップの構成要素を構築します。これらの構成要素は、1 つ のトレーニングラウンドの完全なロジックをキャプチャする最終的な `federated_computation` に結合されます。\n",
        "\n",
        "> 注意: 上記に説明されていない 1 つの技術的な詳細があります。`federated_select` とローカルモデルの構築では、静的に既知の形状が必要であるため、動的なクライアントごとの `num_actual_tokens` サイズを使用できません。代わりに、静的な値 `M` を使用し、必要に応じてパディングを追加します。これは、アルゴリズムのセマンティクスには影響しません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrB3RWrCYHd_"
      },
      "source": [
        "### クライアントトークンをカウントし、`federated_select` にスライスするモデルを決定する\n",
        "\n",
        "各デバイスは、モデルのどの「スライス」がローカルトレーニングデータセットに関連しているかを判断する必要があります。ここでは、クライアントトレーニングデータセットの各トークンを含むサンプルの数を（スパースに）カウントします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRr0ip0ja31O"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def token_count_fn(token_counts, batch):\n",
        "  \"\"\"Adds counts from `batch` to the running `token_counts` sum.\"\"\"\n",
        "  # Sum across the batch dimension.\n",
        "  flat_tokens = tf.sparse.reduce_sum(\n",
        "      batch.tokens, axis=0, output_is_sparse=True)\n",
        "  flat_tokens = tf.cast(flat_tokens, dtype=TOKEN_COUNT_DTYPE)\n",
        "  return tf.sparse.add(token_counts, flat_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTZdZZhFgeYz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: [0 1 4 8]\n",
            "counts: [2 3 1 1]\n"
          ]
        }
      ],
      "source": [
        "# Simple tests\n",
        "# Create the initial zero token counts using empty tensors.\n",
        "initial_token_counts = tf.SparseTensor(\n",
        "    indices=tf.zeros(shape=(0, 1), dtype=TOKEN_DTYPE),\n",
        "    values=tf.zeros(shape=(0,), dtype=TOKEN_COUNT_DTYPE),\n",
        "    dense_shape=(WORD_VOCAB_SIZE,))\n",
        "\n",
        "client_token_counts = batched_dataset1.reduce(initial_token_counts,\n",
        "                                              token_count_fn)\n",
        "tokens = tf.reshape(client_token_counts.indices, (-1,)).numpy()\n",
        "print('tokens:', tokens)\n",
        "np.testing.assert_array_equal(tokens, [0, 1, 4, 8])\n",
        "# The count is the number of *examples* in which the token/word\n",
        "# occurs, not the total number of occurences, since we still featurize\n",
        "# multiple occurences in the same example as a \"1\".\n",
        "counts = client_token_counts.values.numpy()\n",
        "print('counts:', counts)\n",
        "np.testing.assert_array_equal(counts, [2, 3, 1, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ghfTtZgce7"
      },
      "source": [
        "デバイスで最も頻繁に発生する `MAX_TOKENS_SELECTED_PER_CLIENT` トークンに対応するモデルパラメータを選択します。デバイスで発生するトークンの数がこれより少ない場合は、リストを埋めて　`federated_select` を使用できるようにします。\n",
        "\n",
        "（発生確率に基づいて）トークンをランダムに選択するなど、他の戦略の方がおそらく優れていることに注意してください。これにより、（クライアントがデータを持っている）モデルのすべてのスライスが更新されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45YOfL8fh5K8"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def keys_for_client(client_dataset, max_tokens_per_client):\n",
        "  \"\"\"Computes a set of max_tokens_per_client keys.\"\"\"\n",
        "  initial_token_counts = tf.SparseTensor(\n",
        "      indices=tf.zeros((0, 1), dtype=TOKEN_DTYPE),\n",
        "      values=tf.zeros((0,), dtype=TOKEN_COUNT_DTYPE),\n",
        "      dense_shape=(WORD_VOCAB_SIZE,))\n",
        "  client_token_counts = client_dataset.reduce(initial_token_counts,\n",
        "                                              token_count_fn)\n",
        "  # Find the most-frequently occuring tokens\n",
        "  tokens = tf.reshape(client_token_counts.indices, shape=(-1,))\n",
        "  counts = client_token_counts.values\n",
        "  perm = tf.argsort(counts, direction='DESCENDING')\n",
        "  tokens = tf.gather(tokens, perm)\n",
        "  counts = tf.gather(counts, perm)\n",
        "  num_raw_tokens = tf.shape(tokens)[0]\n",
        "  actual_num_tokens = tf.minimum(max_tokens_per_client, num_raw_tokens)\n",
        "  selected_tokens = tokens[:actual_num_tokens]\n",
        "  paddings = [[0, max_tokens_per_client - tf.shape(selected_tokens)[0]]]\n",
        "  padded_tokens = tf.pad(selected_tokens, paddings=paddings)\n",
        "  # Make sure the type is statically determined\n",
        "  padded_tokens = tf.reshape(padded_tokens, shape=(max_tokens_per_client,))\n",
        "\n",
        "  # We will pass these tokens as keys into `federated_select`, which\n",
        "  # requires SELECT_KEY_DTYPE=tf.int32 keys.\n",
        "  padded_tokens = tf.cast(padded_tokens, dtype=SELECT_KEY_DTYPE)\n",
        "  return padded_tokens, actual_num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTyJVDqkjGyG"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "\n",
        "# Case 1: actual_num_tokens > max_tokens_per_client\n",
        "selected_tokens, actual_num_tokens = keys_for_client(batched_dataset1, 3)\n",
        "assert tf.size(selected_tokens) == 3\n",
        "assert actual_num_tokens == 3\n",
        "\n",
        "# Case 2: actual_num_tokens < max_tokens_per_client\n",
        "selected_tokens, actual_num_tokens = keys_for_client(batched_dataset1, 10)\n",
        "assert tf.size(selected_tokens) == 10\n",
        "assert actual_num_tokens == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO0pJ8D-dzp7"
      },
      "source": [
        "### グローバルトークンをローカルトークンにマップする\n",
        "\n",
        "上記の選択により、オンデバイスモデルに使用する `[0, actual_num_tokens)` の範囲の密なトークンのセットが得られます。ただし、読み取ったデータセットには、はるかに大きなグローバル語彙範囲 `[0, WORD_VOCAB_SIZE)` のトークンが含まれています。\n",
        "\n",
        "したがって、グローバルトークンを対応するローカルトークンにマップする必要があります。ローカルトークン ID は、前の手順で計算された `selected_tokens` テンソルへのインデックスによって与えられます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG3uLlqJohXg"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def map_to_local_token_ids(client_data, client_keys):\n",
        "  global_to_local = tf.lookup.StaticHashTable(\n",
        "      # Note int32 -> int64 maps are not supported\n",
        "      tf.lookup.KeyValueTensorInitializer(\n",
        "          keys=tf.cast(client_keys, dtype=TOKEN_DTYPE),\n",
        "          # Note we need to use tf.shape, not the static \n",
        "          # shape client_keys.shape[0]\n",
        "          values=tf.range(0, limit=tf.shape(client_keys)[0],\n",
        "                          dtype=TOKEN_DTYPE)),\n",
        "      # We use -1 for tokens that were not selected, which can occur for clients\n",
        "      # with more than MAX_TOKENS_SELECTED_PER_CLIENT distinct tokens.\n",
        "      # We will simply remove these invalid indices from the batch below.\n",
        "      default_value=-1)\n",
        "\n",
        "  def to_local_ids(sparse_tokens):\n",
        "    indices_t = tf.transpose(sparse_tokens.indices)\n",
        "    batch_indices = indices_t[0]  # First column\n",
        "    tokens = indices_t[1]  # Second column\n",
        "    tokens = tf.map_fn(\n",
        "        lambda global_token_id: global_to_local.lookup(global_token_id), tokens)\n",
        "    # Remove tokens that aren't actually available (looked up as -1):\n",
        "    available_tokens = tokens >= 0\n",
        "    tokens = tokens[available_tokens]\n",
        "    batch_indices = batch_indices[available_tokens]\n",
        "\n",
        "    updated_indices = tf.transpose(\n",
        "        tf.concat([[batch_indices], [tokens]], axis=0))\n",
        "    st = tf.sparse.SparseTensor(\n",
        "        updated_indices,\n",
        "        tf.ones(tf.size(tokens), dtype=FEATURE_DTYPE),\n",
        "        # Each client has at most MAX_TOKENS_SELECTED_PER_CLIENT distinct tokens.\n",
        "        dense_shape=[sparse_tokens.dense_shape[0], MAX_TOKENS_SELECTED_PER_CLIENT])\n",
        "    st = tf.sparse.reorder(st)\n",
        "    return st\n",
        "\n",
        "  return client_data.map(lambda b: BatchType(to_local_ids(b.tokens), b.tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Cn60NIBUWf"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "client_keys, actual_num_tokens = keys_for_client(\n",
        "    batched_dataset3, MAX_TOKENS_SELECTED_PER_CLIENT)\n",
        "client_keys = client_keys[:actual_num_tokens]\n",
        "\n",
        "d = map_to_local_token_ids(batched_dataset3, client_keys)\n",
        "batch  = next(iter(d))\n",
        "all_tokens = tf.gather(batch.tokens.indices, indices=1, axis=1)\n",
        "# Confirm we have local indices in the range [0, MAX):\n",
        "assert tf.math.reduce_max(all_tokens) < MAX_TOKENS_SELECTED_PER_CLIENT\n",
        "assert tf.math.reduce_max(all_tokens) >= 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nvZYfzga1yN"
      },
      "source": [
        "### 各クライアントでローカル（サブ）モデルをトレーニングする\n",
        "\n",
        "注意: `federated_select` は、選択したスライスを、選択キーと同じ順序で `tf.data.Dataset`として返します。したがって、最初に、そのようなデータセットを取得し、それをクライアントモデルのモデルの重みとして使用できる単一の密なテンソルに変換する効用関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFpC_nFjgbtI"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def slices_dataset_to_tensor(slices_dataset):\n",
        "  \"\"\"Convert a dataset of slices to a tensor.\"\"\"\n",
        "  # Use batching to gather all of the slices into a single tensor.\n",
        "  d = slices_dataset.batch(MAX_TOKENS_SELECTED_PER_CLIENT,\n",
        "                           drop_remainder=False)\n",
        "  iter_d = iter(d)\n",
        "  tensor = next(iter_d)\n",
        "  # Make sure we have consumed everything\n",
        "  opt = iter_d.get_next_as_optional()\n",
        "  tf.Assert(tf.logical_not(opt.has_value()), data=[''], name='CHECK_EMPTY')\n",
        "  return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX0gtz5Ylvqf"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "weights = np.random.random(\n",
        "    size=(MAX_TOKENS_SELECTED_PER_CLIENT, TAG_VOCAB_SIZE)).astype(np.float32)\n",
        "model_slices_as_dataset = tf.data.Dataset.from_tensor_slices(weights)\n",
        "weights2 = slices_dataset_to_tensor(model_slices_as_dataset)\n",
        "np.testing.assert_array_equal(weights, weights2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft0ROTYl0laV"
      },
      "source": [
        "各クライアントで実行される単純なローカルトレーニングループを定義するために必要なすべてのコンポーネントが揃いました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG66d8UR_9Gm"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def client_train_fn(model, client_optimizer,\n",
        "                    model_slices_as_dataset, client_data,\n",
        "                    client_keys, actual_num_tokens):\n",
        "  \n",
        "  initial_model_weights = slices_dataset_to_tensor(model_slices_as_dataset)\n",
        "  assert len(model.trainable_variables) == 1\n",
        "  model.trainable_variables[0].assign(initial_model_weights)\n",
        "\n",
        "  # Only keep the \"real\" (unpadded) keys.\n",
        "  client_keys = client_keys[:actual_num_tokens]\n",
        " \n",
        "  client_data = map_to_local_token_ids(client_data, client_keys)\n",
        "\n",
        "  loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "  for features, labels in client_data:\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(features)\n",
        "      loss = loss_fn(labels, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    client_optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  model_weights_delta = model.trainable_weights[0] - initial_model_weights\n",
        "  model_weights_delta = tf.slice(model_weights_delta, begin=[0, 0], \n",
        "                           size=[actual_num_tokens, -1])\n",
        "  return client_keys, model_weights_delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XOe0thb2TQr"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "# Note if you execute this cell a second time, you need to also re-execute\n",
        "# the preceeding cell to avoid \"tf.function-decorated function tried to \n",
        "# create variables on non-first call\" errors.\n",
        "on_device_model = create_logistic_model(MAX_TOKENS_SELECTED_PER_CLIENT,\n",
        "                                        TAG_VOCAB_SIZE)\n",
        "client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "client_keys, actual_num_tokens = keys_for_client(\n",
        "    batched_dataset2, MAX_TOKENS_SELECTED_PER_CLIENT)\n",
        "\n",
        "model_slices_as_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    np.zeros((MAX_TOKENS_SELECTED_PER_CLIENT, TAG_VOCAB_SIZE),\n",
        "             dtype=np.float32))\n",
        "\n",
        "keys, delta = client_train_fn(\n",
        "    on_device_model,\n",
        "    client_optimizer,\n",
        "    model_slices_as_dataset,\n",
        "    client_data=batched_dataset3,\n",
        "    client_keys=client_keys,\n",
        "    actual_num_tokens=actual_num_tokens)\n",
        "\n",
        "print(delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJFTBczKSYH"
      },
      "source": [
        "### IndexedSlices を集約する\n",
        "\n",
        "`tff.federated_aggregate` を使用して、`IndexedSlices` のスパースな連合集約を作成します。この単純な実装には、`density_shape` が事前に静的に認識されているという制約があります。また、この集約は、*セミスパース*（クライアント-&gt;サーバー通信がスパース）ですが、サーバーは `accumulate` および`merge` で集約の密な表現を維持し、この密な表現を出力していることにも注意してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIG_XYd2OYeO"
      },
      "outputs": [],
      "source": [
        "def federated_indexed_slices_sum(slice_indices, slice_values, dense_shape):\n",
        "  \"\"\"\n",
        "  Sums IndexedSlices@CLIENTS to a dense @SERVER Tensor.\n",
        "\n",
        "  Intermediate aggregation is performed by converting to a dense representation,\n",
        "  which may not be suitable for all applications.\n",
        "\n",
        "  Args:\n",
        "    slice_indices: An IndexedSlices.indices tensor @CLIENTS.\n",
        "    slice_values: An IndexedSlices.values tensor @CLIENTS.\n",
        "    dense_shape: A statically known dense shape.\n",
        "\n",
        "  Returns:\n",
        "    A dense tensor placed @SERVER representing the sum of the client's\n",
        "    IndexedSclies.\n",
        "  \"\"\"\n",
        "  slices_dtype = slice_values.type_signature.member.dtype\n",
        "  zero = tff.tf_computation(\n",
        "      lambda: tf.zeros(dense_shape, dtype=slices_dtype))()\n",
        "\n",
        "  @tf.function\n",
        "  def accumulate_slices(dense, client_value):\n",
        "    indices, slices = client_value\n",
        "    # There is no built-in way to add `IndexedSlices`, but \n",
        "    # tf.convert_to_tensor is a quick way to convert to a dense representation\n",
        "    # so we can add them.\n",
        "    return dense + tf.convert_to_tensor(\n",
        "        tf.IndexedSlices(slices, indices, dense_shape))\n",
        "\n",
        "\n",
        "  return tff.federated_aggregate(\n",
        "      (slice_indices, slice_values),\n",
        "      zero=zero,\n",
        "      accumulate=tff.tf_computation(accumulate_slices),\n",
        "      merge=tff.tf_computation(lambda d1, d2: tf.add(d1, d2, name='merge')),\n",
        "      report=tff.tf_computation(lambda d: d))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "769qwlLhKFA9"
      },
      "source": [
        "テストとして最小限の `federated_computation` を作成します"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_FIvwj3UIAq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({<int64[?],float32[?,2]>}@CLIENTS -> float32[6,2]@SERVER)\n"
          ]
        }
      ],
      "source": [
        "dense_shape = (6, 2)\n",
        "indices_type = tff.TensorType(tf.int64, (None,))\n",
        "values_type = tff.TensorType(tf.float32, (None, 2))\n",
        "client_slice_type = tff.type_at_clients(\n",
        "    (indices_type, values_type))\n",
        "\n",
        "@tff.federated_computation(client_slice_type)\n",
        "def test_sum_indexed_slices(indices_values_at_client):\n",
        "  indices, values = indices_values_at_client\n",
        "  return federated_indexed_slices_sum(indices, values, dense_shape)\n",
        "\n",
        "print(test_sum_indexed_slices.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dulcCZg2S_9f"
      },
      "outputs": [],
      "source": [
        "x = tf.IndexedSlices(\n",
        "    values=np.array([[2., 2.1], [0., 0.1], [1., 1.1], [5., 5.1]],\n",
        "                    dtype=np.float32),\n",
        "    indices=[2, 0, 1, 5],\n",
        "    dense_shape=dense_shape)\n",
        "y = tf.IndexedSlices(\n",
        "    values=np.array([[0., 0.3], [3.1, 3.2]], dtype=np.float32),\n",
        "    indices=[1, 3],\n",
        "    dense_shape=dense_shape)\n",
        "\n",
        "# Sum one.\n",
        "result = test_sum_indexed_slices([(x.indices, x.values)])\n",
        "np.testing.assert_array_equal(tf.convert_to_tensor(x), result)\n",
        "\n",
        "# Sum two.\n",
        "expected = [[0., 0.1], [1., 1.4], [2., 2.1], [3.1, 3.2], [0., 0.], [5., 5.1]]\n",
        "result = test_sum_indexed_slices([(x.indices, x.values), (y.indices, y.values)])\n",
        "np.testing.assert_array_almost_equal(expected, result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sXg8fDQhwB7"
      },
      "source": [
        "# `federated_computation` に全てをまとめる\n",
        "\n",
        "TFF を使用して、コンポーネントを `tff.federated_computation` にまとめます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yRpGJr_i3XK"
      },
      "outputs": [],
      "source": [
        "DENSE_MODEL_SHAPE = (WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\n",
        "client_data_type = tff.SequenceType(batched_dataset1.element_spec)\n",
        "model_type = tff.TensorType(tf.float32, shape=DENSE_MODEL_SHAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITawWBbodTdE"
      },
      "source": [
        "連合平均化に基づく基本的なサーバートレーニング関数を使用し、サーバー学習率 1.0 で更新を適用します。クライアント提供のモデルを単純に平均化するのではなく、モデルに更新（デルタ）を適用することが重要です。そうでなければ、モデルの特定のスライスが特定のラウンドでどのクライアントによってもトレーニングされていない場合、その係数はゼロになる可能性があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGgKTpnscdZI"
      },
      "outputs": [],
      "source": [
        "@tff.tf_computation\n",
        "def server_update(current_model_weights, update_sum, num_clients):\n",
        "  average_update = update_sum / num_clients\n",
        "  return current_model_weights + average_update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSFAmgzqBRHA"
      },
      "source": [
        "さらにいくつかの `tff.tf_computation` の要素が必要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soFQAmktUtYp"
      },
      "outputs": [],
      "source": [
        "# Function to select slices from the model weights in federated_select:\n",
        "select_fn = tff.tf_computation(\n",
        "    lambda model_weights, index: tf.gather(model_weights, index))\n",
        "\n",
        "\n",
        "# We need to wrap `client_train_fn` as a `tff.tf_computation`, making\n",
        "# sure we do any operations that might construct `tf.Variable`s outside\n",
        "# of the `tf.function` we are wrapping.\n",
        "@tff.tf_computation\n",
        "def client_train_fn_tff(model_slices_as_dataset, client_data, client_keys,\n",
        "                        actual_num_tokens):\n",
        "  # Note this is amaller than the global model, using\n",
        "  # MAX_TOKENS_SELECTED_PER_CLIENT which is much smaller than WORD_VOCAB_SIZE.\n",
        "  # We would like a model of size `actual_num_tokens`, but we\n",
        "  # can't build the model dynamically, so we will slice off the padded\n",
        "  # weights at the end.\n",
        "  client_model = create_logistic_model(MAX_TOKENS_SELECTED_PER_CLIENT,\n",
        "                                       TAG_VOCAB_SIZE)\n",
        "  client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "  return client_train_fn(client_model, client_optimizer,\n",
        "                         model_slices_as_dataset, client_data, client_keys,\n",
        "                         actual_num_tokens)\n",
        "\n",
        "@tff.tf_computation\n",
        "def keys_for_client_tff(client_data):\n",
        "  return keys_for_client(client_data, MAX_TOKENS_SELECTED_PER_CLIENT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftI2fCGNBJQ-"
      },
      "source": [
        "以上ですべての要素をまとめる準備が整いました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM_2mzkgBOjl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<server_model=float32[13,4]@SERVER,client_data={<tokens=<indices=int64[?,2],values=int32[?],dense_shape=int64[2]>,tags=float32[?,4]>*}@CLIENTS> -> float32[13,4]@SERVER)\n"
          ]
        }
      ],
      "source": [
        "@tff.federated_computation(\n",
        "    tff.type_at_server(model_type), tff.type_at_clients(client_data_type))\n",
        "def sparse_model_update(server_model, client_data):\n",
        "  max_tokens = tff.federated_value(MAX_TOKENS_SELECTED_PER_CLIENT, tff.SERVER)\n",
        "  keys_at_clients, actual_num_tokens = tff.federated_map(\n",
        "      keys_for_client_tff, client_data)\n",
        "\n",
        "  model_slices = tff.federated_select(keys_at_clients, max_tokens, server_model,\n",
        "                                      select_fn)\n",
        "\n",
        "  update_keys, update_slices = tff.federated_map(\n",
        "      client_train_fn_tff,\n",
        "      (model_slices, client_data, keys_at_clients, actual_num_tokens))\n",
        "\n",
        "  dense_update_sum = federated_indexed_slices_sum(update_keys, update_slices,\n",
        "                                                  DENSE_MODEL_SHAPE)\n",
        "  num_clients = tff.federated_sum(tff.federated_value(1.0, tff.CLIENTS))\n",
        "\n",
        "  updated_server_model = tff.federated_map(\n",
        "      server_update, (server_model, dense_update_sum, num_clients))\n",
        "\n",
        "  return updated_server_model\n",
        "\n",
        "\n",
        "print(sparse_model_update.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSvveGwdZFCO"
      },
      "source": [
        "# モデルをトレーニングしましょう\n",
        "\n",
        "トレーニング関数ができたので、試してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn_V-E3FdrLO"
      },
      "outputs": [],
      "source": [
        "server_model = create_logistic_model(WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\n",
        "server_model.compile(  # Compile to make evaluation easy.\n",
        "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.0),  # Unused\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[ \n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.AUC(name='auc'),\n",
        "      tf.keras.metrics.Recall(top_k=2, name='recall_at_2'),\n",
        "  ])\n",
        "\n",
        "def evaluate(model, dataset, name):\n",
        "  metrics = model.evaluate(dataset, verbose=0)\n",
        "  metrics_str = ', '.join([f'{k}={v:.2f}' for k, v in \n",
        "                          (zip(server_model.metrics_names, metrics))])\n",
        "  print(f'{name}: {metrics_str}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw7S9PmfZ3Bw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before training\n",
            "Client 1: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.60\n",
            "Client 2: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.50\n",
            "Client 3: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.40\n",
            "Training on clients [0 1]\n",
            "Training on clients [0 2 1]\n",
            "Training on clients [2 0]\n",
            "Training on clients [1 0 2]\n",
            "Training on clients [2]\n",
            "Training on clients [2 0]\n",
            "Training on clients [1 2 0]\n",
            "Training on clients [0]\n",
            "Training on clients [2]\n",
            "Training on clients [1 2]\n",
            "After training\n",
            "Client 1: loss=0.67, precision=0.80, auc=0.91, recall_at_2=0.80\n",
            "Client 2: loss=0.68, precision=0.67, auc=0.96, recall_at_2=1.00\n",
            "Client 3: loss=0.65, precision=1.00, auc=0.93, recall_at_2=0.80\n"
          ]
        }
      ],
      "source": [
        "print('Before training')\n",
        "evaluate(server_model, batched_dataset1, 'Client 1')\n",
        "evaluate(server_model, batched_dataset2, 'Client 2')\n",
        "evaluate(server_model, batched_dataset3, 'Client 3')\n",
        "\n",
        "model_weights = server_model.trainable_weights[0]\n",
        "\n",
        "client_datasets = [batched_dataset1, batched_dataset2, batched_dataset3]\n",
        "for _ in range(10):  # Run 10 rounds of FedAvg\n",
        "  # We train on 1, 2, or 3 clients per round, selecting\n",
        "  # randomly.\n",
        "  cohort_size = np.random.randint(1, 4)\n",
        "  clients = np.random.choice([0, 1, 2], cohort_size, replace=False)\n",
        "  print('Training on clients', clients)\n",
        "  model_weights = sparse_model_update(\n",
        "      model_weights, [client_datasets[i] for i in clients])\n",
        "server_model.set_weights([model_weights])\n",
        "\n",
        "print('After training')\n",
        "evaluate(server_model, batched_dataset1, 'Client 1')\n",
        "evaluate(server_model, batched_dataset2, 'Client 2')\n",
        "evaluate(server_model, batched_dataset3, 'Client 3')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sparse_federated_learning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
