{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcfrhafzkZbH"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# 量子化認識トレーニングの総合ガイド"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/model_optimization/guide/quantization/training_comprehensive_guide.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/model_optimization/guide/quantization/training_comprehensive_guide.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHubでソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/model_optimization/guide/quantization/training_comprehensive_guide.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbORZA_bQx1G"
      },
      "source": [
        "Keras 量子化認識トレーニングの総合ガイドへようこそ。\n",
        "\n",
        "このページでは、さまざまなユースケースを示し、それぞれで API を使用する方法を説明します。どの API が必要であるかを特定したら、[API ドキュメント](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization)でパラメータと詳細を確認してください。\n",
        "\n",
        "- 量子化認識トレーニングのメリットとサポート対象を確認するには、「[概要](https://www.tensorflow.org/model_optimization/guide/quantization/training.md)」をご覧ください。\n",
        "- 単一のエンドツーエンドの例については、「[量子化認識トレーニングの例](https://www.tensorflow.org/model_optimization/guide/quantization/training_example.md)」をご覧ください。\n",
        "\n",
        "次のユースケースについて説明しています。\n",
        "\n",
        "- 以下のステップで、8 ビット量子化でモデルをデプロイします。\n",
        "    - 量子化認識モデルを定義します。\n",
        "    - Keras HDF5 モデルの場合のみ、特殊なチェックポイント設定と逆シリアル化ロジックを使用します。そうでない場合、トレーニングは標準どおりです。\n",
        "    - 量子化認識モデルから量子化モデルを作成します。\n",
        "- 量子化による実験。\n",
        "    - すべての実験には、デプロイをサポートするパスはありません。\n",
        "    - カスタム Keras レイヤーは実験に該当します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuABqZnXVDvO"
      },
      "source": [
        "## MNIST モデルをビルドする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqnbd7TOfAq9"
      },
      "source": [
        "必要な API の特定と目的の理解については、次を実行できますが、このセクションを読まずに進むことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "lvpH1Hg7ULFz"
      },
      "outputs": [],
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip install -q tf-nightly\n",
        "! pip install -q tensorflow-model-optimization\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "import tempfile\n",
        "\n",
        "input_shape = [20]\n",
        "x_train = np.random.randn(1, 20).astype(np.float32)\n",
        "y_train = tf.keras.utils.to_categorical(np.random.randn(1), num_classes=20)\n",
        "\n",
        "def setup_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(20, input_shape=input_shape),\n",
        "      tf.keras.layers.Flatten()\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def setup_pretrained_weights():\n",
        "  model= setup_model()\n",
        "\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer='adam',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  model.fit(x_train, y_train)\n",
        "\n",
        "  _, pretrained_weights = tempfile.mkstemp('.tf')\n",
        "\n",
        "  model.save_weights(pretrained_weights)\n",
        "\n",
        "  return pretrained_weights\n",
        "\n",
        "def setup_pretrained_model():\n",
        "  model = setup_model()\n",
        "  pretrained_weights = setup_pretrained_weights()\n",
        "  model.load_weights(pretrained_weights)\n",
        "  return model\n",
        "\n",
        "setup_model()\n",
        "pretrained_weights = setup_pretrained_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTHLMLV-ZrUA"
      },
      "source": [
        "##量子化認識モデルを定義する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U6XAUhIe6re"
      },
      "source": [
        "以下のようにしてモデルを定義した場合、バックエンドにデプロイするために使用できるパスが[概要ページ](https://www.tensorflow.org/model_optimization/guide/quantization/training.md)に一覧表示されています。デフォルトでは、8 ビット量子化が使用されます。\n",
        "\n",
        "注意: 量子化認識モデルは、実際には量子化されていません。量子化モデルの作成は別のステップで行われます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybigft1fTn4T"
      },
      "source": [
        "### モデル全体を量子化する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puZvqnp1xsn-"
      },
      "source": [
        "**ユースケース:**\n",
        "\n",
        "- サブクラス化モデルはサポートされていません。\n",
        "\n",
        "**モデルの精度を高めるための<strong>ヒント</strong>:**\n",
        "\n",
        "- 精度を最も低下させるレイヤーの量子化を省略する場合は、「一部のレイヤーを量子化する」を試してください。\n",
        "- 一般的に、ゼロからトレーニングするのではなく、量子化認識トレーニングでファインチューニングする方が優れています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zhzx_azO1WR"
      },
      "source": [
        "モデル全体を量子化認識にするには、モデルに `tfmot.quantization.keras.quantize_model` を適用します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s_EK8reOruu"
      },
      "outputs": [],
      "source": [
        "base_model = setup_model()\n",
        "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
        "\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
        "quant_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTbTLn3dZM7h"
      },
      "source": [
        "### 一部のレイヤーを量子化する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbM8o832xTxV"
      },
      "source": [
        "モデルを量子化すると、精度に悪影響が及ぶことがあります。モデルのレイヤーを選択的に量子化すると、精度、速度、およびモデルサイズ間のトレードオフを調べることができます。\n",
        "\n",
        "**ユースケース:**\n",
        "\n",
        "- 完全に量子化されたモデル（EdgeTPU v1、ほとんどの DSP など）でのみ機能するバックエンドをデプロイするには、「モデル全体を量子化する」をご覧ください。\n",
        "\n",
        "**モデルの精度を高めるための<strong>ヒント</strong>:**\n",
        "\n",
        "- 一般的に、ゼロからトレーニングするのではなく、量子化認識トレーニングでファインチューニングする方が優れています。\n",
        "- 最初のレイヤーではなく、後のレイヤーを量子化してみてください。\n",
        "- クリティカルレイヤー（注意機構など）を量子化しないようにしてください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OCbOUWHsE_v"
      },
      "source": [
        "以下の例では、`Dense` レイヤーのみを量子化しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN0B_QB-ZhE2"
      },
      "outputs": [],
      "source": [
        "# Create a base model\n",
        "base_model = setup_model()\n",
        "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
        "\n",
        "# Helper function uses `quantize_annotate_layer` to annotate that only the \n",
        "# Dense layers should be quantized.\n",
        "def apply_quantization_to_dense(layer):\n",
        "  if isinstance(layer, tf.keras.layers.Dense):\n",
        "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "  return layer\n",
        "\n",
        "# Use `tf.keras.models.clone_model` to apply `apply_quantization_to_dense` \n",
        "# to the layers of the model.\n",
        "annotated_model = tf.keras.models.clone_model(\n",
        "    base_model,\n",
        "    clone_function=apply_quantization_to_dense,\n",
        ")\n",
        "\n",
        "# Now that the Dense layers are annotated,\n",
        "# `quantize_apply` actually makes the model quantization aware.\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "quant_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiA28PrrW11H"
      },
      "source": [
        "この例では量子化するものを決定するためにレイヤーの種類が使用されていますが、特定のレイヤーを量子化する上で最も簡単な方法は、`name` プロパティを設定し、`clone_function` でその名前を探す方法です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjY_JyB808Da"
      },
      "outputs": [],
      "source": [
        "print(base_model.layers[0].name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpb_BydRaSoF"
      },
      "source": [
        "#### 可読性を高められても、モデルの精度を潜在的に低下させる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vqXeYffzSHp"
      },
      "source": [
        "これは、量子化認識トレーニングによるファインチューニングとは使用できないため、上記の例よりも精度に劣る可能性があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQoMH3g3fWwb"
      },
      "source": [
        "**Functional の例**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wow55hg5oiM"
      },
      "outputs": [],
      "source": [
        "# Use `quantize_annotate_layer` to annotate that the `Dense` layer\n",
        "# should be quantized.\n",
        "i = tf.keras.Input(shape=(20,))\n",
        "x = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10))(i)\n",
        "o = tf.keras.layers.Flatten()(x)\n",
        "annotated_model = tf.keras.Model(inputs=i, outputs=o)\n",
        "\n",
        "# Use `quantize_apply` to actually make the model quantization aware.\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "# For deployment purposes, the tool adds `QuantizeLayer` after `InputLayer` so that the\n",
        "# quantized model can take in float inputs instead of only uint8.\n",
        "quant_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIGj-r2of2ls"
      },
      "source": [
        "**Sequential の例**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQOiDUGgfi4y"
      },
      "outputs": [],
      "source": [
        "# Use `quantize_annotate_layer` to annotate that the `Dense` layer\n",
        "# should be quantized.\n",
        "annotated_model = tf.keras.Sequential([\n",
        "  tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=input_shape)),\n",
        "  tf.keras.layers.Flatten()\n",
        "])\n",
        "\n",
        "# Use `quantize_apply` to actually make the model quantization aware.\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "quant_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpvX5IqahV1r"
      },
      "source": [
        "## チェックポイントと逆シリアル化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuZ5wlij1dcJ"
      },
      "source": [
        "**ユースケース:** このコードは、HDF5 モデル形式のみで必要です（HDF5 重みまたはその他の形式では不要です）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6khQg-q7imfH"
      },
      "outputs": [],
      "source": [
        "# Define the model.\n",
        "base_model = setup_model()\n",
        "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
        "\n",
        "# Save or checkpoint the model.\n",
        "_, keras_model_file = tempfile.mkstemp('.h5')\n",
        "quant_aware_model.save(keras_model_file)\n",
        "\n",
        "# `quantize_scope` is needed for deserializing HDF5 models.\n",
        "with tfmot.quantization.keras.quantize_scope():\n",
        "  loaded_model = tf.keras.models.load_model(keras_model_file)\n",
        "\n",
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeNCMDAbnEKU"
      },
      "source": [
        "## 量子化モデルを作成してデプロイする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiYk_KR0rJ2n"
      },
      "source": [
        "通常は、使用するデプロイバックエンドのドキュメントを参照してください。\n",
        "\n",
        "これは、TFLite バックエンドの例です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbBiEetda3R8"
      },
      "outputs": [],
      "source": [
        "base_model = setup_pretrained_model()\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
        "\n",
        "# Typically you train the model here.\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5raSy9ghxkv"
      },
      "source": [
        "## 量子化による実験"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUGpXIET0cy3"
      },
      "source": [
        "**ユースケース**: 以下の API を使用した場合、サポートされているデプロイパスはありません。たとえば、TFLite 変換とカーネル実装は 8 ビット量子化のみをサポートしています。特徴は実験的であり、下位互換性はありません。\n",
        "\n",
        "- `tfmot.quantization.keras.QuantizeConfig`\n",
        "- `tfmot.quantization.keras.quantizers.Quantizer`\n",
        "- `tfmot.quantization.keras.quantizers.LastValueQuantizer`\n",
        "- `tfmot.quantization.keras.quantizers.MovingAverageQuantizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1KI_FCcU7Yn"
      },
      "source": [
        "### セットアップ: DefaultDenseQuantizeConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6nPkJDRUB2G"
      },
      "source": [
        "実験には、レイヤーの重み、活性化、および出力を量子化する方法を記述した `tfmot.quantization.keras.QuantizeConfig` を使用する必要があります。\n",
        "\n",
        "以下は、API デフォルトで `Dense` レイヤーに使用されるのと同じ `QuantizeConfig` を定義する例です。\n",
        "\n",
        "この例では順伝播中に、`get_weights_and_quantizers` に返される `LastValueQuantizer` が入力として `layer.kernel` に呼び出され、出力を生成します。この出力は元の `Dense` レイヤーの順伝播で `set_quantize_weights` に定義されたロジックに従って `layer.kernel` に置き換えられます。これと同じ考え方が活性化と出力に適用されます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9SWK5UQT7VQ"
      },
      "outputs": [],
      "source": [
        "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
        "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
        "\n",
        "class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
        "    # Configure how to quantize weights.\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n",
        "\n",
        "    # Configure how to quantize activations.\n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "      return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
        "\n",
        "    def set_quantize_weights(self, layer, quantize_weights):\n",
        "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
        "      # , in the same order\n",
        "      layer.kernel = quantize_weights[0]\n",
        "\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
        "      # , in the same order.\n",
        "      layer.activation = quantize_activations[0]\n",
        "\n",
        "    # Configure how to quantize outputs (may be equivalent to activations).\n",
        "    def get_output_quantizers(self, layer):\n",
        "      return []\n",
        "\n",
        "    def get_config(self):\n",
        "      return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vJeoGQG9ZX0"
      },
      "source": [
        "### カスタム Keras レイヤーを量子化する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmyhI_bzWb2w"
      },
      "source": [
        "この例では、`DefaultDenseQuantizeConfig` を使用して `CustomLayer` を量子化します。\n",
        "\n",
        "構成の適用は、「量子化による実験」のユースケースと同じです。\n",
        "\n",
        "- `tfmot.quantization.keras.quantize_annotate_layer` を `CustomLayer` に適用し、`QuantizeConfig` を渡します。\n",
        "- `tfmot.quantization.keras.quantize_annotate_model` を使用して、API のデフォルトでモデルの残りを量子化し続けます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_rBOJdyWWEs"
      },
      "outputs": [],
      "source": [
        "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
        "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
        "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
        "\n",
        "class CustomLayer(tf.keras.layers.Dense):\n",
        "  pass\n",
        "\n",
        "model = quantize_annotate_model(tf.keras.Sequential([\n",
        "   quantize_annotate_layer(CustomLayer(20, input_shape=(20,)), DefaultDenseQuantizeConfig()),\n",
        "   tf.keras.layers.Flatten()\n",
        "]))\n",
        "\n",
        "# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
        "# as well as the custom Keras layer.\n",
        "with quantize_scope(\n",
        "  {'DefaultDenseQuantizeConfig': DefaultDenseQuantizeConfig,\n",
        "   'CustomLayer': CustomLayer}):\n",
        "  # Use `quantize_apply` to actually make the model quantization aware.\n",
        "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
        "\n",
        "quant_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnMguvVSnUqD"
      },
      "source": [
        "### 量子化パラメータを変更する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLgH1aFMjTK4"
      },
      "source": [
        "**一般的な過ち:** 32 ビット未満にバイアスを量子化すると、通常、モデルの精度を著しく悪化させてしまいます。\n",
        "\n",
        "この例は、`Dense` レイヤーが重みにデフォルトの 8 ビットではなく 4 ビットを使用するように変更します。モデルの残りは、API のデフォルトを使用し続けます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77jgBjccnTh6"
      },
      "outputs": [],
      "source": [
        "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
        "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
        "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
        "\n",
        "class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n",
        "    # Configure weights to quantize with 4-bit instead of 8-bits.\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "      return [(layer.kernel, LastValueQuantizer(num_bits=4, symmetric=True, narrow_range=False, per_axis=False))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9JDKhaU3FKe"
      },
      "source": [
        "構成の適用は、「量子化による実験」のユースケースと同じです。\n",
        "\n",
        "- `tfmot.quantization.keras.quantize_annotate_layer` を `Dense` レイヤーに適用し、`QuantizeConfig` を渡します。\n",
        "- `tfmot.quantization.keras.quantize_annotate_model` を使用して、API のデフォルトでモデルの残りを量子化し続けます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq5mfyBF3KxV"
      },
      "outputs": [],
      "source": [
        "model = quantize_annotate_model(tf.keras.Sequential([\n",
        "   # Pass in modified `QuantizeConfig` to modify this Dense layer.\n",
        "   quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=(20,)), ModifiedDenseQuantizeConfig()),\n",
        "   tf.keras.layers.Flatten()\n",
        "]))\n",
        "\n",
        "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
        "with quantize_scope(\n",
        "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
        "  # Use `quantize_apply` to actually make the model quantization aware.\n",
        "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
        "\n",
        "quant_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJMKgzh84CCs"
      },
      "source": [
        "### 量子化するレイヤーの一部を変更する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3pij2uO808g"
      },
      "source": [
        "この例は、`Dense` レイヤーが活性化の量子化をスキップするように変更します。モデルの残りは、API のデフォルトを使用し続けます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BaaJPBR8djV"
      },
      "outputs": [],
      "source": [
        "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
        "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
        "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
        "\n",
        "class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "      # Skip quantizing activations.\n",
        "      return []\n",
        "\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "      # Empty since `get_activaations_and_quantizers` returns\n",
        "      # an empty list.\n",
        "      return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OkqHX5r2nT7"
      },
      "source": [
        "構成の適用は、「量子化による実験」のユースケースと同じです。\n",
        "\n",
        "- `tfmot.quantization.keras.quantize_annotate_layer` を `Dense` レイヤーに適用し、`QuantizeConfig` を渡します。\n",
        "- `tfmot.quantization.keras.quantize_annotate_model` を使用して、API のデフォルトでモデルの残りを量子化し続けます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln9MDIZJ2n3F"
      },
      "outputs": [],
      "source": [
        "model = quantize_annotate_model(tf.keras.Sequential([\n",
        "   # Pass in modified `QuantizeConfig` to modify this Dense layer.\n",
        "   quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=(20,)), ModifiedDenseQuantizeConfig()),\n",
        "   tf.keras.layers.Flatten()\n",
        "]))\n",
        "\n",
        "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
        "with quantize_scope(\n",
        "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
        "  # Use `quantize_apply` to actually make the model quantization aware.\n",
        "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
        "\n",
        "quant_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD0sIR6tmmRx"
      },
      "source": [
        "### カスタム量子化アルゴリズムを使用する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4onhF-H1zsn"
      },
      "source": [
        "`tfmot.quantization.keras.quantizers.Quantizer` クラスは、入力に任意のアルゴリズムを適用できるコーラブルです。\n",
        "\n",
        "この例では、入力は重みであり、`FixedRangeQuantizer` **call** 関数の計算を重みに適用します。元の重みの値の代わりに、`FixedRangeQuantizer` の出力が、その重みを使用したものすべてに渡されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt8UioZH49QV"
      },
      "outputs": [],
      "source": [
        "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
        "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
        "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
        "\n",
        "class FixedRangeQuantizer(tfmot.quantization.keras.quantizers.Quantizer):\n",
        "  \"\"\"Quantizer which forces outputs to be between -1 and 1.\"\"\"\n",
        "\n",
        "  def build(self, tensor_shape, name, layer):\n",
        "    # Not needed. No new TensorFlow variables needed.\n",
        "    return {}\n",
        "\n",
        "  def __call__(self, inputs, training, weights, **kwargs):\n",
        "    return tf.keras.backend.clip(inputs, -1.0, 1.0)\n",
        "\n",
        "  def get_config(self):\n",
        "    # Not needed. No __init__ parameters to serialize.\n",
        "    return {}\n",
        "\n",
        "\n",
        "class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n",
        "    # Configure weights to quantize with 4-bit instead of 8-bits.\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "      # Use custom algorithm defined in `FixedRangeQuantizer` instead of default Quantizer.\n",
        "      return [(layer.kernel, FixedRangeQuantizer())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu5ZeJ_Y2UxW"
      },
      "source": [
        "構成の適用は、「量子化による実験」のユースケースと同じです。\n",
        "\n",
        "- `tfmot.quantization.keras.quantize_annotate_layer` を `Dense` レイヤーに適用し、`QuantizeConfig` を渡します。\n",
        "- `tfmot.quantization.keras.quantize_annotate_model` を使用して、API のデフォルトでモデルの残りを量子化し続けます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItC_3mwT2U87"
      },
      "outputs": [],
      "source": [
        "model = quantize_annotate_model(tf.keras.Sequential([\n",
        "   # Pass in modified `QuantizeConfig` to modify this `Dense` layer.\n",
        "   quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=(20,)), ModifiedDenseQuantizeConfig()),\n",
        "   tf.keras.layers.Flatten()\n",
        "]))\n",
        "\n",
        "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
        "with quantize_scope(\n",
        "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
        "  # Use `quantize_apply` to actually make the model quantization aware.\n",
        "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
        "\n",
        "quant_aware_model.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "training_comprehensive_guide.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
