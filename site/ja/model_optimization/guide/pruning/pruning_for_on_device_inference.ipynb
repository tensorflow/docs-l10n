{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKT-NezBJ4Nd"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DwBljPxTJ4Ng"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIb8SDeKJ4Nh"
      },
      "source": [
        "# XNNPACK を使用したオンデバイスの推論のプルーニング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX1fje9OJ4Ni"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/model_optimization/guide/pruning/pruning_for_on_device_inference\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     TensorFlow.org で表示</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/model_optimization/guide/pruning/pruning_for_on_device_inference.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colabで実行</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/model_optimization/guide/pruning/pruning_for_on_device_inference.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     GitHubでソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/model_optimization/guide/pruning/pruning_for_on_device_inference.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uCVQVRMJ4Nj"
      },
      "source": [
        "[XNNPACK](https://github.com/google/XNNPACK) を使用したオンデバイスの推論のレイテンシーを改善するための Keras 重みプルーニングのガイドへようこそ。\n",
        "\n",
        "このガイドでは、新しく導入された `tfmot.sparsity.keras.PruningPolicy` API と[XNNPACK スパース推論](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#sparse-inference)を使用して最新の CPU で畳み込みモデルを高速化する方法を実演します。\n",
        "\n",
        "このガイドでは、モデル作成プロセスの次の手順について説明します。\n",
        "\n",
        "- 密なベースラインを構築してトレーニングする\n",
        "- プルーニングを使ってモデルを微調整する\n",
        "- TFLite に変換する\n",
        "- オンデバイスのベンチマーク\n",
        "\n",
        "このガイドでは、プルーニングを使用したモデルの微調整のベストプラクティスについては説明しません。このトピックの詳細については、[包括的なガイド](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md)をご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF5sWYUZJ4Nk"
      },
      "source": [
        "## セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re0qdmOAJ4Nk"
      },
      "outputs": [],
      "source": [
        "! pip install -q tensorflow\n",
        "! pip install -q tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIn7sB8-J4Nk"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOeRh5hvJ4Nl"
      },
      "source": [
        "## 密なモデルを構築してトレーニングする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNxUSmleJ4Nl"
      },
      "source": [
        "[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) データセットの分類タスク用に、単純なベースライン CNN を構築してトレーニングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws4cmZCJJ4Nm"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR10 dataset.\n",
        "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
        "    'cifar10',\n",
        "    split=['train[:90%]', 'train[90%:]', 'test'],\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.image.convert_image_dtype(image, tf.float32), label\n",
        "\n",
        "# Load the data in batches of 128 images.\n",
        "batch_size = 128\n",
        "def prepare_dataset(ds, buffer_size=None):\n",
        "  ds = ds.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  ds = ds.cache()\n",
        "  if buffer_size:\n",
        "    ds = ds.shuffle(buffer_size)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return ds\n",
        "\n",
        "ds_train = prepare_dataset(ds_train,\n",
        "                           buffer_size=ds_info.splits['train'].num_examples)\n",
        "ds_val = prepare_dataset(ds_val)\n",
        "ds_test = prepare_dataset(ds_test)\n",
        "\n",
        "# Build the dense baseline model.\n",
        "dense_model = keras.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
        "    keras.layers.ZeroPadding2D(padding=1),\n",
        "    keras.layers.Conv2D(\n",
        "        filters=8,\n",
        "        kernel_size=(3, 3),\n",
        "        strides=(2, 2),\n",
        "        padding='valid'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.Conv2D(filters=16, kernel_size=(1, 1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.ZeroPadding2D(padding=1),\n",
        "    keras.layers.DepthwiseConv2D(\n",
        "        kernel_size=(3, 3), strides=(2, 2), padding='valid'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.Conv2D(filters=32, kernel_size=(1, 1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.GlobalAveragePooling2D(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Compile and train the dense model for 10 epochs.\n",
        "dense_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "dense_model.fit(\n",
        "  ds_train,\n",
        "  epochs=10,\n",
        "  validation_data=ds_val)\n",
        "\n",
        "# Evaluate the dense model.\n",
        "_, dense_model_accuracy = dense_model.evaluate(ds_test, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32h7h4YYJ4Nn"
      },
      "source": [
        "## 疎なモデルを構築する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXAXjJcuJ4Nn"
      },
      "source": [
        "[包括的なガイド](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md)の手順を使用して、`tfmot.sparsity.keras.prune_low_magnitude` 関数を適用します。パラメータは、プルーニングによるオンデバイスの高速化（`tfmot.sparsity.keras.PruneForLatencyOnXNNPack`）を対象とします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1WQt5dmJ4Nn"
      },
      "outputs": [],
      "source": [
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after after 5 epochs.\n",
        "end_epoch = 5\n",
        "\n",
        "num_iterations_per_epoch = len(ds_train)\n",
        "end_step =  num_iterations_per_epoch * end_epoch\n",
        "\n",
        "# Define parameters for pruning.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.25,\n",
        "                                                               final_sparsity=0.75,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step),\n",
        "      'pruning_policy': tfmot.sparsity.keras.PruneForLatencyOnXNNPack()\n",
        "}\n",
        "\n",
        "# Try to apply pruning wrapper with pruning policy parameter.\n",
        "try:\n",
        "  model_for_pruning = prune_low_magnitude(dense_model, **pruning_params)\n",
        "except ValueError as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzDggTmYJ4No"
      },
      "source": [
        "`prune_low_magnitude` を呼び出すと、`ValueError` になり、`Could not find a GlobalAveragePooling2D layer with keepdims = True in all output branches` というメッセージが表示されます。このメッセージは、モデルのプルーニングがポリシー `tfmot.sparsity.keras.PruneForLatencyOnXNNPack` でサポートされていないことを示しています。具体的には、レイヤー `GlobalAveragePooling2D` にはパラメータ `keepdims = True` が必要です。修正して、`prune_low_magnitude` 関数を再度適用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvALAbZeJ4No"
      },
      "outputs": [],
      "source": [
        "fixed_dense_model = keras.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
        "    keras.layers.ZeroPadding2D(padding=1),\n",
        "    keras.layers.Conv2D(\n",
        "        filters=8,\n",
        "        kernel_size=(3, 3),\n",
        "        strides=(2, 2),\n",
        "        padding='valid'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.Conv2D(filters=16, kernel_size=(1, 1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.ZeroPadding2D(padding=1),\n",
        "    keras.layers.DepthwiseConv2D(\n",
        "        kernel_size=(3, 3), strides=(2, 2), padding='valid'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.Conv2D(filters=32, kernel_size=(1, 1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.GlobalAveragePooling2D(keepdims=True),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Use the pretrained model for pruning instead of training from scratch.\n",
        "fixed_dense_model.set_weights(dense_model.get_weights())\n",
        "\n",
        "# Try to reapply pruning wrapper.\n",
        "model_for_pruning = prune_low_magnitude(fixed_dense_model, **pruning_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51K8A0XCJ4Np"
      },
      "source": [
        "`prune_low_magnitude` の呼び出すとエラーは発生しません。つまり、モデルは `tfmot.sparsity.keras.PruneForLatencyOnXNNPack` ポリシーで完全にサポートされており、[XNNPACK スパース推論](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#sparse-inference)を使用して高速化できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG4yrdNUJ4Np"
      },
      "source": [
        "### 疎なモデルの微調整"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqhqM7AtJ4Np"
      },
      "source": [
        "[プルーニングの例](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras.md)に従って、密なモデルの重みを使用して疎なモデルを微調整します。25％ のスパース性（重みの 25％ がゼロに設定されている）でモデルの微調整を開始し、75％ のスパース性で終了します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzdS6AgRJ4Np"
      },
      "outputs": [],
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "model_for_pruning.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.fit(\n",
        "  ds_train,\n",
        "  epochs=15,\n",
        "  validation_data=ds_val,\n",
        "  callbacks=callbacks)\n",
        "\n",
        "# Evaluate the dense model.\n",
        "_, pruned_model_accuracy = model_for_pruning.evaluate(ds_test, verbose=0)\n",
        "\n",
        "print('Dense model test accuracy:', dense_model_accuracy)\n",
        "print('Pruned model test accuracy:', pruned_model_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeI9QSIMJ4Nq"
      },
      "source": [
        "ログには、レイヤーごとのスパース性の進行状況が示されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGDkSxKJJ4Nq"
      },
      "outputs": [],
      "source": [
        "#docs_infra: no_execute\n",
        "%tensorboard --logdir={logdir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hJ5PX_y7psT"
      },
      "source": [
        "プルーニングによる微調整後、テストの精度は、密なモデルと比較して適度に改善（43％ から 44％）しました。[TFLite ベンチマーク](https://www.tensorflow.org/lite/performance/measurement)を使用してオンデバイスのレイテンシを比較してみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xCRpxczJ4Nq"
      },
      "source": [
        "## モデルの変換とベンチマーク\n",
        "\n",
        "プルーニングされたモデルを TFLite に変換するには、`strip_pruning` 関数を使用して、`PruneLowMagnitude` ラッパーを元のレイヤーに置き換える必要があります。また、プルーニングされたモデル（`model_for_pruning`）の重みはほとんどゼロであるため、最適化 `tf.lite.Optimize.EXPERIMENTAL_SPARSITY` を適用して、結果の TFLite モデルを効率的に保存できます。この最適化フラグは、密なモデルには必要ありません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAJr2XKCJ4Nr"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(dense_model)\n",
        "dense_tflite_model = converter.convert()\n",
        "\n",
        "_, dense_tflite_file = tempfile.mkstemp('.tflite')\n",
        "with open(dense_tflite_file, 'wb') as f:\n",
        "  f.write(dense_tflite_model)\n",
        "\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY]\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLClpRYq7psT"
      },
      "source": [
        "[TFLite モデルベンチマークツール](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark)の指示に従って、ツールを作成し、高密度でプルーニングされた TFLite モデルと共に Android デバイスにアップロードし、デバイスで両方のモデルをベンチマークします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qamJwAM7psU"
      },
      "outputs": [],
      "source": [
        "! adb shell /data/local/tmp/benchmark_model \\\n",
        "    --graph=/data/local/tmp/dense_model.tflite \\\n",
        "    --use_xnnpack=true \\\n",
        "    --num_runs=100 \\\n",
        "    --num_threads=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpTxyOcd7psU"
      },
      "outputs": [],
      "source": [
        "! adb shell /data/local/tmp/benchmark_model \\\n",
        "    --graph=/data/local/tmp/pruned_model.tflite \\\n",
        "    --use_xnnpack=true \\\n",
        "    --num_runs=100 \\\n",
        "    --num_threads=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKOVVoYD7psU"
      },
      "source": [
        "Pixel 4 のベンチマークでは、平均推論時間は密なモデルでは *17us*、プルーニングされたモデルでは *12us* でした。オンデバイスのベンチマークは、このような小さなモデルでも、レイテンシが明らかに **5us**（**30％**）改善されていることを示しています。私たちの経験では、[ MobileNetV3](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3) や [EfficientNet-lite](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite) に基づくと、より大きなモデルでも同様のパフォーマンス向上が示されます。スピードアップは、モデル全体に対する 1x1 畳み込みの相対的な寄与に基づいて異なります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqL_Nhuw7psU"
      },
      "source": [
        "## 結論"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Icj3vV7psU"
      },
      "source": [
        "このチュートリアルでは、TF MOT API と XNN Pack の新機能を使用して、オンデバイスのパフォーマンスを高速化するために疎なモデルを作成する方法を実演しました 。これらの疎なモデルは、密なモデルよりも小さく、高速ですが、品質は密のモデルと同等または上回る場合もあります。\n",
        "\n",
        "この新しい機能は、モデルをデバイスにデプロイする際に特に重要となります。ぜひ試してみてください。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pruning_for_on_device_inference.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
