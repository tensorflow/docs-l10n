{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACbjNjyO4f_8"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCM50vaM4jiK"
      },
      "outputs": [],
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qOVy-_vmuUP"
      },
      "source": [
        "# 最近傍とテキスト埋め込みによるセマンティック検索\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/semantic_approximate_nearest_neighbors\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/hub/tutorials/semantic_approximate_nearest_neighbors.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a>   </td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/hub/tutorials/semantic_approximate_nearest_neighbors.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub で表示</a></td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/hub/tutorials/semantic_approximate_nearest_neighbors.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a>   </td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/universal-sentence-encoder/2\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub モデルを参照</a>   </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hks9F5qq6m2"
      },
      "source": [
        "このチュートリアルでは、[TensorFlow Hub](https://tfhub.dev)（TF-Hub）が提供する入力データから埋め込みを生成し、抽出された埋め込みを使用して最近傍（ANN）インデックスを構築する方法を説明します。構築されたインデックスは、リアルタイムに類似性の一致と検索を行うために使用できます。\n",
        "\n",
        "大規模なコーパスのデータを取り扱う場合、特定のクエリに対して最も類似するアイテムをリアルタイムで見つけるために、レポジトリ全体をスキャンして完全一致を行うというのは、効率的ではありません。そのため、おおよその類似性一致アルゴリズムを使用することで、正確な最近傍の一致を見つける際の精度を少しだけ犠牲にし、速度を大幅に向上させることができます。\n",
        "\n",
        "このチュートリアルでは、ニュースの見出しのコーパスに対してリアルタイムテキスト検索を行い、クエリに最も類似する見出しを見つけ出す例を示します。この検索はキーワード検索とは異なり、テキスト埋め込みにエンコードされた意味的類似性をキャプチャします。\n",
        "\n",
        "このチュートリアルの手順は次のとおりです。\n",
        "\n",
        "1. サンプルデータをダウンロードする。\n",
        "2. TF-Hub モジュールを使用して、データの埋め込みを生成する。\n",
        "3. 埋め込みの ANN インデックスを構築する。\n",
        "4. インデックスを使って、類似性の一致を実施する。\n",
        "\n",
        "TF-Hub モジュールから埋め込みを生成するには、[TensorFlow Transform](https://beam.apache.org/documentation/programming-guide/)（TF-Transform）を使った [Apache Beam](https://www.tensorflow.org/tfx/tutorials/transform/simple) を使用します。また、最近傍インデックスの構築には、Spotify の [ANNOY](https://github.com/spotify/annoy) ライブラリを使用します。ANN フレームワークのベンチマークは、こちらの [Github リポジトリ](https://github.com/erikbern/ann-benchmarks)をご覧ください。\n",
        "\n",
        "このチュートリアルでは TensorFlow 1.0 を使用し、TF1 の [Hub モジュール](https://www.tensorflow.org/hub/tf1_hub_module)のみと連携します。更新版は、[このチュートリアルの TF2 バージョン](https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_semantic_approximate_nearest_neighbors.ipynb)をご覧ください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0jr0QK9qO5P"
      },
      "source": [
        "## セットアップ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whMRj9qeqed4"
      },
      "source": [
        "必要なライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmXkLPoaqS--"
      },
      "outputs": [],
      "source": [
        "!pip install -q apache_beam\n",
        "!pip install -q 'scikit_learn~=0.23.0'  # For gaussian_random_matrix.\n",
        "!pip install -q annoy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-vBZiCCqld0"
      },
      "source": [
        "必要なライブラリをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NTYbdWcseuK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import apache_beam as beam\n",
        "import annoy\n",
        "from sklearn.random_projection import gaussian_random_matrix\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GF0GnLqGdPQ"
      },
      "outputs": [],
      "source": [
        "# TFT needs to be installed afterwards\n",
        "!pip install -q tensorflow_transform==0.24\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx0SZa6-7b-f"
      },
      "outputs": [],
      "source": [
        "print('TF version: {}'.format(tf.__version__))\n",
        "print('TF-Hub version: {}'.format(hub.__version__))\n",
        "print('TF-Transform version: {}'.format(tft.__version__))\n",
        "print('Apache Beam version: {}'.format(beam.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Imq876rLWx"
      },
      "source": [
        "## 1. サンプルデータをダウンロードする\n",
        "\n",
        "[A Million News Headlines](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SYBGZL#) データセットには、15 年にわたって発行されたニュースの見出しが含まれます。出典は、有名なオーストラリア放送協会（ABC）です。このニュースデータセットは、2003 年の始めから 2017 年の終わりまでの特筆すべき世界的なイベントについて、オーストラリアにより焦点を当てた記録が含まれます。\n",
        "\n",
        "**形式**: 1）発行日と 2）見出しのテキストの 2 列をタブ区切りにしたデータ。このチュートリアルで関心があるのは、見出しのテキストのみです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpF57n8e5C9D"
      },
      "outputs": [],
      "source": [
        "!wget 'https://dataverse.harvard.edu/api/access/datafile/3450625?format=tab&gbrecs=true' -O raw.tsv\n",
        "!wc -l raw.tsv\n",
        "!head raw.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reeoc9z0zTxJ"
      },
      "source": [
        "単純化するため、見出しのテキストのみを維持し、発行日は削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INPWa4upv_yJ"
      },
      "outputs": [],
      "source": [
        "!rm -r corpus\n",
        "!mkdir corpus\n",
        "\n",
        "with open('corpus/text.txt', 'w') as out_file:\n",
        "  with open('raw.tsv', 'r') as in_file:\n",
        "    for line in in_file:\n",
        "      headline = line.split('\\t')[1].strip().strip('\"')\n",
        "      out_file.write(headline+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-oedX40z6o2"
      },
      "outputs": [],
      "source": [
        "!tail corpus/text.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls0Zh7kYz3PM"
      },
      "source": [
        "## TF-Hub モジュールを読み込むためのヘルパー関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSt_jmyKz3Xp"
      },
      "outputs": [],
      "source": [
        "def load_module(module_url):\n",
        "  embed_module = hub.Module(module_url)\n",
        "  placeholder = tf.placeholder(dtype=tf.string)\n",
        "  embed = embed_module(placeholder)\n",
        "  session = tf.Session()\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  print('TF-Hub module is loaded.')\n",
        "\n",
        "  def _embeddings_fn(sentences):\n",
        "    computed_embeddings = session.run(\n",
        "        embed, feed_dict={placeholder: sentences})\n",
        "    return computed_embeddings\n",
        "\n",
        "  return _embeddings_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AngMtH50jNb"
      },
      "source": [
        "## 2. データの埋め込みを生成する\n",
        "\n",
        "このチュートリアルでは、[ユニバーサルセンテンスエンコーダ](https://tfhub.dev/google/universal-sentence-encoder/2)を使用して、見出しデータの埋め込みを生成します。その後で、文章レベルの意味の類似性を計算するために、文章埋め込みを簡単に使用することが可能となります。埋め込み生成プロセスは、Apache Beam と TF-Transform を使用して実行します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_DvXnDB1pEX"
      },
      "source": [
        "### 埋め込み抽出メソッド"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL7OEY1E0A35"
      },
      "outputs": [],
      "source": [
        "encoder = None\n",
        "\n",
        "def embed_text(text, module_url, random_projection_matrix):\n",
        "  # Beam will run this function in different processes that need to\n",
        "  # import hub and load embed_fn (if not previously loaded)\n",
        "  global encoder\n",
        "  if not encoder:\n",
        "    encoder = hub.Module(module_url)\n",
        "  embedding = encoder(text)\n",
        "  if random_projection_matrix is not None:\n",
        "    # Perform random projection for the embedding\n",
        "    embedding = tf.matmul(\n",
        "        embedding, tf.cast(random_projection_matrix, embedding.dtype))\n",
        "  return embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_don5gXy9D59"
      },
      "source": [
        "### TFT preprocess_fn メソッドの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwYlrzzK9ECE"
      },
      "outputs": [],
      "source": [
        "def make_preprocess_fn(module_url, random_projection_matrix=None):\n",
        "  '''Makes a tft preprocess_fn'''\n",
        "\n",
        "  def _preprocess_fn(input_features):\n",
        "    '''tft preprocess_fn'''\n",
        "    text = input_features['text']\n",
        "    # Generate the embedding for the input text\n",
        "    embedding = embed_text(text, module_url, random_projection_matrix)\n",
        "    \n",
        "    output_features = {\n",
        "        'text': text, \n",
        "        'embedding': embedding\n",
        "        }\n",
        "        \n",
        "    return output_features\n",
        "  \n",
        "  return _preprocess_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ492LN7A-NZ"
      },
      "source": [
        "### データセットのメタデータの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2D4332VA-2V"
      },
      "outputs": [],
      "source": [
        "def create_metadata():\n",
        "  '''Creates metadata for the raw data'''\n",
        "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "  from tensorflow_transform.tf_metadata import schema_utils\n",
        "  feature_spec = {'text': tf.FixedLenFeature([], dtype=tf.string)}\n",
        "  schema = schema_utils.schema_from_feature_spec(feature_spec)\n",
        "  metadata = dataset_metadata.DatasetMetadata(schema)\n",
        "  return metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zlSLPzRBm6H"
      },
      "source": [
        "### Beam パイプライン"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCGUIB172m2G"
      },
      "outputs": [],
      "source": [
        "def run_hub2emb(args):\n",
        "  '''Runs the embedding generation pipeline'''\n",
        "\n",
        "  options = beam.options.pipeline_options.PipelineOptions(**args)\n",
        "  args = namedtuple(\"options\", args.keys())(*args.values())\n",
        "\n",
        "  raw_metadata = create_metadata()\n",
        "  converter = tft.coders.CsvCoder(\n",
        "      column_names=['text'], schema=raw_metadata.schema)\n",
        "\n",
        "  with beam.Pipeline(args.runner, options=options) as pipeline:\n",
        "    with tft_beam.Context(args.temporary_dir):\n",
        "      # Read the sentences from the input file\n",
        "      sentences = ( \n",
        "          pipeline\n",
        "          | 'Read sentences from files' >> beam.io.ReadFromText(\n",
        "              file_pattern=args.data_dir)\n",
        "          | 'Convert to dictionary' >> beam.Map(converter.decode)\n",
        "      )\n",
        "\n",
        "      sentences_dataset = (sentences, raw_metadata)\n",
        "      preprocess_fn = make_preprocess_fn(args.module_url, args.random_projection_matrix)\n",
        "      # Generate the embeddings for the sentence using the TF-Hub module\n",
        "      embeddings_dataset, _ = (\n",
        "          sentences_dataset\n",
        "          | 'Extract embeddings' >> tft_beam.AnalyzeAndTransformDataset(preprocess_fn)\n",
        "      )\n",
        "\n",
        "      embeddings, transformed_metadata = embeddings_dataset\n",
        "      # Write the embeddings to TFRecords files\n",
        "      embeddings | 'Write embeddings to TFRecords' >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "          file_path_prefix='{}/emb'.format(args.output_dir),\n",
        "          file_name_suffix='.tfrecords',\n",
        "          coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHbq4t2gCDAG"
      },
      "source": [
        "### ランダムプロジェクションの重み行列を生成する\n",
        "\n",
        "[ランダムプロジェクション](https://en.wikipedia.org/wiki/Random_projection)は、ユークリッド空間に存在する一連の点の次元を縮小するために使用される、単純でありながら高性能のテクニックです。理論的背景については、[Johnson-Lindenstrauss の補題](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma)をご覧ください。\n",
        "\n",
        "ランダムプロジェクションを使用して埋め込みの次元を縮小するということは、ANN インデックスの構築とクエリに必要となる時間を短縮できるということです。\n",
        "\n",
        "このチュートリアルでは、[Scikit-learn](https://en.wikipedia.org/wiki/Random_projection#Gaussian_random_projection) ライブラリの[ガウスランダムプロジェクション](https://scikit-learn.org/stable/modules/random_projection.html#gaussian-random-projection)を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1aYPeOUCDIP"
      },
      "outputs": [],
      "source": [
        "def generate_random_projection_weights(original_dim, projected_dim):\n",
        "  random_projection_matrix = None\n",
        "  if projected_dim and original_dim > projected_dim:\n",
        "    random_projection_matrix = gaussian_random_matrix(\n",
        "        n_components=projected_dim, n_features=original_dim).T\n",
        "    print(\"A Gaussian random weight matrix was creates with shape of {}\".format(random_projection_matrix.shape))\n",
        "    print('Storing random projection matrix to disk...')\n",
        "    with open('random_projection_matrix', 'wb') as handle:\n",
        "      pickle.dump(random_projection_matrix, \n",
        "                  handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "  return random_projection_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHxZX2Z3Nk64"
      },
      "source": [
        "### パラメータの設定\n",
        "\n",
        "ランダムプロジェクションを使用せずに、元の埋め込み空間を使用してインデックスを構築する場合は、`projected_dim` パラメータを `None` に設定します。これにより、高次元埋め込みのインデックス作成ステップが減速することに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "feMVXFL0NlIM"
      },
      "outputs": [],
      "source": [
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/2' #@param {type:\"string\"}\n",
        "projected_dim = 64  #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On-MbzD922kb"
      },
      "source": [
        "### パイプラインの実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3I1Wv4i21yY"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "output_dir = pathlib.Path(tempfile.mkdtemp())\n",
        "temporary_dir = pathlib.Path(tempfile.mkdtemp())\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  original_dim = load_module(module_url)(['']).shape[1]\n",
        "  random_projection_matrix = None\n",
        "\n",
        "  if projected_dim:\n",
        "    random_projection_matrix = generate_random_projection_weights(\n",
        "        original_dim, projected_dim)\n",
        "\n",
        "args = {\n",
        "    'job_name': 'hub2emb-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S')),\n",
        "    'runner': 'DirectRunner',\n",
        "    'batch_size': 1024,\n",
        "    'data_dir': 'corpus/*.txt',\n",
        "    'output_dir': output_dir,\n",
        "    'temporary_dir': temporary_dir,\n",
        "    'module_url': module_url,\n",
        "    'random_projection_matrix': random_projection_matrix,\n",
        "}\n",
        "\n",
        "print(\"Pipeline args are set.\")\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS9obmeP4ZOA"
      },
      "outputs": [],
      "source": [
        "!rm -r {output_dir}\n",
        "!rm -r {temporary_dir}\n",
        "\n",
        "print(\"Running pipeline...\")\n",
        "%time run_hub2emb(args)\n",
        "print(\"Pipeline is done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAwOo7gQWvVd"
      },
      "outputs": [],
      "source": [
        "!ls {output_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVnee4e6U90u"
      },
      "source": [
        "生成された埋め込みをいくつか読み取ります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K7pGXlXOj1N"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "embed_file = os.path.join(output_dir, 'emb-00000-of-00001.tfrecords')\n",
        "sample = 5\n",
        "record_iterator =  tf.io.tf_record_iterator(path=embed_file)\n",
        "for string_record in itertools.islice(record_iterator, sample):\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(string_record)\n",
        "  text = example.features.feature['text'].bytes_list.value\n",
        "  embedding = np.array(example.features.feature['embedding'].float_list.value)\n",
        "  print(\"Embedding dimensions: {}\".format(embedding.shape[0]))\n",
        "  print(\"{}: {}\".format(text, embedding[:10]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agGoaMSgY8wN"
      },
      "source": [
        "## 3. 埋め込みの ANN インデックスを構築する\n",
        "\n",
        "[ANNOY](https://github.com/spotify/annoy)（Approximate Nearest Neighbors Oh Yeah）は、特定のクエリ点に近い空間内のポイントを検索するための、Python バインディングを使った C++ ライブラリです。メモリにマッピングされた、大規模な読み取り専用ファイルベースのデータ構造も作成します。[Spotify](https://www.spotify.com) が構築したもので、おすすめの音楽に使用されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcPDspU3WjgH"
      },
      "outputs": [],
      "source": [
        "def build_index(embedding_files_pattern, index_filename, vector_length, \n",
        "    metric='angular', num_trees=100):\n",
        "  '''Builds an ANNOY index'''\n",
        "\n",
        "  annoy_index = annoy.AnnoyIndex(vector_length, metric=metric)\n",
        "  # Mapping between the item and its identifier in the index\n",
        "  mapping = {}\n",
        "\n",
        "  embed_files = tf.gfile.Glob(embedding_files_pattern)\n",
        "  print('Found {} embedding file(s).'.format(len(embed_files)))\n",
        "\n",
        "  item_counter = 0\n",
        "  for f, embed_file in enumerate(embed_files):\n",
        "    print('Loading embeddings in file {} of {}...'.format(\n",
        "      f+1, len(embed_files)))\n",
        "    record_iterator = tf.io.tf_record_iterator(\n",
        "      path=embed_file)\n",
        "\n",
        "    for string_record in record_iterator:\n",
        "      example = tf.train.Example()\n",
        "      example.ParseFromString(string_record)\n",
        "      text = example.features.feature['text'].bytes_list.value[0].decode(\"utf-8\")\n",
        "      mapping[item_counter] = text\n",
        "      embedding = np.array(\n",
        "        example.features.feature['embedding'].float_list.value)\n",
        "      annoy_index.add_item(item_counter, embedding)\n",
        "      item_counter += 1\n",
        "      if item_counter % 100000 == 0:\n",
        "        print('{} items loaded to the index'.format(item_counter))\n",
        "\n",
        "  print('A total of {} items added to the index'.format(item_counter))\n",
        "\n",
        "  print('Building the index with {} trees...'.format(num_trees))\n",
        "  annoy_index.build(n_trees=num_trees)\n",
        "  print('Index is successfully built.')\n",
        "  \n",
        "  print('Saving index to disk...')\n",
        "  annoy_index.save(index_filename)\n",
        "  print('Index is saved to disk.')\n",
        "  print(\"Index file size: {} GB\".format(\n",
        "    round(os.path.getsize(index_filename) / float(1024 ** 3), 2)))\n",
        "  annoy_index.unload()\n",
        "\n",
        "  print('Saving mapping to disk...')\n",
        "  with open(index_filename + '.mapping', 'wb') as handle:\n",
        "    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  print('Mapping is saved to disk.')\n",
        "  print(\"Mapping file size: {} MB\".format(\n",
        "    round(os.path.getsize(index_filename + '.mapping') / float(1024 ** 2), 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgyOQhUq6FNE"
      },
      "outputs": [],
      "source": [
        "embedding_files = \"{}/emb-*.tfrecords\".format(output_dir)\n",
        "embedding_dimension = projected_dim\n",
        "index_filename = \"index\"\n",
        "\n",
        "!rm {index_filename}\n",
        "!rm {index_filename}.mapping\n",
        "\n",
        "%time build_index(embedding_files, index_filename, embedding_dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic31Tm5cgAd5"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maGxDl8ufP-p"
      },
      "source": [
        "## 4. インデックスを使って、類似性の一致を実施する\n",
        "\n",
        "ANN インデックスを使用して、入力クエリに意味的に近いニュースの見出しを検索できるようになりました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dIs8W78fYPp"
      },
      "source": [
        "### インデックスとマッピングファイルを読み込む"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlTTrbQHayvb"
      },
      "outputs": [],
      "source": [
        "index = annoy.AnnoyIndex(embedding_dimension)\n",
        "index.load(index_filename, prefault=True)\n",
        "print('Annoy index is loaded.')\n",
        "with open(index_filename + '.mapping', 'rb') as handle:\n",
        "  mapping = pickle.load(handle)\n",
        "print('Mapping file is loaded.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6liFMSUh08J"
      },
      "source": [
        "### 類似性の一致メソッド"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUxjTag8hc16"
      },
      "outputs": [],
      "source": [
        "def find_similar_items(embedding, num_matches=5):\n",
        "  '''Finds similar items to a given embedding in the ANN index'''\n",
        "  ids = index.get_nns_by_vector(\n",
        "  embedding, num_matches, search_k=-1, include_distances=False)\n",
        "  items = [mapping[i] for i in ids]\n",
        "  return items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjerNpmZja0A"
      },
      "source": [
        "### 特定のクエリから埋め込みを抽出する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0IIXzfBjZ19"
      },
      "outputs": [],
      "source": [
        "# Load the TF-Hub module\n",
        "print(\"Loading the TF-Hub module...\")\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  embed_fn = load_module(module_url)\n",
        "print(\"TF-Hub module is loaded.\")\n",
        "\n",
        "random_projection_matrix = None\n",
        "if os.path.exists('random_projection_matrix'):\n",
        "  print(\"Loading random projection matrix...\")\n",
        "  with open('random_projection_matrix', 'rb') as handle:\n",
        "    random_projection_matrix = pickle.load(handle)\n",
        "  print('random projection matrix is loaded.')\n",
        "\n",
        "def extract_embeddings(query):\n",
        "  '''Generates the embedding for the query'''\n",
        "  query_embedding =  embed_fn([query])[0]\n",
        "  if random_projection_matrix is not None:\n",
        "    query_embedding = query_embedding.dot(random_projection_matrix)\n",
        "  return query_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCoCNROujEIO"
      },
      "outputs": [],
      "source": [
        "extract_embeddings(\"Hello Machine Learning!\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE_Q60nCk_ZB"
      },
      "source": [
        "### クエリを入力して、類似性の最も高いアイテムを検索する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wC0uLjvfk5nB"
      },
      "outputs": [],
      "source": [
        "#@title { run: \"auto\" }\n",
        "query = \"confronting global challenges\" #@param {type:\"string\"}\n",
        "print(\"Generating embedding for the query...\")\n",
        "%time query_embedding = extract_embeddings(query)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Finding relevant items in the index...\")\n",
        "%time items = find_similar_items(query_embedding, 10)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Results:\")\n",
        "print(\"=========\")\n",
        "for item in items:\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwtMtyOeDKwt"
      },
      "source": [
        "## 今後の学習\n",
        "\n",
        "[tensorflow.org/hub](https://www.tensorflow.org/) では、TensorFlow についてさらに学習し、TF-Hub API ドキュメントを確認することができます。また、[tfhub.dev](https://www.tensorflow.org/hub/) では、その他のテキスト埋め込みモジュールや画像特徴量ベクトルモジュールなど、利用可能な TensorFlow Hub モジュールを検索することができます。\n",
        "\n",
        "さらに、Google の [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/) もご覧ください。機械学習の実用的な導入をテンポよく学習できます。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ls0Zh7kYz3PM",
        "_don5gXy9D59",
        "SQ492LN7A-NZ"
      ],
      "name": "semantic_approximate_nearest_neighbors.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
