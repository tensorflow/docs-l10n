{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXehiGc3Kr2I"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-6LKjmi8Ktoh"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/spice\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/hub/tutorials/spice.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"> Google Colab で実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/hub/tutorials/spice.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/hub/tutorials/spice.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Download notebook</a></td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/spice/2\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub モデルを参照</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPQKw4x4bL8w"
      },
      "source": [
        "# SPICE によるピッチ検出\n",
        "\n",
        "この Colab では、TensorFlow Hub からダウンロードした SPICE モデルの使用方法を紹介します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfKwZlPnPwD1"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -q -y timidity libsndfile1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYrIdOS8SW3b"
      },
      "outputs": [],
      "source": [
        "# All the imports to deal with sound data\n",
        "!pip install pydub numba==0.48 librosa music21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p09o78LGYdnz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "from librosa import display as librosadisplay\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import statistics\n",
        "import sys\n",
        "\n",
        "from IPython.display import Audio, Javascript\n",
        "from scipy.io import wavfile\n",
        "\n",
        "from base64 import b64decode\n",
        "\n",
        "import music21\n",
        "from pydub import AudioSegment\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "\n",
        "print(\"tensorflow: %s\" % tf.__version__)\n",
        "#print(\"librosa: %s\" % librosa.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHxox8hXc3w1"
      },
      "source": [
        "# 音声入力ファイル\n",
        "\n",
        "これが最も困難な部分です。あなたの歌声を録音しましょう！:)\n",
        "\n",
        "音声ファイルの取得には、次の 4 つの方法があります。\n",
        "\n",
        "1. Colab で直接音声を録音する\n",
        "2. ご利用の PC からアップロードする\n",
        "3. Google Drive に保存されたファイルを使用する\n",
        "4. ウェブからファイルをダウンロードする\n",
        "\n",
        "以下の 4 つの方法から 1 つを選択してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HaCAHOqiVu5B"
      },
      "outputs": [],
      "source": [
        "#@title [Run this] Definition of the JS code to record audio straight from the browser\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(sec=5):\n",
        "  try:\n",
        "    from google.colab import output\n",
        "  except ImportError:\n",
        "    print('No possible to import output from google.colab')\n",
        "    return ''\n",
        "  else:\n",
        "    print('Recording')\n",
        "    display(Javascript(RECORD))\n",
        "    s = output.eval_js('record(%d)' % (sec*1000))\n",
        "    fname = 'recorded_audio.wav'\n",
        "    print('Saving to', fname)\n",
        "    b = b64decode(s.split(',')[1])\n",
        "    with open(fname, 'wb') as f:\n",
        "      f.write(b)\n",
        "    return fname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "sBpWWkTzfUYR"
      },
      "outputs": [],
      "source": [
        "#@title Select how to input your audio  { run: \"auto\" }\n",
        "INPUT_SOURCE = 'https://storage.googleapis.com/download.tensorflow.org/data/c-scale-metronome.wav' #@param [\"https://storage.googleapis.com/download.tensorflow.org/data/c-scale-metronome.wav\", \"RECORD\", \"UPLOAD\", \"./drive/My Drive/YOUR_MUSIC_FILE.wav\"] {allow-input: true}\n",
        "\n",
        "print('You selected', INPUT_SOURCE)\n",
        "\n",
        "if INPUT_SOURCE == 'RECORD':\n",
        "  uploaded_file_name = record(5)\n",
        "elif INPUT_SOURCE == 'UPLOAD':\n",
        "  try:\n",
        "    from google.colab import files\n",
        "  except ImportError:\n",
        "    print(\"ImportError: files from google.colab seems to not be available\")\n",
        "  else:\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "      print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "          name=fn, length=len(uploaded[fn])))\n",
        "    uploaded_file_name = next(iter(uploaded))\n",
        "    print('Uploaded file: ' + uploaded_file_name)\n",
        "elif INPUT_SOURCE.startswith('./drive/'):\n",
        "  try:\n",
        "    from google.colab import drive\n",
        "  except ImportError:\n",
        "    print(\"ImportError: files from google.colab seems to not be available\")\n",
        "  else:\n",
        "    drive.mount('/content/drive')\n",
        "    # don't forget to change the name of the file you\n",
        "    # will you here!\n",
        "    gdrive_audio_file = 'YOUR_MUSIC_FILE.wav'\n",
        "    uploaded_file_name = INPUT_SOURCE\n",
        "elif INPUT_SOURCE.startswith('http'):\n",
        "  !wget --no-check-certificate 'https://storage.googleapis.com/download.tensorflow.org/data/c-scale-metronome.wav' -O c-scale.wav\n",
        "  uploaded_file_name = 'c-scale.wav'\n",
        "else:\n",
        "  print('Unrecognized input format!')\n",
        "  print('Please select \"RECORD\", \"UPLOAD\", or specify a file hosted on Google Drive or a file from the web to download file to download')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S2BvIoDf9nf"
      },
      "source": [
        "# 音声データを準備する\n",
        "\n",
        "音声データを入手したので、期待される形式に変換して聴いてみましょう！\n",
        "\n",
        "SPICE モデルでは、入力として、サンプリングレート 16 kHz の音声ファイルが必要です。また、チャンネル数は 1 つ（Mono）である必要があります。\n",
        "\n",
        "この部分の作業を支援するために、wav ファイルをモデルが期待する形式に変換する関数（`convert_audio_for_model`）を用意しました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ1362i-JoFI"
      },
      "outputs": [],
      "source": [
        "# Function that converts the user-created audio to the format that the model \n",
        "# expects: bitrate 16kHz and only one channel (mono).\n",
        "\n",
        "EXPECTED_SAMPLE_RATE = 16000\n",
        "\n",
        "def convert_audio_for_model(user_file, output_file='converted_audio_file.wav'):\n",
        "  audio = AudioSegment.from_file(user_file)\n",
        "  audio = audio.set_frame_rate(EXPECTED_SAMPLE_RATE).set_channels(1)\n",
        "  audio.export(output_file, format=\"wav\")\n",
        "  return output_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL9pftZ2nPm9"
      },
      "outputs": [],
      "source": [
        "# Converting to the expected format for the model\n",
        "# in all the input 4 input method before, the uploaded file name is at\n",
        "# the variable uploaded_file_name\n",
        "converted_audio_file = convert_audio_for_model(uploaded_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TslkX2AOZN0p"
      },
      "outputs": [],
      "source": [
        "# Loading audio samples from the wav file:\n",
        "sample_rate, audio_samples = wavfile.read(converted_audio_file, 'rb')\n",
        "\n",
        "# Show some basic information about the audio.\n",
        "duration = len(audio_samples)/sample_rate\n",
        "print(f'Sample rate: {sample_rate} Hz')\n",
        "print(f'Total duration: {duration:.2f}s')\n",
        "print(f'Size of the input: {len(audio_samples)}')\n",
        "\n",
        "# Let's listen to the wav file.\n",
        "Audio(audio_samples, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBicZu5AgcpR"
      },
      "source": [
        "まずはじめに、歌声の波形を見てみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAa2M3CLZcWW"
      },
      "outputs": [],
      "source": [
        "# We can visualize the audio as a waveform.\n",
        "_ = plt.plot(audio_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1eI0b8qgn08"
      },
      "source": [
        "より詳しいビジュアライゼーションとして、経時的に周波数を示す[スペクトログラム](https://en.wikipedia.org/wiki/Spectrogram) があります。\n",
        "\n",
        "ここでは、対数による周波数スケールを使用して、歌声をより鮮明に視覚化させます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGR4UZtpZvWI"
      },
      "outputs": [],
      "source": [
        "MAX_ABS_INT16 = 32768.0\n",
        "\n",
        "def plot_stft(x, sample_rate, show_black_and_white=False):\n",
        "  x_stft = np.abs(librosa.stft(x, n_fft=2048))\n",
        "  fig, ax = plt.subplots()\n",
        "  fig.set_size_inches(20, 10)\n",
        "  x_stft_db = librosa.amplitude_to_db(x_stft, ref=np.max)\n",
        "  if(show_black_and_white):\n",
        "    librosadisplay.specshow(data=x_stft_db, y_axis='log', \n",
        "                             sr=sample_rate, cmap='gray_r')\n",
        "  else:\n",
        "    librosadisplay.specshow(data=x_stft_db, y_axis='log', sr=sample_rate)\n",
        "\n",
        "  plt.colorbar(format='%+2.0f dB')\n",
        "\n",
        "plot_stft(audio_samples / MAX_ABS_INT16 , sample_rate=EXPECTED_SAMPLE_RATE)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGCzo_cjjH-7"
      },
      "source": [
        "ここで、最後の変換を行う必要があります。音声サンプルは int16 形式です。これらを -1 と 1 の間の浮動小数点数に正規化する必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv4H4O1Xb8T8"
      },
      "outputs": [],
      "source": [
        "audio_samples = audio_samples / float(MAX_ABS_INT16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTdo_TwljVUV"
      },
      "source": [
        "# モデルを実行する\n",
        "\n",
        "ようやく簡単な作業です。**TensorFlow Hub** でモデルを読み込み、音声をフィードしましょう。SPICE から ピッチと不確実性の 2 つの出力が得られます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUptYSTAbc3I"
      },
      "source": [
        "**TensorFlow Hub** は、機械学習モデルの再利用可能な部分の公開、発見、および消費のためのライブラリです。ユーザーの抱える課題の解決する機械学習の使用を簡単にすることができます。\n",
        "\n",
        "モデルを読み込むには、Hub モジュールと、モデルにポイントする URL のみが必要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri0A0DSXY_Yd"
      },
      "outputs": [],
      "source": [
        "# Loading the SPICE model is easy:\n",
        "model = hub.load(\"https://tfhub.dev/google/spice/2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQV5H6J4suMT"
      },
      "source": [
        "**注意:** ここでの豆知識は、Hub のすべてのモデル URL は、ダウンロードだけでなく、ドキュメントの読み取りにも利用できるということです。そのため、ブラウザでそのリンクを開くと、モデルの使用方法を読み、どのようにしてトレーニングされたのかという詳細も知ることができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUVICjIps9hI"
      },
      "source": [
        "モデルが読み込まれ、データの準備が完了したので、結果を取得するための 3 行を追加しましょう。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP55fXBYcBhb"
      },
      "outputs": [],
      "source": [
        "# We now feed the audio to the SPICE tf.hub model to obtain pitch and uncertainty outputs as tensors.\n",
        "model_output = model.signatures[\"serving_default\"](tf.constant(audio_samples, tf.float32))\n",
        "\n",
        "pitch_outputs = model_output[\"pitch\"]\n",
        "uncertainty_outputs = model_output[\"uncertainty\"]\n",
        "\n",
        "# 'Uncertainty' basically means the inverse of confidence.\n",
        "confidence_outputs = 1.0 - uncertainty_outputs\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(20, 10)\n",
        "plt.plot(pitch_outputs, label='pitch')\n",
        "plt.plot(confidence_outputs, label='confidence')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blJwFWR4kMul"
      },
      "source": [
        "信頼度の低い（confidence &lt; 0.9）すべてのピッチ推定値を取り除いて、残りのピッチをグラフ化して、結果を理解しやすくしましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1MRmcm2cEkM"
      },
      "outputs": [],
      "source": [
        "confidence_outputs = list(confidence_outputs)\n",
        "pitch_outputs = [ float(x) for x in pitch_outputs]\n",
        "\n",
        "indices = range(len (pitch_outputs))\n",
        "confident_pitch_outputs = [ (i,p)  \n",
        "  for i, p, c in zip(indices, pitch_outputs, confidence_outputs) if  c >= 0.9  ]\n",
        "confident_pitch_outputs_x, confident_pitch_outputs_y = zip(*confident_pitch_outputs)\n",
        " \n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(20, 10)\n",
        "ax.set_ylim([0, 1])\n",
        "plt.scatter(confident_pitch_outputs_x, confident_pitch_outputs_y, )\n",
        "plt.scatter(confident_pitch_outputs_x, confident_pitch_outputs_y, c=\"r\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNBZ7ZblkxOm"
      },
      "source": [
        "SPICE が返すピッチ値の範囲は 0 から 1 です。この値を Hz 単位の絶対ピッチ値に変換しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-CnpKzmcQi9"
      },
      "outputs": [],
      "source": [
        "def output2hz(pitch_output):\n",
        "  # Constants taken from https://tfhub.dev/google/spice/2\n",
        "  PT_OFFSET = 25.58\n",
        "  PT_SLOPE = 63.07\n",
        "  FMIN = 10.0;\n",
        "  BINS_PER_OCTAVE = 12.0;\n",
        "  cqt_bin = pitch_output * PT_SLOPE + PT_OFFSET;\n",
        "  return FMIN * 2.0 ** (1.0 * cqt_bin / BINS_PER_OCTAVE)\n",
        "    \n",
        "confident_pitch_values_hz = [ output2hz(p) for p in confident_pitch_outputs_y ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24yK0a6HjCSZ"
      },
      "source": [
        "では、どれほど良い予測が得られるか確認しましょう。予測されるピッチを元のスペクトログラムにオーバーレイ表示します。ピッチ予測をより見やすくするために、スペクトログラムをモノクロに変更してます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1kaAcX9rrDo"
      },
      "outputs": [],
      "source": [
        "plot_stft(audio_samples / MAX_ABS_INT16 , \n",
        "          sample_rate=EXPECTED_SAMPLE_RATE, show_black_and_white=True)\n",
        "# Note: conveniently, since the plot is in log scale, the pitch outputs \n",
        "# also get converted to the log scale automatically by matplotlib.\n",
        "plt.scatter(confident_pitch_outputs_x, confident_pitch_values_hz, c=\"r\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NskqpiHLxq6V"
      },
      "source": [
        "# 音符に変換する\n",
        "\n",
        "ピッチ値を得たので、次はこの値を音符に変換することにしましょう！この作業はそれ自体が困難となる部分です。次の 2 つの項目を考慮する必要があります。\n",
        "\n",
        "1. 休符（歌声がないところ）\n",
        "2. 各音符のサイズ（オフセット） "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDOlm9PLTTjt"
      },
      "source": [
        "### 1: 出力にゼロを追加して、歌声がない部分を示す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uSQ3bJmTZmo"
      },
      "outputs": [],
      "source": [
        "pitch_outputs_and_rests = [\n",
        "    output2hz(p) if c >= 0.9 else 0\n",
        "    for i, p, c in zip(indices, pitch_outputs, confidence_outputs)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fM0UwlsTt4w"
      },
      "source": [
        "### 2: 音符のオフセットを追加する\n",
        "\n",
        "自由に歌う場合、メロディーには音符が表現する絶対ピッチ値に対するオフセットがある場合があります。したがって、予測を音符に変換するには、この潜在的なオフセットを修正する必要があります。次のコードはこれを計算しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsJu-P5ksdFW"
      },
      "outputs": [],
      "source": [
        "A4 = 440\n",
        "C0 = A4 * pow(2, -4.75)\n",
        "note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
        "\n",
        "def hz2offset(freq):\n",
        "  # This measures the quantization error for a single note.\n",
        "  if freq == 0:  # Rests always have zero error.\n",
        "    return None\n",
        "  # Quantized note.\n",
        "  h = round(12 * math.log2(freq / C0))\n",
        "  return 12 * math.log2(freq / C0) - h\n",
        "\n",
        "\n",
        "# The ideal offset is the mean quantization error for all the notes\n",
        "# (excluding rests):\n",
        "offsets = [hz2offset(p) for p in pitch_outputs_and_rests if p != 0]\n",
        "print(\"offsets: \", offsets)\n",
        "\n",
        "ideal_offset = statistics.mean(offsets)\n",
        "print(\"ideal offset: \", ideal_offset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K17It_qT2DtE"
      },
      "source": [
        "ヒューリスティックを使用して歌われた可能性の最も高い音符のシーケンスの予測を試みることができるようになりました。上記で計算された理想的なオフセットは 1 つの材料ではありますが、速度（8 個など、どれくらいの予測をたてるのか）と量子化を始める時間オフセットを知る必要もあります。単純にしておくために、異なる速度と時間オフセットを試して、量子化誤差を測定し、最終的に、この誤差を最小限に抑える値を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMUTI4L52ZHA"
      },
      "outputs": [],
      "source": [
        "def quantize_predictions(group, ideal_offset):\n",
        "  # Group values are either 0, or a pitch in Hz.\n",
        "  non_zero_values = [v for v in group if v != 0]\n",
        "  zero_values_count = len(group) - len(non_zero_values)\n",
        "\n",
        "  # Create a rest if 80% is silent, otherwise create a note.\n",
        "  if zero_values_count > 0.8 * len(group):\n",
        "    # Interpret as a rest. Count each dropped note as an error, weighted a bit\n",
        "    # worse than a badly sung note (which would 'cost' 0.5).\n",
        "    return 0.51 * len(non_zero_values), \"Rest\"\n",
        "  else:\n",
        "    # Interpret as note, estimating as mean of non-rest predictions.\n",
        "    h = round(\n",
        "        statistics.mean([\n",
        "            12 * math.log2(freq / C0) - ideal_offset for freq in non_zero_values\n",
        "        ]))\n",
        "    octave = h // 12\n",
        "    n = h % 12\n",
        "    note = note_names[n] + str(octave)\n",
        "    # Quantization error is the total difference from the quantized note.\n",
        "    error = sum([\n",
        "        abs(12 * math.log2(freq / C0) - ideal_offset - h)\n",
        "        for freq in non_zero_values\n",
        "    ])\n",
        "    return error, note\n",
        "\n",
        "\n",
        "def get_quantization_and_error(pitch_outputs_and_rests, predictions_per_eighth,\n",
        "                               prediction_start_offset, ideal_offset):\n",
        "  # Apply the start offset - we can just add the offset as rests.\n",
        "  pitch_outputs_and_rests = [0] * prediction_start_offset + \\\n",
        "                            pitch_outputs_and_rests\n",
        "  # Collect the predictions for each note (or rest).\n",
        "  groups = [\n",
        "      pitch_outputs_and_rests[i:i + predictions_per_eighth]\n",
        "      for i in range(0, len(pitch_outputs_and_rests), predictions_per_eighth)\n",
        "  ]\n",
        "\n",
        "  quantization_error = 0\n",
        "\n",
        "  notes_and_rests = []\n",
        "  for group in groups:\n",
        "    error, note_or_rest = quantize_predictions(group, ideal_offset)\n",
        "    quantization_error += error\n",
        "    notes_and_rests.append(note_or_rest)\n",
        "\n",
        "  return quantization_error, notes_and_rests\n",
        "\n",
        "\n",
        "best_error = float(\"inf\")\n",
        "best_notes_and_rests = None\n",
        "best_predictions_per_note = None\n",
        "\n",
        "for predictions_per_note in range(20, 65, 1):\n",
        "  for prediction_start_offset in range(predictions_per_note):\n",
        "\n",
        "    error, notes_and_rests = get_quantization_and_error(\n",
        "        pitch_outputs_and_rests, predictions_per_note,\n",
        "        prediction_start_offset, ideal_offset)\n",
        "\n",
        "    if error < best_error:      \n",
        "      best_error = error\n",
        "      best_notes_and_rests = notes_and_rests\n",
        "      best_predictions_per_note = predictions_per_note\n",
        "\n",
        "# At this point, best_notes_and_rests contains the best quantization.\n",
        "# Since we don't need to have rests at the beginning, let's remove these:\n",
        "while best_notes_and_rests[0] == 'Rest':\n",
        "  best_notes_and_rests = best_notes_and_rests[1:]\n",
        "# Also remove silence at the end.\n",
        "while best_notes_and_rests[-1] == 'Rest':\n",
        "  best_notes_and_rests = best_notes_and_rests[:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMZbWA3aVqee"
      },
      "source": [
        "では、量子化された音符を楽譜として書き出しましょう！\n",
        "\n",
        "これを行うには、[music21](http://web.mit.edu/music21/) と [Open Sheet Music Display](https://github.com/opensheetmusicdisplay/opensheetmusicdisplay) の 2 つのライブラリを使用します。\n",
        "\n",
        "**注意:** 単純に行えるように、ここではすべての音符の長さが同じ（半音）であると仮定しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVrk_IOIzpQR"
      },
      "outputs": [],
      "source": [
        "# Creating the sheet music score.\n",
        "sc = music21.stream.Score()\n",
        "# Adjust the speed to match the actual singing.\n",
        "bpm = 60 * 60 / best_predictions_per_note\n",
        "print ('bpm: ', bpm)\n",
        "a = music21.tempo.MetronomeMark(number=bpm)\n",
        "sc.insert(0,a)\n",
        "\n",
        "for snote in best_notes_and_rests:   \n",
        "    d = 'half'\n",
        "    if snote == 'Rest':      \n",
        "      sc.append(music21.note.Rest(type=d))\n",
        "    else:\n",
        "      sc.append(music21.note.Note(snote, type=d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "CEleCWHtG2s4"
      },
      "outputs": [],
      "source": [
        "#@title [Run this] Helper function to use Open Sheet Music Display (JS code) to show a music score\n",
        "\n",
        "from IPython.core.display import display, HTML, Javascript\n",
        "import json, random\n",
        "\n",
        "def showScore(score):\n",
        "    xml = open(score.write('musicxml')).read()\n",
        "    showMusicXML(xml)\n",
        "    \n",
        "def showMusicXML(xml):\n",
        "    DIV_ID = \"OSMD_div\"\n",
        "    display(HTML('<div id=\"'+DIV_ID+'\">loading OpenSheetMusicDisplay</div>'))\n",
        "    script = \"\"\"\n",
        "    var div_id = {{DIV_ID}};\n",
        "    function loadOSMD() { \n",
        "        return new Promise(function(resolve, reject){\n",
        "            if (window.opensheetmusicdisplay) {\n",
        "                return resolve(window.opensheetmusicdisplay)\n",
        "            }\n",
        "            // OSMD script has a 'define' call which conflicts with requirejs\n",
        "            var _define = window.define // save the define object \n",
        "            window.define = undefined // now the loaded script will ignore requirejs\n",
        "            var s = document.createElement( 'script' );\n",
        "            s.setAttribute( 'src', \"https://cdn.jsdelivr.net/npm/opensheetmusicdisplay@0.7.6/build/opensheetmusicdisplay.min.js\" );\n",
        "            //s.setAttribute( 'src', \"/custom/opensheetmusicdisplay.js\" );\n",
        "            s.onload=function(){\n",
        "                window.define = _define\n",
        "                resolve(opensheetmusicdisplay);\n",
        "            };\n",
        "            document.body.appendChild( s ); // browser will try to load the new script tag\n",
        "        }) \n",
        "    }\n",
        "    loadOSMD().then((OSMD)=>{\n",
        "        window.openSheetMusicDisplay = new OSMD.OpenSheetMusicDisplay(div_id, {\n",
        "          drawingParameters: \"compacttight\"\n",
        "        });\n",
        "        openSheetMusicDisplay\n",
        "            .load({{data}})\n",
        "            .then(\n",
        "              function() {\n",
        "                openSheetMusicDisplay.render();\n",
        "              }\n",
        "            );\n",
        "    })\n",
        "    \"\"\".replace('{{DIV_ID}}',DIV_ID).replace('{{data}}',json.dumps(xml))\n",
        "    display(Javascript(script))\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTu4phq4WeAI"
      },
      "outputs": [],
      "source": [
        "# rendering the music score\n",
        "showScore(sc)\n",
        "print(best_notes_and_rests)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGPXm6Z83U2g"
      },
      "source": [
        "音符を MIDI ファイルに変換して、聴いてみましょう。\n",
        "\n",
        "このファイルを作成するには、前に作成したストリームを使用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klYoWjgmPaod"
      },
      "outputs": [],
      "source": [
        "# Saving the recognized musical notes as a MIDI file\n",
        "converted_audio_file_as_midi = converted_audio_file[:-4] + '.mid'\n",
        "fp = sc.write('midi', fp=converted_audio_file_as_midi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz7Mj3Qx1lpR"
      },
      "outputs": [],
      "source": [
        "wav_from_created_midi = converted_audio_file_as_midi.replace(' ', '_') + \"_midioutput.wav\"\n",
        "print(wav_from_created_midi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahss5EOiWDDp"
      },
      "source": [
        "Colab で聴くには、wav に変換し直す必要があります。簡単な方法は、Timidty を使用することです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmeJ-UITV2nq"
      },
      "outputs": [],
      "source": [
        "!timidity $converted_audio_file_as_midi -Ow -o $wav_from_created_midi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnvwmyNj7kCC"
      },
      "source": [
        "最後に、モデルが推論し、予測されたピッチから MIDI 経由で作成され、音符から作成された音声を聴いてみましょう！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNLBB0zJV6vN"
      },
      "outputs": [],
      "source": [
        "Audio(wav_from_created_midi)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "spice.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
