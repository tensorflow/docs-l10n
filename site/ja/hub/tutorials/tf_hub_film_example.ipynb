{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNLUPuRpkFv_"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DQcWZm0FkPk-"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2022 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exbxve1rHlrF"
      },
      "source": [
        "# Frame interpolation using the FILM model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMWFVTlbrQ8m"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf_hub_film_example\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf_hub_film_example.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_film_example.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/tf_hub_film_example.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/film/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61H28S7ArUAZ"
      },
      "source": [
        "Frame interpolation is the task of synthesizing many in-between images from a given set of images. The technique is often used for frame rate upsampling or creating slow-motion video effects.\n",
        "\n",
        "In this colab, you will use the FILM model to do frame interpolation. The colab also provides code snippets to create videos from the interpolated in-between images.\n",
        "\n",
        "For more information on FILM research, you can read more here:\n",
        "- Google AI Blog: [Large Motion Frame Interpolation](https://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html)\n",
        "- Project Page: FILM: [Frame Interpolation for Large Motion](https://film-net.github.io/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVX7s6zMulsu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi5t2OEJsGBW"
      },
      "outputs": [],
      "source": [
        "!pip install mediapy\n",
        "!sudo apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA1tq39MjOiF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "from typing import Generator, Iterable, List, Optional\n",
        "import mediapy as media"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTgXmeYGnT7q"
      },
      "source": [
        "## Load the model from TFHub\n",
        "\n",
        "To load a model from TensorFlow Hub you need the tfhub library and the model handle which is its documentation url."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GojhvyAtjUt0"
      },
      "outputs": [],
      "source": [
        "model = hub.load(\"https://tfhub.dev/google/film/1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOQJPsu2CwPk"
      },
      "source": [
        "## Util function to load images from a url or locally\n",
        "\n",
        "This function loads an image and make it ready to be used by the model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPnh5uhQvFln"
      },
      "outputs": [],
      "source": [
        "_UINT8_MAX_F = float(np.iinfo(np.uint8).max)\n",
        "\n",
        "def load_image(img_url: str):\n",
        "  \"\"\"Returns an image with shape [height, width, num_channels], with pixels in [0..1] range, and type np.float32.\"\"\"\n",
        "\n",
        "  if (img_url.startswith(\"https\")):\n",
        "    user_agent = {'User-agent': 'Colab Sample (https://tensorflow.org)'}\n",
        "    response = requests.get(img_url, headers=user_agent)\n",
        "    image_data = response.content\n",
        "  else:\n",
        "    image_data = tf.io.read_file(img_url)\n",
        "\n",
        "  image = tf.io.decode_image(image_data, channels=3)\n",
        "  image_numpy = tf.cast(image, dtype=tf.float32).numpy()\n",
        "  return image_numpy / _UINT8_MAX_F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjDFns1zp5y6"
      },
      "source": [
        "FILM's model input is a dictionary with the keys `time`, `x0`, `x1`:\n",
        "\n",
        "- `time`: position of the interpolated frame. Midway is `0.5`.\n",
        "- `x0`: is the initial frame.\n",
        "- `x1`: is the final frame.\n",
        "\n",
        "Both frames need to be normalized (done in the function `load_image` above) where each pixel is in the range of `[0..1]`.\n",
        "\n",
        "`time` is a value between `[0..1]` and it says where the generated image should be. 0.5 is midway between the input images.\n",
        "\n",
        "All three values need to have a batch dimension too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEQNQlHGsWSM"
      },
      "outputs": [],
      "source": [
        "# using images from the FILM repository (https://github.com/google-research/frame-interpolation/)\n",
        "\n",
        "image_1_url = \"https://github.com/google-research/frame-interpolation/blob/main/photos/one.png?raw=true\"\n",
        "image_2_url = \"https://github.com/google-research/frame-interpolation/blob/main/photos/two.png?raw=true\"\n",
        "\n",
        "time = np.array([0.5], dtype=np.float32)\n",
        "\n",
        "image1 = load_image(image_1_url)\n",
        "image2 = load_image(image_2_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6_MQE9EuF_K"
      },
      "outputs": [],
      "source": [
        "input = {\n",
        "    'time': np.expand_dims(time, axis=0), # adding the batch dimension to the time\n",
        "     'x0': np.expand_dims(image1, axis=0), # adding the batch dimension to the image\n",
        "     'x1': np.expand_dims(image2, axis=0)  # adding the batch dimension to the image\n",
        "}\n",
        "mid_frame = model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZkzYE2bptfD"
      },
      "source": [
        "The model outputs a couple of results but what you'll use here is the `image` key, whose value is the interpolated frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eClVbNFhA5Py"
      },
      "outputs": [],
      "source": [
        "print(mid_frame.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE2csH3u8ePe"
      },
      "outputs": [],
      "source": [
        "frames = [image1, mid_frame['image'][0].numpy(), image2]\n",
        "\n",
        "media.show_images(frames, titles=['input image one', 'generated image', 'input image two'], height=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS1AT8kn-f_l"
      },
      "source": [
        "Let's create a video from the generated frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFc53B3p37SH"
      },
      "outputs": [],
      "source": [
        "media.show_video(frames, fps=3, title='FILM interpolated video')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5AOFNkj-lfO"
      },
      "source": [
        "## Define a Frame Interpolator Library\n",
        "\n",
        "As you can see, the transition is not too smooth. \n",
        "\n",
        "To improve that you'll need many more interpolated frames.\n",
        "\n",
        "You could just keep running the model many times with intermediary images but there is a better solution.\n",
        "\n",
        "To generate many interpolated images and have a  smoother video you'll create an interpolator library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsoDv_9geoZn"
      },
      "outputs": [],
      "source": [
        "\"\"\"A wrapper class for running a frame interpolation based on the FILM model on TFHub\n",
        "\n",
        "Usage:\n",
        "  interpolator = Interpolator()\n",
        "  result_batch = interpolator(image_batch_0, image_batch_1, batch_dt)\n",
        "  Where image_batch_1 and image_batch_2 are numpy tensors with TF standard\n",
        "  (B,H,W,C) layout, batch_dt is the sub-frame time in range [0..1], (B,) layout.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def _pad_to_align(x, align):\n",
        "  \"\"\"Pads image batch x so width and height divide by align.\n",
        "\n",
        "  Args:\n",
        "    x: Image batch to align.\n",
        "    align: Number to align to.\n",
        "\n",
        "  Returns:\n",
        "    1) An image padded so width % align == 0 and height % align == 0.\n",
        "    2) A bounding box that can be fed readily to tf.image.crop_to_bounding_box\n",
        "      to undo the padding.\n",
        "  \"\"\"\n",
        "  # Input checking.\n",
        "  assert np.ndim(x) == 4\n",
        "  assert align > 0, 'align must be a positive number.'\n",
        "\n",
        "  height, width = x.shape[-3:-1]\n",
        "  height_to_pad = (align - height % align) if height % align != 0 else 0\n",
        "  width_to_pad = (align - width % align) if width % align != 0 else 0\n",
        "\n",
        "  bbox_to_pad = {\n",
        "      'offset_height': height_to_pad // 2,\n",
        "      'offset_width': width_to_pad // 2,\n",
        "      'target_height': height + height_to_pad,\n",
        "      'target_width': width + width_to_pad\n",
        "  }\n",
        "  padded_x = tf.image.pad_to_bounding_box(x, **bbox_to_pad)\n",
        "  bbox_to_crop = {\n",
        "      'offset_height': height_to_pad // 2,\n",
        "      'offset_width': width_to_pad // 2,\n",
        "      'target_height': height,\n",
        "      'target_width': width\n",
        "  }\n",
        "  return padded_x, bbox_to_crop\n",
        "\n",
        "\n",
        "class Interpolator:\n",
        "  \"\"\"A class for generating interpolated frames between two input frames.\n",
        "\n",
        "  Uses the Film model from TFHub\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, align: int = 64) -> None:\n",
        "    \"\"\"Loads a saved model.\n",
        "\n",
        "    Args:\n",
        "      align: 'If >1, pad the input size so it divides with this before\n",
        "        inference.'\n",
        "    \"\"\"\n",
        "    self._model = hub.load(\"https://tfhub.dev/google/film/1\")\n",
        "    self._align = align\n",
        "\n",
        "  def __call__(self, x0: np.ndarray, x1: np.ndarray,\n",
        "               dt: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Generates an interpolated frame between given two batches of frames.\n",
        "\n",
        "    All inputs should be np.float32 datatype.\n",
        "\n",
        "    Args:\n",
        "      x0: First image batch. Dimensions: (batch_size, height, width, channels)\n",
        "      x1: Second image batch. Dimensions: (batch_size, height, width, channels)\n",
        "      dt: Sub-frame time. Range [0,1]. Dimensions: (batch_size,)\n",
        "\n",
        "    Returns:\n",
        "      The result with dimensions (batch_size, height, width, channels).\n",
        "    \"\"\"\n",
        "    if self._align is not None:\n",
        "      x0, bbox_to_crop = _pad_to_align(x0, self._align)\n",
        "      x1, _ = _pad_to_align(x1, self._align)\n",
        "\n",
        "    inputs = {'x0': x0, 'x1': x1, 'time': dt[..., np.newaxis]}\n",
        "    result = self._model(inputs, training=False)\n",
        "    image = result['image']\n",
        "\n",
        "    if self._align is not None:\n",
        "      image = tf.image.crop_to_bounding_box(image, **bbox_to_crop)\n",
        "    return image.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeGYaNBd_7a5"
      },
      "source": [
        "## Frame and Video Generation Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOJxup6s_1DP"
      },
      "outputs": [],
      "source": [
        "def _recursive_generator(\n",
        "    frame1: np.ndarray, frame2: np.ndarray, num_recursions: int,\n",
        "    interpolator: Interpolator) -> Generator[np.ndarray, None, None]:\n",
        "  \"\"\"Splits halfway to repeatedly generate more frames.\n",
        "\n",
        "  Args:\n",
        "    frame1: Input image 1.\n",
        "    frame2: Input image 2.\n",
        "    num_recursions: How many times to interpolate the consecutive image pairs.\n",
        "    interpolator: The frame interpolator instance.\n",
        "\n",
        "  Yields:\n",
        "    The interpolated frames, including the first frame (frame1), but excluding\n",
        "    the final frame2.\n",
        "  \"\"\"\n",
        "  if num_recursions == 0:\n",
        "    yield frame1\n",
        "  else:\n",
        "    # Adds the batch dimension to all inputs before calling the interpolator,\n",
        "    # and remove it afterwards.\n",
        "    time = np.full(shape=(1,), fill_value=0.5, dtype=np.float32)\n",
        "    mid_frame = interpolator(\n",
        "        np.expand_dims(frame1, axis=0), np.expand_dims(frame2, axis=0), time)[0]\n",
        "    yield from _recursive_generator(frame1, mid_frame, num_recursions - 1,\n",
        "                                    interpolator)\n",
        "    yield from _recursive_generator(mid_frame, frame2, num_recursions - 1,\n",
        "                                    interpolator)\n",
        "\n",
        "\n",
        "def interpolate_recursively(\n",
        "    frames: List[np.ndarray], num_recursions: int,\n",
        "    interpolator: Interpolator) -> Iterable[np.ndarray]:\n",
        "  \"\"\"Generates interpolated frames by repeatedly interpolating the midpoint.\n",
        "\n",
        "  Args:\n",
        "    frames: List of input frames. Expected shape (H, W, 3). The colors should be\n",
        "      in the range[0, 1] and in gamma space.\n",
        "    num_recursions: Number of times to do recursive midpoint\n",
        "      interpolation.\n",
        "    interpolator: The frame interpolation model to use.\n",
        "\n",
        "  Yields:\n",
        "    The interpolated frames (including the inputs).\n",
        "  \"\"\"\n",
        "  n = len(frames)\n",
        "  for i in range(1, n):\n",
        "    yield from _recursive_generator(frames[i - 1], frames[i],\n",
        "                                    times_to_interpolate, interpolator)\n",
        "  # Separately yield the final frame.\n",
        "  yield frames[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1R2KjhEAHu0"
      },
      "outputs": [],
      "source": [
        "times_to_interpolate = 6\n",
        "interpolator = Interpolator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZUo8tg1AYvZ"
      },
      "source": [
        "## Running the Interpolator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMMNjs7sAWTG"
      },
      "outputs": [],
      "source": [
        "input_frames = [image1, image2]\n",
        "frames = list(\n",
        "    interpolate_recursively(input_frames, times_to_interpolate,\n",
        "                                        interpolator))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9mHHyCAAhrM"
      },
      "outputs": [],
      "source": [
        "print(f'video with {len(frames)} frames')\n",
        "media.show_video(frames, fps=30, title='FILM interpolated video')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0AZKeMVFwAc"
      },
      "source": [
        "For more information, you can visit [FILM's model repository](https://github.com/google-research/frame-interpolation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8764ry3SGDks"
      },
      "source": [
        "## Citation\n",
        "\n",
        "If you find this model and code useful in your works, please acknowledge it appropriately by citing:\n",
        "\n",
        "```\n",
        "@inproceedings{reda2022film,\n",
        " title = {FILM: Frame Interpolation for Large Motion},\n",
        " author = {Fitsum Reda and Janne Kontkanen and Eric Tabellion and Deqing Sun and Caroline Pantofaru and Brian Curless},\n",
        " booktitle = {The European Conference on Computer Vision (ECCV)},\n",
        " year = {2022}\n",
        "}\n",
        "```\n",
        "\n",
        "```\n",
        "@misc{film-tf,\n",
        "  title = {Tensorflow 2 Implementation of \"FILM: Frame Interpolation for Large Motion\"},\n",
        "  author = {Fitsum Reda and Janne Kontkanen and Eric Tabellion and Deqing Sun and Caroline Pantofaru and Brian Curless},\n",
        "  year = {2022},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  howpublished = {\\url{https://github.com/google-research/frame-interpolation}}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tf_hub_film_example.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
