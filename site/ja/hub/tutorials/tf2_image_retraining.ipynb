{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScitaPqhKtuW"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvztxQ6VsK2k"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYM61xrTsP5d"
      },
      "source": [
        "# 画像分類器を再トレーニングする\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf2_image_retraining\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> TensorFlow.orgで表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/hub/tutorials/tf2_image_retraining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"> Google Colab で実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/hub/tutorials/tf2_image_retraining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/hub/tutorials/tf2_image_retraining.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード/a0}</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub モデルを見る</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1otmJgmbahf"
      },
      "source": [
        "## はじめに\n",
        "\n",
        "画像分類モデルには数百個のパラメータがあります。モデルをゼロからトレーニングするには、ラベル付きの多数のトレーニングデータと膨大なトレーニング性能が必要となります。転移学習とは、関連するタスクでトレーニングされたモデルの一部を取り出して新しいモデルで再利用することで、学習の大部分を省略するテクニックを指します。\n",
        "\n",
        "この Colab では、より大規模で一般的な ImageNet データセットでトレーニングされた、TensorFlow Hub のトレーニング済み TF2 SavedModel を使用して画像特徴量を抽出することで、5 種類の花を分類する Keras モデルの構築方法を実演します。オプションとして、特徴量抽出器を新たに追加される分類器とともにトレーニング（「ファインチューニング」）することができます。\n",
        "\n",
        "### 代替ツールをお探しですか？\n",
        "\n",
        "これは、TensorFlow のコーディングチュートリアルです。TensorFlow または TF Lite モデルを構築するだけのツールをお探しの方は、PIP パッケージ `tensorflow-hub[make_image_classifier]` によって[インストールされる](https://www.tensorflow.org/hub/installation) [make_image_classifier](https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier) コマンドラインツール、または[こちら](https://colab.sandbox.google.com/github/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/image_classification.ipynb)の TF Lite Colab をご覧ください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL54LWCHt5q5"
      },
      "source": [
        "## セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlauq-4FWGZM"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmaHHH7Pvmth"
      },
      "source": [
        "## 使用する TF2 SavedModel モジュールを選択する\n",
        "\n",
        "手始めに、[https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4) を使用します。同じ URL を、SavedModel を識別するコードに使用できます。またブラウザで使用すれば、そのドキュメントを表示することができます。（ここでは TF1 Hub 形式のモデルは機能しないことに注意してください。）\n",
        "\n",
        "画像特徴量ベクトルを生成するその他の TF2 モデルは、[こちら](https://tfhub.dev/s?module-type=image-feature-vector&tf-version=tf2)をご覧ください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlsEcKVeuCnf"
      },
      "outputs": [],
      "source": [
        "module_selection = (\"mobilenet_v2_100_224\", 224) #@param [\"(\\\"mobilenet_v2_100_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "handle_base, pixels = module_selection\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(handle_base)\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
        "\n",
        "BATCH_SIZE = 32 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTY8qzyYv3vl"
      },
      "source": [
        "## Flowers データセットをセットアップする\n",
        "\n",
        "入力は、選択されたモジュールに合わせてサイズ変更されます。データセットを拡張することで（読み取られるたびに画像をランダムに歪みを加える）、特にファインチューニング時のトレーニングが改善されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBtFK1hO8KsO"
      },
      "outputs": [],
      "source": [
        "data_dir = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umB5tswsfTEQ"
      },
      "outputs": [],
      "source": [
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
        "                   interpolation=\"bilinear\")\n",
        "\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    **datagen_kwargs)\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\n",
        "\n",
        "do_data_augmentation = False #@param {type:\"boolean\"}\n",
        "if do_data_augmentation:\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      horizontal_flip=True,\n",
        "      width_shift_range=0.2, height_shift_range=0.2,\n",
        "      shear_range=0.2, zoom_range=0.2,\n",
        "      **datagen_kwargs)\n",
        "else:\n",
        "  train_datagen = valid_datagen\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_gVStowW3G"
      },
      "source": [
        "## モデルを定義する\n",
        "\n",
        "Hub モジュールを使用して、線形分類器を `feature_extractor_layer` の上に配置するだけで定義できます。\n",
        "\n",
        "高速化するため、トレーニング不可能な `feature_extractor_layer` から始めますが、ファインチューニングを実施して精度を高めることもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaJW3XrPyFiF"
      },
      "outputs": [],
      "source": [
        "do_fine_tuning = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50FYNIb1dmJH"
      },
      "outputs": [],
      "source": [
        "print(\"Building model with\", MODULE_HANDLE)\n",
        "model = tf.keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes,\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "## モデルをトレーニングする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f3yBUvkd_VJ"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_YKX2Qnfg6x"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=validation_steps).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYOw0fTO1W4x"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ8DKKgeKv4-"
      },
      "source": [
        "検証データの画像でモデルが機能するか試してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi1iCNB9K1Ai"
      },
      "outputs": [],
      "source": [
        "def get_class_string_from_index(index):\n",
        "   for class_string, class_index in valid_generator.class_indices.items():\n",
        "      if class_index == index:\n",
        "         return class_string\n",
        "\n",
        "x, y = next(valid_generator)\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0])\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "print(\"True label: \" + get_class_string_from_index(true_index))\n",
        "print(\"Predicted label: \" + get_class_string_from_index(predicted_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCsAsQM1IRvA"
      },
      "source": [
        "最後に次のようにして、トレーニングされたモデルを、TF Serving または TF Lite（モバイル）用に保存することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGvTi69oIc2d"
      },
      "outputs": [],
      "source": [
        "saved_model_path = \"/tmp/saved_flowers_model\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzW4oNRjILaq"
      },
      "source": [
        "## オプション: TensorFlow Lite にデプロイする\n",
        "\n",
        "[TensorFlow Lite](https://www.tensorflow.org/lite) では、TensorFlow モデルをモバイルおよび IoT デバイスにデプロイすることができます。以下のコードには、トレーニングされたモデルを TF Lite に変換して、[TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization) のポストトレーニングツールを適用する方法が示されています。最後に、結果の質を調べるために、変換したモデルを TF Lite Interpreter で実行しています。\n",
        "\n",
        "- 最適化せずに変換すると、前と同じ結果が得られます（丸め誤差まで）。\n",
        "- データなしで最適化して変換すると、モデルの重みを 8 ビットに量子化しますが、それでもニューラルネットワークアクティベーションの推論では浮動小数点数計算が使用されます。これにより、モデルのサイズが約 4 倍に縮小されるため、モバイルデバイスの CPU レイテンシが改善されます。\n",
        "- 最上部の、ニューラルネットワークアクティベーションの計算は、量子化の範囲を調整するために小規模な参照データセットが提供される場合、8 ビット整数に量子化されます。モバイルデバイスでは、これにより推論がさらに高速化されるため、EdgeTPU などのアクセラレータで実行することが可能となります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va1Vo92fSyV6"
      },
      "outputs": [],
      "source": [
        "#@title Optimization settings\n",
        "# docs_infra: no_execute\n",
        "# TODO(b/156102192)\n",
        "optimize_lite_model = False  #@param {type:\"boolean\"}\n",
        "#@markdown Setting a value greater than zero enables quantization of neural network activations. A few dozen is already a useful amount.\n",
        "num_calibration_examples = 60  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "representative_dataset = None\n",
        "if optimize_lite_model and num_calibration_examples:\n",
        "  # Use a bounded number of training examples without labels for calibration.\n",
        "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
        "  representative_dataset = lambda: itertools.islice(\n",
        "      ([image[None, ...]] for batch, _ in train_generator for image in batch),\n",
        "      num_calibration_examples)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "if optimize_lite_model:\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  if representative_dataset:  # This is optional, see above.\n",
        "    converter.representative_dataset = representative_dataset\n",
        "lite_model_content = converter.convert()\n",
        "\n",
        "with open(\"/tmp/lite_flowers_model\", \"wb\") as f:\n",
        "  f.write(lite_model_content)\n",
        "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
        "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wqEmD0xIqeG"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
        "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
        "def lite_model(images):\n",
        "  interpreter.allocate_tensors()\n",
        "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "  interpreter.invoke()\n",
        "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMMK-fZrKrk8"
      },
      "outputs": [],
      "source": [
        "#@markdown For rapid experimentation, start with a moderate number of examples.\n",
        "# docs_infra: no_execute\n",
        "num_eval_examples = 50  #@param {type:\"slider\", min:0, max:700}\n",
        "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
        "                for batch in train_generator\n",
        "                for (image, label) in zip(*batch))\n",
        "count = 0\n",
        "count_lite_tf_agree = 0\n",
        "count_lite_correct = 0\n",
        "for image, label in eval_dataset:\n",
        "  probs_lite = lite_model(image[None, ...])[0]\n",
        "  probs_tf = model(image[None, ...]).numpy()[0]\n",
        "  y_lite = np.argmax(probs_lite)\n",
        "  y_tf = np.argmax(probs_tf)\n",
        "  y_true = np.argmax(label)\n",
        "  count +=1\n",
        "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
        "  if y_lite == y_true: count_lite_correct += 1\n",
        "  if count >= num_eval_examples: break\n",
        "print(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
        "print(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ScitaPqhKtuW"
      ],
      "name": "tf2_image_retraining.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
