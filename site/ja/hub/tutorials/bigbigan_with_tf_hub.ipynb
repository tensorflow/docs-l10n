{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLOYL1PJAAtK"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fJWQ8WSAFhh"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1NTVIH6ABK-"
      },
      "source": [
        "# BigBiGAN による画像生成\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/bigbigan_with_tf_hub\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     TensorFlow.org で表示</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/hub/tutorials/bigbigan_with_tf_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colabで実行</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/hub/tutorials/bigbigan_with_tf_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/hub/tutorials/bigbigan_with_tf_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "  <td>     <a href=\"https://tfhub.dev/s?q=experts%2Fbert\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub モデルを参照</a>   </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVvOoEhswyZg"
      },
      "source": [
        "このノートブックは <a>TF Hub</a> で利用できる <em>BigBiGAN</em> モデルのデモです。\n",
        "\n",
        "BigBiGAN は、教師なし表現学習に使用可能な*エンコーダ*モジュールを追加することによって、標準的な (Big)GAN を拡張します。大まかに言えば、エンコーダは与えられた実データ `x` で潜在性 `z` を予測してジェネレータを反転させます。これらのモデルの詳細については、[arXiv の BigBiGAN 論文](https://arxiv.org/abs/1907.02544) [1] をご覧ください。\n",
        "\n",
        "ランタイムに接続した後、以下の指示に従ってください。\n",
        "\n",
        "1. （オプション）下記の最初のコードセルで選択した **`module_path`** を更新して、異なるエンコーダアーキテクチャ用の BigBiGAN ジェネレータを読み込みます。\n",
        "2. **Runtime &gt; Run all** をクリックして各セルを順番に実行します。その後、BigBiGAN のサンプルや再構成の可視化を含む出力は、以下のように自動的に表示されます。\n",
        "\n",
        "注意: 問題が生じる場合は、**Runtime &gt; Restart and run all...** をクリックすると、ランタイムを再起動して始めからすべてのセルの再実行ができます。\n",
        "\n",
        "[1] Jeff Donahue・Karen Simonyan『[Large Scale Adversarial Representation Learning](https://arxiv.org/abs/1907.02544)』*arxiv:1907.02544* (2019)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtGFwUKOA9jt"
      },
      "source": [
        "まず、モジュールのパスを設定します。デフォルトでは **`https://tfhub.dev/deepmind/bigbigan-resnet50/1`** から小さい ResNet-50 ベースのエンコーダの BigBiGAN モデルを読み込みます。最良の表現学習結果を得るためにもっと大きな RevNet-50-x4 ベースのモデルを読み込む場合には、アクティブな **`module_path`** の設定をコメントアウトして、その他の設定をアンコメントします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoY9pl0FBoUS"
      },
      "outputs": [],
      "source": [
        "module_path = 'https://tfhub.dev/deepmind/bigbigan-resnet50/1'  # ResNet-50\n",
        "# module_path = 'https://tfhub.dev/deepmind/bigbigan-revnet50x4/1'  # RevNet-50 x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr01cszC_vcC"
      },
      "source": [
        "## セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPdT-hYj1XXQ"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import IPython.display\n",
        "import PIL.Image\n",
        "from pprint import pformat\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouePZy6-CFJl"
      },
      "source": [
        "## 関数を定義して画像を表示する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBQPtmrY2N91"
      },
      "outputs": [],
      "source": [
        "def imgrid(imarray, cols=4, pad=1, padval=255, row_major=True):\n",
        "  \"\"\"Lays out a [N, H, W, C] image array as a single image grid.\"\"\"\n",
        "  pad = int(pad)\n",
        "  if pad < 0:\n",
        "    raise ValueError('pad must be non-negative')\n",
        "  cols = int(cols)\n",
        "  assert cols >= 1\n",
        "  N, H, W, C = imarray.shape\n",
        "  rows = N // cols + int(N % cols != 0)\n",
        "  batch_pad = rows * cols - N\n",
        "  assert batch_pad >= 0\n",
        "  post_pad = [batch_pad, pad, pad, 0]\n",
        "  pad_arg = [[0, p] for p in post_pad]\n",
        "  imarray = np.pad(imarray, pad_arg, 'constant', constant_values=padval)\n",
        "  H += pad\n",
        "  W += pad\n",
        "  grid = (imarray\n",
        "          .reshape(rows, cols, H, W, C)\n",
        "          .transpose(0, 2, 1, 3, 4)\n",
        "          .reshape(rows*H, cols*W, C))\n",
        "  if pad:\n",
        "    grid = grid[:-pad, :-pad]\n",
        "  return grid\n",
        "\n",
        "def interleave(*args):\n",
        "  \"\"\"Interleaves input arrays of the same shape along the batch axis.\"\"\"\n",
        "  if not args:\n",
        "    raise ValueError('At least one argument is required.')\n",
        "  a0 = args[0]\n",
        "  if any(a.shape != a0.shape for a in args):\n",
        "    raise ValueError('All inputs must have the same shape.')\n",
        "  if not a0.shape:\n",
        "    raise ValueError('Inputs must have at least one axis.')\n",
        "  out = np.transpose(args, [1, 0] + list(range(2, len(a0.shape) + 1)))\n",
        "  out = out.reshape(-1, *a0.shape[1:])\n",
        "  return out\n",
        "\n",
        "def imshow(a, format='png', jpeg_fallback=True):\n",
        "  \"\"\"Displays an image in the given format.\"\"\"\n",
        "  a = a.astype(np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  PIL.Image.fromarray(a).save(data, format)\n",
        "  im_data = data.getvalue()\n",
        "  try:\n",
        "    disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  except IOError:\n",
        "    if jpeg_fallback and format != 'jpeg':\n",
        "      print ('Warning: image was too large to display in format \"{}\"; '\n",
        "             'trying jpeg instead.').format(format)\n",
        "      return imshow(a, format='jpeg')\n",
        "    else:\n",
        "      raise\n",
        "  return disp\n",
        "\n",
        "def image_to_uint8(x):\n",
        "  \"\"\"Converts [-1, 1] float array to [0, 255] uint8.\"\"\"\n",
        "  x = np.asarray(x)\n",
        "  x = (256. / 2.) * (x + 1.)\n",
        "  x = np.clip(x, 0, 255)\n",
        "  x = x.astype(np.uint8)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ASXPMb6CaXR"
      },
      "source": [
        "## BigBiGAN TF Hub モジュールを読み込んで利用可能な機能を表示する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuG7G1ToCtaf"
      },
      "outputs": [],
      "source": [
        "# module = hub.Module(module_path, trainable=True, tags={'train'})  # training\n",
        "module = hub.Module(module_path)  # inference\n",
        "\n",
        "for signature in module.get_signature_names():\n",
        "  print('Signature:', signature)\n",
        "  print('Inputs:', pformat(module.get_input_info_dict(signature)))\n",
        "  print('Outputs:', pformat(module.get_output_info_dict(signature)))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAY-AmcNCj9_"
      },
      "source": [
        "## ラッパークラスを定義して様々な関数へのアクセスを容易にする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTKHkxfx1dAL"
      },
      "outputs": [],
      "source": [
        "class BigBiGAN(object):\n",
        "\n",
        "  def __init__(self, module):\n",
        "    \"\"\"Initialize a BigBiGAN from the given TF Hub module.\"\"\"\n",
        "    self._module = module\n",
        "\n",
        "  def generate(self, z, upsample=False):\n",
        "    \"\"\"Run a batch of latents z through the generator to generate images.\n",
        "\n",
        "    Args:\n",
        "      z: A batch of 120D Gaussian latents, shape [N, 120].\n",
        "\n",
        "    Returns: a batch of generated RGB images, shape [N, 128, 128, 3], range\n",
        "      [-1, 1].\n",
        "    \"\"\"\n",
        "    outputs = self._module(z, signature='generate', as_dict=True)\n",
        "    return outputs['upsampled' if upsample else 'default']\n",
        "\n",
        "  def make_generator_ph(self):\n",
        "    \"\"\"Creates a tf.placeholder with the dtype & shape of generator inputs.\"\"\"\n",
        "    info = self._module.get_input_info_dict('generate')['z']\n",
        "    return tf.placeholder(dtype=info.dtype, shape=info.get_shape())\n",
        "\n",
        "  def gen_pairs_for_disc(self, z):\n",
        "    \"\"\"Compute generator input pairs (G(z), z) for discriminator, given z.\n",
        "\n",
        "    Args:\n",
        "      z: A batch of latents (120D standard Gaussians), shape [N, 120].\n",
        "\n",
        "    Returns: a tuple (G(z), z) of discriminator inputs.\n",
        "    \"\"\"\n",
        "    # Downsample 256x256 image x for 128x128 discriminator input.\n",
        "    x = self.generate(z)\n",
        "    return x, z\n",
        "\n",
        "  def encode(self, x, return_all_features=False):\n",
        "    \"\"\"Run a batch of images x through the encoder.\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (256x256 RGB images), shape [N, 256, 256, 3], range\n",
        "        [-1, 1].\n",
        "      return_all_features: If True, return all features computed by the encoder.\n",
        "        Otherwise (default) just return a sample z_hat.\n",
        "\n",
        "    Returns: the sample z_hat of shape [N, 120] (or a dict of all features if\n",
        "      return_all_features).\n",
        "    \"\"\"\n",
        "    outputs = self._module(x, signature='encode', as_dict=True)\n",
        "    return outputs if return_all_features else outputs['z_sample']\n",
        "\n",
        "  def make_encoder_ph(self):\n",
        "    \"\"\"Creates a tf.placeholder with the dtype & shape of encoder inputs.\"\"\"\n",
        "    info = self._module.get_input_info_dict('encode')['x']\n",
        "    return tf.placeholder(dtype=info.dtype, shape=info.get_shape())\n",
        "\n",
        "  def enc_pairs_for_disc(self, x):\n",
        "    \"\"\"Compute encoder input pairs (x, E(x)) for discriminator, given x.\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (256x256 RGB images), shape [N, 256, 256, 3], range\n",
        "        [-1, 1].\n",
        "\n",
        "    Returns: a tuple (downsample(x), E(x)) of discriminator inputs.\n",
        "    \"\"\"\n",
        "    # Downsample 256x256 image x for 128x128 discriminator input.\n",
        "    x_down = tf.nn.avg_pool(x, ksize=2, strides=2, padding='SAME')\n",
        "    z = self.encode(x)\n",
        "    return x_down, z\n",
        "\n",
        "  def discriminate(self, x, z):\n",
        "    \"\"\"Compute the discriminator scores for pairs of data (x, z).\n",
        "\n",
        "    (x, z) must be batches with the same leading batch dimension, and joint\n",
        "      scores are computed on corresponding pairs x[i] and z[i].\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (128x128 RGB images), shape [N, 128, 128, 3], range\n",
        "        [-1, 1].\n",
        "      z: A batch of latents (120D standard Gaussians), shape [N, 120].\n",
        "\n",
        "    Returns:\n",
        "      A dict of scores:\n",
        "        score_xz: the joint scores for the (x, z) pairs.\n",
        "        score_x: the unary scores for x only.\n",
        "        score_z: the unary scores for z only.\n",
        "    \"\"\"\n",
        "    inputs = dict(x=x, z=z)\n",
        "    return self._module(inputs, signature='discriminate', as_dict=True)\n",
        "\n",
        "  def reconstruct_x(self, x, use_sample=True, upsample=False):\n",
        "    \"\"\"Compute BigBiGAN reconstructions of images x via G(E(x)).\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (256x256 RGB images), shape [N, 256, 256, 3], range\n",
        "        [-1, 1].\n",
        "      use_sample: takes a sample z_hat ~ E(x). Otherwise, deterministically\n",
        "        use the mean. (Though a sample z_hat may be far from the mean z,\n",
        "        typically the resulting recons G(z_hat) and G(z) are very\n",
        "        similar.\n",
        "      upsample: if set, upsample the reconstruction to the input resolution\n",
        "        (256x256). Otherwise return the raw lower resolution generator output\n",
        "        (128x128).\n",
        "\n",
        "    Returns: a batch of recons G(E(x)), shape [N, 256, 256, 3] if\n",
        "      `upsample`, otherwise [N, 128, 128, 3].\n",
        "    \"\"\"\n",
        "    if use_sample:\n",
        "      z = self.encode(x)\n",
        "    else:\n",
        "      z = self.encode(x, return_all_features=True)['z_mean']\n",
        "    recons = self.generate(z, upsample=upsample)\n",
        "    return recons\n",
        "\n",
        "  def losses(self, x, z):\n",
        "    \"\"\"Compute per-module BigBiGAN losses given data & latent sample batches.\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (256x256 RGB images), shape [N, 256, 256, 3], range\n",
        "        [-1, 1].\n",
        "      z: A batch of latents (120D standard Gaussians), shape [M, 120].\n",
        "\n",
        "    For the original BigBiGAN losses, pass batches of size N=M=2048, with z's\n",
        "    sampled from a 120D standard Gaussian (e.g., np.random.randn(2048, 120)),\n",
        "    and x's sampled from the ImageNet (ILSVRC2012) training set with the\n",
        "    \"ResNet-style\" preprocessing from:\n",
        "\n",
        "        https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_preprocessing.py\n",
        "\n",
        "    Returns:\n",
        "      A dict of per-module losses:\n",
        "        disc: loss for the discriminator.\n",
        "        enc: loss for the encoder.\n",
        "        gen: loss for the generator.\n",
        "    \"\"\"\n",
        "    # Compute discriminator scores on (x, E(x)) pairs.\n",
        "    # Downsample 256x256 image x for 128x128 discriminator input.\n",
        "    scores_enc_x_dict = self.discriminate(*self.enc_pairs_for_disc(x))\n",
        "    scores_enc_x = tf.concat([scores_enc_x_dict['score_xz'],\n",
        "                              scores_enc_x_dict['score_x'],\n",
        "                              scores_enc_x_dict['score_z']], axis=0)\n",
        "\n",
        "    # Compute discriminator scores on (G(z), z) pairs.\n",
        "    scores_gen_z_dict = self.discriminate(*self.gen_pairs_for_disc(z))\n",
        "    scores_gen_z = tf.concat([scores_gen_z_dict['score_xz'],\n",
        "                              scores_gen_z_dict['score_x'],\n",
        "                              scores_gen_z_dict['score_z']], axis=0)\n",
        "\n",
        "    disc_loss_enc_x = tf.reduce_mean(tf.nn.relu(1. - scores_enc_x))\n",
        "    disc_loss_gen_z = tf.reduce_mean(tf.nn.relu(1. + scores_gen_z))\n",
        "    disc_loss = disc_loss_enc_x + disc_loss_gen_z\n",
        "\n",
        "    enc_loss = tf.reduce_mean(scores_enc_x)\n",
        "    gen_loss = tf.reduce_mean(-scores_gen_z)\n",
        "\n",
        "    return dict(disc=disc_loss, enc=enc_loss, gen=gen_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L5SFfH4C9gu"
      },
      "source": [
        "## サンプル、再構成、ディスクリミネータスコア、損失の計算に使用するテンソルを作成する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goxtzcb-19NA"
      },
      "outputs": [],
      "source": [
        "bigbigan = BigBiGAN(module)\n",
        "\n",
        "# Make input placeholders for x (`enc_ph`) and z (`gen_ph`).\n",
        "enc_ph = bigbigan.make_encoder_ph()\n",
        "gen_ph = bigbigan.make_generator_ph()\n",
        "\n",
        "# Compute samples G(z) from encoder input z (`gen_ph`).\n",
        "gen_samples = bigbigan.generate(gen_ph)\n",
        "\n",
        "# Compute reconstructions G(E(x)) of encoder input x (`enc_ph`).\n",
        "recon_x = bigbigan.reconstruct_x(enc_ph, upsample=True)\n",
        "\n",
        "# Compute encoder features used for representation learning evaluations given\n",
        "# encoder input x (`enc_ph`).\n",
        "enc_features = bigbigan.encode(enc_ph, return_all_features=True)\n",
        "\n",
        "# Compute discriminator scores for encoder pairs (x, E(x)) given x (`enc_ph`)\n",
        "# and generator pairs (G(z), z) given z (`gen_ph`).\n",
        "disc_scores_enc = bigbigan.discriminate(*bigbigan.enc_pairs_for_disc(enc_ph))\n",
        "disc_scores_gen = bigbigan.discriminate(*bigbigan.gen_pairs_for_disc(gen_ph))\n",
        "\n",
        "# Compute losses.\n",
        "losses = bigbigan.losses(enc_ph, gen_ph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly7LWnSUDQ_P"
      },
      "source": [
        "## TensorFlow セッションを作成して変数を初期化する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPnzCHDWFJwx"
      },
      "outputs": [],
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcEVS26D-ues"
      },
      "source": [
        "# ジェネレーターのサンプル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYSA8Zvb-w7S"
      },
      "source": [
        "まず最初に、事前トレーニング済みの BigBiGAN ジェネレータからのサンプルを、標準のガウス（`np.random.randn`経由）からジェネレータの入力 `z` をサンプリングして可視化し、生成される画像を表示します。ここでは、標準的な GAN の能力を超えることは行わず（エンコーダは無視して）ジェネレータのみを使用しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zfpvw8fGNMr"
      },
      "outputs": [],
      "source": [
        "feed_dict = {gen_ph: np.random.randn(32, 120)}\n",
        "_out_samples = sess.run(gen_samples, feed_dict=feed_dict)\n",
        "print('samples shape:', _out_samples.shape)\n",
        "imshow(imgrid(image_to_uint8(_out_samples), cols=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v58CTfl8jTc"
      },
      "source": [
        "# TF-Flowers データセットから `test_images` を読み込む"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0kmzQ4EqKJt"
      },
      "source": [
        "BigBiGAN は ImageNet 上でトレーニングを行いますが、このデモに使用するには大きすぎるため、再構成の可視化およびエンコーダ特徴量計算の入力として、もっと小さな TF-Flowers [1] データセットを使用します。\n",
        "\n",
        "このセルでは TF-Flowers を読み込んで（必要に応じてデータセットをダウンロードします）、256x256 の RGB 画像サンプルの固定バッチを NumPy 配列の `test_images` に格納します。\n",
        "\n",
        "[1] https://www.tensorflow.org/datasets/catalog/tf_flowers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBgpkMdkUjL-"
      },
      "outputs": [],
      "source": [
        "def get_flowers_data():\n",
        "  \"\"\"Returns a [32, 256, 256, 3] np.array of preprocessed TF-Flowers samples.\"\"\"\n",
        "  import tensorflow_datasets as tfds\n",
        "  ds, info = tfds.load('tf_flowers', split='train', with_info=True)\n",
        "\n",
        "  # Just get the images themselves as we don't need labels for this demo.\n",
        "  ds = ds.map(lambda x: x['image'])\n",
        "\n",
        "  # Filter out small images (with minor edge length <256).\n",
        "  ds = ds.filter(lambda x: tf.reduce_min(tf.shape(x)[:2]) >= 256)\n",
        "\n",
        "  # Take the center square crop of the image and resize to 256x256.\n",
        "  def crop_and_resize(image):\n",
        "    imsize = tf.shape(image)[:2]\n",
        "    minor_edge = tf.reduce_min(imsize)\n",
        "    start = (imsize - minor_edge) // 2\n",
        "    stop = start + minor_edge\n",
        "    cropped_image = image[start[0] : stop[0], start[1] : stop[1]]\n",
        "    resized_image = tf.image.resize_bicubic([cropped_image], [256, 256])[0]\n",
        "    return resized_image\n",
        "  ds = ds.map(crop_and_resize)\n",
        "\n",
        "  # Convert images from [0, 255] uint8 to [-1, 1] float32.\n",
        "  ds = ds.map(lambda image: tf.cast(image, tf.float32) / (255. / 2.) - 1)\n",
        "\n",
        "  # Take the first 32 samples.\n",
        "  ds = ds.take(32)\n",
        "\n",
        "  return np.array(list(tfds.as_numpy(ds)))\n",
        "\n",
        "test_images = get_flowers_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAFJQU597n2A"
      },
      "source": [
        "# 再構成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmCQ9N9b7ptM"
      },
      "source": [
        "ここでは、実画像をエンコーダに通してジェネレータに戻し、画像 `x` から `G(E(x))` を計算して BigBiGAN の再構成を可視化します。以下には、左列に入力画像`x`を、右列に対応する再構成を表示します。\n",
        "\n",
        "再構成はピクセルが完璧に入力画像と一致しているわけではなく、むしろ入力の低レベルの詳細情報の大部分を「忘れる」一方で、高レベルのセマンティックな情報はキャプチャする傾向があることに注意してください。このことは、表現学習のアプローチで表示する画像の高レベルのセマンティックな情報の型をキャプチャするように BigBiGAN エンコーダが学習する可能性があることを示唆しています。\n",
        "\n",
        "また、256x256 入力の未加工画像の再構成は、ジェネレータが 128x128 の低解像度で生成することにも注意してください。可視化が目的なのでアップサンプルしています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2F3eq8aFRle"
      },
      "outputs": [],
      "source": [
        "test_images_batch = test_images[:16]\n",
        "_out_recons = sess.run(recon_x, feed_dict={enc_ph: test_images_batch})\n",
        "print('reconstructions shape:', _out_recons.shape)\n",
        "\n",
        "inputs_and_recons = interleave(test_images_batch, _out_recons)\n",
        "print('inputs_and_recons shape:', inputs_and_recons.shape)\n",
        "imshow(imgrid(image_to_uint8(inputs_and_recons), cols=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPpW3qdbEpXL"
      },
      "source": [
        "# エンコーダの特徴量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gAW76YxEsZa"
      },
      "source": [
        "ここでは、標準的な表現学習評価に使用されるエンコーダから特徴量を計算する方法を示します。\n",
        "\n",
        "これらの特徴量は、線形分類器または最近傍法をベースとする分類器で使用する場合があります。全体平均プーリングの後に取得する標準的特徴（主な `avepool_feat`）を含むと共に、もっと大きな「BN+CReLU」特徴量（主な`bn_crelu_feat`）を使用して、最良の結果が得られるようにしています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpZYe5S_FQEw"
      },
      "outputs": [],
      "source": [
        "_out_features = sess.run(enc_features, feed_dict={enc_ph: test_images_batch})\n",
        "print('AvePool features shape:', _out_features['avepool_feat'].shape)\n",
        "print('BN+CReLU features shape:', _out_features['bn_crelu_feat'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGzahsms2w9a"
      },
      "source": [
        "# 識別器のスコアと損失"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2_5BIBN21Hr"
      },
      "source": [
        "最後に、エンコーダとジェネレータのペアのバッチについて、識別器のスコアと損失を計算します。これらの損失をオプティマイザに渡して BigBiGAN のトレーニングを行うことができます。\n",
        "\n",
        "上の画像のバッチをエンコーダ入力`x`として使用し、エンコーダスコアを`D(x, E(x))`として計算します。ジェネレータの入力には`np.random.randn`を使用して 120D の標準ガウスから`z`をサンプリングし、ジェネレータのスコアを`D(G(z), z)`として計算します。\n",
        "\n",
        "識別器は `(x, z)` のペアに対する結合スコア `score_xz`、および `<code data-md-type=\"codespan\">x` と `z` の単独スコア `score_x` と `score_z` を予測します。これは、エンコーダのペアには高い（正の）スコアを、ジェネレータのペアには低い（負の）スコアを与えるようにトレーニングされています。大抵は以下のようになり、単独スコア `score_z` はどちらの場合でも負となりますが、エンコーダ出力 `E(x)` がガウスからの実際のサンプルに似ていることを示しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JJ8Go0dr22-"
      },
      "outputs": [],
      "source": [
        "feed_dict = {enc_ph: test_images, gen_ph: np.random.randn(32, 120)}\n",
        "_out_scores_enc, _out_scores_gen, _out_losses = sess.run(\n",
        "    [disc_scores_enc, disc_scores_gen, losses], feed_dict=feed_dict)\n",
        "print('Encoder scores:', {k: v.mean() for k, v in _out_scores_enc.items()})\n",
        "print('Generator scores:', {k: v.mean() for k, v in _out_scores_gen.items()})\n",
        "print('Losses:', _out_losses)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9v58CTfl8jTc"
      ],
      "name": "bigbigan_with_tf_hub.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
