{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCs7P9JTMlzV"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqn-HYw-Mkea"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stRetE8gMlmZ"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/wav2vec2_saved_model_finetuning\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/wav2vec2_saved_model_finetuning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/wav2vec2_saved_model_finetuning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub で表示</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/wav2vec2_saved_model_finetuning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
        "  <td>     <a href=\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\"> TF Hub モデルを参照</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndG8MjmJeicp"
      },
      "source": [
        "# LM ヘッドを使用した Wav2Vec2 の微調整\n",
        "\n",
        "このノートブックでは、事前にトレーニングされた wav2vec2 モデルを [TFHub](https://tfhub.dev) からロードし、事前にトレーニングされたモデルの上に言語モデリングヘッド（LM）を追加することにより、[LibriSpeech データセット](https://huggingface.co/datasets/librispeech_asr)で微調整します。基礎となるタスクは、**自動音声認識**のモデルを構築することです。つまり、音声が与えられた場合、モデルはそれをテキストに変換できる必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWk8nL6Ui-_0"
      },
      "source": [
        "## セットアップ\n",
        "\n",
        "このノートブックを実行する前に、GPU ランタイムを使用していることを確認してください（`Runtime` &gt; `Change runtime type` &gt; `GPU`）。次のセルは、[`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2) パッケージとその依存関係をインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seqTlMyeZvM4"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main\n",
        "!sudo apt-get install -y libsndfile1-dev\n",
        "!pip3 install -q SoundFile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuJL8-f0zn5"
      },
      "source": [
        "## `TFHub` を使用したモデルのセットアップ\n",
        "\n",
        "まず、いくつかのライブラリ/モジュールをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3_fgx4eZvM7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from wav2vec2 import Wav2Vec2Config\n",
        "\n",
        "config = Wav2Vec2Config()\n",
        "\n",
        "print(\"TF version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0rVUxyWsS5f"
      },
      "source": [
        "最初に TFHub からモデルをダウンロードし、モデルの署名を [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) でラップして、他の Keras レイヤーと同じようにこのモデルを使用できるようにします。幸い、`hub.KerasLayer` はたった1行で両方を実行できます。\n",
        "\n",
        "**注意: **`hub.KerasLayer` を使用してモデルをロードすると、モデルは少し不透明になりますが、モデルをより細かく制御する必要がある場合は、 `tf.keras.models.load_model(...)` を使用してモデルをロードできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO6QRC7KZvM9"
      },
      "outputs": [],
      "source": [
        "pretrained_layer = hub.KerasLayer(\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\", trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCputyVBv2e9"
      },
      "source": [
        "モデルエクスポートスクリプトに興味がある場合は、この[スクリプト](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/export2hub.py)を参照できます。オブジェクト `pretrained_layer` は、[`Wav2Vec2Model`](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/wav2vec2/modeling.py) のフリーズバージョンです。これらの事前トレーニング済みの重みは、[このスクリプト](https://huggingface.co/facebook/wav2vec2-base)を使用して HuggingFacePyTorch[ 事前トレーニング済み](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/convert_torch_to_tf.py)の重みから変換されました。\n",
        "\n",
        "もともと、wav2vec2 はマスクされた時間ステップの真の量子化された潜在的な音声表現を識別することを目的として、マスクされた言語モデリングアプローチで事前にトレーニングされていました。トレーニングの目的について詳しくは、[wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) をお読みください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SseDnCr7hyhC"
      },
      "source": [
        "ここで、次のセルで役立つ定数とハイパーパラメータをいくつか定義します。モデルのシグネチャは静的シーケンス長 `246000` のみを受け入れるため、`AUDIO_MAXLEN` は意図的に `246000` に設定されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiILuMBERxlO"
      },
      "outputs": [],
      "source": [
        "AUDIO_MAXLEN = 246000\n",
        "LABEL_MAXLEN = 256\n",
        "BATCH_SIZE = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V4gTgGLgXvO"
      },
      "source": [
        "次のセルでは、`pretrained_layer` と高密度レイヤー（LM ヘッド）を [Keras の Functional API](https://www.tensorflow.org/guide/keras/functional) でラップします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3CUN1KEB10Q"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(AUDIO_MAXLEN,))\n",
        "hidden_states = pretrained_layer(inputs)\n",
        "outputs = tf.keras.layers.Dense(config.vocab_size)(hidden_states)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zDXuoMXhDMo"
      },
      "source": [
        "各タイムステップで語彙内の各トークンの確率を予測するため、高密度レイヤー（上記で定義）の出力ディメンションは `vocab_size` です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPp18ZHRtnq-"
      },
      "source": [
        "## トレーニング状態のセットアップ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATQy1ZK3vFr7"
      },
      "source": [
        "TensorFlow では、`model.call` または `model.build` が初めて呼び出されたときにのみモデルの重みが作成されるため、次のセルでモデルの重みが作成されます。さらに、`model.summary()` を実行して、トレーニング可能なパラメータの総数を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgL5wyaXZvM-"
      },
      "outputs": [],
      "source": [
        "model(tf.random.uniform(shape=(BATCH_SIZE, AUDIO_MAXLEN)))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxxA4Fevp7m"
      },
      "source": [
        "次に、モデルをトレーニングできるように `loss_fn` とオプティマイザを定義する必要があります。次のセルで定義されます。簡単にするため、`Adam` オプティマイザを使用します。`CTCLoss` は、入力サブパーツを出力サブパーツと簡単に位置合わせできないタスク（`ASR` など）に使用される一般的な損失タイプです。 CTC-loss の詳細については、このすばらしい[ブログ投稿](https://distill.pub/2017/ctc/)をご覧ください。\n",
        "\n",
        "（[`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2)パッケージからの）`CTCLoss`は、`config`、`model_input_shape`、`division_factor` の 3 つの引数を受け入れます。`division_factor=1` の場合、損失は単純に合計されるため、それに応じて `division_factor` を渡して、バッチ全体の平均を取得します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glDepVEHZvM_"
      },
      "outputs": [],
      "source": [
        "from wav2vec2 import CTCLoss\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "loss_fn = CTCLoss(config, (BATCH_SIZE, AUDIO_MAXLEN), division_factor=BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mvTuOXpwsQe"
      },
      "source": [
        "## データの読み込みと前処理\n",
        "\n",
        "LibriSpeech データセットを[公式ウェブサイト](http://www.openslr.org/12)からダウンロードして設定しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4kIEC77cBCM"
      },
      "outputs": [],
      "source": [
        "!wget https://www.openslr.org/resources/12/dev-clean.tar.gz -P ./data/train/\n",
        "!tar -xf ./data/train/dev-clean.tar.gz -C ./data/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsQpmpn6jrMI"
      },
      "source": [
        "**注意: **このノートブックはデモンストレーションのみを目的としているため、`dev-clean` 構成を使用しており、少量のデータが必要です。完全なトレーニングデータは、[LibriSpeech Webサイト](http://www.openslr.org/12)から簡単にダウンロードできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynxAjtGHGFpM"
      },
      "outputs": [],
      "source": [
        "ls ./data/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBMiORo0xJD0"
      },
      "source": [
        "データセットは LibriSpeech ディレクトリにあります。これらのファイルを調べてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkIu_Wt4ZvNA"
      },
      "outputs": [],
      "source": [
        "data_dir = \"./data/train/LibriSpeech/dev-clean/2428/83705/\"\n",
        "all_files = os.listdir(data_dir)\n",
        "\n",
        "flac_files = [f for f in all_files if f.endswith(\".flac\")]\n",
        "txt_files = [f for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "print(\"Transcription files:\", txt_files, \"\\nSound files:\", flac_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEObi_Apk3ZD"
      },
      "source": [
        "なるほど。各サブディレクトリには、多くの `.flac` ファイルと `.txt` ファイルがあります。`.txt` ファイルには、そのサブディレクトリに存在するすべての音声サンプル（つまり、`.flac` ファイル）のテキスト文字起こしが含まれています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYW6WKJflO2e"
      },
      "source": [
        "このテキストデータは次のようにロードできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEBKxQblHPwq"
      },
      "outputs": [],
      "source": [
        "def read_txt_file(f):\n",
        "  with open(f, \"r\") as f:\n",
        "    samples = f.read().split(\"\\n\")\n",
        "    samples = {s.split()[0]: \" \".join(s.split()[1:]) for s in samples if len(s.split()) > 2}\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldkf_ceb0_YW"
      },
      "source": [
        "同様に、`.flac` ファイルから音声サンプルをロードするための関数を定義します。\n",
        "\n",
        "wav2vec2 は `16K` の頻度で事前トレーニングされているため、`REQUIRED_SAMPLE_RATE` は `16000` に設定されます。頻度によるデータ分布の大きな変化なしに、微調整することをお勧めします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOJ3OzPsTyXv"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "REQUIRED_SAMPLE_RATE = 16000\n",
        "\n",
        "def read_flac_file(file_path):\n",
        "  with open(file_path, \"rb\") as f:\n",
        "      audio, sample_rate = sf.read(f)\n",
        "  if sample_rate != REQUIRED_SAMPLE_RATE:\n",
        "      raise ValueError(\n",
        "          f\"sample rate (={sample_rate}) of your files must be {REQUIRED_SAMPLE_RATE}\"\n",
        "      )\n",
        "  file_id = os.path.split(file_path)[-1][:-len(\".flac\")]\n",
        "  return {file_id: audio}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sxDN8P4nWkW"
      },
      "source": [
        "次に、ランダムなサンプルをいくつか選び、視覚化してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI5J-2Dfm_wT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "import random\n",
        "\n",
        "file_id = random.choice([f[:-len(\".flac\")] for f in flac_files])\n",
        "flac_file_path, txt_file_path = os.path.join(data_dir, f\"{file_id}.flac\"), os.path.join(data_dir, \"2428-83705.trans.txt\")\n",
        "\n",
        "print(\"Text Transcription:\", read_txt_file(txt_file_path)[file_id], \"\\nAudio:\")\n",
        "Audio(filename=flac_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8jJ7Ed81p_A"
      },
      "source": [
        "次に、すべての音声とテキストのサンプルを組み合わせて、その目的のために（次のセルで）関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI-5YCzaTsei"
      },
      "outputs": [],
      "source": [
        "def fetch_sound_text_mapping(data_dir):\n",
        "  all_files = os.listdir(data_dir)\n",
        "\n",
        "  flac_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".flac\")]\n",
        "  txt_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "  txt_samples = {}\n",
        "  for f in txt_files:\n",
        "    txt_samples.update(read_txt_file(f))\n",
        "\n",
        "  speech_samples = {}\n",
        "  for f in flac_files:\n",
        "    speech_samples.update(read_flac_file(f))\n",
        "\n",
        "  assert len(txt_samples) == len(speech_samples)\n",
        "\n",
        "  samples = [(speech_samples[file_id], txt_samples[file_id]) for file_id in speech_samples.keys() if len(speech_samples[file_id]) < AUDIO_MAXLEN]\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx95Lxvu0nT4"
      },
      "source": [
        "いくつかのサンプルを見てみましょう..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ls7X_jqIz4R"
      },
      "outputs": [],
      "source": [
        "samples = fetch_sound_text_mapping(data_dir)\n",
        "samples[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjhSWfsnlCL"
      },
      "source": [
        "注意: このノートブックで少量のデータセットを操作する際に、このデータはメモリに読み込まれます。ただし、完全なデータセット（〜300 GB）でトレーニングするには、データを遅延ロードする必要があります。[このスクリプト](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/data_utils.py)を参照して、詳細を確認できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg8Zia1kzw0J"
      },
      "source": [
        "今すぐデータを前処理しましょう!!!\n",
        "\n",
        "まず、`gsoc-wav2vec2` パッケージを使用してトークナイザーとプロセッサーを定義します。次に、非常に簡単な前処理を行います。`processor` はフレーム軸に対して生の音声を正規化し、`tokenizer` はモデル出力を文字列に変換し（定義された語彙を使用）、特別なトークンを削除します（トークナイザーの構成によって異なります）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaat_hMLNVHF"
      },
      "outputs": [],
      "source": [
        "from wav2vec2 import Wav2Vec2Processor\n",
        "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
        "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  label = tokenizer(text)\n",
        "  return tf.constant(label, dtype=tf.int32)\n",
        "\n",
        "def preprocess_speech(audio):\n",
        "  audio = tf.constant(audio, dtype=tf.float32)\n",
        "  return processor(tf.transpose(audio))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyKl8QP-zRFC"
      },
      "source": [
        "次に、上記のセルで定義した前処理関数を呼び出す Python ジェネレーターを定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoQrRalwMpQ6"
      },
      "outputs": [],
      "source": [
        "def inputs_generator():\n",
        "  for speech, text in samples:\n",
        "    yield preprocess_speech(speech), preprocess_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vlm3ySFULsG"
      },
      "source": [
        "## `tf.data.Dataset` のセットアップ\n",
        "\n",
        "次のセルは、`.from_generator(...)` メソッドを使用して `tf.data.Dataset` オブジェクトをセットアップします。上記のセルで定義した `generator` オブジェクトを使用します。\n",
        "\n",
        "**注意: **分散トレーニング（特に TPU）の場合、`.from_generator(...)` は現在機能しないため、`.tfrecord` 形式で保存されたデータでトレーニングすることをお勧めします（注意: TFRecord は、TPU が最大限に機能するように、理想的には GCS バケット内に保存する必要があります）。\n",
        "\n",
        "LibriSpeech データを tfrecords に変換する方法の詳細については、[このスクリプト](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/make_tfrecords.py) を参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbQ_dMwGO62h"
      },
      "outputs": [],
      "source": [
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None),  dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        ")\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(inputs_generator, output_signature=output_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXBbNsRyPyw3"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(flac_files)\n",
        "SEED = 42\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DAUmns3pXfr"
      },
      "source": [
        "データセットを複数のバッチに渡すため、次のセルでバッチを準備しましょう。ここで、バッチ内のすべてのシーケンスを一定の長さにパディングする必要があります。そのために `.padded_batch(...)` メソッドを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okhko1IWRida"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(AUDIO_MAXLEN, LABEL_MAXLEN), padding_values=(0.0, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A45CjQG5qSbV"
      },
      "source": [
        "アクセラレータ（GPU/TPU など）は非常に高速であり、データの読み込み部分が CPU で発生するため、トレーニング中にデータの読み込み（および前処理）がボトルネックになることがよくあります。これにより、特に多くのオンライン前処理が含まれる場合や、データが GCS バケットからオンラインでストリーミングされる場合に、トレーニング時間が大幅に増加する可能性があります。これらの問題に対応するために、`tf.data.Dataset` は `.prefetch(...)` メソッドを提供します。この方法は、モデルが現在のバッチで（GPU/TPU で）予測を行っている間に、次のいくつかのバッチを（CPU で）並行して準備するのに役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-bKu2YjRior"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqk2cs6LxVIh"
      },
      "source": [
        "このノートブックはデモンストレーションを目的に作成されているため、最初に `num_train_batches` を取得し、それだけでトレーニングを実行します。ただし、データセット全体でトレーニングすることをお勧めします。同様に、`num_val_batches` のみを評価します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6GO5oYUxXtz"
      },
      "outputs": [],
      "source": [
        "num_train_batches = 10\n",
        "num_val_batches = 4\n",
        "\n",
        "train_dataset = dataset.take(num_train_batches)\n",
        "val_dataset = dataset.skip(num_train_batches).take(num_val_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzAOI78tky08"
      },
      "source": [
        "## モデルのトレーニング\n",
        "\n",
        "モデルをトレーニングするため、モデルを `.compile(...)` でコンパイルした後に、`.fit(...)` メソッドを直接呼び出します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuBY2sZElgwg"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer, loss=loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qswxafSl0HjO"
      },
      "source": [
        "上記のセルは、トレーニング状態を設定します。これで、`.fit(...)` メソッドを使用してトレーニングを開始できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtuSfnj1l-I_"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
        "history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySvp8r2E1q_V"
      },
      "source": [
        "後で推論を実行できるように、`.save(...)` メソッドを使用してモデルを保存しましょう。[TFHub のドキュメント](https://www.tensorflow.org/hub/publish)に従って、この SavedModel を TFHub にエクスポートすることもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0KEYcwydwjF"
      },
      "outputs": [],
      "source": [
        "save_dir = \"finetuned-wav2vec2\"\n",
        "model.save(save_dir, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkOpp9rZ211t"
      },
      "source": [
        "注意: このモデルは推論のみに使用するため、`include_optimizer=False` を設定しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJfPlTgezD0i"
      },
      "source": [
        "## 評価\n",
        "\n",
        "次に、検証データセットの単語誤り率を計算します\n",
        "\n",
        "**単語誤り率**（WER）は、自動音声認識システムのパフォーマンスを測定するための一般的な指標です。WER は、単語レベルで機能するレーベンシュタイン距離から導出されます。単語誤り率は WER = (S + D + I) / N = (S + D + I) / (S + D + C) で計算でき、S は置換の数、D は削除の数、I は挿入の数、C は正しい単語の数、N は参照内の単語の数です（N=S+D+C）。この値は、誤って予測された単語の割合を示します。\n",
        "\n",
        "WER の詳細については、[この論文](https://www.isca-speech.org/archive_v0/interspeech_2004/i04_2765.html)を参照してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_91Y7-r3xu"
      },
      "source": [
        "[HuggingFace データセット](https://huggingface.co/docs/datasets/)ライブラリの `load_metric(...)` 関数を使用します。最初に `pip` を使用して `datasets` ライブラリをインストールしてから、`metric`オブジェクトを定義しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW9F_oVDU1TZ"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q datasets\n",
        "\n",
        "from datasets import load_metric\n",
        "metric = load_metric(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssWXWc7CZvNB"
      },
      "outputs": [],
      "source": [
        "@tf.function(jit_compile=True)\n",
        "def eval_fwd(batch):\n",
        "  logits = model(batch, training=False)\n",
        "  return tf.argmax(logits, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFh1myg1x4ua"
      },
      "source": [
        "次に、検証データの評価を実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQTFVjZghckJ"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for speech, labels in tqdm(val_dataset, total=num_val_batches):\n",
        "    predictions  = eval_fwd(speech)\n",
        "    predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "    references = [tokenizer.decode(label, group_tokens=False) for label in labels.numpy().tolist()]\n",
        "    metric.add_batch(references=references, predictions=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCc8qBesv3e"
      },
      "source": [
        "`tokenizer.decode(...)` メソッドを使用して、予測とラベルをテキストにデコードし直し、後で `WER` 計算用のメトリックに追加します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI_URj8Wtb2g"
      },
      "source": [
        "では、次のセルでメトリック値を計算しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a83wekLgWMod"
      },
      "outputs": [],
      "source": [
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cD1OgVEjl4"
      },
      "source": [
        "**注意: **モデルは非常に小さなデータでトレーニングされており、ASR のようなタスクでは、音声からテキストへのマッピングを学習するために大量のデータが必要になることが多いため、ここでのメトリック値は意味がありません。良い結果を得るには、おそらく大きなデータでトレーニングする必要があります。このノートブックは、事前にトレーニングされた音声モデルを微調整するためのテンプレートを提供します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G14o706kdTE1"
      },
      "source": [
        "## 推論\n",
        "\n",
        "トレーニングプロセスに満足し、モデルを `save_dir` に保存したので、このモデルを推論に使用する方法を確認します。\n",
        "\n",
        "まず、`tf.keras.models.load_model(...)` を使用してモデルをロードします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrTrExiUdaED"
      },
      "outputs": [],
      "source": [
        "finetuned_model = tf.keras.models.load_model(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luodSroz20SR"
      },
      "source": [
        "推論を実行するための音声サンプルをダウンロードしましょう。次のサンプルを音声サンプルに置き換えることもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUE0shded6Ej"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/SA2.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycBjU_U53FjL"
      },
      "source": [
        "`soundfile.read(...)` を使用して音声サンプルを読み取り、モデルのシグネチャを満たすために `AUDIO_MAXLEN` にパディングします。次に、<br>`Wav2Vec2Processor` インスタンスを使用してその音声サンプルを正規化し、モデルにフィードします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7CARje4d5_H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "speech, _ = sf.read(\"SA2.wav\")\n",
        "speech = np.pad(speech, (0, AUDIO_MAXLEN - len(speech)))\n",
        "speech = tf.expand_dims(processor(tf.constant(speech)), 0)\n",
        "\n",
        "outputs = finetuned_model(speech)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUSttSPa30qP"
      },
      "source": [
        "上で定義した `Wav2Vec2tokenizer` インスタンスを使用して、数値をデコードしてテキストシーケンスに戻しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYdJqxQ4llgI"
      },
      "outputs": [],
      "source": [
        "predictions = tf.argmax(outputs, axis=-1)\n",
        "predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXC757bztJc"
      },
      "source": [
        "モデルはこのノートブックの大きなデータでトレーニングされたことがないため、この予測は非常にランダムです（このノートブックは完全なトレーニングを行うためのものではないためです）。このモデルを完全な LibriSpeech データセットでトレーニングすると、適切な予測が得られます。\n",
        "\n",
        "ようやくこのノートブックの最後にたどり着きました。しかし、音声関連のタスクについての TensorFlow の学習はこれで終わりではありません。この[リポジトリ](https://github.com/tulasiram58827/TTS_TFLite)には、さらにすばらしいチュートリアルがいくつか含まれています。このノートブックでバグが発生した場合は、[ここ](https://github.com/vasudevgupta7/gsoc-wav2vec2/issues)で問題を作成してください。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rWk8nL6Ui-_0",
        "wvuJL8-f0zn5",
        "oPp18ZHRtnq-",
        "1mvTuOXpwsQe",
        "7Vlm3ySFULsG",
        "CzAOI78tky08",
        "SJfPlTgezD0i",
        "G14o706kdTE1"
      ],
      "name": "wav2vec2_saved_model_finetuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
