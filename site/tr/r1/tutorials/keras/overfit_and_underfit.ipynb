{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fTFj8ft5dlbS"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "lzyBOpYMdp3F"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "m_x4KfSJ7Vt7"
      },
      "outputs": [],
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C9HmC2T4ld5B"
      },
      "source": [
        "# Aşırı uyum (overfitting) ve yetersiz uyum (underfitting) nedir ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kRTxFhXAlnl1"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/tr/r1/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/tr/r1/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4hj2R9kBjERd"
      },
      "source": [
        "Note: Bu dökümanlar TensorFlow gönüllü kullanıcıları tarafından çevirilmiştir.\n",
        "Topluluk tarafından sağlananan çeviriler gönüllülerin ellerinden geldiğince\n",
        "güncellendiği için [Resmi İngilizce dökümanlar](https://www.tensorflow.org/?hl=en)\n",
        "ile bire bir aynı olmasını garantileyemeyiz. Eğer bu tercümeleri iyileştirmek\n",
        "için önerileriniz var ise lütfen [tensorflow/docs](https://github.com/tensorflow/docs)\n",
        "havuzuna pull request gönderin. Gönüllü olarak çevirilere katkıda bulunmak için\n",
        "[docs-tr@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-tr)\n",
        "listesi ile iletişime geçebilirsiniz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "19rPukKZsPG6"
      },
      "source": [
        "Bu örneğimizde, 'tf.keras' API'sini kullanacağız. Tensorflow 'tf.keras' API'si ile ilgili daha fazla bilgiye [Keras guide](https://www.tensorflow.org/r1/guide/keras) linki üzerinden ulaşabilirsiniz.\n",
        "\n",
        "Önceki örneklerimizde (film yorumlarının sınıflandırılması, yakıt verimliliğinin tahminlenmesi) gördüğünüz gibi, doğrulama seti üzerinden ölçtüğümüz model doğruluğu, belirli bir epoch sayısında tepe noktasına ulaşıp, sonrasında azalmaya başlamaktadır. \n",
        "\n",
        "Diğer bir deyişle, modelimiz eğitim veri setine *aşırı uyumlu (overfit)* hale gelmektedir. Aşırı uyum ile nasıl baş edeceğimiz önemlidir. *Eğitim veri seti* ile çok yüksek doğruluk değerine sahip olmamız mümkün olsa bile, asıl amacımız modelin daha önce görmediği veriler ile yani *test veri seti* ile yüksek doğrulukta sonuç üretebilmesi, bu şekilde genelleştirilmesidir.\n",
        "\n",
        "Aşırı uyumun tersi *yetersiz uyum (underfittig)* 'dur. Yetersiz uyum, test verisi ile değerlendirdiğimizde modelde hala belirli bir iyileşme alanının olması durumudur. Yetersiz uyum belirli nedenler sonucunda oluşur: Eğer model yeterince güçlü değil ise, çok fazla regülarize edildi ise, yeterince uzun süre eğitilmedi ise. Bu, ağın eğitim verisinden ilgili çıkarımları gerektiği kadar yapamadığı anlamına gelmektedir. \n",
        "\n",
        "Bunun yanında eğer modeli çok uzun süre eğitirseniz, moldel eğitim veri setine aşırı uyum sağlayarak, yaptığı çıkarımlar ile test veri setinde iyi şekilde çalışabilmesi için gerekli olan genelleme yeteneğini kaybedecektir. Modelimizin daha iyi performans göstermesi için dengeyi sağlamalıyız. Bu nedenle, aşağıda inceleyeceğimiz gibi, modelin uygun sayıda epoch ile nasıl eğitileceğini anlamak önemli bir beceridir. \n",
        "\n",
        "Aşırı uyumu engellemek için en iyi çözüm daha fazla eğitim verisinin kullanılmasıdır. Daha fazla veri ile eğitilen modeller doğal olarak daha iyi genelleme yeteneğine sahip olurlar. Bunun mümkün olmadığı durumlarda, ikinci en iyi çözüm *regularization* tekniklerini kullanmak olur. Bu teknikleri kullandığımızda, modelimizin sahip olacağı bilgi tipi ve miktarını sınırlarız. Model ağımız daha az miktarda çıkarımı hafızasına alırsa, optimizasyon süreci en önemli çıkarımları yapması için modeli zorlayacaktır. Bunun sonucunda modelin genelleme yeteneği artacaktır. \n",
        "\n",
        "Bu notebook'ta, IMBD film yorumlarının sınıflandırılması modelimizin iyileştirilmesi için en çok kullanılan iki regularizasyon tekniği olan ağırlık regularizasyonu (weigth regularization) ve dropout teknikleri kullanılacaktır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OP5sztkoETj0"
      },
      "outputs": [],
      "source": [
        "# keras.datasets.imdb is broken in 1.13 and 1.14, by np 1.16.3\n",
        "!pip install tf_nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5pZ8A2liqvgk"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1cweoTiruj8O"
      },
      "source": [
        "## IMDB veri setini indirelim\n",
        "\n",
        "Önceki notebook'ta yer alan `emmbeding` işlemi yerine, bu örnekte cümlelere `multi-hot encoding` işlemi uygulayacağız. Modelimiz hızlı bir şekilde eğitim verisine aşırı uyum gösterecektir. Bu şekilde, aşırı uyumun nasıl oluştuğunu ve nasıl baş edeceğimizi göstermiş olacağız. \n",
        "\n",
        "`Multi-hot-encoding` ile listemizi 0'lar ve 1'ler içeren vektöre dönüştürelim. Örnek olarak bu işlem ile `[3, 5]` dizisi, 3 ve 5 indisleri 1 olan bunun dışındaki tüm değerleri 0 olan 10,000 boyutlu bir vektöre çevrilir. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QpzE4iqZtJly"
      },
      "outputs": [],
      "source": [
        "NUM_WORDS = 10000\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n",
        "\n",
        "def multi_hot_sequences(sequences, dimension):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, word_indices in enumerate(sequences):\n",
        "        results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "\n",
        "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
        "test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MzWVeXe3NBTn"
      },
      "source": [
        "Sonuçta oluşan `multi-hot` vektörlerden birine bakalım. Kelime indisleri frekanslarına göre sıralanmıştır, yani aşağıdaki grafikte görüldüğü gibi 0 indeksine yaklaştıkça daha fazla 1 değeri görürüz: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "71kr5rG4LkGM"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lglk41MwvU5o"
      },
      "source": [
        "## Aşırı uyumu açıklayalım\n",
        "\n",
        "Aşırı uyumu engellemenin en basit yolu model boyutunun küçültülmesidir. Model boyutunu, modelin katman sayısına ve katmanları içersinde yere alan birim sayısına bağlı olan öğrenebilir parametrelerin sayısı belirler. Derin öğrenmede, modelde yer alan öğrenebilir parametrelerin sayısı modelin \"kapasitesi\" belirlemektedir. Daha basit haliyle, daha fazla parametreye sahip model, daha fazla \"hafıza kapasitesine\" sahiptir. Bu nedenle, model kusursuz bir şekilde eğitim verisine uyum sağlayacaktır. Genelleme yeteneğine sahip olmayan bu modelin, daha önce görmediği veriler ile başarılı bir şekilde tahminleme yapması mümkün değildir. \n",
        "\n",
        "Her zaman aklımızda tutmamız gerekir: derin öğrenme modelleri eğitim verisine uyum sağlama eğilimindedirler, asıl başarılması gereken bu uyum değil modele genelleme yeteneği kazandırabilmektir. \n",
        "\n",
        "Buna karşın, eğer ağın hafızaya alma kaynakları kısıtlı ise, öğrenmesi kolay olmayacaktır. Eğitim sürecinde kayıp değerini küçültmek ve daha fazla tahminleme gücü kazanmak için, modelin sıkıştırılmış çıkarımlar oluşturması gerekecektir. Bununla birlikte, modeli küçük oluşturusak eğitim veri setine uyum sağlamakta zorluk çekecektir.  \"Çok fazla kapasite\" ile \"olması gerektiğinden az kapasite\" arasında bir denge oluşturulması gerekmektedir.\n",
        "\n",
        "Ne yazık ki, doğru büyüklükte bir model yapısını (modeldeki katman sayısı ve katmanların büyüklüğü) belirleyebileceğimiz sihirli bir formül bulunmamaktadır. Farklı model yapılarını deneyerek, problemimizin çözümü için en uygun modeli belirlememiz gerekmektedir. \n",
        "\n",
        "Uygun model boyutunu belirlemek için, göreceli olarak az katman ve parametreye sahip bir yapıyla başlamak en iyisidir. Doğrulama kayıp değerlerinde azalma görmemeye başladığımız zamana kadar, modelin katman sayısını veya katmanlar içerisindeki birimleri artırabiliriz. Film yorumlarının sınıflandırılmasını sağlayan ağımızda bunu deneyelim: \n",
        "\n",
        "Referans olarak, sadece ```Yoğun-Dense``` katmanlar içeren basit bir modelle başlayacağız. Sonrasında bu modelin daha küçük ve büyük versiyonlarını oluşturarak sonuçları karşılaştıracağız."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ReKHdC2EgVu"
      },
      "source": [
        "### Referans modeli oluşturalım"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QKgdXPx9usBa"
      },
      "outputs": [],
      "source": [
        "baseline_model = keras.Sequential([\n",
        "    # `input_shape` is only required here so that `.summary` works. \n",
        "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "])\n",
        "\n",
        "baseline_model.compile(optimizer='adam',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['accuracy', 'binary_crossentropy'])\n",
        "\n",
        "baseline_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LqG3MXF5xSjR"
      },
      "outputs": [],
      "source": [
        "baseline_history = baseline_model.fit(train_data,\n",
        "                                      train_labels,\n",
        "                                      epochs=20,\n",
        "                                      batch_size=512,\n",
        "                                      validation_data=(test_data, test_labels),\n",
        "                                      verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L-DGRBbGxI6G"
      },
      "source": [
        "### Daha küçük bir model oluşturalım"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SrfoVQheYSO5"
      },
      "source": [
        "Yukarıda oluşturduğumuz referans modele göre daha az gizli birim içeren modelimizi oluşturalım:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jksi-XtaxDAh"
      },
      "outputs": [],
      "source": [
        "smaller_model = keras.Sequential([\n",
        "    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(4, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "])\n",
        "\n",
        "smaller_model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', 'binary_crossentropy'])\n",
        "\n",
        "smaller_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jbngCZliYdma"
      },
      "source": [
        "Ve modelimizi aynı veriler ile eğitelim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ofn1AwDhx-Fe"
      },
      "outputs": [],
      "source": [
        "smaller_history = smaller_model.fit(train_data,\n",
        "                                    train_labels,\n",
        "                                    epochs=20,\n",
        "                                    batch_size=512,\n",
        "                                    validation_data=(test_data, test_labels),\n",
        "                                    verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vIPuf23FFaVn"
      },
      "source": [
        "### Daha büyük bir model oluşturalım\n",
        "\n",
        "Daha büyük bir model oluşturabilir ve nasıl hızlı bir şekilde aşırı uyum gösterdiğini görebiliriz. Sonrasında, problem için gereğinden fazla kapasiteye sahip bu modeli, karşılaştıma grubumuza ekleyelim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ghQwwqwqvQM9"
      },
      "outputs": [],
      "source": [
        "bigger_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "])\n",
        "\n",
        "bigger_model.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy','binary_crossentropy'])\n",
        "\n",
        "bigger_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D-d-i5DaYmr7"
      },
      "source": [
        "Ve yeniden, aynı veriler ile modeli eğitelim: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U1A99dhqvepf"
      },
      "outputs": [],
      "source": [
        "bigger_history = bigger_model.fit(train_data, train_labels,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=512,\n",
        "                                  validation_data=(test_data, test_labels),\n",
        "                                  verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fy3CMUZpzH3d"
      },
      "source": [
        "### Eğitim ve doğrulama kayıp değerlerini grafikte gösterelim\n",
        "\n",
        "<!--TODO(markdaoust): This should be a one-liner with tensorboard -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HSlo1F4xHuuM"
      },
      "source": [
        "Grafikteki düz çizgiler eğitim kayıp değerini, kesikli çizgiler ise doğrulama kayıp değerini (düşük kayıp değerlerinin daha iyi modelin göstergesi olduğunu hatırlayalım) göstermektedir. Küçük modelimizin referans modele göre daha geç (4ncü epoch yerine 6ncı epochta) aşırı uyum gösterdiğini görebiliriz. Bunun yanında küçük modelimizin performansı, aşırı uyum sonrasında daha yavaş düşmektedir. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0XmKDtOWzOpk"
      },
      "outputs": [],
      "source": [
        "def plot_history(histories, key='binary_crossentropy'):\n",
        "  plt.figure(figsize=(16,10))\n",
        "    \n",
        "  for name, history in histories:\n",
        "    val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                   '--', label=name.title()+' Val')\n",
        "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "             label=name.title()+' Train')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(key.replace('_',' ').title())\n",
        "  plt.legend()\n",
        "\n",
        "  plt.xlim([0,max(history.epoch)])\n",
        "\n",
        "\n",
        "plot_history([('baseline', baseline_history),\n",
        "              ('smaller', smaller_history),\n",
        "              ('bigger', bigger_history)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bi6hBhdnSfjA"
      },
      "source": [
        "Büyük modelimizin neredeyse hemen, ilk eopch sonrası, aşırı uyum gösterdiğini görüyoruz. Ağımızın kapasitesi arttıkça, modelin uyum gösterme hızı artıyor, bunun sonucunda eğitim kayıpları hızlıca azalıyor. Bunun yanında modelimiz eğitim verisine aşırı uyum göstermeye daha yatkın hale geliyor, bundan dolayı eğitim ve doğrulama kayıpları arasında büyük fark görüyoruz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ASdv7nsgEFhx"
      },
      "source": [
        "## Stratejiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4rHoVWcswFLa"
      },
      "source": [
        "### Modelimize ağırlık regülarizasyonu (weigth regularization) ekleyelim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kRxWepNawbBK"
      },
      "source": [
        "Occam's Razor prensibini biliyor olabilirsiniz: bu prensibe göre bir konuyla ilgili iki farklı açıklamadan doğru olması en olası seçenek, \"en basit\" açıklamadır yani konuyla ilgili en az varsayım içeren açıklamadır. Bu prensip, sinir ağ modelleri içinde geçerlidir: çeşitli eğitim veri setleri ve model yapıları göz önüne alındığında, veriyi açıklayan farklı ağırlık değer setleri (farklı modeller) mevcuttur ve bunların arasından en basit olanlar kompleks modellere göre daha az aşırı uyum olasılığı taşımaktadır.\n",
        "\n",
        "Bu bağlamda \"basit model\", parametre değer dağılımının düzensizliğinin (entropy) az olduğu modeldir (veya yukarıdaki bölümde gördüğümüz gibi tümüyle daha az parametreye sahip modeldir). Bu nedenle aşırı uyumu engellemenin bir yoluda, ağırlıklarının küçük değerler almasına zorlayarak modelin karmaşıklığına sınırlama getirmektir. Bunun sonucunda ağırlık değerlerinin dağılımı daha \"düzenli\" olur. Buna \"ağırlık regülarizasyonu\" denir ve uygulamada büyük ağırlık değerleri için modelin kayıp fonksiyonuna ilave bir ceza değeri eklenmesiyle gerçekleştirilir. Bu ceza değerini iki farklı şekilde oluşturabiliriz: \n",
        "\n",
        "* [L1 regülarizasyonu](https://developers.google.com/machine-learning/glossary/#L1_regularization), bu yöntemde ceza değerleri, ağırlıkların mutlak değerleriyle orantılı olarak eklenir (buna ağırlıkların \"L1 normu\" denir).\n",
        "\n",
        "* [L2 regülarizasyonu](https://developers.google.com/machine-learning/glossary/#L2_regularization), bu yöntemde ceza değerleri, ağırlık katsayı değerlerinin karesiyle orantılı şekilde eklenir (buna ağırlıkların \"L2 normu\" denir). L2 regüarizasyonu aynı zamanda weigth decay olarakta adlandırılır. Farklı adlandırmalar karışıklığa neden olabilir: weigth decay ile L2 regularization aslında matematiksel olarak aynı işlemlerdir. \n",
        "\n",
        "`tf.keras`'ta bu işlem, katmanlara ağırlık regülarizasyon parametre değerlerinin girilmesi ile uygulanır. Şimdi, modelimize L2 ağırlık regülarizasyonunu birlikte ekleyelim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HFGmcwduwVyQ"
      },
      "outputs": [],
      "source": [
        "l2_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                       activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                       activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "])\n",
        "\n",
        "l2_model.compile(optimizer='adam',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy', 'binary_crossentropy'])\n",
        "\n",
        "l2_model_history = l2_model.fit(train_data, train_labels,\n",
        "                                epochs=20,\n",
        "                                batch_size=512,\n",
        "                                validation_data=(test_data, test_labels),\n",
        "                                verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bUUHoXb7w-_C"
      },
      "source": [
        "```l2(0.001)``` 'nin anlamı şudur; katmanın ağırlık matrisi içerisindeki her bir katsayı, ağın toplam kayıp değerine ```0.001 * weight_coefficient_value**2``` ceza değerini ekleyecektir. Bu ceza değeri sadece eğitim sürecinde ekleneceği için, kayıp değerleri eğitim sürecinde test sürecine göre çok daha büyük olacaktır. \n",
        "\n",
        "L2 regülarizasyon ceza değerinin sonuçlara etkisini aşağıda görebiliriz:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7wkfLyxBZdh_"
      },
      "outputs": [],
      "source": [
        "plot_history([('baseline', baseline_history),\n",
        "              ('l2', l2_model_history)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kx1YHMsVxWjP"
      },
      "source": [
        "Göreceğiniz gibi, L2 regülarizasyonunun uygulandığı model, referans modelimiz ile aynı sayıda parametreye sahip olmasına karşın, aşırı uyuma çok daha dirençli bir hale geldi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HmnBNOOVxiG8"
      },
      "source": [
        "### Modelimize 'dropout' ekleyelim\n",
        "\n",
        "Dropout, Toronto Üniversitesi'nden Hilton ve öğrencileri tarafından geliştirilmiş olan bir yöntemdir. Sinir ağları için en etkin ve en yaygın kullanılan regülarizasyon yöntemlerinden biridir. Dropout, eğitim sürecinde katmanlara ait rastgele seçilen belirli sayıdaki çıktı özelliğinin \"bırakılması\" (değerinin sıfıra eşitlenmesiyle) anlamına gelmektedir. Örneğin, ele aldığımız katmanın eğitim sürecinde beslenen girdi değeri ile çıktı olarak [0.2, 0.5, 1.3, 0.8, 1.1] vektörü oluşturduğunu varsayalım; dropout uygulandıktan sonra bu vektör gösterildiği şekliyle rastgele dağılmış olarak çeşitli sıfır değerleri içerecektir; [0, 0.5, \n",
        "1.3, 0, 1.1]. \"Dropout oranı\" sıfırlanacak özelliklerin oranıdır; genelde 0.2 ile 0.5 arasında bir değer alır. Test sürecinde, daha fazla aktif birime sahip olmak için hiç bir birim sıfırlanmaz, bunun yerine dengeyi korumak için katmanların çıktı değerleri droput oranı ile aynı oranda küçültülür.\n",
        "\n",
        "tf.keras API'si Dropout katmanıyla, modelimize dropout işlemini uygulayabiliriz. \n",
        "\n",
        "Aşırı uyumu ne oranda azalttığını değerlendirebilmek için, IMDB ağına iki adet Dropout katmanı ekleyelim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OFEYvtrHxSWS"
      },
      "outputs": [],
      "source": [
        "dpt_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "])\n",
        "\n",
        "dpt_model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy','binary_crossentropy'])\n",
        "\n",
        "dpt_model_history = dpt_model.fit(train_data, train_labels,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=512,\n",
        "                                  validation_data=(test_data, test_labels),\n",
        "                                  verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SPZqwVchx5xp"
      },
      "outputs": [],
      "source": [
        "plot_history([('baseline', baseline_history),\n",
        "              ('dropout', dpt_model_history)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gjfnkEeQyAFG"
      },
      "source": [
        "Droput eklenmesi, referans modele göre açık bir iyileşme sunmaktadır.\n",
        "\n",
        "\n",
        "Tekrardan özetleyelim: sinir ağlarında aşırı uyumu engellemek için yaygın olarak kullanılan yöntemler aşağıda listelenmiştir:\n",
        "\n",
        "* Daha fazla eğitim verisi kullanarak.\n",
        "* Ağın kapasitesini düşürerek.\n",
        "* Ağırlık regülarizasyonu ekleyerek.\n",
        "* Droput ekleyerek.\n",
        "\n",
        "Ve bu notebook'ta yer almayan diğer iki önemli yöntem ise: data-augmentation ve batch normalization yöntemleridir."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fTFj8ft5dlbS"
      ],
      "name": "overfit_and_underfit.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
