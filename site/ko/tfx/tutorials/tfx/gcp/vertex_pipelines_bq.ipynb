{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pknVo1kM2wI2"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SoFqANDE222Y"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x1ypzczQCwy"
      },
      "source": [
        "# TFX 및 Vertex 파이프라인을 사용하여 BigQuery에서 데이터 읽기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_445qeKq8e3-"
      },
      "source": [
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_bq\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "<td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tfx/tutorials/tfx/gcp/vertex_pipelines_bq.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a> </td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tfx/tutorials/tfx/gcp/vertex_pipelines_bq.ipynb\"><img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "<td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/tfx/docs/tutorials/tfx/gcp/vertex_pipelines_bq.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a> </td>\n",
        "<td><a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?q=download_url%3Dhttps://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tfx/tutorials/tfx/gcp/vertex_pipelines_bq.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Google Cloud Vertex AI Workbench에서 실행하기</a></td>\n",
        "</table></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VuwrlnvQJ5k"
      },
      "source": [
        "이 노트북 기반 튜토리얼에서는 [Google Cloud BigQuery](https://cloud.google.com/bigquery)를 ML 모델을 학습시키기 위한 데이터 소스로 사용합니다. ML 파이프라인은 TFX를 사용하여 구성되고 Google Cloud Vertex Pipelines에서 실행됩니다.\n",
        "\n",
        "이 노트북은 [Vertex 파이프라인용 단순 TFX 파이프라인 튜토리얼](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple)에서 구축한 TFX 파이프라인을 기반으로 합니다. 해당 튜토리얼을 아직 읽지 않았다면 이 노트북을 계속 진행하기 전에 꼭 읽기 바랍니다.\n",
        "\n",
        "[BigQuery](https://cloud.google.com/bigquery)는 비즈니스 민첩성을 위해 설계된 확장성이 뛰어나고 비용 효율적인 서버리스 멀티 클라우드 데이터 웨어하우스입니다. TFX를 사용하여 BigQuery에서 학습 데이터를 읽고 [학습된 모델을 BigQuery에 게시](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/extensions/google_cloud_big_query/Pusher)할 수 있습니다.\n",
        "\n",
        "이 튜토리얼에서는 BigQuery에서 TFX 파이프라인으로 데이터를 읽는 `BigQueryExampleGen` 구성 요소를 사용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNA00n3irPgE"
      },
      "source": [
        "이 노트북은 [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb) 또는 [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks)에서 실행하기 위한 것입니다. 이 중 하나도 사용하지 않는 경우, 위의 \"Google Colab에서 실행하기\" 버튼을 클릭하면 됩니다.\n",
        "\n",
        "## 설정\n",
        "\n",
        "[Vertex 파이프라인용 단순 TFX 파이프라인 튜토리얼](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple)을 마쳤다면 실제 작동하는 GCP 프로젝트와 GCS 버킷이 있을 것이며 이것이 이 튜토리얼에 필요한 전부입니다. 예비 튜토리얼을 아직 보지 않았다면 먼저 읽어보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJbPaFzKrPgN"
      },
      "source": [
        "### Python 패키지 설치하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVWOEGgMrPgO"
      },
      "source": [
        "TFX 및 KFP를 포함한 필수 Python 패키지를 설치하여 ML 파이프라인을 작성하고 Vertex 파이프라인에 작업을 제출합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osJJdvmIrPgP"
      },
      "outputs": [],
      "source": [
        "# Use the latest version of pip.\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade \"tfx[kfp]<2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gT1MYvflVBB"
      },
      "source": [
        "### shapely 설치 제거하기\n",
        "\n",
        "TODO(b/263441833) ImportError를 피하는 임시 솔루션입니다. 다른 추가 종속성을 제거하는 대신 최신 버전의 Bigquery를 지원하여 처리하는 것이 이상적입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOK-jepulVUU"
      },
      "outputs": [],
      "source": [
        "!pip uninstall shapely -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5GiQFjprPgP"
      },
      "source": [
        "#### 런타임을 다시 시작했습니까?\n",
        "\n",
        "Google Colab을 사용하는 경우, \"런타임 다시 시작\" 버튼을 클릭하거나 \"런타임 &gt; 렁타임 다시 시작...\" 메뉴를 사용하여 런타임을 다시 시작해야 합니다. 이는 Colab이 패키지를 로드하는 방식 때문입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3TRhlDvrPgQ"
      },
      "source": [
        "Colab에 있지 않다면 다음 셀을 사용하여 런타임을 다시 시작할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYKpuhamrPgQ"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "import sys\n",
        "if not 'google.colab' in sys.modules:\n",
        "  # Automatically restart kernel after installs\n",
        "  import IPython\n",
        "  app = IPython.Application.instance()\n",
        "  app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjGI8rzFrPgQ"
      },
      "source": [
        "### 이 노트북을 사용하려면 Google에 로그인하세요\n",
        "\n",
        "Colab에서 이 노트북을 실행하는 경우 사용자 계정으로 인증하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY8IqqnmrPgQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI3AVxMPrPgQ"
      },
      "source": [
        "**AI Platform Notebooks를 사용 중인 경우** 이어지는 섹션을 실행하기 전에 다음을 실행하여 Google Cloud에 인증합니다.\n",
        "\n",
        "```sh\n",
        "gcloud auth login\n",
        "```\n",
        "\n",
        "**터미널 창**(메뉴에서 **파일** &gt; **새로 만들기**를 통해 열 수 있음)에서 실행하면 됩니다. 이 작업은 노트북 인스턴스당 한 번만 수행하면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3pkMt6zrPgQ"
      },
      "source": [
        "패키지 버전을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvZS3XW2rPgR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))\n",
        "import kfp\n",
        "print('KFP version: {}'.format(kfp.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDtLdSkvqPHe"
      },
      "source": [
        "### 변수 설정하기\n",
        "\n",
        "아래에서 파이프라인을 사용자 지정하는 데 사용되는 몇 가지 변수를 설정합니다. 다음 정보가 필요합니다.\n",
        "\n",
        "- GCP 프로젝트 ID 및 번호. [프로젝트 ID 및 번호 확인](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects)을 참조하세요.\n",
        "- 파이프라인을 실행할 GCP 영역. Vertex 파이프라인을 사용할 수 있는 영역에 대한 자세한 내용은 [Vertex AI 위치 가이드](https://cloud.google.com/vertex-ai/docs/general/locations#feature-availability)를 참조하세요.\n",
        "- 파이프라인 출력을 저장하기 위한 Google Cloud Storage 버킷\n",
        "\n",
        "**실행하기 전에 아래 셀에 필요한 값을 입력하세요**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcUseqJaE2XN"
      },
      "outputs": [],
      "source": [
        "GOOGLE_CLOUD_PROJECT = ''         # <--- ENTER THIS\n",
        "GOOGLE_CLOUD_PROJECT_NUMBER = ''  # <--- ENTER THIS\n",
        "GOOGLE_CLOUD_REGION = ''          # <--- ENTER THIS\n",
        "GCS_BUCKET_NAME = ''              # <--- ENTER THIS\n",
        "\n",
        "if not (GOOGLE_CLOUD_PROJECT and  GOOGLE_CLOUD_PROJECT_NUMBER and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
        "    from absl import logging\n",
        "    logging.error('Please set all required parameters.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAaCPLjgiJrO"
      },
      "source": [
        "프로젝트를 사용하도록 `gcloud`를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkWdxe4TXRHk"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project {GOOGLE_CLOUD_PROJECT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPN6UL5CazNy"
      },
      "outputs": [],
      "source": [
        "PIPELINE_NAME = 'penguin-bigquery'\n",
        "\n",
        "# Path to various pipeline artifact.\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "# Paths for users' Python module.\n",
        "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "# Paths for users' data.\n",
        "DATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "# This is the path where your model will be pushed for serving.\n",
        "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szKDDoD_KipW"
      },
      "source": [
        "기본적으로 Vertex 파이프라인은 `[project-number]-compute@developer.gserviceaccount.com` 형식의 기본 GCE VM 서비스 계정을 사용합니다. 파이프라인에서 BigQuery에 액세스하려면 이 계정에 BigQuery를 사용할 수 있는 권한을 부여해야 합니다. 계정에 'BigQuery 사용자' 역할을 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aii8K3dJEyj"
      },
      "outputs": [],
      "source": [
        "!gcloud projects add-iam-policy-binding {GOOGLE_CLOUD_PROJECT} \\\n",
        "  --member=serviceAccount:{GOOGLE_CLOUD_PROJECT_NUMBER}-compute@developer.gserviceaccount.com \\\n",
        "  --role=roles/bigquery.user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3ktk1j9s1PP"
      },
      "source": [
        "서비스 계정 및 IAM 구성에 대한 자세한 내용은 [Vertex 설명서](https://cloud.google.com/vertex-ai/docs/pipelines/configure-project)를 참조하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH6gizcpSwWV"
      },
      "source": [
        "## 파이프라인 생성하기\n",
        "\n",
        "TFX 파이프라인은 [Vertex 파이프라인용 단순 TFX 파이프라인 튜토리얼](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple)에서 했던 것처럼 Python API를 사용하여 정의됩니다. 이전에는 CSV 파일에서 데이터를 읽는 `CsvExampleGen`을 사용했습니다. 이 튜토리얼에서는 BigQuery에서 데이터를 읽는 [`BigQueryExampleGen`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/extensions/google_cloud_big_query/BigQueryExampleGen) 구성 요소를 사용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNg73Slwn8nq"
      },
      "source": [
        "### BigQuery 쿼리 준비하기\n",
        "\n",
        "동일한 [Palmer Penguins 데이터세트](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)를 사용할 것입니다. 그러나 동일한 CSV 파일을 사용하여 채워진 BigQuery 테이블 `tfx-oss-public.palmer_penguins.palmer_penguins`에서 이를 읽습니다.\n",
        "\n",
        "Google Colab을 사용하는 경우 BigQuery 테이블의 내용을 직접 검사할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb_Kj1U8pBhZ"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "%%bigquery --project {GOOGLE_CLOUD_PROJECT}\n",
        "SELECT *\n",
        "FROM `tfx-oss-public.palmer_penguins.palmer_penguins`\n",
        "LIMIT 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arvdbM5jpjNm"
      },
      "source": [
        "레이블인 `species`을 제외한 모든 특성은 이미 0~1로 정규화되었습니다. 펭귄의 `species`을 예측하는 분류 모델을 구축할 것입니다.\n",
        "\n",
        "`BigQueryExampleGen`에는 가져올 데이터를 지정하는 쿼리가 필요합니다. 테이블에 있는 모든 행의 모든 필드를 사용하기 때문에 쿼리는 매우 간단합니다. <a>BigQuery 표준 SQL 구문</a>에 따라 필요 시 필드 이름을 지정하고 <code>WHERE</code> 조건을 추가할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AwysGAVnfJA"
      },
      "outputs": [],
      "source": [
        "QUERY = \"SELECT * FROM `tfx-oss-public.palmer_penguins.palmer_penguins`\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjDv93eS5xV"
      },
      "source": [
        "### 모델 코드를 작성합니다.\n",
        "\n",
        "[단순 TFX 파이프라인 튜토리얼](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple)에서와 동일한 모델 코드를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aES7Hv5QTDK3"
      },
      "outputs": [],
      "source": [
        "_trainer_module_file = 'penguin_trainer.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnc67uQNTDfW"
      },
      "outputs": [],
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n",
        "\n",
        "from typing import List\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.public import tfxio\n",
        "\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "_FEATURE_KEYS = [\n",
        "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
        "]\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "_EVAL_BATCH_SIZE = 10\n",
        "\n",
        "# Since we're not generating or creating a schema, we will instead create\n",
        "# a feature spec.  Since there are a fairly small number of features this is\n",
        "# manageable for this dataset.\n",
        "_FEATURE_SPEC = {\n",
        "    **{\n",
        "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
        "           for feature in _FEATURE_KEYS\n",
        "       },\n",
        "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
        "}\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[str],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              schema: schema_pb2.Schema,\n",
        "              batch_size: int) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    schema: schema of the input data.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "      schema=schema).repeat()\n",
        "\n",
        "\n",
        "def _make_keras_model() -> tf.keras.Model:\n",
        "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model.\n",
        "  \"\"\"\n",
        "  # The model below is built with Functional API, please refer to\n",
        "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
        "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
        "  d = keras.layers.concatenate(inputs)\n",
        "  for _ in range(2):\n",
        "    d = keras.layers.Dense(8, activation='relu')(d)\n",
        "  outputs = keras.layers.Dense(3)(d)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  model.summary(print_fn=logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
        "  # version provided by pipeline author. A schema can also derived from TFT\n",
        "  # graph if a Transform component is used. In the case when either is missing,\n",
        "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
        "  # feature_spec, but the schema returned would be very primitive.\n",
        "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
        "\n",
        "  train_dataset = _input_fn(\n",
        "      fn_args.train_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(\n",
        "      fn_args.eval_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  model = _make_keras_model()\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps)\n",
        "\n",
        "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
        "  # directory.\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LsYx8MpYvPv"
      },
      "source": [
        "파이프라인 구성 요소에서 액세스할 수 있는 GCS에 모듈 파일을 복사합니다. 모델 학습은 GCP에서 이루어지므로 이 모델 정의를 업로드해야 합니다.\n",
        "\n",
        "그렇지 않으면 모듈 파일을 포함하는 컨테이너 이미지를 빌드하고 이미지를 사용하여 파이프라인을 실행할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMMs5wuNYAbc"
      },
      "outputs": [],
      "source": [
        "!gsutil cp {_trainer_module_file} {MODULE_ROOT}/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3OkNz3gTLwM"
      },
      "source": [
        "### 파이프라인 정의 작성하기\n",
        "\n",
        "TFX 파이프라인을 생성하는 함수를 정의합니다. `query`를 인수로 사용하는 `BigQueryExampleGen`을 사용해야 합니다. 이전 튜토리얼에서 한 가지 더 변경된 점은 실행 시 구성 요소에 전달되는 `beam_pipeline_args`를 전달해야 한다는 것입니다. `beam_pipeline_args`를 사용하여 BigQuery에 추가 매개변수를 전달합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M49yYVNBTPd4"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "def _create_pipeline(pipeline_name: str, pipeline_root: str, query: str,\n",
        "                     module_file: str, serving_model_dir: str,\n",
        "                     beam_pipeline_args: Optional[List[str]],\n",
        "                     ) -> tfx.dsl.Pipeline:\n",
        "  \"\"\"Creates a TFX pipeline using BigQuery.\"\"\"\n",
        "\n",
        "  # NEW: Query data in BigQuery as a data source.\n",
        "  example_gen = tfx.extensions.google_cloud_big_query.BigQueryExampleGen(\n",
        "      query=query)\n",
        "\n",
        "  # Uses user-provided Python function that trains a model.\n",
        "  trainer = tfx.components.Trainer(\n",
        "      module_file=module_file,\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
        "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
        "\n",
        "  # Pushes the model to a file destination.\n",
        "  pusher = tfx.components.Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      push_destination=tfx.proto.PushDestination(\n",
        "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)))\n",
        "\n",
        "  components = [\n",
        "      example_gen,\n",
        "      trainer,\n",
        "      pusher,\n",
        "  ]\n",
        "\n",
        "  return tfx.dsl.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      components=components,\n",
        "      # NEW: `beam_pipeline_args` is required to use BigQueryExampleGen.\n",
        "      beam_pipeline_args=beam_pipeline_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJbq07THU2GV"
      },
      "source": [
        "## Vertex 파이프라인에서 파이프라인을 실행합니다\n",
        "\n",
        "[Vertex 파이프라인용 단순 TFX 파이프라인 튜토리얼](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple)에서 했던 것처럼 Vertex 파이프라인을 사용하여 파이프라인을 실행합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mp0AkmrPdUb"
      },
      "source": [
        "또한 BigQueryExampleGen에 대해 `beam_pipeline_args`를 전달해야 합니다. 여기에는 GCP 프로젝트 이름 및 BigQuery 실행을 위한 임시 저장소와 같은 구성이 포함됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAtfOZTYWJu-"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "import os\n",
        "\n",
        "# We need to pass some GCP related configs to BigQuery. This is currently done\n",
        "# using `beam_pipeline_args` parameter.\n",
        "BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [\n",
        "   '--project=' + GOOGLE_CLOUD_PROJECT,\n",
        "   '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),\n",
        "   ]\n",
        "\n",
        "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
        "\n",
        "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
        "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
        "    output_filename=PIPELINE_DEFINITION_FILE)\n",
        "_ = runner.run(\n",
        "    _create_pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        query=QUERY,\n",
        "        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n",
        "        serving_model_dir=SERVING_MODEL_DIR,\n",
        "        beam_pipeline_args=BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWyITYSDd8w4"
      },
      "source": [
        "생성된 정의 파일은 kfp 클라이언트를 사용하여 제출할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI71jlEvWMV7"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import pipeline_jobs\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
        "\n",
        "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
        "                                display_name=PIPELINE_NAME)\n",
        "job.submit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3k9f5IVQXcQ"
      },
      "source": [
        "이제 위의 출력에 있는 링크를 방문하거나 [Google Cloud Console](https://console.cloud.google.com/)에서 'Vertex AI &gt; 파이프라인'을 방문하여 진행 상황을 확인할 수 있습니다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pknVo1kM2wI2"
      ],
      "name": "vertex_pipelines_bq.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
