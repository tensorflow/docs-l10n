{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAttKaKmT435"
      },
      "source": [
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/transform/census\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> TensorFlow.org에서 보기</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tfx/tutorials/transform/census.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tfx/tutorials/transform/census.ipynb\"><img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소그 보기</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tfx/tutorials/transform/census.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a></td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tghWegsjhpkt"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rSGJWC5biBiG"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPt5BHTwy_0F"
      },
      "source": [
        "# TensorFlow Transform으로 데이터 전처리하기\n",
        "\n",
        "***TensorFlow Extended(TFX)의 특성 엔지니어링 구성 요소***\n",
        "\n",
        "이 예제 colab 노트북은 <a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/transform/get_started\">TensorFlow Transform</a>(`tf.Transform`)을 사용하여 모델을 훈련하고 프로덕션에서 추론을 제공하는 데 정확히 동일한 코드를 사용하여 데이터를 사전 처리하는 방법에 대한 좀 더 고급스러운 예를 제공합니다.\n",
        "\n",
        "TensorFlow Transform은 훈련 데이터세트에 대한 전체 전달이 필요한 특성 생성을 포함하여 TensorFlow에 대한 입력 데이터를 전처리하기 위한 라이브러리입니다. 예를 들어 TensorFlow Transform을 사용하여 다음을 수행할 수 있습니다.\n",
        "\n",
        "- 평균과 표준 편차를 이용하여 입력값 정규화\n",
        "- 모든 입력값에 대해 어휘를 생성하여 문자열을 정수로 변환\n",
        "- 관찰된 데이터 분포를 기반으로 부동 소수점을 버킷에 할당하여 정수로 변환\n",
        "\n",
        "TensorFlow는 단일 예제 또는 예제 배치에 대한 조작을 기본적으로 지원합니다. `tf.Transform`은 이러한 기능을 확장하여 전체 훈련 데이터세트에 대한 전체 전달을 지원합니다.\n",
        "\n",
        "`tf.Transform`의 출력은 훈련과 제공 모두에 사용할 수 있는 TensorFlow 그래프로 내보내집니다. 훈련과 제공 모두에 동일한 그래프를 사용하면 두 단계에 동일한 변환이 적용되므로 왜곡을 방지할 수 있습니다.\n",
        "\n",
        "핵심 포인트: `tf.Transform` 및 이것이 Apache Beam과 작동하는 방식을 이해하려면 Apache Beam 자체에 대해 약간 알아야 합니다. <a target=\"_blank\" href=\"https://beam.apache.org/documentation/programming-guide/\">Beam 프로그래밍 가이드</a>가 좋은 출발점을 제공합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQUubddMvnP"
      },
      "source": [
        "##이 예제에서 수행하는 작업\n",
        "\n",
        "이 예에서 우리는 <a target=\"_blank\" href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult\">인구 조사 데이터를 포함하는 널리 사용되는 데이터 세트</a>를 처리하고 분류를 수행하도록 모델을 훈련할 것입니다. 그 과정에서 `tf.Transform`을 사용하여 데이터를 변환할 것입니다.\n",
        "\n",
        "핵심 포인트: 모델러 및 개발자로서 이 데이터가 어떻게 사용되는지, 그리고 모델의 예측이 초래할 수 있는 잠재적인 이점과 피해에 대해 생각해보세요. 이와 같은 모델은 사회적 편견과 불균형을 강화시킬 수 있습니다. 기능이 해결하려는 문제와 관련이 있습니까? 아니면 편견을 유발합니까? 자세한 내용은 <a target=\"_blank\" href=\"https://developers.google.com/machine-learning/fairness-overview/\">ML 공정성</a>에 대해 읽어보세요.\n",
        "\n",
        "참고: <a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/model_analysis\">TensorFlow 모델 분석</a>은 모델이 사회적 바이어스와 격차를 강화할 수 있는 방법을 이해하는 것을 포함하여 모델이 데이터의 다양한 세그먼트에 대해 예측을 얼마나 잘 수행하는지 이해하기 위한 강력한 도구입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeonII4omTr1"
      },
      "source": [
        "### TensorFlow Transform 설치하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ak6XDO5mT3m"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0mXLOJR_-dv"
      },
      "outputs": [],
      "source": [
        "# This cell is only necessary because packages were installed while python was\n",
        "# running. It avoids the need to restart the runtime when running in Colab.\n",
        "import pkg_resources\n",
        "import importlib\n",
        "\n",
        "importlib.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RptgLn2RYuK3"
      },
      "source": [
        "## 가져오기 및 전역\n",
        "\n",
        "먼저 필요한 항목을 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4QXVIM7iglN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TF: {}'.format(tf.__version__))\n",
        "\n",
        "import apache_beam as beam\n",
        "print('Beam: {}'.format(beam.__version__))\n",
        "\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "print('Transform: {}'.format(tft.__version__))\n",
        "\n",
        "from tfx_bsl.public import tfxio\n",
        "from tfx_bsl.coders.example_coder import RecordBatchToExamplesEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sutRmRNSGT5p"
      },
      "source": [
        "다음으로 데이터 파일을 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKEYRl2g_vzl"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data\n",
        "!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.test\n",
        "\n",
        "train_path = './adult.data'\n",
        "test_path = './adult.test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxOxaaOYRfl7"
      },
      "source": [
        "### 열 이름 지정하기\n",
        "\n",
        "데이터세트의 열을 참조하기 위한 몇 가지 편리한 목록을 만들 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bsr1nLHqyg_"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'native-country',\n",
        "]\n",
        "\n",
        "NUMERIC_FEATURE_KEYS = [\n",
        "    'age',\n",
        "    'capital-gain',\n",
        "    'capital-loss',\n",
        "    'hours-per-week',\n",
        "    'education-num'\n",
        "]\n",
        "\n",
        "ORDERED_CSV_COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n",
        "]\n",
        "\n",
        "LABEL_KEY = 'label'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R52dXlw0G0CN"
      },
      "source": [
        "다음은 데이터에 대한 빠른 미리보기입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "312cQ5vwGjOu"
      },
      "outputs": [],
      "source": [
        "pandas_train = pd.read_csv(train_path, header=None, names=ORDERED_CSV_COLUMNS)\n",
        "\n",
        "pandas_train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzjzjR3351j0"
      },
      "outputs": [],
      "source": [
        "one_row = dict(pandas_train.loc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk2b8IPd4uPr"
      },
      "outputs": [],
      "source": [
        "COLUMN_DEFAULTS = [\n",
        "  '' if isinstance(v, str) else 0.0\n",
        "  for v in  dict(pandas_train.loc[1]).values()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LefAguV5ICMc"
      },
      "source": [
        "테스트 데이터에는 건너뛰어야 있는 1개의 헤더 행과 각 행의 끝에 후행 \".\"가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RasgDIUKHCpV"
      },
      "outputs": [],
      "source": [
        "pandas_test = pd.read_csv(test_path, header=1, names=ORDERED_CSV_COLUMNS)\n",
        "\n",
        "pandas_test.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9aH5ZnDdD_z"
      },
      "outputs": [],
      "source": [
        "testing = os.getenv(\"WEB_TEST_BROWSER\", False)\n",
        "if testing:\n",
        "  pandas_train = pandas_train.loc[:1]\n",
        "  pandas_test = pandas_test.loc[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTn4at8rurk"
      },
      "source": [
        "###특성과 스키마 정의. 입력에 있는 열의 유형을 기반으로 스키마를 정의해 보겠습니다. 무엇보다도 이를 올바르게 가져오는 데 도움이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oS2RfyCrzMr"
      },
      "outputs": [],
      "source": [
        "RAW_DATA_FEATURE_SPEC = dict(\n",
        "    [(name, tf.io.FixedLenFeature([], tf.string))\n",
        "     for name in CATEGORICAL_FEATURE_KEYS] +\n",
        "    [(name, tf.io.FixedLenFeature([], tf.float32))\n",
        "     for name in NUMERIC_FEATURE_KEYS] + \n",
        "    [(LABEL_KEY, tf.io.FixedLenFeature([], tf.string))]\n",
        ")\n",
        "\n",
        "SCHEMA = tft.DatasetMetadata.from_feature_spec(RAW_DATA_FEATURE_SPEC).schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j6M7ObpaLHi"
      },
      "source": [
        "### [선택 사항] tf.train.Example proto 인코딩 및 디코딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgGO9-GkZ5Kv"
      },
      "source": [
        "이 튜토리얼에서는 몇몇 위치에서 데이터세트의 예제를 `tf.train.Example` proto와 상호 변환해야 합니다.\n",
        "\n",
        "아래의 숨겨진 `encode_example` 함수는 데이터세트의 특성 사전을 `tf.train.Example`로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbhndy7uWqYp"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def encode_example(input_features):\n",
        "  input_features = dict(input_features)\n",
        "  output_features = {}\n",
        "  \n",
        "  for key in CATEGORICAL_FEATURE_KEYS:\n",
        "    value = input_features[key]\n",
        "    feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[value.strip().encode()]))\n",
        "    output_features[key] = feature \n",
        "\n",
        "  for key in NUMERIC_FEATURE_KEYS:\n",
        "    value = input_features[key]\n",
        "    feature = tf.train.Feature(\n",
        "        float_list=tf.train.FloatList(value=[value]))\n",
        "    output_features[key] = feature \n",
        "\n",
        "  label_value = input_features.get(LABEL_KEY, None)\n",
        "  if label_value is not None:\n",
        "    output_features[LABEL_KEY]  = tf.train.Feature(\n",
        "        bytes_list = tf.train.BytesList(value=[label_value.strip().encode()]))\n",
        "\n",
        "  example = tf.train.Example(\n",
        "      features = tf.train.Features(feature=output_features)\n",
        "  )\n",
        "  return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qx7fSVmmwIQ"
      },
      "source": [
        "이제 데이터세트 예제를 `Example` protos로 변환할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWd95yxJceXy"
      },
      "outputs": [],
      "source": [
        "tf_example = encode_example(pandas_train.loc[0])\n",
        "tf_example.features.feature['age']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EutF2aPXbAUd"
      },
      "outputs": [],
      "source": [
        "serialized_example_batch = tf.constant([\n",
        "  encode_example(pandas_train.loc[i]).SerializeToString()\n",
        "  for i in range(3)\n",
        "])\n",
        "\n",
        "serialized_example_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTqlJcI_m6az"
      },
      "source": [
        "직렬화된 예제 proto의 배치를 다시 텐서 사전으로 변환할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXlrur1vc4n_"
      },
      "outputs": [],
      "source": [
        "decoded_tensors = tf.io.parse_example(\n",
        "    serialized_example_batch,\n",
        "    features=RAW_DATA_FEATURE_SPEC\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUAcdCrEdDe3"
      },
      "source": [
        "어떤 경우에는 레이블이 전달되지 않으므로 레이블이 선택 사항이 되도록 인코딩 함수가 작성됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEt3nPr_o59f"
      },
      "outputs": [],
      "source": [
        "features_dict = dict(pandas_train.loc[0])\n",
        "features_dict.pop(LABEL_KEY)\n",
        "\n",
        "LABEL_KEY in features_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0yqvsHtpDdX"
      },
      "source": [
        "`Example` proto를 만들 때 레이블 키가 포함되지 않습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N5FMXO7dRzM"
      },
      "outputs": [],
      "source": [
        "no_label_example = encode_example(features_dict)\n",
        "\n",
        "LABEL_KEY in no_label_example.features.feature.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdXy9lo4t45d"
      },
      "source": [
        "###하이퍼파라미터 설정 및 기본 하우스키핑\n",
        "\n",
        "훈련에 사용되는 상수 및 하이퍼파라미터."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WHyOkC9uL71"
      },
      "outputs": [],
      "source": [
        "NUM_OOV_BUCKETS = 1\n",
        "\n",
        "EPOCH_SPLITS = 10\n",
        "TRAIN_NUM_EPOCHS = 2*EPOCH_SPLITS\n",
        "NUM_TRAIN_INSTANCES = len(pandas_train)\n",
        "NUM_TEST_INSTANCES = len(pandas_test)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "STEPS_PER_TRAIN_EPOCH = tf.math.ceil(NUM_TRAIN_INSTANCES/BATCH_SIZE/EPOCH_SPLITS)\n",
        "EVALUATION_STEPS = tf.math.ceil(NUM_TEST_INSTANCES/BATCH_SIZE)\n",
        "\n",
        "# Names of temp files\n",
        "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
        "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
        "EXPORTED_MODEL_DIR = 'exported_model_dir'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG2uO-88c6R9"
      },
      "outputs": [],
      "source": [
        "if testing:\n",
        "  TRAIN_NUM_EPOCHS = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a1ns5KswDb2"
      },
      "source": [
        "##`tf.Transform`으로 전처리하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKd3mCLNVYmg"
      },
      "source": [
        "###`tf.Transform` 전처리 함수 생성. *전처리 함수*는 tf.Transform의 가장 중요한 개념입니다. 전처리 함수는 데이터세트의 변환이 실제로 일어나는 곳으로, 텐서 사전을 수락하고 반환합니다. 여기서 텐서는 [`Tensor`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor) 또는 [`SparseTensor`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/SparseTensor)를 의미합니다. 일반적으로 전처리 함수의 핵심을 구성하는 두 가지 주요 API 호출 그룹이 있습니다.\n",
        "\n",
        "1. **TensorFlow Ops:** 일반적으로 TensorFlow 연산을 의미하는 텐서를 수락하고 반환하는 모든 함수로, 원시 데이터를 변환된 데이터로 한 번에 하나의 특성 벡터씩 변환하는 TensorFlow 연산을 그래프에 추가합니다. 이는 훈련 및 제공 기간 동안 모든 예에 대해 실행됩니다.\n",
        "2. **Tensorflow 변환 분석기/맵퍼:** tf.Transform에서 제공하는 모든 분석기/맵퍼입니다. 이들은 또한 텐서를 수락 및 반환하며 일반적으로 Tensorflow 연산과 Beam 계산을 조합적으로 포함하지만 TensorFlow 연산과 달리 전체 훈련 데이터세트에 대한 전체 패스가 필요한 분석 중에 Beam 파이프라인에서만 실행됩니다. Beam 계산은 훈련하기 전과 훈련하는 동안 한 번만 실행되며 일반적으로 전체 훈련 데이터세트에 대해 전체 패스를 만듭니다. 그래프에 추가되는 `tf.constant` 텐서가 생성됩니다. 예를 들어, `tft.min`은 훈련 데이터세트에 대해 텐서의 최솟값을 계산합니다.\n",
        "\n",
        "주의: 전처리 함수를 추론 제공에 적용할 때 훈련 중에 분석기가 생성한 상수는 변경되지 않습니다. 데이터에 추세 또는 계절성 요소가 있는 경우 그에 따라 계획하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZopPfpaH4sB"
      },
      "source": [
        "다음은 이 데이터세트에 대한 `preprocessing_fn`로, 여러 가지 작업을 수행합니다.\n",
        "\n",
        "1. `tft.scale_to_0_1`을 사용하여 숫자 특성을 `[0,1]` 범위로 확장합니다.\n",
        "2. `tft.compute_and_apply_vocabulary`를 사용하여 각 범주 특성에 대한 어휘를 계산하고 각 입력에 대한 정수 ID를 `tf.int64`로 반환합니다. 이는 문자열 및 정수 범주형 입력에 모두 적용됩니다.\n",
        "3. 표준 TensorFlow 작업을 사용하여 데이터에 일부 수동 변환을 적용합니다. 여기에서 이러한 작업은 레이블에 적용되지만 특성도 변환할 수 있습니다. TensorFlow 작업은 다음과 같은 몇 가지 작업을 수행합니다.\n",
        "    - 레이블에 대한 조회 테이블을 만듭니다(`tf.init_scope`는 함수가 처음 호출될 때만 테이블이 생성되도록 함).\n",
        "    - 레이블의 텍스트를 정규화합니다.\n",
        "    - 레이블을 one-hot으로 변환합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDrzuYH0WFc2"
      },
      "outputs": [],
      "source": [
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "  # Since we are modifying some features and leaving others unchanged, we\n",
        "  # start by setting `outputs` to a copy of `inputs.\n",
        "  outputs = inputs.copy()\n",
        "\n",
        "  # Scale numeric columns to have range [0, 1].\n",
        "  for key in NUMERIC_FEATURE_KEYS:\n",
        "    outputs[key] = tft.scale_to_0_1(inputs[key])\n",
        "\n",
        "  # For all categorical columns except the label column, we generate a\n",
        "  # vocabulary but do not modify the feature.  This vocabulary is instead\n",
        "  # used in the trainer, by means of a feature column, to convert the feature\n",
        "  # from a string to an integer id.\n",
        "  for key in CATEGORICAL_FEATURE_KEYS:\n",
        "    outputs[key] = tft.compute_and_apply_vocabulary(\n",
        "        tf.strings.strip(inputs[key]),\n",
        "        num_oov_buckets=NUM_OOV_BUCKETS,\n",
        "        vocab_filename=key)\n",
        "\n",
        "  # For the label column we provide the mapping from string to index.\n",
        "  table_keys = ['>50K', '<=50K']\n",
        "  with tf.init_scope():\n",
        "    initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=table_keys,\n",
        "        values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
        "        key_dtype=tf.string,\n",
        "        value_dtype=tf.int64)\n",
        "    table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
        "\n",
        "  # Remove trailing periods for test data when the data is read with tf.data.\n",
        "  # label_str  = tf.sparse.to_dense(inputs[LABEL_KEY])\n",
        "  label_str = inputs[LABEL_KEY]\n",
        "  label_str = tf.strings.regex_replace(label_str, r'\\.$', '')\n",
        "  label_str = tf.strings.strip(label_str)\n",
        "  data_labels = table.lookup(label_str)\n",
        "  transformed_label = tf.one_hot(\n",
        "      indices=data_labels, depth=len(table_keys), on_value=1.0, off_value=0.0)\n",
        "  outputs[LABEL_KEY] = tf.reshape(transformed_label, [-1, len(table_keys)])\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA1Eg2JXFzzZ"
      },
      "source": [
        "## 구문\n",
        "\n",
        "이제 모든 것을 한곳으로 모으고 <a target=\"_blank\" href=\"https://beam.apache.org/\">Apache Beam</a>을 사용하여 실행할 준비가 완료되었습니다.\n",
        "\n",
        "Apache Beam은 <a target=\"_blank\" href=\"https://beam.apache.org/documentation/programming-guide/#applying-transforms\">변환을 정의하고 호출하기 위해 특수 구문</a>을 사용합니다.  예를 들어 다음 줄을 보겠습니다.\n",
        "\n",
        "```\n",
        "result = pass_this | 'name this step' >> to_this_call\n",
        "```\n",
        "\n",
        "`to_this_call` 메서드는 `pass_this`라는 개체를 호출 및 전달하고 <a target=\"_blank\" href=\"https://stackoverflow.com/questions/50519662/what-does-the-redirection-mean-in-apache-beam-python\">이 연산을 스택 추적에서 <code>name this step</code>이라고 합니다.</a> `to_this_call`에 대한 호출의 결과는 `result`에서 반환됩니다. 다음과 같이 함께 연결된 파이프라인의 단계를 종종 볼 수 있습니다.\n",
        "\n",
        "```\n",
        "result = apache_beam.Pipeline() | 'first step' >> do_this_first() | 'second step' >> do_this_last()\n",
        "```\n",
        "\n",
        "그리고 새 파이프라인으로 시작했기 때문에 다음과 같이 계속할 수 있습니다.\n",
        "\n",
        "```\n",
        "next_result = result | 'doing more stuff' >> another_function()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgAGOAdFWRn2"
      },
      "source": [
        "### 데이터 변환하기\n",
        "\n",
        "이제 Apache Beam 파이프라인에서 데이터 변환을 시작할 준비가 되었습니다.\n",
        "\n",
        "1. `tfxio.CsvTFXIO` CSV 판독기를 사용하여 데이터를 읽습니다(파이프라인에서 텍스트 행을 처리하려면 대신 `tfxio.BeamRecordCsvTFXIO` 사용).\n",
        "2. 위에서 정의한 `preprocessing_fn`을 사용하여 데이터를 분석하고 변환합니다.\n",
        "3. 결과를 `Example` protos의 `TFRecord`로 결과를 작성합니다. 나중에 모델 훈련에 이를 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCeYucVoRRfo"
      },
      "outputs": [],
      "source": [
        "def transform_data(train_data_file, test_data_file, working_dir):\n",
        "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
        "\n",
        "  Read in the data using the CSV reader, and transform it using a\n",
        "  preprocessing pipeline that scales numeric data and converts categorical data\n",
        "  from strings to int64 values indices, by creating a vocabulary for each\n",
        "  category.\n",
        "\n",
        "  Args:\n",
        "    train_data_file: File containing training data\n",
        "    test_data_file: File containing test data\n",
        "    working_dir: Directory to write transformed data and metadata to\n",
        "  \"\"\"\n",
        "\n",
        "  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
        "  # of the block.\n",
        "  with beam.Pipeline() as pipeline:\n",
        "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "      # Create a TFXIO to read the census data with the schema. To do this we\n",
        "      # need to list all columns in order since the schema doesn't specify the\n",
        "      # order of columns in the csv.\n",
        "      # We first read CSV files and use BeamRecordCsvTFXIO whose .BeamSource()\n",
        "      # accepts a PCollection[bytes] because we need to patch the records first\n",
        "      # (see \"FixCommasTrainData\" below). Otherwise, tfxio.CsvTFXIO can be used\n",
        "      # to both read the CSV files and parse them to TFT inputs:\n",
        "      # csv_tfxio = tfxio.CsvTFXIO(...)\n",
        "      # raw_data = (pipeline | 'ToRecordBatches' >> csv_tfxio.BeamSource())\n",
        "      train_csv_tfxio = tfxio.CsvTFXIO(\n",
        "          file_pattern=train_data_file,\n",
        "          telemetry_descriptors=[],\n",
        "          column_names=ORDERED_CSV_COLUMNS,\n",
        "          schema=SCHEMA)\n",
        "\n",
        "      # Read in raw data and convert using CSV TFXIO.\n",
        "      raw_data = (\n",
        "          pipeline |\n",
        "          'ReadTrainCsv' >> train_csv_tfxio.BeamSource())\n",
        "\n",
        "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
        "      # the schema to read the CSV data, but we also need it to interpret\n",
        "      # raw_data.\n",
        "      cfg = train_csv_tfxio.TensorAdapterConfig()\n",
        "      raw_dataset = (raw_data, cfg)\n",
        "\n",
        "      # The TFXIO output format is chosen for improved performance.\n",
        "      transformed_dataset, transform_fn = (\n",
        "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(\n",
        "              preprocessing_fn, output_record_batches=True))\n",
        "\n",
        "      # Transformed metadata is not necessary for encoding.\n",
        "      transformed_data, _ = transformed_dataset\n",
        "\n",
        "      # Extract transformed RecordBatches, encode and write them to the given\n",
        "      # directory.\n",
        "      coder = RecordBatchToExamplesEncoder()\n",
        "      _ = (\n",
        "          transformed_data\n",
        "          | 'EncodeTrainData' >>\n",
        "          beam.FlatMapTuple(lambda batch, _: coder.encode(batch))\n",
        "          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n",
        "\n",
        "      # Now apply transform function to test data.  In this case we remove the\n",
        "      # trailing period at the end of each line, and also ignore the header line\n",
        "      # that is present in the test data file.\n",
        "      test_csv_tfxio = tfxio.CsvTFXIO(\n",
        "          file_pattern=test_data_file,\n",
        "          skip_header_lines=1,\n",
        "          telemetry_descriptors=[],\n",
        "          column_names=ORDERED_CSV_COLUMNS,\n",
        "          schema=SCHEMA)\n",
        "      raw_test_data = (\n",
        "          pipeline\n",
        "          | 'ReadTestCsv' >> test_csv_tfxio.BeamSource())\n",
        "\n",
        "      raw_test_dataset = (raw_test_data, test_csv_tfxio.TensorAdapterConfig())\n",
        "\n",
        "      # The TFXIO output format is chosen for improved performance.\n",
        "      transformed_test_dataset = (\n",
        "          (raw_test_dataset, transform_fn)\n",
        "          | tft_beam.TransformDataset(output_record_batches=True))\n",
        "\n",
        "      # Transformed metadata is not necessary for encoding.\n",
        "      transformed_test_data, _ = transformed_test_dataset\n",
        "\n",
        "      # Extract transformed RecordBatches, encode and write them to the given\n",
        "      # directory.\n",
        "      _ = (\n",
        "          transformed_test_data\n",
        "          | 'EncodeTestData' >>\n",
        "          beam.FlatMapTuple(lambda batch, _: coder.encode(batch))\n",
        "          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n",
        "\n",
        "      # Will write a SavedModel and metadata to working_dir, which can then\n",
        "      # be read by the tft.TFTransformOutput class.\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huaj5EgCVRD9"
      },
      "source": [
        "파이프라인을 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjC7eDWFyA8K"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import pathlib\n",
        "\n",
        "output_dir = os.path.join(tempfile.mkdtemp(), 'keras')\n",
        "\n",
        "\n",
        "transform_data(train_path, test_path, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqln2AClsA0z"
      },
      "source": [
        "출력 디렉터리를 `tft.TFTransformOutput`으로 래핑합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXd4Mgj6sAGB"
      },
      "outputs": [],
      "source": [
        "tf_transform_output = tft.TFTransformOutput(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59hNe7oY9vqG"
      },
      "outputs": [],
      "source": [
        "tf_transform_output.transformed_feature_spec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBlL2EIVVF8"
      },
      "source": [
        "디렉터리를 보면 세 가지가 포함되어 있음을 알 수 있습니다.\n",
        "\n",
        "1. `train_transformed` 및 `test_transformed` 데이터 파일\n",
        "2. `transform_fn` 디렉터리(`tf.saved_model`)\n",
        "3. The `transformed_metadata`\n",
        "\n",
        "다음 섹션에서는 이러한 아티팩트를 사용하여 모델을 훈련하는 방법을 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG6nrHEP2L65"
      },
      "outputs": [],
      "source": [
        "!ls -l {output_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnaMyRMJ03bR"
      },
      "source": [
        "##전처리된 데이터를 사용하여 tf.keras로 모델 훈련하기\n",
        "\n",
        "훈련과 적용 모두에 동일한 코드를 사용하여 왜곡을 방지하는 데 `tf.Transform`이 어떻게 이용되는지 보여주기 위해 모델을 훈련할 것입니다. 모델을 훈련하고 훈련된 모델을 운영에 적합하게 준비하려면 입력 함수를 생성해야 합니다. 훈련 입력 함수와 적용 입력 함수의 주된 차이점은 훈련 데이터에는 레이블이 포함되고 운영 데이터에는 포함되지 않는다는 것입니다. 인수와 반환도 약간 다릅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8xCZKNc2wAS"
      },
      "source": [
        "###훈련을 위한 입력 함수 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StezlX-Uv0ae"
      },
      "source": [
        "이전 섹션의 파이프라인을 실행하면 변환된 데이터가 포함된 `TFRecord` 파일이 생성됩니다.\n",
        "\n",
        "다음 코드는 `tf.data.experimental.make_batched_features_dataset` 및 `tft.TFTransformOutput.transformed_feature_spec`을 사용하여 이러한 데이터 파일을 `tf.data.Dataset`으로 읽습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775Y7BTpHBmb"
      },
      "outputs": [],
      "source": [
        "def _make_training_input_fn(tf_transform_output, train_file_pattern,\n",
        "                            batch_size):\n",
        "  \"\"\"An input function reading from transformed data, converting to model input.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    transformed_examples: Base filename of examples.\n",
        "    batch_size: Batch size.\n",
        "\n",
        "  Returns:\n",
        "    The input data for training or eval, in the form of k.\n",
        "  \"\"\"\n",
        "  def input_fn():\n",
        "    return tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=train_file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=tf_transform_output.transformed_feature_spec(),\n",
        "        reader=tf.data.TFRecordDataset,\n",
        "        label_key=LABEL_KEY,\n",
        "        shuffle=True)\n",
        "\n",
        "  return input_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b8BgvBvkCnX"
      },
      "outputs": [],
      "source": [
        "train_file_pattern = pathlib.Path(output_dir)/f'{TRANSFORMED_TRAIN_DATA_FILEBASE}*'\n",
        "\n",
        "input_fn = _make_training_input_fn(\n",
        "    tf_transform_output=tf_transform_output,\n",
        "    train_file_pattern = str(train_file_pattern),\n",
        "    batch_size = 10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0PwPLBqxsg2"
      },
      "source": [
        "아래에서 변환된 데이터 샘플을 볼 수 있습니다. `education-num` 및 `hourd-per-week`와 같은 숫자 열이 [0,1] 범위의 부동 소수점으로 변환되고 문자열 열이 ID로 변환되는 방식에 주목하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpiS26IWlD-1"
      },
      "outputs": [],
      "source": [
        "for example, label in input_fn().take(1):\n",
        "  break\n",
        "\n",
        "pd.DataFrame(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaMzMnij88_v"
      },
      "outputs": [],
      "source": [
        "label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyNTX7CO8AAz"
      },
      "source": [
        "### 모델 훈련 및 평가하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdg9jXuLWuyK"
      },
      "source": [
        "모델 구성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK4brUuDTAJ4"
      },
      "outputs": [],
      "source": [
        "def build_keras_model(working_dir):\n",
        "  inputs = build_keras_inputs(working_dir)\n",
        "\n",
        "  encoded_inputs = encode_inputs(inputs)\n",
        "\n",
        "  stacked_inputs = tf.concat(tf.nest.flatten(encoded_inputs), axis=1)\n",
        "  output = tf.keras.layers.Dense(100, activation='relu')(stacked_inputs)\n",
        "  output = tf.keras.layers.Dense(50, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dense(2)(output)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fJwIbdCRFER"
      },
      "outputs": [],
      "source": [
        "def build_keras_inputs(working_dir):\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
        "  feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  # Build the `keras.Input` objects.\n",
        "  inputs = {}\n",
        "  for key, spec in feature_spec.items():\n",
        "    if isinstance(spec, tf.io.VarLenFeature):\n",
        "      inputs[key] = tf.keras.layers.Input(\n",
        "          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n",
        "    elif isinstance(spec, tf.io.FixedLenFeature):\n",
        "      inputs[key] = tf.keras.layers.Input(\n",
        "          shape=spec.shape, name=key, dtype=spec.dtype)\n",
        "    else:\n",
        "      raise ValueError('Spec type is not supported: ', key, spec)\n",
        "\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dHD5SoqRqOh"
      },
      "outputs": [],
      "source": [
        "def encode_inputs(inputs):\n",
        "  encoded_inputs = {}\n",
        "  for key in inputs:\n",
        "    feature = tf.expand_dims(inputs[key], -1)\n",
        "    if key in CATEGORICAL_FEATURE_KEYS:\n",
        "      num_buckets = tf_transform_output.num_buckets_for_transformed_feature(key)\n",
        "      encoding_layer = (\n",
        "          tf.keras.layers.CategoryEncoding(\n",
        "              num_tokens=num_buckets, output_mode='binary', sparse=False))\n",
        "      encoded_inputs[key] = encoding_layer(feature)\n",
        "    else:\n",
        "      encoded_inputs[key] = feature\n",
        "  \n",
        "  return encoded_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xNhSq8lTTx3"
      },
      "outputs": [],
      "source": [
        "model = build_keras_model(output_dir)\n",
        "\n",
        "tf.keras.utils.plot_model(model,rankdir='LR', show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQSpw_XzXVn1"
      },
      "source": [
        "데이터세트를 빌드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afi3NOC0OMUa"
      },
      "outputs": [],
      "source": [
        "def get_dataset(working_dir, filebase):\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  data_path_pattern = os.path.join(\n",
        "      working_dir,\n",
        "      filebase + '*')\n",
        "  \n",
        "  input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      data_path_pattern,\n",
        "      batch_size=BATCH_SIZE)\n",
        "  \n",
        "  dataset = input_fn()\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fE_3jyzX_h2"
      },
      "source": [
        "모델을 훈련하고 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i_lhWH8IZrk"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(\n",
        "    model,\n",
        "    working_dir):\n",
        "  \"\"\"Train the model on training data and evaluate on test data.\n",
        "\n",
        "  Args:\n",
        "    working_dir: The location of the Transform output.\n",
        "    num_train_instances: Number of instances in train set\n",
        "    num_test_instances: Number of instances in test set\n",
        "\n",
        "  Returns:\n",
        "    The results from the estimator's 'evaluate' method\n",
        "  \"\"\"\n",
        "  train_dataset = get_dataset(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)\n",
        "  validation_dataset = get_dataset(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)\n",
        "\n",
        "  model = build_keras_model(working_dir)\n",
        "\n",
        "  history = train_model(model, train_dataset, validation_dataset)\n",
        "\n",
        "  metric_values = model.evaluate(validation_dataset,\n",
        "                                 steps=EVALUATION_STEPS,\n",
        "                                 return_dict=True)\n",
        "  return model, history, metric_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcVsByIsViRy"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataset, validation_dataset):\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_dataset, validation_data=validation_dataset,\n",
        "      epochs=TRAIN_NUM_EPOCHS,\n",
        "      steps_per_epoch=STEPS_PER_TRAIN_EPOCH,\n",
        "      validation_steps=EVALUATION_STEPS)\n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5xoioogYTle"
      },
      "outputs": [],
      "source": [
        "model, history, metric_values = train_and_evaluate(model, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQCbdPIQeXeZ"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Eval')\n",
        "plt.ylim(0,max(plt.ylim()))\n",
        "plt.legend()\n",
        "plt.title('Loss');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYeuthrs27vl"
      },
      "source": [
        "### 새 데이터 변환하기\n",
        "\n",
        "이전 섹션에서 훈련 프로세스는 `transform_dataset` 함수에서 `tft_beam.AnalyzeAndTransformDataset`에 의해 생성된 변환된 데이터의 하드 카피를 사용했습니다.\n",
        "\n",
        "새 데이터에서 작업하려면 `tft_beam.WriteTransformFn`에 의해 저장된 `preprocessing_fn`의 최종 버전을 로드해야 합니다.\n",
        "\n",
        "`TFTransformOutput.transform_features_layer` 메서드는 출력 디렉터리에서 `preprocessing_fn` SavedModel을 로드합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxi9aS106CLd"
      },
      "source": [
        "소스 파일에서 처리되지 않은 새 배치를 로드하는 함수는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMHDZhp82ZjM"
      },
      "outputs": [],
      "source": [
        "def read_csv(file_name, batch_size):\n",
        "  return tf.data.experimental.make_csv_dataset(\n",
        "        file_pattern=file_name,\n",
        "        batch_size=batch_size,\n",
        "        column_names=ORDERED_CSV_COLUMNS,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        prefetch_buffer_size=0,\n",
        "        ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AradAjmW2vyd"
      },
      "outputs": [],
      "source": [
        "for ex in read_csv(test_path, batch_size=5):\n",
        "  break\n",
        "\n",
        "pd.DataFrame(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX1f6SgM6LZc"
      },
      "source": [
        "`tft.TransformFeaturesLayer`를 로드하여 `preprocessing_fn`으로 이 데이터를 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nma2Bzi--11x"
      },
      "outputs": [],
      "source": [
        "ex2 = ex.copy()\n",
        "ex2.pop('fnlwgt')\n",
        "\n",
        "tft_layer = tf_transform_output.transform_features_layer()\n",
        "t_ex = tft_layer(ex2)\n",
        "\n",
        "label = t_ex.pop(LABEL_KEY)\n",
        "pd.DataFrame(t_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P43ixyQNz1zq"
      },
      "source": [
        "`tft_layer`는 특성의 일부만 전달되는 경우에도 변환을 실행할 만큼 충분히 똑똑합니다. 예를 들어, 두 개의 특성만 전달하면 해당 특성의 변환된 버전만 다시 얻을 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swEPuZsR0Y5S"
      },
      "outputs": [],
      "source": [
        "ex2 = pd.DataFrame(ex)[['education', 'hours-per-week']]\n",
        "ex2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s4SxutV1DTI"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(tft_layer(dict(ex2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5wo3dN-vhFL"
      },
      "source": [
        "다음은 특성 사양에 없는 특성을 삭제하고 레이블이 제공된 특성에 있는 경우 `(features, label)` 쌍을 반환하는 보다 강력한 버전입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdMKDnafJh64"
      },
      "outputs": [],
      "source": [
        "class Transform(tf.Module):\n",
        "  def __init__(self, working_dir):\n",
        "    self.working_dir = working_dir\n",
        "    self.tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "    self.tft_layer = tf_transform_output.transform_features_layer()\n",
        "  \n",
        "  @tf.function\n",
        "  def __call__(self, features):\n",
        "    raw_features = {}\n",
        "\n",
        "    for key, val in features.items():\n",
        "      # Skip unused keys\n",
        "      if key not in RAW_DATA_FEATURE_SPEC:\n",
        "        continue\n",
        "\n",
        "      raw_features[key] = val\n",
        "\n",
        "    # Apply the `preprocessing_fn`.\n",
        "    transformed_features = tft_layer(raw_features)\n",
        "    \n",
        "    if LABEL_KEY in transformed_features:\n",
        "      # Pop the label and return a (features, labels) pair.\n",
        "      data_labels = transformed_features.pop(LABEL_KEY)\n",
        "      return (transformed_features, data_labels)\n",
        "    else:\n",
        "      return transformed_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm5HI578Ku1B"
      },
      "outputs": [],
      "source": [
        "transform = Transform(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jeenwN_3ZRj"
      },
      "outputs": [],
      "source": [
        "t_ex, t_label = transform(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIavZAqALO8H"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(t_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVQead0fwVuy"
      },
      "source": [
        "이제 `Dataset.map`을 사용하여 즉시 새 데이터에 해당 변환을 적용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN3IO6u1Mk83"
      },
      "outputs": [],
      "source": [
        "model.evaluate(\n",
        "    read_csv(test_path, batch_size=5).map(transform),\n",
        "    steps=EVALUATION_STEPS,\n",
        "    return_dict=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymlco3hfU_-E"
      },
      "source": [
        "### 모델 내보내기\n",
        "\n",
        "따라서 훈련된 모델과 `preporcessing_fn`을 새 데이터에 적용하는 메서드가 준비되었습니다. 직렬화된 `tf.train.Example` proto를 입력으로 받아들이는 새 모델로 이를 결합합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ2WICuwEwqC"
      },
      "outputs": [],
      "source": [
        "class ServingModel(tf.Module):\n",
        "  def __init__(self, model, working_dir):\n",
        "    self.model = model\n",
        "    self.working_dir = working_dir\n",
        "    self.transform = Transform(working_dir)\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\n",
        "  def __call__(self, serialized_tf_examples):\n",
        "    # parse the tf.train.Example\n",
        "    feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
        "    feature_spec.pop(LABEL_KEY)\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "    # Apply the `preprocessing_fn`\n",
        "    transformed_features = self.transform(parsed_features)\n",
        "    # Run the model\n",
        "    outputs = self.model(transformed_features)\n",
        "    # Format the output\n",
        "    classes_names = tf.constant([['0', '1']])\n",
        "    classes = tf.tile(classes_names, [tf.shape(outputs)[0], 1])\n",
        "    return {'classes': classes, 'scores': outputs}\n",
        "\n",
        "  def export(self, output_dir):\n",
        "    # Increment the directory number. This is required in order to make this\n",
        "    # model servable with model_server.\n",
        "    save_model_dir = pathlib.Path(output_dir)/'model'\n",
        "    number_dirs = [int(p.name) for p in save_model_dir.glob('*')\n",
        "                  if p.name.isdigit()]\n",
        "    id = max([0] + number_dirs)+1\n",
        "    save_model_dir = save_model_dir/str(id)\n",
        "\n",
        "    # Set the signature to make it visible for serving.\n",
        "    concrete_serving_fn = self.__call__.get_concrete_function()\n",
        "    signatures = {'serving_default': concrete_serving_fn}\n",
        "\n",
        "    # Export the model.\n",
        "    tf.saved_model.save(\n",
        "        self,\n",
        "        str(save_model_dir),\n",
        "        signatures=signatures)\n",
        "    \n",
        "    return save_model_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8TZf2di24L2"
      },
      "source": [
        "모델을 빌드하고 직렬화된 예제 배치에서 테스트 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2mSC1UMGAwJ"
      },
      "outputs": [],
      "source": [
        "serving_model = ServingModel(model, output_dir)\n",
        "\n",
        "serving_model(serialized_example_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWhighof3AK8"
      },
      "source": [
        "모델을 SavedModel로 내보냅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kodDWTJIEr77"
      },
      "outputs": [],
      "source": [
        "saved_model_dir = serving_model.export(output_dir)\n",
        "saved_model_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohbWxp3-3aQu"
      },
      "source": [
        "모델을 다시 로드하고 동일한 예제 배치에서 테스트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nShh6GqcEr78"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load(str(saved_model_dir))\n",
        "run_model = reloaded.signatures['serving_default']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiYJhQySEr78"
      },
      "outputs": [],
      "source": [
        "run_model(serialized_example_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICqetCnSjwp1"
      },
      "source": [
        "##우리가 수행한 작업. 이 예제에서는 `tf.Transform`을 사용하여 인구 조사 데이터의 데이터세트를 전처리하고 정리 및 변환된 데이터로 모델을 훈련했습니다. 또한 추론을 수행하기 위해 운영 환경에 훈련된 모델을 배포할 때 사용할 수 있는 입력 함수를 만들었습니다. 훈련과 추론 모두에 동일한 코드를 사용함으로써 데이터 기울이기 문제를 피할 수 있습니다. 그 과정에서 데이터 정리에 필요한 변환을 수행하기 위해 Apache Beam 변환을 생성하는 방법을 배웠습니다. 또한 이 변환 데이터를 이용해 `tf.keras`로 모델을 훈련하는 방법도 보았습니다. 이것은 TensorFlow Transform으로 수행할 수 있는 작업의 일부일 뿐입니다! `tf.Transform`을 더욱 자세히 살펴보고 이것으로 무엇을 할 수 있는지 알아보기 바랍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APEUSA9boKgT"
      },
      "source": [
        "## [선택 사항] 전처리된 데이터를 사용하여 tf.estimator로 모델 훈련하기\n",
        "\n",
        "> 경고: Estimator는 새 코드에 권장되지 않습니다. Estimator는 <code>v1.Session</code> 스타일 코드를 실행하며, 이 코드는 올바르게 작성하기가 좀 더 어렵고 특히 TF 2 코드와 결합할 경우 예기치 않게 작동할 수 있습니다. Estimator는 <a>호환성 보장</a>이 적용되지만 보안 취약점 외에는 수정 사항이 제공되지 않습니다. 자세한 내용은 [마이그레이션 가이드](https://tensorflow.org/guide/versions)를 참조하세요.\n",
        "\n",
        " <!-- <div class=\"tfo-display-only-on-site\"><devsite-expandable>\n",
        "  <button type=\"button\" class=\"button-red button expand-control\">Show Section</button> -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcBWjr3ioZbl"
      },
      "source": [
        "###훈련을 위한 입력 함수 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFO0MeWQ228a"
      },
      "outputs": [],
      "source": [
        "def _make_training_input_fn(tf_transform_output, transformed_examples,\n",
        "                            batch_size):\n",
        "  \"\"\"Creates an input function reading from transformed data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    transformed_examples: Base filename of examples.\n",
        "    batch_size: Batch size.\n",
        "\n",
        "  Returns:\n",
        "    The input function for training or eval.\n",
        "  \"\"\"\n",
        "  def input_fn():\n",
        "    \"\"\"Input function for training and eval.\"\"\"\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=transformed_examples,\n",
        "        batch_size=batch_size,\n",
        "        features=tf_transform_output.transformed_feature_spec(),\n",
        "        reader=tf.data.TFRecordDataset,\n",
        "        shuffle=True)\n",
        "\n",
        "    transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n",
        "        dataset).get_next()\n",
        "\n",
        "    # Extract features and label from the transformed tensors.\n",
        "    transformed_labels = tf.where(\n",
        "        tf.equal(transformed_features.pop(LABEL_KEY), 1))\n",
        "\n",
        "    return transformed_features, transformed_labels[:,1]\n",
        "\n",
        "  return input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22XOsZ-noez-"
      },
      "source": [
        "###제공을 위한 입력 함수 생성\n",
        "\n",
        "프로덕션에서 사용할 수 있는 입력 함수를 만들고 훈련된 모델을 제공하기에 적합하게 준비하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "NN5FVg343Jea"
      },
      "outputs": [],
      "source": [
        "def _make_serving_input_fn(tf_transform_output):\n",
        "  \"\"\"Creates an input function reading from raw data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "\n",
        "  Returns:\n",
        "    The serving input function.\n",
        "  \"\"\"\n",
        "  raw_feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
        "  # Remove label since it is not available during serving.\n",
        "  raw_feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  def serving_input_fn():\n",
        "    \"\"\"Input function for serving.\"\"\"\n",
        "    # Get raw features by generating the basic serving input_fn and calling it.\n",
        "    # Here we generate an input_fn that expects a parsed Example proto to be fed\n",
        "    # to the model at serving time.  See also\n",
        "    # tf.estimator.export.build_raw_serving_input_receiver_fn.\n",
        "    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "        raw_feature_spec, default_batch_size=None)\n",
        "    serving_input_receiver = raw_input_fn()\n",
        "\n",
        "    # Apply the transform function that was used to generate the materialized\n",
        "    # data.\n",
        "    raw_features = serving_input_receiver.features\n",
        "    transformed_features = tf_transform_output.transform_raw_features(\n",
        "        raw_features)\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(\n",
        "        transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "  return serving_input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc9Edp8A7dsI"
      },
      "source": [
        "###FeatureColumns에 입력 데이터 래핑. 예제 모델은 TensorFlow FeatureColumns에 데이터를 예상합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qOFOvBk7oJX"
      },
      "outputs": [],
      "source": [
        "def get_feature_columns(tf_transform_output):\n",
        "  \"\"\"Returns the FeatureColumns for the model.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: A `TFTransformOutput` object.\n",
        "\n",
        "  Returns:\n",
        "    A list of FeatureColumns.\n",
        "  \"\"\"\n",
        "  # Wrap scalars as real valued columns.\n",
        "  real_valued_columns = [tf.feature_column.numeric_column(key, shape=())\n",
        "                         for key in NUMERIC_FEATURE_KEYS]\n",
        "\n",
        "  # Wrap categorical columns.\n",
        "  one_hot_columns = [\n",
        "      tf.feature_column.indicator_column(\n",
        "          tf.feature_column.categorical_column_with_identity(\n",
        "              key=key,\n",
        "              num_buckets=(NUM_OOV_BUCKETS +\n",
        "                  tf_transform_output.vocabulary_size_by_name(\n",
        "                      vocab_filename=key))))\n",
        "      for key in CATEGORICAL_FEATURE_KEYS]\n",
        "\n",
        "  return real_valued_columns + one_hot_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6FyMzMcpOgT"
      },
      "source": [
        "###모델 학습, 평가 및 내보내기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iGQ0jeq8IWr"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,\n",
        "                       num_test_instances=NUM_TEST_INSTANCES):\n",
        "  \"\"\"Train the model on training data and evaluate on test data.\n",
        "\n",
        "  Args:\n",
        "    working_dir: Directory to read transformed data and metadata from and to\n",
        "        write exported model to.\n",
        "    num_train_instances: Number of instances in train set\n",
        "    num_test_instances: Number of instances in test set\n",
        "\n",
        "  Returns:\n",
        "    The results from the estimator's 'evaluate' method\n",
        "  \"\"\"\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  run_config = tf.estimator.RunConfig()\n",
        "\n",
        "  estimator = tf.estimator.LinearClassifier(\n",
        "      feature_columns=get_feature_columns(tf_transform_output),\n",
        "      config=run_config,\n",
        "      loss_reduction=tf.losses.Reduction.SUM)\n",
        "\n",
        "  # Fit the model using the default optimizer.\n",
        "  train_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
        "      batch_size=BATCH_SIZE)\n",
        "  estimator.train(\n",
        "      input_fn=train_input_fn,\n",
        "      max_steps=TRAIN_NUM_EPOCHS * num_train_instances / BATCH_SIZE)\n",
        "\n",
        "  # Evaluate model on test dataset.\n",
        "  eval_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n",
        "      batch_size=1)\n",
        "\n",
        "  # Export the model.\n",
        "  serving_input_fn = _make_serving_input_fn(tf_transform_output)\n",
        "  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
        "  estimator.export_saved_model(exported_model_dir, serving_input_fn)\n",
        "\n",
        "  return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k8LrDPZpZsK"
      },
      "source": [
        "###모두 결합하기. 인구 조사 데이터를 사전 처리하고, 모델을 훈련하고, 제공에 적합하게 준비하는 데 필요한 모든 요소들을 만들었습니다. 지금까지는 준비 작업만 했고, 이제 실행해볼 차례입니다!\n",
        "\n",
        "참고: 전체 프로세스를 보려면 이 셀의 출력을 스크롤하세요. 결과는 맨 아래에 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_1_2dB6pdc2"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "temp = temp = os.path.join(tempfile.mkdtemp(),'estimator')\n",
        "\n",
        "transform_data(train_path, test_path, temp)\n",
        "results = train_and_evaluate(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_IqGL90GCIq"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6T3aHoRsjgR"
      },
      "source": [
        " </devsite-expandable></div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "APEUSA9boKgT"
      ],
      "name": "census.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
