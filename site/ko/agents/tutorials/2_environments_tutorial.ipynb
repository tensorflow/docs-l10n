{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma19Ks2CTDbZ"
      },
      "source": [
        "##### Copyright 2021 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XljsiF6lYkuV"
      },
      "source": [
        "# 환경\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/2_environments_tutorial\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/2_environments_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h3B-YBHopJI"
      },
      "source": [
        "## 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9c6vCPGovOM"
      },
      "source": [
        "RL(Reinforcement Learning)의 목표는 환경과 상호 작용하여 학습하는 에이전트를 설계하는 것입니다. 표준 RL 설정에서 에이전트는 타임스텝마다 관측 값을 수신하고 행동을 선택합니다. 행동이 환경에 적용되고 환경이 보상과 새로운 관찰 값을 반환합니다. 에이전트는 이익이라고도 하는 보상의 합계를 최대화하기 위한 행동을 선택하도록 정책을 훈련합니다.\n",
        "\n",
        "TF-Agents에서 환경은 Python 또는 TensorFlow로 구현될 수 있습니다. Python 환경은 일반적으로 구현, 이해 및 디버깅하기 더 쉽지만, TensorFlow 환경은 더 효율적이며 자연적인 병렬화가 가능합니다. 가장 일반적인 워크플로는 Python에서 환경을 구현하고 래퍼 중 하나를 사용하여 환경을 TensorFlow로 자동 변환합니다.\n",
        "\n",
        "Python 환경을 먼저 살펴보겠습니다. TensorFlow 환경은 매우 유사한 API를 따릅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_16bQF0anmE"
      },
      "source": [
        "## 설정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qax00bg2a4Jj"
      },
      "source": [
        "tf-agents 또는 gym을 아직 설치하지 않은 경우, 다음을 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKU2iY_7at8Y"
      },
      "outputs": [],
      "source": [
        "!pip install \"gym>=0.21.0\"\n",
        "!pip install tf-agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZAoFNwnRbKK"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-y4p9i9UURn"
      },
      "source": [
        "## Python 환경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPSwHONKMNv9"
      },
      "source": [
        "Python 환경에는 행동을 환경에 적용하는 `step(action) -> next_time_step` 메서드가 있으며 다음 단계에 대한 다음 정보를 반환합니다.\n",
        "\n",
        "1. `observation`: 에이전트가 다음 단계에서 행동을 선택하기 위해 관찰할 수 있는 환경 상태의 일부입니다.\n",
        "2. `reward`: 에이전트는 여러 단계에 걸쳐 이러한 보상의 합계를 최대화하는 방법을 학습하고 있습니다.\n",
        "3. `step_type`: 환경과의 상호 작용은 일반적으로 시퀀스/에피소드의 일부입니다. 체스 게임에서 여러 번의 이동을 예로 들 수 있습니다. step_type은 `FIRST`, `MID` 또는 `LAST`일 수 있으며, 해당 타임스텝이 시퀀스의 첫 번째 단계인지 중간 단계인지 마지막 단계인지를 나타냅니다.\n",
        "4. `discount`: 현재 타임스텝의 보상에 대한 다음 타임스텝의 보상 가중치를 나타내는 부동 소수점입니다.\n",
        "\n",
        "이들은 명명된 튜플 `TimeStep(step_type, reward, discount, observation)`으로 그룹화됩니다.\n",
        "\n",
        "The interface that all Python environments must implement is in `environments/py_environment.PyEnvironment`. The main methods are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlD2Dd2vUTtg"
      },
      "outputs": [],
      "source": [
        "class PyEnvironment(object):\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Return initial_time_step.\"\"\"\n",
        "    self._current_time_step = self._reset()\n",
        "    return self._current_time_step\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Apply action and return new time_step.\"\"\"\n",
        "    if self._current_time_step is None:\n",
        "        return self.reset()\n",
        "    self._current_time_step = self._step(action)\n",
        "    return self._current_time_step\n",
        "\n",
        "  def current_time_step(self):\n",
        "    return self._current_time_step\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    \"\"\"Return time_step_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Return observation_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def action_spec(self):\n",
        "    \"\"\"Return action_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _reset(self):\n",
        "    \"\"\"Return initial_time_step.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _step(self, action):\n",
        "    \"\"\"Apply action and return new time_step.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfF8koryiGPR"
      },
      "source": [
        "`step()` 메서드 외에도, 환경은 새 시퀀스를 시작하고 초기 `TimeStep`을 제공하는 `reset()` 메서드도 제공합니다. `reset` 메서드를 명시적으로 호출할 필요는 없습니다. 에피소드가 끝나거나 step()이 처음 호출될 때 환경이 자동으로 재설정된다고 가정합니다.\n",
        "\n",
        "서브 클래스는 `step()` 또는 `reset()`을 직접 구현하지 않습니다. 대신 `_step()` 및 `_reset()` 메서드를 재정의합니다. 이들 메서드에서 반환된 타임스텝은 `current_time_step()`를 통해 캐시되고 노출됩니다.\n",
        "\n",
        "`observation_spec` 및 `action_spec` 메서드는 관찰 값 및 행동의 이름, 형상, 데이터 유형 및 범위를 각각 설명하는 `(Bounded)ArraySpecs`의 중첩을 반환합니다.\n",
        "\n",
        "TF-Agents에서 목록, 튜플, 명명된 튜플 또는 사전으로 구성된 구조와 같은 트리로 정의된 중첩을 반복해서 참조합니다. 중첩은 관찰 값과 행동의 구조를 유지하기 위해 임의로 구성될 수 있습니다. 관찰 값과 행동이 많은 복잡한 환경에 매우 유용한 것으로 나타났습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r63R-RbjcIRw"
      },
      "source": [
        "### 표준 환경 사용하기\n",
        "\n",
        "TF Agents에는 OpenAI Gym, DeepMind-control 및 Atari와 같은 많은 표준 환경을 위한 래퍼가 내장되어 있으며 `py_environment.PyEnvironment` 인터페이스를 따릅니다. 이들 래핑된 환경은 환경 도구 모음를 사용하여 쉽게 로드할 수 있습니다. OpenAI gym에서 CartPole 환경을 로드하고 행동과 time_step_spec을 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kBPE5T-nb2-"
      },
      "outputs": [],
      "source": [
        "environment = suite_gym.load('CartPole-v0')\n",
        "print('action_spec:', environment.action_spec())\n",
        "print('time_step_spec.observation:', environment.time_step_spec().observation)\n",
        "print('time_step_spec.step_type:', environment.time_step_spec().step_type)\n",
        "print('time_step_spec.discount:', environment.time_step_spec().discount)\n",
        "print('time_step_spec.reward:', environment.time_step_spec().reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWXOC863Apo_"
      },
      "source": [
        "환경이 [0, 1]에서 유형 `int64`의 행동을 기대하고 `TimeSteps`를 반환합니다. 관찰 값은 길이 4의 `float32` 벡터이며, 할인 인자는 [0.0, 1.0]에서 `float32`입니다. 이제 전체 에피소드에 대해 고정된 행동 `(1,)`을 취해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzIbOJ0-0y12"
      },
      "outputs": [],
      "source": [
        "action = np.array(1, dtype=np.int32)\n",
        "time_step = environment.reset()\n",
        "print(time_step)\n",
        "while not time_step.is_last():\n",
        "  time_step = environment.step(action)\n",
        "  print(time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAbBl4_PMtA"
      },
      "source": [
        "### 자신만의 Python 환경 만들기\n",
        "\n",
        "많은 고객에게 일반적인 사용 사례는 TF-Agents의 표준 에이전트(agents/ 참조) 중 하나를 문제에 적용하는 것입니다. 이를 위해서는 문제를 환경으로 만들어야 합니다. Python으로 환경을 구현하는 방법을 살펴보겠습니다.\n",
        "\n",
        "에이전트가 다음과 같은 카드 게임((블랙 잭에서 영감받음)을 플레이하도록 훈련하고 싶다고 가정해 보겠습니다.\n",
        "\n",
        "1. 이 게임은 1에서 10까지의 무한한 카드 한 벌을 사용하여 진행됩니다.\n",
        "2. 매번 에이전트는 두 가지 일을 할 수 있습니다. 새로운 임의의 카드를 얻거나 현재 라운드를 중단합니다.\n",
        "3. 목표는 라운드가 끝날 때 카드의 합계를 가능한 한 21에 가깝게 유지하는 것입니다.\n",
        "\n",
        "게임을 나타내는 환경은 다음과 같습니다.\n",
        "\n",
        "1. Actions: 2 가지 행동이 있습니다. Action 0: 새 카드를 받고, Action 1: 현재 라운드를 종료합니다.\n",
        "2. Observations: 현재 라운드에서의 카드의 합계\n",
        "3. Reward: 목표는 21을 넘어가지 않고 가능한 한 21에 가까워지는 것이므로 라운드가 끝날 때 다음 보상을 사용하여 이를 달성할 수 있습니다. sum_of_cards - 21 if sum_of_cards &lt;= 21, else -21\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HD0cDykPL6I"
      },
      "outputs": [],
      "source": [
        "class CardGameEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    # Make sure episodes don't go on forever.\n",
        "    if action == 1:\n",
        "      self._episode_ended = True\n",
        "    elif action == 0:\n",
        "      new_card = np.random.randint(1, 11)\n",
        "      self._state += new_card\n",
        "    else:\n",
        "      raise ValueError('`action` should be 0 or 1.')\n",
        "\n",
        "    if self._episode_ended or self._state >= 21:\n",
        "      reward = self._state - 21 if self._state <= 21 else -21\n",
        "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
        "    else:\n",
        "      return ts.transition(\n",
        "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYEwyX7QsqeX"
      },
      "source": [
        "위의 환경을 올바르게 정의하여 모든 작업을 수행했는지 확인합니다. 자신의 환경을 만들 때, 생성된 관찰 값 및 time_steps가 사양에 정의된 올바른 형상 및 유형을 따르는지 확인해야 합니다. 이들은 TensorFlow 그래프를 생성하는 데 사용되므로 잘못 정의하면 디버깅하기 어려운 문제가 발생할 수 있습니다.\n",
        "\n",
        "환경을 검증하기 위해 임의의 정책을 사용하여 행동을 생성하고 5가지 이상의 에피소드를 반복하여 환경이 의도한 대로 동작하는지 확인합니다. 환경 사양을 따르지 않는 time_step을 수신하면 오류가 발생합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hhm-5R7spVx"
      },
      "outputs": [],
      "source": [
        "environment = CardGameEnv()\n",
        "utils.validate_py_environment(environment, episodes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_36eM7MvkNOg"
      },
      "source": [
        "이제 환경이 의도한 대로 동작하고 있음을 알았으므로 고정된 정책을 사용하여 이 환경을 실행하겠습니다. 3장의 카드를 요청한 후 라운드를 종료합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FILylafAkMEx"
      },
      "outputs": [],
      "source": [
        "get_new_card_action = np.array(0, dtype=np.int32)\n",
        "end_round_action = np.array(1, dtype=np.int32)\n",
        "\n",
        "environment = CardGameEnv()\n",
        "time_step = environment.reset()\n",
        "print(time_step)\n",
        "cumulative_reward = time_step.reward\n",
        "\n",
        "for _ in range(3):\n",
        "  time_step = environment.step(get_new_card_action)\n",
        "  print(time_step)\n",
        "  cumulative_reward += time_step.reward\n",
        "\n",
        "time_step = environment.step(end_round_action)\n",
        "print(time_step)\n",
        "cumulative_reward += time_step.reward\n",
        "print('Final Reward = ', cumulative_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vBLPN3ioyGx"
      },
      "source": [
        "### 환경 래퍼\n",
        "\n",
        "An environment wrapper takes a Python environment and returns a modified version of the environment. Both the original environment and the modified environment are instances of `py_environment.PyEnvironment`, and multiple wrappers can be chained together.\n",
        "\n",
        "일부 일반적인 래퍼는 `environments/wrappers.py`에서 찾을 수 있습니다. 예를 들면, 다음과 같습니다.\n",
        "\n",
        "1. `ActionDiscretizeWrapper`: 연속 행동 공간을 불연속 행동 공간으로 변환합니다.\n",
        "2. `RunStats`: 수행한 단계 수, 완료된 에피소드 수 등과 같은 환경의 실행 통계를 캡처합니다.\n",
        "3. `TimeLimit`: 정해진 수의 단계 후에 에피소드를 종료합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8aIybRdnFfb"
      },
      "source": [
        "#### 예 1: 행동 불연속화 래퍼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIaxJRUpvfyc"
      },
      "source": [
        "InvertedPendulum은 범위 `[-2, 2]`에서 연속 행동을 허용하는 PyBullet 환경입니다. 이 환경에서 DQN과 같은 불연속 행동 에이전트를 훈련하려면 행동 공간을 불연속화(양자화)해야 합니다. 이것이 바로 `ActionDiscretizeWrapper`가 하는 일입니다. 래핑 전후의 `action_spec`을 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJxEoZ4HoyjR"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('Pendulum-v1')\n",
        "print('Action Spec:', env.action_spec())\n",
        "\n",
        "discrete_action_env = wrappers.ActionDiscretizeWrapper(env, num_actions=5)\n",
        "print('Discretized Action Spec:', discrete_action_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njFjW8bmwTWJ"
      },
      "source": [
        "The wrapped `discrete_action_env` is an instance of `py_environment.PyEnvironment` and can be treated like a regular Python environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l5dwAhsP_F_"
      },
      "source": [
        "## TensorFlow Environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZG39AjBkTjr"
      },
      "source": [
        "The interface for TF environments is defined in `environments/tf_environment.TFEnvironment` and looks very similar to the Python environments. TF Environments differ from Python envs in a couple of ways:\n",
        "\n",
        "- 배열 대신 텐서 객체를 생성합니다.\n",
        "- TF Environments는 사양과 비교하여 생성된 텐서에 배치 차원을 추가합니다.\n",
        "\n",
        "Converting the Python environments into TFEnvs allows tensorflow to parallelize operations. For example, one could define a `collect_experience_op` that collects data from the environment and adds to a `replay_buffer`, and a `train_op` that reads from the `replay_buffer` and trains the agent, and run them in parallel naturally in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKBDDZqKTxsL"
      },
      "outputs": [],
      "source": [
        "class TFEnvironment(object):\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    \"\"\"Describes the `TimeStep` tensors returned by `step()`.\"\"\"\n",
        "\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Defines the `TensorSpec` of observations provided by the environment.\"\"\"\n",
        "\n",
        "  def action_spec(self):\n",
        "    \"\"\"Describes the TensorSpecs of the action expected by `step(action)`.\"\"\"\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
        "    return self._reset()\n",
        "\n",
        "  def current_time_step(self):\n",
        "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
        "    return self._current_time_step()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\"\n",
        "    return self._step(action)\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _reset(self):\n",
        "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _current_time_step(self):\n",
        "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _step(self, action):\n",
        "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFkBIA92ThWf"
      },
      "source": [
        "`current_time_step()` 메서드는 현재 time_step을 반환하고 필요한 경우 환경을 초기화합니다.\n",
        "\n",
        "`reset()` 메서드는 환경에서 강제로 재설정하고 current_step을 반환합니다.\n",
        "\n",
        "`action`이 이전 `time_step`에 의존하지 않으면 `tf.control_dependency`가 `Graph` 모드에 필요합니다.\n",
        "\n",
        "지금은 `TFEnvironments`를 만드는 방법을 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6wS3AaLdVLT"
      },
      "source": [
        "### 자신만의 TensorFlow Environment 만들기\n",
        "\n",
        "이것은 Pytohn에서 환경을 만드는 것보다 더 복잡하므로 이 colab에서는 다루지 않습니다. [여기](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/tf_environment_test.py)에서 예제를 볼 수 있습니다. 더 일반적인 사용 사례는 Python으로 환경을 구현하고 `TFPyEnvironment` 래퍼를 사용하여 환경을 TensorFlow로 래핑하는 것입니다(아래 참조)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Ny2lb-dU5R"
      },
      "source": [
        "### TensorFlow에서 Python Environment 래핑하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv4-UcurZ8nb"
      },
      "source": [
        "`TFPyEnvironment` 래퍼를 사용하여 모든 Python 환경을 TensorFlow 환경으로 쉽게 래핑할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYerqyNfnVRL"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
        "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
        "print(\"Action Specs:\", tf_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3WFrnX9CNpC"
      },
      "source": [
        "이제 사양의 유형은 `(Bounded)TensorSpec`입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQPvC1ARYALj"
      },
      "source": [
        "### 사용 예"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov7EIrk8dKUU"
      },
      "source": [
        "#### 간단한 예제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdvFqUqbdB7u"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "# reset() creates the initial time_step after resetting the environment.\n",
        "time_step = tf_env.reset()\n",
        "num_steps = 3\n",
        "transitions = []\n",
        "reward = 0\n",
        "for i in range(num_steps):\n",
        "  action = tf.constant([i % 2])\n",
        "  # applies the action and returns the new TimeStep.\n",
        "  next_time_step = tf_env.step(action)\n",
        "  transitions.append([time_step, action, next_time_step])\n",
        "  reward += next_time_step.reward\n",
        "  time_step = next_time_step\n",
        "\n",
        "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
        "print('\\n'.join(map(str, np_transitions)))\n",
        "print('Total reward:', reward.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWs48LNsdLnc"
      },
      "source": [
        "#### 전체 에피소드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t561kUXMk-KM"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "time_step = tf_env.reset()\n",
        "rewards = []\n",
        "steps = []\n",
        "num_episodes = 5\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
        "    time_step = tf_env.step(action)\n",
        "    episode_steps += 1\n",
        "    episode_reward += time_step.reward.numpy()\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps)\n",
        "  time_step = tf_env.reset()\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)\n",
        "\n",
        "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
        "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2_environments_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
