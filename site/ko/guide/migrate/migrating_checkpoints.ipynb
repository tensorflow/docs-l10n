{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bYaCABobL5q"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FlUw7tSKbtg4"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61dp4Hg5ksTC"
      },
      "source": [
        "# 모델 체크포인트 마이그레이션하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/migrate/migrating_checkpoints\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/guide/migrate/migrating_checkpoints.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/guide/migrate/migrating_checkpoints.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/guide/migrate/migrating_checkpoints.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avuMwzscPnHh"
      },
      "source": [
        "참고: `tf.compat.v1.Saver`를 사용하여 저장한 체크포인트는 종종 *TF1 또는 이름 기반* 체크포인트라고 합니다. `tf.train.Checkpoint`를 사용하여 저장한 체크포인트는 *TF2 또는 객체 기반* 체크포인트라고 합니다.\n",
        "\n",
        "## 개요\n",
        "\n",
        "이 가이드에서는 [`tf.compat.v1.Saver`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver)로 체크포인트를 저장하고 로드하는 모델이 있으며, TF2 [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) API를 사용하거나 TF2 모델의 기존 체크포인트를 사용하려는 경우를 가정합니다.\n",
        "\n",
        "다음은 발생할 수 있는 몇 가지 일반적인 시나리오입니다.\n",
        "\n",
        "**시나리오 1**\n",
        "\n",
        "이전 훈련에서 실행하는 기존 TF1 체크포인트가 있으며 이를 TF2로 로드하거나 변환해야 합니다.\n",
        "\n",
        "- TF2에서 TF1 체크포인트를 로드하려면 [*TF2에서 TF1 체크포인트 로드하기* 코드 조각을 참조하세요.](#load-tf1-in-tf2)\n",
        "- 체크포인트를 TF2로 변환하려면 [*체크포인트 변환*](#checkpoint-conversion)을 참조하세요.\n",
        "\n",
        "**시나리오 2**\n",
        "\n",
        "변수 이름과 경로를 변경할 위험이 있는 방식으로 모델을 조정하고 있으며(예: `get_variable`에서 명시적 `tf.Variable`를 생성하도록 점진적으로 마이그레이션하는 경우) 작업 도중에 기존 체크포인트의 저장/로드를 유지하려고 합니다.\n",
        "\n",
        "[*모델 마이그레이션을 진행하는 동안 체크포인트 호환성을 유지하는 방법*](#maintain-checkpoint-compat) 섹션을 참조하세요.\n",
        "\n",
        "**시나리오 3**\n",
        "\n",
        "훈련 코드와 체크포인트를 TF2로 마이그레이션하고 있지만 현재 추론 파이프라인은 운영 안정성을 위해 계속해서 TF1 체크포인트를 필요로 합니다.\n",
        "\n",
        "*옵션 1*\n",
        "\n",
        "훈련을 진행할 때 TF1과 TF2 체크포인트를 모두 저장합니다.\n",
        "\n",
        "- [*TF2에서 TF1 체크포인트 저장하기*](#save-tf1-in-tf2) 참조\n",
        "\n",
        "*옵션 2*\n",
        "\n",
        "TF2 체크포인트를 TF1로 변환합니다.\n",
        "\n",
        "- [*체크포인트 변환하기*](#checkpoint-conversion) 참조\n",
        "\n",
        "---\n",
        "\n",
        "아래의 예제는 TF1/TF2에서 체크포인트를 저장하고 로드하는 모든 조합을 보여주기에 모델을 마이그레이션하는 방법을 결정할 때 약간의 유연성을 가질 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaYgaekzOAHf"
      },
      "source": [
        "## 설치하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcvTd5QhZ78L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "\n",
        "def print_checkpoint(save_path):\n",
        "  reader = tf.train.load_checkpoint(save_path)\n",
        "  shapes = reader.get_variable_to_shape_map()\n",
        "  dtypes = reader.get_variable_to_dtype_map()\n",
        "  print(f\"Checkpoint at '{save_path}':\")\n",
        "  for key in shapes:\n",
        "    print(f\"  (key='{key}', shape={shapes[key]}, dtype={dtypes[key].name}, \"\n",
        "          f\"value={reader.get_tensor(key)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO8Q6QkulJlj"
      },
      "source": [
        "## TF1에서 TF2로 변경하기\n",
        "\n",
        "이 섹션은 TF1과 TF2 사이에 변경된 사항과 \"이름 기반\"(TF1) 대비 \"객체 기반\"(TF2) 체크포인트의 의미에 대해 궁금해하는 내용을 포함합니다.\n",
        "\n",
        "두 가지 유형의 체크포인트는 실제로 동일한 형식으로 저장되며 이는 본질적으로 키-값 테이블입니다. 키가 생성되는 방식만 다릅니다.\n",
        "\n",
        "이름 기반 체크포인트의 키는 **변수 이름**입니다. 객체 기반 체크포인트의 키는 **루트 객체에서 변수까지의 경로**를 나타냅니다. 아래의 예제를 통해 이것이 의미하는 바를 더 잘 이해할 수 있습니다.\n",
        "\n",
        "먼저 일부 체크포인트를 저장합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YXzbXvOWvdF"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    saver = tf1.train.Saver()\n",
        "    sess.run(a.assign(1))\n",
        "    sess.run(b.assign(2))\n",
        "    sess.run(c.assign(3))\n",
        "    saver.save(sess, 'tf1-ckpt')\n",
        "\n",
        "print_checkpoint('tf1-ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raOych1UaJzl"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(5.0, name='a')\n",
        "b = tf.Variable(6.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(7.0, name='c')\n",
        "\n",
        "ckpt = tf.train.Checkpoint(variables=[a, b, c])\n",
        "save_path_v2 = ckpt.save('tf2-ckpt')\n",
        "print_checkpoint(save_path_v2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYyLhTYszcpl"
      },
      "source": [
        "`tf2-ckpt`의 키를 보면 모두 각 변수의 객체 경로를 참조한다는 것을 알 수 있습니다. 예를 들어, 변수 `a`는 `variables` 목록의 첫 번째 요소이므로 해당 키는 `variables/0/...`이 됩니다(.ATTRIBUTES/VARIABLE_VALUE 상수는 무시해도 됨).\n",
        "\n",
        "`Checkpoint` 객체를 자세히 살펴보면 아래와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLOxvoosg4Al"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "c = tf.Variable(0.)\n",
        "root = ckpt = tf.train.Checkpoint(variables=[a, b, c])\n",
        "print(\"root type =\", type(root).__name__)\n",
        "print(\"root.variables =\", root.variables)\n",
        "print(\"root.variables[0] =\", root.variables[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qaed1yAm3Ar"
      },
      "source": [
        "아래 코드 조각으로 실험해보고 객체 구조에 따라 체크포인트 키가 어떻게 변경되는지 확인해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdHJXlZOyDnn"
      },
      "outputs": [],
      "source": [
        "module = tf.Module()\n",
        "module.d = tf.Variable(0.)\n",
        "test_ckpt = tf.train.Checkpoint(v={'a': a, 'b': b}, \n",
        "                                c=c,\n",
        "                                module=module)\n",
        "test_ckpt_path = test_ckpt.save('root-tf2-ckpt')\n",
        "print_checkpoint(test_ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iWitEsayDWs"
      },
      "source": [
        "*TF2가 이 메커니즘을 사용하는 이유는 무엇인가요?*\n",
        "\n",
        "TF2에는 더 이상 전역 그래프가 없기 때문에 변수 이름을 신뢰할 수 없으며 프로그램이 서로 일관성이 없을 수 있습니다. TF2는 변수는 레이어가 소유하고, 레이어는 모델이 소유하는 객체 지향 모델링 접근 방식을 권장합니다.\n",
        "\n",
        "```\n",
        "variable = tf.Variable(...)\n",
        "layer.variable_name = variable\n",
        "model.layer_name = layer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kv9SmyVjGLA"
      },
      "source": [
        "## 모델 마이그레이션을 진행하는 동안 체크포인트 호환성을 유지하는 방법\n",
        "\n",
        "<a name=\"maintain-checkpoint-compat\"></a>\n",
        "\n",
        "*모든 변수가 올바른 값으로 초기화되었는지 확인*하면 작업/함수가 올바른 계산을 수행하고 있는지 검증할 수 있기에  마이그레이션 프로세스에서 중요합니다. 이렇게 하려면 다양한 마이그레이션 단계에서 모델 간의 **체크포인트 호환성**을 고려해야 합니다. 기본적으로 이 섹션은 *모델을 변경하는 동안 어떻게 해야 동일한 체크포인트를 계속 사용할 수 있나요*라는 질문에 답합니다.\n",
        "\n",
        "다음은 유연성을 높이기 위해 체크포인트 호환성을 유지하는 세 가지 방법입니다.\n",
        "\n",
        "1. 모델이 이전과 **동일한 변수 이름**을 갖습니다.\n",
        "2. 모델이 다른 변수 이름을 가지며, 체크포인트의 변수 이름을 새 이름에 매핑하는 **할당 매핑**을 유지합니다.\n",
        "3. 모델이 다른 변수 이름을 가지며, 모든 변수를 저장하는 **TF2 체크포인트 객체**를 유지합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5JhCyPZDx43"
      },
      "source": [
        "### 변수 이름이 일치하는 경우\n",
        "\n",
        "긴 제목: 변수 이름이 일치할 때 체크포인트를 다시 사용하는 방법\n",
        "\n",
        "짧은 대답: `tf1.train.Saver` 또는 `tf.train.Checkpoint`를 사용하여 기존 체크포인트를 직접 로드할 수 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "`tf.compat.v1.keras.utils.track_tf1_style_variables`를 사용하는 경우 모델 변수 이름이 이전과 동일한지 확인합니다. 변수 이름이 일치하는지 수동으로 확인할 수도 있습니다.\n",
        "\n",
        "마이그레이션된 모델의 변수 이름이 일치하면 `tf.train.Checkpoint` 또는 `tf.compat.v1.train.Saver`를 직접 사용하여 체크포인트를 로드할 수도 있습니다. 두 API 모두 Eager 모드 및 그래프 모드와 호환되므로 마이그레이션의 모든 단계에서 사용할 수 있습니다.\n",
        "\n",
        "참고: `tf.train.Checkpoint`를 사용하여 TF1 체크포인트를 로드할 수 있지만 복잡한 이름 일치 작업 없이 `tf.compat.v1.Saver`를 사용하여 TF2 체크포인트를 로드할 수 없습니다.\n",
        "\n",
        "다음은 다른 모델에 동일한 체크포인트를 사용하는 예제입니다. 먼저 `tf1.train.Saver`를 사용하여 TF1 체크포인트를 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijlHS96URsfR"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    saver = tf1.train.Saver()\n",
        "    sess.run(a.assign(1))\n",
        "    sess.run(b.assign(2))\n",
        "    sess.run(c.assign(3))\n",
        "    save_path = saver.save(sess, 'tf1-ckpt')\n",
        "print_checkpoint(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg7nWZphQD9u"
      },
      "source": [
        "아래 예제는 `tf.compat.v1.Saver`를 사용하여 Eager 모드로 체크포인트를 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4K16m0PPncQ"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.0, name='a')\n",
        "b = tf.Variable(0.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(0.0, name='c')\n",
        "\n",
        "# With the removal of collections in TF2, you must pass in the list of variables\n",
        "# to the Saver object:\n",
        "saver = tf1.train.Saver(var_list=[a, b, c])\n",
        "saver.restore(sess=None, save_path=save_path)\n",
        "print(f\"loaded values of [a, b, c]:  [{a.numpy()}, {b.numpy()}, {c.numpy()}]\")\n",
        "\n",
        "# Saving also works in eager (sess must be None).\n",
        "path = saver.save(sess=None, save_path='tf1-ckpt-saved-in-eager')\n",
        "print_checkpoint(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWnq1f5yAPkq"
      },
      "source": [
        "다음 코드 조각은 TF2 API `tf.train.Checkpoint`를 사용하여 체크포인트를 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StyrzwGvW1YZ"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.0, name='a')\n",
        "b = tf.Variable(0.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(0.0, name='c')\n",
        "\n",
        "# Without the name_scope, name=\"scoped/c\" works too:\n",
        "c_2 = tf.Variable(0.0, name='scoped/c')\n",
        "\n",
        "print(\"Variable names: \")\n",
        "print(f\"  a.name = {a.name}\")\n",
        "print(f\"  b.name = {b.name}\")\n",
        "print(f\"  c.name = {c.name}\")\n",
        "print(f\"  c_2.name = {c_2.name}\")\n",
        "\n",
        "# Restore the values with tf.train.Checkpoint\n",
        "ckpt = tf.train.Checkpoint(variables=[a, b, c, c_2])\n",
        "ckpt.restore(save_path)\n",
        "print(f\"loaded values of [a, b, c, c_2]:  [{a.numpy()}, {b.numpy()}, {c.numpy()}, {c_2.numpy()}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYYgbj8F7Yb7"
      },
      "source": [
        "#### TF2의 변수 이름\n",
        "\n",
        "- 여전히 변수에는 모두 설정할 수 있는 `name` 인수가 있습니다.\n",
        "- 또한 Keras 모델은 변수의 접두사로 설정하는 `name` 인수를 사용합니다.\n",
        "- `v1.name_scope` 함수를 변수 이름의 접두어를 지정하는데 사용할 수 있습니다. 이 함수는 `tf.variable_scope`와는 매우 다릅니다. 이름에만 영향을 미치며 변수를 추적하거나 재사용을 관장하지 않습니다.\n",
        "\n",
        "`tf.compat.v1.keras.utils.track_tf1_style_variables` 데코레이터는 `tf.variable_scope`와 `tf.compat.v1.get_variable`의 이름 지정 및 재사용 의미 체계가 변경되지 않도록 유지하여 변수 이름과 TF1 체크포인트 호환성을 유지하는 데 도움이 되는 shim입니다. 자세한 정보는 [모델 매핑 가이드](./model_mapping.ipynb)를 참조하세요.\n",
        "\n",
        "**참고 1: shim을 사용하는 경우 TF2 API를 사용하여 체크포인트를 로드합니다(사전 훈련된 TF1 체크포인트를 사용하는 경우에도).**\n",
        "\n",
        "*체크포인트 Keras* 섹션을 참조하세요.\n",
        "\n",
        "**참고 2: `get_variable`에서 `tf.Variable`로 마이그레이션하는 경우:**\n",
        "\n",
        "Shim으로 데코레이팅한 레이어 또는 모듈이 `tf.compat.v1.get_variable` 대신 `tf.Variable`을 사용하는 일부 변수(또는 Keras 레이어/모델)로 구성되어 있고 객체 지향 방식의 속성와 추적이 첨부된 경우 TF1.x 그래프/세션 대 Eager 실행을 진행하는 동안 다른 변수 이름 지정 의미 체계를 가질 수 있습니다.\n",
        "\n",
        "즉, TF2에서 실행할 경우 *이름이 예상과 다를 수 있습니다*.\n",
        "\n",
        "경고: Eager 실행에서 변수의 이름은 중복될 수 있으며 이는 이름 기반 체크포인트의 여러 변수를 동일한 이름에 매핑해야 하는 경우 문제를 일으킬 수 있습니다. `tf.name_scope` 및 레이어 생성자 또는 `tf.Variable` `name` 인수를 사용하여 레이어 및 변수 이름을 명시적으로 조정하여 변수 이름을 조정하고 중복이 없는지 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkUQJUUyjOJz"
      },
      "source": [
        "### 할당 매핑 유지 관리하기\n",
        "\n",
        "할당 매핑은 일반적으로 TF1 모델 사이에 가중치를 전이할 때 사용하며 변수 이름이 변경되는 경우 모델 마이그레이션을 진행할 때에도 사용할 수 있습니다.\n",
        "\n",
        "이 매핑은 [`tf.compat.v1.train.init_from_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/init_from_checkpoint), [`tf.compat.v1.train.Saver`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver)와 함께 사용할 수 있으며, [`tf.train.load_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/load_checkpoint)를 사용하여 변수 또는 범위 이름이 변경되었을 수 있는 모델에 가중치를 로드할 수 있습니다.\n",
        "\n",
        "이 섹션의 예제에서는 이전에 저장된 체크포인트를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PItyo7DdJ6Ek"
      },
      "outputs": [],
      "source": [
        "print_checkpoint('tf1-ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPryV_WBJrI3"
      },
      "source": [
        "#### `init_from checkpoint`를 사용하여 로드하기\n",
        "\n",
        "[`tf1.train.init_from_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/init_from_checkpoint)는 할당 연산을 생성하는 대신 변수 이니셜라이저에 값을 배치하기 때문에 그래프/세션에 있는 동안 호출해야 합니다.\n",
        "\n",
        "`assignment_map` 인수를 사용하여 변수가 로드되는 방식을 구성할 수 있습니다. 다음 설명서 내용을 확인하세요.\n",
        "\n",
        "> 할당 매핑이 지원하는 구문:\n",
        "\n",
        "- `'checkpoint_scope_name/': 'scope_name/'` - 텐서 이름이 일치하는 `checkpoint_scope_name`의 현재 `scope_name`에 있는 모든 변수를 로드합니다.\n",
        "- `'checkpoint_scope_name/some_other_variable': 'scope_name/variable_name'` - `checkpoint_scope_name/some_other_variable`에서 `scope_name/variable_name` 변수를 초기화합니다.\n",
        "- `'scope_variable_name': variable` - 체크포인트에서 텐서 'scope_variable_name'을 사용하여 주어진 `tf.Variable` 객체를 초기화합니다.\n",
        "- `'scope_variable_name': list(variable)` - 체크포인트에서 텐서 'scope_variable_name'을 사용하여 분할된 변수 목록을 초기화합니다.\n",
        "- `'/': 'scope_name/'` - 체크포인트의 루트에서 현재 `scope_name`의 모든 변수를 로드합니다(예: 범위 없음).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM_7OzRpdH0A"
      },
      "outputs": [],
      "source": [
        "# Restoring with tf1.train.init_from_checkpoint:\n",
        "\n",
        "# A new model with a different scope for the variables.\n",
        "with tf.Graph().as_default() as g:\n",
        "  with tf1.variable_scope('new_scope'):\n",
        "    a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    # The assignment map will remap all variables in the checkpoint to the\n",
        "    # new scope:\n",
        "    tf1.train.init_from_checkpoint(\n",
        "        'tf1-ckpt',\n",
        "        assignment_map={'/': 'new_scope/'})\n",
        "    # `init_from_checkpoint` adds the initializers to these variables.\n",
        "    # Use `sess.run` to run these initializers.\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za_8xhFWKVlH"
      },
      "source": [
        "#### `tf1.train.Saver`을 사용하여 로드하기\n",
        "\n",
        "`init_from_checkpoint`와 달리 [`tf.compat.v1.train.Saver`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver)는 그래프와 Eager 모드에서 모두 실행됩니다. `var_list` 인수는 변수 이름을 `tf.Variable` 객체에 매핑해야 한다는 점을 제외하고 선택적으로 사전을 허용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiKNmdGJgoX9"
      },
      "outputs": [],
      "source": [
        "# Restoring with tf1.train.Saver (works in both graph and eager):\n",
        "\n",
        "# A new model with a different scope for the variables.\n",
        "with tf1.variable_scope('new_scope'):\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                      initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                      initializer=tf1.zeros_initializer())\n",
        "  c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "# Initialize the saver with a dictionary with the original variable names:\n",
        "saver = tf1.train.Saver({'a': a, 'b': b, 'scoped/c': c})\n",
        "saver.restore(sess=None, save_path='tf1-ckpt')\n",
        "print(\"Restored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JsgCXt3Ly-h"
      },
      "source": [
        "#### `tf.train.load_checkpoint`을 사용하여 로드하기\n",
        "\n",
        "이 옵션은 변수 값을 정밀하게 제어해야 하는 경우에 적합합니다. 다시 말하지만 이 옵션은그래프 모드와 Eager 모드 모두에서 작동합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc39Bh6JMso6"
      },
      "outputs": [],
      "source": [
        "# Restoring with tf.train.load_checkpoint (works in both graph and eager):\n",
        "\n",
        "# A new model with a different scope for the variables.\n",
        "with tf.Graph().as_default() as g:\n",
        "  with tf1.variable_scope('new_scope'):\n",
        "    a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    # It may be easier writing a loop if your model has a lot of variables.\n",
        "    reader = tf.train.load_checkpoint('tf1-ckpt')\n",
        "    sess.run(a.assign(reader.get_tensor('a')))\n",
        "    sess.run(b.assign(reader.get_tensor('b')))\n",
        "    sess.run(c.assign(reader.get_tensor('scoped/c')))\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBSTJVCNDKed"
      },
      "source": [
        "### TF2 체크포인트 객체 유지 관리하기\n",
        "\n",
        "마이그레이션을 진행하는 동안 변수 및 범위 이름이 많이 변경될 수 있는 경우 `tf.train.Checkpoint` 및 TF2 체크포인트를 사용합니다. TF2는 변수 이름 대신 **객체 구조**를 사용합니다(자세한 내용은 *TF1에서 TF2로의 변경사항* 참조).\n",
        "\n",
        "즉, `tf.train.Checkpoint`를 생성하여 체크포인트를 저장하거나 복원할 때 동일한 **순서**(목록의 경우)와 **키**(`Checkpoint` 이니셜라이저에 대한 사전 및 키워드 인수의 경우)를 사용하는지 확인합니다. 체크포인트 호환성의 몇 가지 예제는 다음과 같습니다.\n",
        "\n",
        "```\n",
        "ckpt = tf.train.Checkpoint(foo=[var_a, var_b])\n",
        "\n",
        "# compatible with ckpt\n",
        "tf.train.Checkpoint(foo=[var_a, var_b])\n",
        "\n",
        "# not compatible with ckpt\n",
        "tf.train.Checkpoint(foo=[var_b, var_a])\n",
        "tf.train.Checkpoint(bar=[var_a, var_b])\n",
        "```\n",
        "\n",
        "아래 코드 샘플은 \"동일한\" `tf.train.Checkpoint`를 사용하여 다른 이름의 변수를 로드하는 방식을 보여줍니다. 먼저 TF2 체크포인트를 저장합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCSkz_-Tbct6"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(1))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(2))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(3))\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    print(\"[a, b, c]: \", sess.run([a, b, c]))\n",
        "\n",
        "    # Save a TF2 checkpoint\n",
        "    ckpt = tf.train.Checkpoint(unscoped=[a, b], scoped=[c])\n",
        "    tf2_ckpt_path = ckpt.save('tf2-ckpt')\n",
        "    print_checkpoint(tf2_ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62MWdZMxezeP"
      },
      "source": [
        "변수/범위 이름이 변경되더라도 `tf.train.Checkpoint`를 계속 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh61SGeqb27b"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a_different_name', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b_different_name', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  with tf1.variable_scope('different_scope'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    print(\"Initialized [a, b, c]: \", sess.run([a, b, c]))\n",
        "\n",
        "    ckpt = tf.train.Checkpoint(unscoped=[a, b], scoped=[c])\n",
        "    # `assert_consumed` validates that all checkpoint objects are restored from\n",
        "    # the checkpoint. `run_restore_ops` is required when running in a TF1\n",
        "    # session.\n",
        "    ckpt.restore(tf2_ckpt_path).assert_consumed().run_restore_ops()\n",
        "\n",
        "    # Removing `assert_consumed` is fine if you want to skip the validation.\n",
        "    # ckpt.restore(tf2_ckpt_path).run_restore_ops()\n",
        "\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unDPmL-kldr2"
      },
      "source": [
        "Eager 모드에서는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79S0zMAnfzx7"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "c = tf.Variable(0.)\n",
        "print(\"Initialized [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])\n",
        "\n",
        "# The keys \"scoped\" and \"unscoped\" are no longer relevant, but are used to\n",
        "# maintain compatibility with the saved checkpoints.\n",
        "ckpt = tf.train.Checkpoint(unscoped=[a, b], scoped=[c])\n",
        "\n",
        "ckpt.restore(tf2_ckpt_path).assert_consumed().run_restore_ops()\n",
        "print(\"Restored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKfNAr8l3aFg"
      },
      "source": [
        "## Estimator의 TF2 체크포인트\n",
        "\n",
        "위의 섹션에서는 모델을 마이그레이션하는 동안 체크포인트 호환성을 유지하는 방식을 설명합니다. 체크포인트가 저장/로드되는 방식이 약간 다르지만 이러한 개념은 Estimator 모델에도 적용됩니다. TF2 API를 사용하도록 Estimator 모델을 마이그레이션할 때 *모델이 계속 Estimator를 사용하는 동안* TF1에서 TF2 체크포인트로 전환하길 바랄 수 있습니다. 이 섹션은 그렇게 하는 방법을 보여줍니다.\n",
        "\n",
        "[`tf.estimator.Estimator`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) 및 [`MonitoredSession`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MonitoredSession)에는 `scaffold`라는 [`tf.compat.v1.train.Scaffold`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Scaffold) 객체 저장 메커니즘이 있습니다. `Scaffold`는 `tf1.train.Saver` 또는 `tf.train.Checkpoint`를 포함할 수 있으며, 이는 `Estimator` 및 `MonitoredSession`를 활성화하여 TF1 또는 TF2 스타일의 체크포인트를 저장합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8AT_oO-5TXU"
      },
      "outputs": [],
      "source": [
        "# A model_fn that saves a TF1 checkpoint\n",
        "def model_fn_tf1_ckpt(features, labels, mode):\n",
        "  # This model adds 2 to the variable `v` in every train step.\n",
        "  train_step = tf1.train.get_or_create_global_step()\n",
        "  v = tf1.get_variable('var', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode,\n",
        "      predictions=v,\n",
        "      train_op=tf.group(v.assign_add(2), train_step.assign_add(1)),\n",
        "      loss=tf.constant(1.),\n",
        "      scaffold=None\n",
        "  )\n",
        "\n",
        "!rm -rf est-tf1\n",
        "est = tf.estimator.Estimator(model_fn_tf1_ckpt, 'est-tf1')\n",
        "\n",
        "def train_fn():\n",
        "  return tf.data.Dataset.from_tensor_slices(([1,2,3], [4,5,6]))\n",
        "est.train(train_fn, steps=1)\n",
        "\n",
        "latest_checkpoint = tf.train.latest_checkpoint('est-tf1')\n",
        "print_checkpoint(latest_checkpoint)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttH6cUrl7jK2"
      },
      "outputs": [],
      "source": [
        "# A model_fn that saves a TF2 checkpoint\n",
        "def model_fn_tf2_ckpt(features, labels, mode):\n",
        "  # This model adds 2 to the variable `v` in every train step.\n",
        "  train_step = tf1.train.get_or_create_global_step()\n",
        "  v = tf1.get_variable('var', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  ckpt = tf.train.Checkpoint(var_list={'var': v}, step=train_step)\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode,\n",
        "      predictions=v,\n",
        "      train_op=tf.group(v.assign_add(2), train_step.assign_add(1)),\n",
        "      loss=tf.constant(1.),\n",
        "      scaffold=tf1.train.Scaffold(saver=ckpt)\n",
        "  )\n",
        "\n",
        "!rm -rf est-tf2\n",
        "est = tf.estimator.Estimator(model_fn_tf2_ckpt, 'est-tf2',\n",
        "                             warm_start_from='est-tf1')\n",
        "\n",
        "def train_fn():\n",
        "  return tf.data.Dataset.from_tensor_slices(([1,2,3], [4,5,6]))\n",
        "est.train(train_fn, steps=1)\n",
        "\n",
        "latest_checkpoint = tf.train.latest_checkpoint('est-tf2')\n",
        "print_checkpoint(latest_checkpoint)  \n",
        "\n",
        "assert est.get_variable_value('var_list/var/.ATTRIBUTES/VARIABLE_VALUE') == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYVYgahE8daL"
      },
      "source": [
        "`v`의 최종 값은 `est-tf1`에서 웜 스타트된 후 추가 5단계를 훈련하여 `16`가 되어야 합니다. 훈련 단계 값은 `warm_start` 체크포인트로부터 전달되지 않습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq8EjblQUIA2"
      },
      "source": [
        "## Keras 체크포인트하기\n",
        "\n",
        "Keras로 빌드한 모델은 여전히 `tf1.train.Saver` 및 `tf.train.Checkpoint`를 사용하여 기존 가중치를 로드합니다. 모델이 완전히 마이그레이션되면 특히 `ModelCheckpoint` 콜백을 사용하여 훈련하는 경우 `model.save_weights` 및 `model.load_weights`를 사용하도록 전환합니다.\n",
        "\n",
        "체크포인트와 Keras에 대해 알아야 할 몇 가지 사항:\n",
        "\n",
        "**초기화 vs 빌드**\n",
        "\n",
        "Keras 모델과 레이어는 완전히 생성되기 전에 **2단계**를 거쳐야 합니다. 첫 번째 단계는 Python 객체의 *초기화*인 `layer = tf.keras.layers.Dense(x)`입니다. 두 번째 단계는 대부분의 가중치가 실제로 생성되는 *빌드* 단계인 `layer.build(input_shape)`입니다. 모델을 호출하거나 단일 `train`, `eval` 또는 `predict` 단계(처음에만 해당)를 실행하여 모델을 빌드할 수도 있습니다.\n",
        "\n",
        "`model.load_weights(path).assert_consumed()`에서 오류가 발생하는 것이 확인되면 모델/레이어가 빌드되지 않았을 가능성이 높습니다.\n",
        "\n",
        "**TF2 체크포인트를 사용하는 Keras**\n",
        "\n",
        "`tf.train.Checkpoint(model).write`는 `model.save_weights`와 동일합니다. `tf.train.Checkpoint(model).read`와 `model.load_weights`도 동일합니다. `Checkpoint(model) != Checkpoint(model=model)`에 유의해야 합니다.\n",
        "\n",
        "**Keras의 `build()` 단계와 함께 동작하는 TF2 체크포인트**\n",
        "\n",
        "`tf.train.Checkpoint.restore`에는 *지연된 복원*이라는 메커니즘이 있으며 변수가 아직 생성되지 않은 경우 `tf.Module`과 Keras 객체를 사용하여 변수 값을 저장합니다. 이렇게 하면 *초기화된* 모델로 가중치를 로드하고 나중에 *빌드*할 수 있습니다.\n",
        "\n",
        "```\n",
        "m = YourKerasModel()\n",
        "status = m.load_weights(path)\n",
        "\n",
        "# This call builds the model. The variables are created with the restored\n",
        "# values.\n",
        "m.predict(inputs)\n",
        "\n",
        "status.assert_consumed()\n",
        "```\n",
        "\n",
        "이 메커니즘 때문에 Keras 모델과 함께 TF2 체크포인트 로드 API를 사용하는 것이 좋습니다(기존 TF1 체크포인트를 [모델 매핑 shim](./model_mapping.ipynb)으로 복원하는 경우도 동일). 자세한 내용은 [체크포인트 가이드](https://www.tensorflow.org/guide/checkpoint#delayed_restorations)를 참조합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO2NucRtqMm6"
      },
      "source": [
        "## 코드 조각\n",
        "\n",
        "아래의 코드 조각은 체크포인트 저장 API의 TF1/TF2 버전 호환성을 보여줍니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3SSc74olkX3"
      },
      "source": [
        "### TF2에서 TF1 체크포인트 저장하기\n",
        "\n",
        "<a name=\"save-tf1-in-tf2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2ZPk8BPloE1"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(1.0, name='a')\n",
        "b = tf.Variable(2.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(3.0, name='c')\n",
        "\n",
        "saver = tf1.train.Saver(var_list=[a, b, c])\n",
        "path = saver.save(sess=None, save_path='tf1-ckpt-saved-in-eager')\n",
        "print_checkpoint(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxyN5khVjhmA"
      },
      "source": [
        "### TF2에서 TF1 체크포인트 로드하기\n",
        "\n",
        "<a name=\"load-tf1-in-tf2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5kSXy3FmA79"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0., name='a')\n",
        "b = tf.Variable(0., name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(0., name='c')\n",
        "print(\"Initialized [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])\n",
        "saver = tf1.train.Saver(var_list=[a, b, c])\n",
        "saver.restore(sess=None, save_path='tf1-ckpt-saved-in-eager')\n",
        "print(\"Restored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul3V4pEwloeN"
      },
      "source": [
        "### TF1에서 TF2 체크포인트 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhuP_2EIlRm4"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(1))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(2))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(3))\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    ckpt = tf.train.Checkpoint(\n",
        "        var_list={v.name.split(':')[0]: v for v in tf1.global_variables()})\n",
        "    tf2_in_tf1_path = ckpt.save('tf2-ckpt-saved-in-session')\n",
        "    print_checkpoint(tf2_in_tf1_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiViCjCDgxhz"
      },
      "source": [
        "### TF1에서 TF2 체크포인트 로드하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-4hIPZvmXlb"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(0))\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    print(\"Initialized [a, b, c]: \", sess.run([a, b, c]))\n",
        "    ckpt = tf.train.Checkpoint(\n",
        "        var_list={v.name.split(':')[0]: v for v in tf1.global_variables()})\n",
        "    ckpt.restore('tf2-ckpt-saved-in-session-1').run_restore_ops()\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRrSE2X6sgAM"
      },
      "source": [
        "## 체크포인트 변환하기\n",
        "\n",
        "<a name=\"checkpoint-conversion\"></a>\n",
        "\n",
        "체크포인트를 로드하고 다시 저장하는 방식으로 TF1과 TF2 사이의 체크포인트를 변환할 수 있습니다. 대안은 아래 코드에 표시된 `tf.train.load_checkpoint`입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9KByaLous4q"
      },
      "source": [
        "### TF1 체크포인트를 TF2로 변환하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG8grCv2smAb"
      },
      "outputs": [],
      "source": [
        "def convert_tf1_to_tf2(checkpoint_path, output_prefix):\n",
        "  \"\"\"Converts a TF1 checkpoint to TF2.\n",
        "\n",
        "  To load the converted checkpoint, you must build a dictionary that maps\n",
        "  variable names to variable objects.\n",
        "  ```\n",
        "  ckpt = tf.train.Checkpoint(vars={name: variable})  \n",
        "  ckpt.restore(converted_ckpt_path)\n",
        "  ```\n",
        "\n",
        "  Args:\n",
        "    checkpoint_path: Path to the TF1 checkpoint.\n",
        "    output_prefix: Path prefix to the converted checkpoint.\n",
        "\n",
        "  Returns:\n",
        "    Path to the converted checkpoint.\n",
        "  \"\"\"\n",
        "  vars = {}\n",
        "  reader = tf.train.load_checkpoint(checkpoint_path)\n",
        "  dtypes = reader.get_variable_to_dtype_map()\n",
        "  for key in dtypes.keys():\n",
        "    vars[key] = tf.Variable(reader.get_tensor(key))\n",
        "  return tf.train.Checkpoint(vars=vars).save(output_prefix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyvqK6Sb3dad"
      },
      "source": [
        "코드 조각 `Save a TF1 checkpoint in TF2`에 저장된 체크포인트를 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcHLN4lPvYvw"
      },
      "outputs": [],
      "source": [
        "# Make sure to run the snippet in `Save a TF1 checkpoint in TF2`.\n",
        "print_checkpoint('tf1-ckpt-saved-in-eager')\n",
        "converted_path = convert_tf1_to_tf2('tf1-ckpt-saved-in-eager', \n",
        "                                     'converted-tf1-to-tf2')\n",
        "print(\"\\n[Converted]\")\n",
        "print_checkpoint(converted_path)\n",
        "\n",
        "# Try loading the converted checkpoint.\n",
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "c = tf.Variable(0.)\n",
        "ckpt = tf.train.Checkpoint(vars={'a': a, 'b': b, 'scoped/c': c})\n",
        "ckpt.restore(converted_path).assert_consumed()\n",
        "print(\"\\nRestored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fokg6ybZvE20"
      },
      "source": [
        "### TF2 체크포인트를 TF1로 변환하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPQsXQveuQiC"
      },
      "outputs": [],
      "source": [
        "def convert_tf2_to_tf1(checkpoint_path, output_prefix):\n",
        "  \"\"\"Converts a TF2 checkpoint to TF1.\n",
        "\n",
        "  The checkpoint must be saved using a \n",
        "  `tf.train.Checkpoint(var_list={name: variable})`\n",
        "\n",
        "  To load the converted checkpoint with `tf.compat.v1.Saver`:\n",
        "  ```\n",
        "  saver = tf.compat.v1.train.Saver(var_list={name: variable}) \n",
        "\n",
        "  # An alternative, if the variable names match the keys:\n",
        "  saver = tf.compat.v1.train.Saver(var_list=[variables]) \n",
        "  saver.restore(sess, output_path)\n",
        "  ```\n",
        "  \"\"\"\n",
        "  vars = {}\n",
        "  reader = tf.train.load_checkpoint(checkpoint_path)\n",
        "  dtypes = reader.get_variable_to_dtype_map()\n",
        "  for key in dtypes.keys():\n",
        "    # Get the \"name\" from the \n",
        "    if key.startswith('var_list/'):\n",
        "      var_name = key.split('/')[1]\n",
        "      # TF2 checkpoint keys use '/', so if they appear in the user-defined name,\n",
        "      # they are escaped to '.S'.\n",
        "      var_name = var_name.replace('.S', '/')\n",
        "      vars[var_name] = tf.Variable(reader.get_tensor(key))\n",
        "  \n",
        "  return tf1.train.Saver(var_list=vars).save(sess=None, save_path=output_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjZD_OSf1mKX"
      },
      "source": [
        "코드 조각 `Save a TF2 checkpoint in TF1`에 저장된 체크포인트를 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc1MVeV6z2DB"
      },
      "outputs": [],
      "source": [
        "# Make sure to run the snippet in `Save a TF2 checkpoint in TF1`.\n",
        "print_checkpoint('tf2-ckpt-saved-in-session-1')\n",
        "converted_path = convert_tf2_to_tf1('tf2-ckpt-saved-in-session-1',\n",
        "                                    'converted-tf2-to-tf1')\n",
        "print(\"\\n[Converted]\")\n",
        "print_checkpoint(converted_path)\n",
        "\n",
        "# Try loading the converted checkpoint.\n",
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(0))\n",
        "  with tf1.Session() as sess:\n",
        "    saver = tf1.train.Saver([a, b, c])\n",
        "    saver.restore(sess, converted_path)\n",
        "    print(\"\\nRestored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBMfArLQ0jb-"
      },
      "source": [
        "## 관련 가이드\n",
        "\n",
        "- [수치의 일치와 정확성 검증하기](./validate_correctness.ipynb)\n",
        "- [모델 매핑 가이드](./model_mapping.ipynb) 및 `tf.compat.v1.keras.utils.track_tf1_style_variables`\n",
        "- [TF2 체크포인트 가이드](https://www.tensorflow.org/guide/checkpoint)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "migrating_checkpoints.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
