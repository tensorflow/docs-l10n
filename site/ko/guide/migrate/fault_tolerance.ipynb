{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJcYs_ERTnnI"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HMUDt0CiUJk9"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77z2OchJTk0l"
      },
      "source": [
        "# 내결함성 메커니즘 마이그레이션하기\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/migrate/fault_tolerance\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서보기</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/guide/migrate/fault_tolerance.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/guide/migrate/fault_tolerance.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/guide/migrate/fault_tolerance.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4O6fPyYTxZv"
      },
      "source": [
        "내결함성은 매개변수 및 모델과 같은 추적 가능한 객체의 상태를 주기적으로 저장하는 메커니즘을 말합니다. 훈련하는 동안 프로그램/머신 오류가 발생한 경우 이를 사용하여 복구할 수 있습니다.\n",
        "\n",
        "이 가이드에서는 먼저 `tf.estimator.RunConfig`를 사용하여 메트릭 저장 설정을 지정하고 TensorFlow 1에서 `tf.estimator.Estimator`를 사용하여 훈련에 내결함성을 추가하는 방법을 보여줍니다. 그런 다음 Tensorflow 2에서 훈련에 내결함성을 구현하는 방법 2가지를 배우게 됩니다.\n",
        "\n",
        "- Keras `Model.fit` API를 사용하는 경우 해당 API로 `tf.keras.callbacks.BackupAndRestore` 콜백을 전달할 수 있습니다.\n",
        "- 사용자 정의 훈련 루프(`tf.GradientTape` 사용)를 사용하는 경우 `tf.train.Checkpoint` 및 `tf.train.CheckpointManager` API를 사용하여 체크포인트를 임의로 저장할 수 있습니다.\n",
        "\n",
        "이 두 가지 메서드 모두 [체크포인트](../../guide/checkpoint.ipynb) 파일의 훈련 상태를 백업하고 복원합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHJfmkCFUhQf"
      },
      "source": [
        "## 설치하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVQubuDzdmA"
      },
      "source": [
        "`tf.keras.callbacks.BackupAndRestore`의 `save_freq` 인수를 사용하여 특정 단계에서 체크포인트의 빈도를 저장하는 기능이 TensorFlow 2.10부터 도입되었으므로 `tf-nightly`를 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGW0XhXkxY_q"
      },
      "outputs": [],
      "source": [
        "!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXnPvQi8Ui1F"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tww-uIoiUlsT"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtlucRG_Uro_"
      },
      "source": [
        "## TensorFlow 1: `tf.estimator.RunConfig`를 사용하여 체크포인트 저장하기\n",
        "\n",
        "TensorFlow 1에서는 `tf.estimator.RunConfig`를 구성하여 모든 단계마다 체크포인트를 저장하도록 `tf.estimator`를 구성할 수 있습니다.\n",
        "\n",
        "이 예제에서는 다섯 번째 체크포인트를 진행하는 동안 인위적으로 오류를 발생시키는 후크를 먼저 작성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8shCkV2jKcc"
      },
      "outputs": [],
      "source": [
        "class InterruptHook(tf1.train.SessionRunHook):\n",
        "  # A hook for artificially interrupting training.\n",
        "  def begin(self):\n",
        "    self._step = -1\n",
        "\n",
        "  def before_run(self, run_context):\n",
        "    self._step += 1\n",
        "\n",
        "  def after_run(self, run_context, run_values):\n",
        "    if self._step == 5:\n",
        "      raise RuntimeError('Interruption')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXbQ6cFlkoIM"
      },
      "source": [
        "다음으로 모든 체크포인트를 저장하고 MNIST 데이터세트를 사용하도록 `tf.estimator.Estimator`를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EKXzi4Qj2Eb"
      },
      "outputs": [],
      "source": [
        "feature_columns = [tf1.feature_column.numeric_column(\"x\", shape=[28, 28])]\n",
        "config = tf1.estimator.RunConfig(save_summary_steps=1,\n",
        "                                 save_checkpoints_steps=1)\n",
        "\n",
        "path = tempfile.mkdtemp()\n",
        "\n",
        "classifier = tf1.estimator.DNNClassifier(\n",
        "    feature_columns=feature_columns,\n",
        "    hidden_units=[256, 32],\n",
        "    optimizer=tf1.train.AdamOptimizer(0.001),\n",
        "    n_classes=10,\n",
        "    dropout=0.2,\n",
        "    model_dir=path,\n",
        "    config = config\n",
        ")\n",
        "\n",
        "train_input_fn = tf1.estimator.inputs.numpy_input_fn(\n",
        "    x={\"x\": x_train},\n",
        "    y=y_train.astype(np.int32),\n",
        "    num_epochs=10,\n",
        "    batch_size=50,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGP7Nyenk1gr"
      },
      "source": [
        "모델 훈련을 시작합니다. 앞에서 정의한 후크로 의해 인위적인 예외가 발생합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWKMsmt6jYSL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  classifier.train(input_fn=train_input_fn,\n",
        "                   hooks=[InterruptHook()],\n",
        "                   max_steps=10)\n",
        "except Exception as e:\n",
        "  print(f'{type(e).__name__}:{e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DekxJkgWk-4N"
      },
      "source": [
        "마지막으로 저장한 체크포인트를 사용하여 `tf.estimator.Estimator`를 다시 빌드하고 훈련을 계속 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqMVTiJMjcH7"
      },
      "outputs": [],
      "source": [
        "classifier = tf1.estimator.DNNClassifier(\n",
        "    feature_columns=feature_columns,\n",
        "    hidden_units=[256, 32],\n",
        "    optimizer=tf1.train.AdamOptimizer(0.001),\n",
        "    n_classes=10,\n",
        "    dropout=0.2,\n",
        "    model_dir=path,\n",
        "    config = config\n",
        ")\n",
        "classifier.train(input_fn=train_input_fn,\n",
        "                   max_steps = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5LtVtmvYx7J"
      },
      "source": [
        "## TensorFlow 2: 콜백 및 `Model.fit`으로 백업 및 복원하기\n",
        "\n",
        "TensorFlow 2에서는 훈련에 Keras `Model.fit` API를 사용하는 경우 `tf.keras.callbacks.BackupAndRestore` 콜백을 제공하여 내결함성 기능을 추가할 수 있습니다.\n",
        "\n",
        "이를 보여주기 위해 우선적으로 네 번째 epoch 체크포인트를 진행하는 동안 인위적으로 오류를 발생시키는 Keras `Callback` 클래스를 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci3yB6A5lwJu"
      },
      "outputs": [],
      "source": [
        "class InterruptAtEpoch(tf.keras.callbacks.Callback):\n",
        "  # A callback for artificially interrupting training.\n",
        "  def __init__(self, interrupting_epoch=3):\n",
        "    self.interrupting_epoch = interrupting_epoch\n",
        "\n",
        "  def on_epoch_end(self, epoch, log=None):\n",
        "    if epoch == self.interrupting_epoch:\n",
        "      raise RuntimeError('Interruption')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhU3VTYZoDh-"
      },
      "source": [
        "그런 다음 간단한 Keras 모델을 정의 및 인스턴스화하고, 손실 함수를 정의하고, `Model.compile`을 호출하고, epoch 경계에서 임시 디렉터리에 체크포인트를 저장하는 `tf.keras.callbacks.BackupAndRestore` 콜백을 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VOQLDNkl2bl"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model = create_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss,\n",
        "              metrics=['accuracy'])\n",
        "log_dir = tempfile.mkdtemp()\n",
        "backup_restore_callback = tf.keras.callbacks.BackupAndRestore(\n",
        "    backup_dir = log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRRWmZqsvMrq"
      },
      "source": [
        "`Model.fit`을 사용하여 모델 훈련을 시작합니다. 훈련을 진행하는 동안 위에서 인스턴스화한 `tf.keras.callbacks.BackupAndRestore` 덕분에 체크포인트가 저장되지만 `InterruptAtEpoch` 클래스는 인위적으로 예외를 발생시켜 네 번째 epoch 이후에 실패를 시뮬레이션합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bVO79qWl4Uv"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  model.fit(x=x_train,\n",
        "            y=y_train,\n",
        "            epochs=10,\n",
        "            steps_per_epoch=100,\n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=[backup_restore_callback, InterruptAtEpoch()])\n",
        "except Exception as e:\n",
        "  print(f'{type(e).__name__}:{e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWidh234vcRf"
      },
      "source": [
        "그런 다음 Keras 모델을 인스턴스화하고 `Model.compile`을 호출한 다음 이전에 저장한 체크포인트의 `Model.fit`을 사용하여 모델을 계속 훈련합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IWPH0Cmn2wi"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss,\n",
        "              metrics=['accuracy'],\n",
        "              steps_per_execution=10)\n",
        "model.fit(x=x_train,\n",
        "            y=y_train,\n",
        "            epochs=10,\n",
        "            steps_per_epoch=100,\n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=[backup_restore_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP2dnpMPxtYj"
      },
      "source": [
        "140번째 단계에서 인위적으로 오류를 발생시키는 다른 `Callback` 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YardkAaBxr-c"
      },
      "outputs": [],
      "source": [
        "class InterruptAtStep(tf.keras.callbacks.Callback):\n",
        "  # A callback for artificially interrupting training.\n",
        "  def __init__(self, interrupting_step=140):\n",
        "    self.total_step_count = 0\n",
        "    self.interrupting_step = interrupting_step\n",
        "\n",
        "  def on_batch_begin(self, batch, logs=None):\n",
        "    self.total_step_count += 1\n",
        "\n",
        "  def on_batch_end(self, batch, logs=None):\n",
        "    if self.total_step_count == self.interrupting_step:\n",
        "      print(\"\\nInterrupting at step count\", self.total_step_count)\n",
        "      raise RuntimeError('Interruption')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af3VpehxyTpb"
      },
      "source": [
        "참고: 이 섹션에서는 Tensorflow 2.10이 릴리스될 때까지 `tf-nightly`에서만 사용할 수 있는 특성을 사용합니다.\n",
        "\n",
        "체크포인트가 30단계마다 저장되도록 하려면 `BackupAndRestore` 콜백의 `save_freq`를 `30`으로 설정합니다. `InterruptAtStep`이 epoch 1 및 40단계(총 단계 수 140)에서 실패를 시뮬레이션하기 위해 인위적으로 예외를 발생시킵니다. 체크포인트는 epoch 1과 20단계에서 마지막으로 저장될 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHHCENDPyUHS"
      },
      "outputs": [],
      "source": [
        "log_dir_2 = tempfile.mkdtemp()\n",
        "\n",
        "backup_restore_callback = tf.keras.callbacks.BackupAndRestore(\n",
        "    backup_dir = log_dir_2, save_freq=30\n",
        ")\n",
        "model = create_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss,\n",
        "              metrics=['accuracy'])\n",
        "try:\n",
        "  model.fit(x=x_train,\n",
        "            y=y_train,\n",
        "            epochs=10,\n",
        "            steps_per_epoch=100,\n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=[backup_restore_callback, InterruptAtStep()])\n",
        "except Exception as e:\n",
        "  print(f'{type(e).__name__}:{e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-ggMFEHynMR"
      },
      "source": [
        "그런 다음 Keras 모델을 인스턴스화하고 `Model.compile`을 호출한 다음 이전에 저장한 체크포인트의 `Model.fit`을 사용하여 모델을 계속 훈련합니다. 훈련은 epoch 2와 21단계부터 시작합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT7Kx30NEqly"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss,\n",
        "              metrics=['accuracy'],\n",
        "              steps_per_execution=10)\n",
        "model.fit(x=x_train,\n",
        "            y=y_train,\n",
        "            epochs=10,\n",
        "            steps_per_epoch=100,\n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=[backup_restore_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdWexHUUaEB6"
      },
      "source": [
        "## TensorFlow 2: 사용자 정의 훈련 루프를 사용하여 수동 체크포인트 작성하기\n",
        "\n",
        "TensorFlow 2에서 사용자 정의 훈련 루프를 사용하는 경우 `tf.train.Checkpoint` 및 `tf.train.CheckpointManager` API로 내결함성 메커니즘을 구현할 수 있습니다.\n",
        "\n",
        "이 예제는 다음을 수행하는 방법을 보여줍니다.\n",
        "\n",
        "- 저장하려는 추적 가능한 객체를 속성으로 설정한 체크포인트를 수동으로 생성하려면 `tf.train.Checkpoint` 객체를 사용합니다.\n",
        "- 여러 체크포인트를 관리하려면 `tf.train.CheckpointManager`를 사용합니다.\n",
        "\n",
        "먼저 Keras 모델, 옵티마이저, 손실 함수를 정의하고 인스턴스화합니다. 그런 다음 추적 가능한 상태가 있는 두 객체(모델 및 옵티마이저)를 관리하는 `Checkpoint`와 임시 디렉터리에서 여러 체크포인트를 기록하고 유지하는 `CheckpointManager`를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPnIRKC8aDwE"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "log_dir = tempfile.mkdtemp()\n",
        "epochs = 5\n",
        "steps_per_epoch = 5\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
        "checkpoint_manager = tf.train.CheckpointManager(\n",
        "            checkpoint, log_dir, max_to_keep=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2tK4fm6xNkJ"
      },
      "source": [
        "이제 새 epoch가 시작될 때마다 첫 번째 epoch 이후 마지막 체크포인트를 로드하는 사용자 정의 훈련 루프를 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhQphF5jxPWU"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "  if epoch > 0:\n",
        "      tf.train.load_checkpoint(save_path)\n",
        "  print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "  for step in range(steps_per_epoch):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      logits = model(x_train, training=True)\n",
        "      loss_value = loss_fn(y_train, logits)\n",
        "\n",
        "      grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    save_path = checkpoint_manager.save()\n",
        "    print(f\"Checkpoint saved to {save_path}\")\n",
        "    print(f\"Training loss at step {step}: {loss_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQUS8nO9FZlH"
      },
      "source": [
        "## 다음 단계\n",
        "\n",
        "TensorFlow 2의 내결함성 및 체크포인트에 대해 자세히 알아보려면 다음 문서를 고려합니다.\n",
        "\n",
        "- `tf.keras.callbacks.BackupAndRestore` 콜백 API 설명서.\n",
        "- `tf.train.Checkpoint` 및 `tf.train.CheckpointManager` API 설명서.\n",
        "- *체크포인트 작성* 섹션 등 [체크포인트 훈련하기](../../guide/checkpoint.ipynb) 가이드.\n",
        "\n",
        "[분산 훈련](../..guide/distributed_training.ipynb)과 관련된 다음 자료도 유용할 수 있습니다.\n",
        "\n",
        "- [Keras를 사용하는 다중 작업자 훈련](../../tutorials/distribute/multi_worker_with_keras.ipynb) 가이드의 *내결함성* 섹션.\n",
        "- [매개변수 서버 훈련](../../tutorials/distribute/parameter_server_training.ipynb) 가이드의 *작업 실패 처리하기* 섹션."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "fault_tolerance.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
