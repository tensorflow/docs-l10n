{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhGuhbZ6M5tl"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AwOEIRJC6Une"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Core API를 사용하는 바이너리 분류를 위한 로지스틱 회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBIlTPscrIT9"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/core/logistic_regression_core\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> TensorFlow.org에서 보기</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauaqJ7WhIhO"
      },
      "source": [
        "이 가이드에서는 [TensorFlow Core 하위 수준 API](https://www.tensorflow.org/guide/core)를 사용하여 [로지스틱 회귀](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external}로 [바이너리 분류](https://developers.google.com/machine-learning/glossary#binary_classification){:.external}를 수행하는 방법을 보여줍니다. 이 가이드는 종양 분류를 위해 [위스콘신 유방암 데이터세트](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external}를 사용합니다.\n",
        "\n",
        "[로지스틱 회귀](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external}는 바이너리 분류에서 가장 널리 사용하는 알고리즘 중 하나입니다. 특성이 있는 예제 세트를 제공하 경우 로지스틱 회귀는 0과 1 사이의 값을 출력하고자 하며, 이는 특정 클래스에 속하는 각 예제의 확률로 해석할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nchsZfwEVtVs"
      },
      "source": [
        "## 설치하기\n",
        "\n",
        "이 튜토리얼에서는 CSV 파일을 [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html){:.external}으로 읽어오는 경우에는 [pandas](https://pandas.pydata.org){:.external}를, 데이터세트에서 쌍별 관계를 플로팅하는 경우에는 [seaborn](https://seaborn.pydata.org){:.external}을, 혼동 행렬을 계산하는 경우에는 [Scikit-learn](https://scikit-learn.org/){:.external}을, 시각화를 생성하는 경우에는 [matplotlib](https://matplotlib.org/){:.external}을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lZoUK6AVTos"
      },
      "outputs": [],
      "source": [
        "!pip install -q seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.metrics as sk_metrics\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Preset matplotlib figure sizes.\n",
        "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
        "\n",
        "print(tf.__version__)\n",
        "# To make the results reproducible, set the random seed value.\n",
        "tf.random.set_seed(22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9ne3FZ-On"
      },
      "source": [
        "## 데이터 로드하기\n",
        "\n",
        "그런 다음에는 [UCI 머신러닝 리포지토리](https://archive.ics.uci.edu/ml/){:.external}의 [위스콘신 유방암 데이터세트](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external}를 로드합니다. 이 데이터세트에는 종양의 반경, 질감 및 오목한 정도와 같은 다양한 특성이 포함되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "outputs": [],
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
        "\n",
        "features = ['radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness',\n",
        "            'concavity', 'concave_poinits', 'symmetry', 'fractal_dimension']\n",
        "column_names = ['id', 'diagnosis']\n",
        "\n",
        "for attr in ['mean', 'ste', 'largest']:\n",
        "  for feature in features:\n",
        "    column_names.append(feature + \"_\" + attr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3VR1aTP92nV"
      },
      "source": [
        "[`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html){:.external}를 사용하여 데이터세트를 pandas [DataFrame](){:.external}로 읽어옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvR2Bzb691lJ"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(url, names=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB9eq6Zq-IZ4"
      },
      "outputs": [],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Z1V6Dg-La_"
      },
      "source": [
        "처음 5개 행을 표시합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWxktwbv-KPp"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-Wn2jzVC1W"
      },
      "source": [
        "[`pandas.DataFrame.sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html){:.external}, [`pandas.DataFrame.drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html){:.external}, [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html){:.external}을 사용하여 데이터세트를 훈련 세트와 테스트 세트로 분할합니다. 대상 레이블에서 특성을 분할했는지 확인해야 합니다. 테스트 세트는 보이지 않는 데이터에서 모델의 일반화 가능성을 평가하는 데 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2O60B-IVG9Q"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset.sample(frac=0.75, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i06vHFv_QB24"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19JaochhaQ3m"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset.drop(train_dataset.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmHRcbAfaSag"
      },
      "outputs": [],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6JxBhBc_wwO"
      },
      "outputs": [],
      "source": [
        "# The `id` column can be dropped since each row is unique\n",
        "x_train, y_train = train_dataset.iloc[:, 2:], train_dataset.iloc[:, 1]\n",
        "x_test, y_test = test_dataset.iloc[:, 2:], test_dataset.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWuJTKEDM-f"
      },
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "이 데이터세트는 예제별로 수집한 10개의 종양 측정값 각각에 대한 평균, 표준 오차 및 최댓값을 포함하고 있습니다. `\"diagnosis\"` 대상 열은 악성 종양을 나타내는 `'M'`과 양성 종양 진단을 나타내는 `'B'`가 있는 범주형 변수입니다. 이 열은 모델 훈련을 위해 숫자 바이너리 형식으로 변환해야 합니다.\n",
        "\n",
        "[`pandas.Series.map`](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html){:.external} 함수는 바이너리 값을 범주에 매핑하는 경우에 유용합니다.\n",
        "\n",
        "전처리를 완료한 후 데이터세트도 `tf.convert_to_tensor` 함수를 사용하여 텐서로 변환해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEJHhN65a2VV"
      },
      "outputs": [],
      "source": [
        "y_train, y_test = y_train.map({'B': 0, 'M': 1}), y_test.map({'B': 0, 'M': 1})\n",
        "x_train, y_train = tf.convert_to_tensor(x_train, dtype=tf.float32), tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "x_test, y_test = tf.convert_to_tensor(x_test, dtype=tf.float32), tf.convert_to_tensor(y_test, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ubs136WLNp"
      },
      "source": [
        "[`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html){:.external}을 사용하여 훈련 세트에서 몇 쌍의 평균 기반 특성의 결합 분포를 검토하고 대상과 어떤 관련이 있는지 관찰합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRKO_x8gWKv-"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(train_dataset.iloc[:, 1:6], hue = 'diagnosis', diag_kind='kde');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YOG5iKYKW_3"
      },
      "source": [
        "이 페어플롯은 반경, 둘레 및 면적과 같은 특정 특성이 높은 상관 관계가 있음을 보여줍니다. 이것은 종양 반경이 둘레와 면적의 계산 모두와 직접 관련되기 때문인 것으로 예상됩니다. 또한 악성 진단은 많은 특성에 대해 오른쪽으로 더 많이 치우친 것처럼 보입니다.\n",
        "\n",
        "전체 통계도 확인해야 합니다. 각 특성이 매우 다양한 값의 범위를 어떻게 다루는지 주목하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi2FzC3T21jR"
      },
      "outputs": [],
      "source": [
        "train_dataset.describe().transpose()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8pDCIFjMla8"
      },
      "source": [
        "일관성 없는 범위가 주어지는 경우 각 특성이 제로 평균과 단위 분산을 갖도록 데이터를 표준화하는 것이 좋습니다. 이러한 프로세스를 [정규화](https://developers.google.com/machine-learning/glossary#normalization){:.external}라고 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrzKNFNjLQDl"
      },
      "outputs": [],
      "source": [
        "class Normalize(tf.Module):\n",
        "  def __init__(self, x):\n",
        "    # Initialize the mean and standard deviation for normalization\n",
        "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
        "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0))\n",
        "\n",
        "  def norm(self, x):\n",
        "    # Normalize the input\n",
        "    return (x - self.mean)/self.std\n",
        "\n",
        "  def unnorm(self, x):\n",
        "    # Unnormalize the input\n",
        "    return (x * self.std) + self.mean\n",
        "\n",
        "norm_x = Normalize(x_train)\n",
        "x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3CrycBXA2s"
      },
      "source": [
        "## 로지스틱 회귀\n",
        "\n",
        "로지스틱 회귀 모델을 구축하기 전에 기존 선형 회귀와 비교하며 차이점을 이해하는 것이 중요합니다.\n",
        "\n",
        "### 로지스틱 회귀 기본 사항\n",
        "\n",
        "선형 회귀는 입력 항목의 선형 조합을 반환합니다. 이 경우 출력의 제한이 없습니다. [로지스틱 회귀](https://developers.google.com/machine-learning/glossary#logistic_regression){:.external}의 출력은 `(0, 1)` 범위 내에서 이루어집니다.각 예제는 해당 예제가 *positive* 클래스에 속할 확률을 나타냅니다.\n",
        "\n",
        "로지스틱 회귀는 기존 선형 회귀 `(-∞, ∞)`의 연속 출력을 확률 `(0, 1)`에 매핑합니다. 이 변환도 대칭적이기에 선형 출력의 부호를 뒤집으면 원래 확률의 반대가 됩니다.\n",
        "\n",
        "$Y$는 클래스 `1`에 속할 확률을 나타냅니다(종양은 악성임). 선형 회귀 출력을 클래스 `0`이 아닌 클래스 `1`에 있는 [로그 오즈(log odds)](https://developers.google.com/machine-learning/glossary#log-odds){:.external} 비율로 해석하여 원하는 매핑 결과를 얻을 수 있습니다.\n",
        "\n",
        "$$\\ln(\\frac{Y}{1-Y}) = wX + b$$\n",
        "\n",
        "$wX + b = z$를 설정하면 $Y$에 대한 이 방정식을 풀 수 있습니다.\n",
        "\n",
        "$$Y = \\frac{e^{z}}{1 + e^{z}} = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "$\\frac{1}{1 + e^{-z}}$ 표현식은 [시그모이드 함수](https://developers.google.com/machine-learning/glossary#sigmoid_function){:.external} $\\sigma(z)$로 알려져 있습니다. 따라서 로지스틱 회귀 방정식은 $Y = \\sigma(wX + b)$로 작성할 수 있습니다.\n",
        "\n",
        "이 튜토리얼의 데이터세트는 고차원 특성 행렬을 처리합니다. 따라서 위의 수식은 다음과 같이 행렬 벡터 형식으로 다시 작성해야 합니다.\n",
        "\n",
        "$${\\mathrm{Y}} = \\sigma({\\mathrm{X}}w + b)$$\n",
        "\n",
        "여기서,\n",
        "\n",
        "- $\\underset{m\\times 1}{\\mathrm{Y}}$: 대상 벡터\n",
        "- $\\underset{m\\times n}{\\mathrm{X}}$: 특성 행렬\n",
        "- $\\underset{n\\times 1}w$: 가중치 벡터\n",
        "- $b$: 바이어스\n",
        "- $\\sigma$: 출력 벡터의 각 요소에 적용되는 시그모이드 함수\n",
        "\n",
        "먼저 선형 출력 `(-∞, ∞)`이 `0`과 `1` 사이에 있도록 변환하는 시그모이드 함수를 시각화합니다. 시그모이드 함수는 `tf.math.sigmoid`에서 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThHaV_RmucZl"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-10, 10, 500)\n",
        "x = tf.cast(x, tf.float32)\n",
        "f = lambda x : (1/20)*x + 0.6\n",
        "plt.plot(x, tf.math.sigmoid(x))\n",
        "plt.ylim((-0.1,1.1))\n",
        "plt.title(\"Sigmoid function\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMXEhrZuKECV"
      },
      "source": [
        "### 로그 손실 함수\n",
        "\n",
        "[로그 손실](https://developers.google.com/machine-learning/glossary#Log_Loss){:.external} 또는 바이너리 교차 엔트로피 손실은 로지스틱 회귀가 있는 바이너리 분류 문제를 다루는 이상적인 손실 함수입니다. 각 예제에서 로그 손실은 예측 확률과 예제의 실제 값 사이의 유사성을 수량화합니다. 이는 다음 수식에 의해 결정됩니다.\n",
        "\n",
        "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}y_i\\cdot\\log(\\hat{y}_i) + (1- y_i)\\cdot\\log(1 - \\hat{y}_i)$$\n",
        "\n",
        "여기서,\n",
        "\n",
        "- $\\hat{y}$: 예측 확률의 벡터\n",
        "- $y$: 실제 대상의 벡터\n",
        "\n",
        "`tf.nn.sigmoid_cross_entropy_with_logits` 함수를 사용하여 로그 손실을 계산할 수 있습니다. 이 함수는 시그모이드 활성화를 회귀 출력에 자동으로 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVBInnSqS36W"
      },
      "outputs": [],
      "source": [
        "def log_loss(y_pred, y):\n",
        "  # Compute the log loss function\n",
        "  ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
        "  return tf.reduce_mean(ce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_mutLj0KNUb"
      },
      "source": [
        "### 경사 하강 업데이트 규칙\n",
        "\n",
        "TensorFlow Core API는 `tf.GradientTape`를 사용하는 자동 미분을 지원합니다. 로지스틱 회귀 [그래디언트 업데이트](https://developers.google.com/machine-learning/glossary#gradient_descent){:.external} 이면의 수학에 대해 궁금한 경우 다음과 같은 간단한 설명을 참고해 주세요.\n",
        "\n",
        "위의 로그 손실 수식에서 각 $\\hat{y}_i$는 $\\sigma({\\mathrm{X_i}}w + b)$와 같이 입력에서 다시 작성할 수 있음을 기억해야 합니다.\n",
        "\n",
        "목표는 로그 손실을 최소화하는 $w^*$와 $b^*$를 찾는 것입니다.\n",
        "\n",
        "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}y_i\\cdot\\log(\\sigma({\\mathrm{X_i}}w + b)) + (1- y_i)\\cdot\\log(1 - \\sigma({\\mathrm{X_i}}w + b))$$\n",
        "\n",
        "$w$에 대해 그래디언트 $L$를 선택하면 다음을 얻습니다.\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m}(\\sigma({\\mathrm{X}}w + b) - y)X$$\n",
        "\n",
        "$b$에 대해 그래디언트 $L$를 선택하면 다음을 얻습니다.\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}\\sigma({\\mathrm{X_i}}w + b) - y_i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTCndUecKZho"
      },
      "source": [
        "이제 로지스틱 회귀 모델을 빌드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0sXM7qLlKfZ"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.built = False\n",
        "    \n",
        "  def __call__(self, x, train=True):\n",
        "    # Initialize the model parameters on the first call\n",
        "    if not self.built:\n",
        "      # Randomly generate the weights and the bias term\n",
        "      rand_w = tf.random.uniform(shape=[x.shape[-1], 1], seed=22)\n",
        "      rand_b = tf.random.uniform(shape=[], seed=22)\n",
        "      self.w = tf.Variable(rand_w)\n",
        "      self.b = tf.Variable(rand_b)\n",
        "      self.built = True\n",
        "    # Compute the model output\n",
        "    z = tf.add(tf.matmul(x, self.w), self.b)\n",
        "    z = tf.squeeze(z, axis=1)\n",
        "    if train:\n",
        "      return z\n",
        "    return tf.sigmoid(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObQu9fDnXGL"
      },
      "source": [
        "검증하려면 훈련되지 않은 모델이 훈련 데이터의 작은 하위 집합에 대해 `(0, 1)` 범위의 값을 출력하는지 확인해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bIovC0Z4QHJ"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ2ievISyf0p"
      },
      "outputs": [],
      "source": [
        "y_pred = log_reg(x_train_norm[:5], train=False)\n",
        "y_pred.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PribnwDHUksC"
      },
      "source": [
        "다음으로, 훈련하는 동안 올바른 분류의 비율을 계산하는 정확성 함수를 작성합니다. 예측 확률에서 분류를 검색하려면 임계값보다 높은 모든 확률이 클래스 `1`에 속하는 임계값을 설정해야 합니다. 이는 기본값으로 `0.5`로 설정할 수 있는 구성 가능한 하이퍼 매개변수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssnVcKg7oMe6"
      },
      "outputs": [],
      "source": [
        "def predict_class(y_pred, thresh=0.5):\n",
        "  # Return a tensor with  `1` if `y_pred` > `0.5`, and `0` otherwise\n",
        "  return tf.cast(y_pred > thresh, tf.float32)\n",
        "\n",
        "def accuracy(y_pred, y):\n",
        "  # Return the proportion of matches between `y_pred` and `y`\n",
        "  y_pred = tf.math.sigmoid(y_pred)\n",
        "  y_pred_class = predict_class(y_pred)\n",
        "  check_equal = tf.cast(y_pred_class == y,tf.float32)\n",
        "  acc_val = tf.reduce_mean(check_equal)\n",
        "  return acc_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_0KHQ25_2dF"
      },
      "source": [
        "### 모델 훈련하기\n",
        "\n",
        "훈련에 미니 배치를 사용하면 메모리 효율성이 높아지고 더 빠른 수렴이 가능해집니다. `tf.data.Dataset` API에는 배치와 셔플에 사용할 수 있는 유용한 함수가 있습니다. API를 사용하면 간단하고 재사용 가능한 부분으로부터 복잡한 입력 파이프라인을 빌드할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJD7-4U0etqa"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0]).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test))\n",
        "test_dataset = test_dataset.shuffle(buffer_size=x_test.shape[0]).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLiWZZPBSDip"
      },
      "source": [
        "이제 로지스틱 회귀 모델의 훈련 루프를 작성합니다. 루프는 모델의 매개변수를 반복적으로 업데이트하기 위해 입력에 대한 로그 손실 함수와 그래디언트를 활용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNC3D1DGsGgK"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "epochs = 200\n",
        "learning_rate = 0.01\n",
        "train_losses, test_losses = [], []\n",
        "train_accs, test_accs = [], []\n",
        "\n",
        "# Set up the training loop and begin training\n",
        "for epoch in range(epochs):\n",
        "  batch_losses_train, batch_accs_train = [], []\n",
        "  batch_losses_test, batch_accs_test = [], []\n",
        "\n",
        "  # Iterate over the training data\n",
        "  for x_batch, y_batch in train_dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred_batch = log_reg(x_batch)\n",
        "      batch_loss = log_loss(y_pred_batch, y_batch)\n",
        "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
        "    # Update the parameters with respect to the gradient calculations\n",
        "    grads = tape.gradient(batch_loss, log_reg.variables)\n",
        "    for g,v in zip(grads, log_reg.variables):\n",
        "      v.assign_sub(learning_rate * g)\n",
        "    # Keep track of batch-level training performance\n",
        "    batch_losses_train.append(batch_loss)\n",
        "    batch_accs_train.append(batch_acc)\n",
        "\n",
        "  # Iterate over the testing data\n",
        "  for x_batch, y_batch in test_dataset:\n",
        "    y_pred_batch = log_reg(x_batch)\n",
        "    batch_loss = log_loss(y_pred_batch, y_batch)\n",
        "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
        "    # Keep track of batch-level testing performance\n",
        "    batch_losses_test.append(batch_loss)\n",
        "    batch_accs_test.append(batch_acc)\n",
        "\n",
        "  # Keep track of epoch-level model performance\n",
        "  train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n",
        "  test_loss, test_acc = tf.reduce_mean(batch_losses_test), tf.reduce_mean(batch_accs_test)\n",
        "  train_losses.append(train_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accs.append(test_acc)\n",
        "  if epoch % 20 == 0:\n",
        "    print(f\"Epoch: {epoch}, Training log loss: {train_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoLiAg7fYft7"
      },
      "source": [
        "### 성능 평가\n",
        "\n",
        "시간 경과에 따른 모델 손실 및 정확성의 변화를 관찰합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv3oCQPvWhr0"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(epochs), train_losses, label = \"Training loss\")\n",
        "plt.plot(range(epochs), test_losses, label = \"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Log loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Log loss vs training iterations\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2HDVGLPODIE"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(epochs), train_accs, label = \"Training accuracy\")\n",
        "plt.plot(range(epochs), test_accs, label = \"Testing accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy vs training iterations\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jonKhUzuPyfa"
      },
      "outputs": [],
      "source": [
        "print(f\"Final training log loss: {train_losses[-1]:.3f}\")\n",
        "print(f\"Final testing log Loss: {test_losses[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3DF4qyrPyke"
      },
      "outputs": [],
      "source": [
        "print(f\"Final training accuracy: {train_accs[-1]:.3f}\")\n",
        "print(f\"Final testing accuracy: {test_accs[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrj1TbOJasjA"
      },
      "source": [
        "이 모델은 훈련 데이터세트에서 종양을 분류할 경우 높은 정확성과 낮은 손실을 보여주며 보이지 않는 테스트 데이터에도 잘 일반화됩니다. 한 단계 더 나아가 전체 정확성 점수보다 더 많은 인사이트를 제공하는 오류율을 탐색할 수도 있습니다. 바이너리 분류 문제에서 가장 많이 사용되는 두 가지 오류율은 FPR(거짓양성률)과 FNR(거짓음성률)입니다.\n",
        "\n",
        "이 문제에서 FPR은 실제로 양성인 종양에서 악성 종양 예측의 비율입니다. 반대로, FNR은 실제로 악성인 종양에서 양성 종양 예측의 비율입니다.\n",
        "\n",
        "분류의 정확성을 평가하는 [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix){:.external}를 사용하여 오차 행렬을 계산하고 matplotlib를 사용하여 행렬을 표시합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJO7YkA8ZDMU"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(y, y_classes, typ):\n",
        "  # Compute the confusion matrix and normalize it\n",
        "  plt.figure(figsize=(10,10))\n",
        "  confusion = sk_metrics.confusion_matrix(y.numpy(), y_classes.numpy())\n",
        "  confusion_normalized = confusion / confusion.sum(axis=1)\n",
        "  axis_labels = range(2)\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.4f', square=True)\n",
        "  plt.title(f\"Confusion matrix: {typ}\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "y_pred_train, y_pred_test = log_reg(x_train_norm, train=False), log_reg(x_test_norm, train=False)\n",
        "train_classes, test_classes = predict_class(y_pred_train), predict_class(y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ5DFcleiDFm"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(y_train, train_classes, 'Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtfcsAp_iCNR"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(y_test, test_classes, 'Testing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlivxaDmTnGq"
      },
      "source": [
        "이 예제의 맥락에서 오류율 측정을 관찰하고 그 중요성을 해석합니다. 암 진단과 같은 많은 의료 테스트 연구에서 낮은 거짓음성률을 보장하기 위해 높은 거짓양성률을 갖는 것은 완벽하게 수용 가능하며 실제로 권장되는 이유는 악성 종양 진단(거짓 음성)을 놓치는 위험이 양성 종양을 악성으로 잘못 분류하는 것(거짓 양성)보다 훨씬 낫기 때문입니다\n",
        "\n",
        "FPR 및 FNR을 제어하려면 확률 예측을 분류하기 전에 임계값 하이퍼 매개변수를 변경해 봅니다. 임계값이 낮을수록 모델의 악성 종양 분류 가능성이 전반적으로 높아집니다. 이는 필연적으로 거짓 양성과 FPR의 수를 증가시키지만 거짓 음성 및 FNR의 수를 줄이는 데에도 도움이 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADEN2rb4Nhj"
      },
      "source": [
        "## 모델 저장하기\n",
        "\n",
        "먼저 원시 데이터를 선택하고 다음 연산을 수행하는 내보내기 모듈을 만들어 봅니다.\n",
        "\n",
        "- 정규화\n",
        "- 확률 예측\n",
        "- 클래스 예측\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KPRHCzg4ZxH"
      },
      "outputs": [],
      "source": [
        "class ExportModule(tf.Module):\n",
        "  def __init__(self, model, norm_x, class_pred):\n",
        "    # Initialize pre- and post-processing functions\n",
        "    self.model = model\n",
        "    self.norm_x = norm_x\n",
        "    self.class_pred = class_pred\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], dtype=tf.float32)])\n",
        "  def __call__(self, x):\n",
        "    # Run the `ExportModule` for new data points\n",
        "    x = self.norm_x.norm(x)\n",
        "    y = self.model(x, train=False)\n",
        "    y = self.class_pred(y)\n",
        "    return y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzRclo5-yjO"
      },
      "outputs": [],
      "source": [
        "log_reg_export = ExportModule(model=log_reg,\n",
        "                              norm_x=norm_x,\n",
        "                              class_pred=predict_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtofGIBN_qFd"
      },
      "source": [
        "현재 상태로 모델을 저장하기 위해 `tf.saved_model.save` 함수를 사용할 수 있습니다. 저장한 모델을 로드하고 예측하려면 `tf.saved_model.load` 함수를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4Qum1Ts_pmF"
      },
      "outputs": [],
      "source": [
        "models = tempfile.mkdtemp()\n",
        "save_path = os.path.join(models, 'log_reg_export')\n",
        "tf.saved_model.save(log_reg_export, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KPILr1i_M_c"
      },
      "outputs": [],
      "source": [
        "log_reg_loaded = tf.saved_model.load(save_path)\n",
        "test_preds = log_reg_loaded(x_test)\n",
        "test_preds[:10].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgGQuV-yqYZH"
      },
      "source": [
        "## 결론\n",
        "\n",
        "이 노트북에서는 로지스틱 회귀 문제를 처리하는 몇 가지 기술을 소개했습니다. 다음은 도움이 될 수 있는 몇 가지 추가 팁입니다.\n",
        "\n",
        "- 구성 가능성이 높은 머신러닝 워크플로를 구축하는 데 [TensorFlow Core API](https://www.tensorflow.org/guide/core)를 사용할 수 있습니다.\n",
        "- 오류율 분석은 전체 정확성 점수 이면의 분류 모델의 성능에 대한 더 많은 인사이트를 얻을 수 있는 좋은 방법입니다.\n",
        "- 과대적합은 로지스틱 회귀 모델의 또 다른 일반적인 문제이지만 이 튜토리얼에서는 문제가 되지 않았습니다. 이에 대한 추가적인 도움이 필요하면 [과대적합 및 과소적합](../../tutorials/keras/overfit_and_underfit.ipynb) 튜토리얼을 참조하세요.\n",
        "\n",
        "TensorFlow Core API를 사용하는 더 많은 예제는 [가이드](https://www.tensorflow.org/guide/core)를 확인하세요. 데이터 로드 및 준비에 대해 자세히 알아보려면 [이미지 데이터 로드](../../load_data/images.ipynb) 또는 [CSV 데이터 로드](../../load_data/csv.ipynb) 튜토리얼을 참고하세요."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "logistic_regression_core.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
