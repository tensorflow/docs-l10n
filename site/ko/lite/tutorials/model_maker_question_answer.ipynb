{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# TensorFlow Lite Model Maker를 사용한 BERT 질문 답변"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5Y7snSuG51"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/tutorials/model_maker_question_answer\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3q-gvm3cI8"
      },
      "source": [
        "The [TensorFlow Lite Model Maker library](https://www.tensorflow.org/lite/guide/model_maker) simplifies the process of adapting and converting a TensorFlow model to particular input data when deploying this model for on-device ML applications.\n",
        "\n",
        "이 노트북에서는 질문 답변 작업에 일반적으로 사용되는 질문 답변 모델을 조정하고 변환하는 데 Model Maker 라이브러리를 사용하는 방법을 보여주는 엔드 투 엔드 예를 소개합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxEHFTk755qw"
      },
      "source": [
        "# BERT 질문 답변 작업 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFbKTCF25-SG"
      },
      "source": [
        "이 라이브러리에서 지원되는 작업은 질문에 대한 답변을 추출하는 작업입니다. 즉, 구절과 질문이 주어졌을 때 구절의 범위가 답변으로 제공됩니다. 아래 이미지는 질문 답변의 예입니다.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_squad_showcase.png\" width=\"500\"></p>\n",
        "\n",
        "<p align=\"center\">\n",
        "    <em>답변은 구절의 범위입니다(이미지 제공: <a href=\"https://rajpurkar.github.io/mlx/qa-and-squad/\">SQuAD 블로그</a>).</em>\n",
        "</p>\n",
        "\n",
        "질문 답변 작업 모델의 경우, 입력은 이미 전처리된 구절과 질문 쌍이어야 하며 출력은 구절의 각 토큰에 대한 시작 로짓과 끝 로짓이어야 합니다. 구절과 질문의 길이에 따라 입력 크기를 설정하고 조정할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb7P4WQta8Ub"
      },
      "source": [
        "## 엔드 투 엔드 개요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7cIHjIfbDlG"
      },
      "source": [
        "다음 코드 조각은 몇 줄의 코드로 모델을 가져오는 방법을 보여줍니다. 전체 프로세스에는 (1) 모델 선택, (2) 데이터 로드, (3) 모델 재훈련, (4) 평가, (5) TensorFlow Lite 형식으로 내보내기의 5 단계가 포함됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQPdlxZBYuZG"
      },
      "source": [
        "```python\n",
        "# Chooses a model specification that represents the model.\n",
        "spec = model_spec.get('mobilebert_qa')\n",
        "\n",
        "# Gets the training data and validation data.\n",
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)\n",
        "\n",
        "# Fine-tunes the model.\n",
        "model = question_answer.create(train_data, model_spec=spec)\n",
        "\n",
        "# Gets the evaluation result.\n",
        "metric = model.evaluate(validation_data)\n",
        "\n",
        "# Exports the model to the TensorFlow Lite format with metadata in the export directory.\n",
        "model.export(export_dir)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exScAdvBbNEi"
      },
      "source": [
        "다음 섹션에서 코드를 자세히 설명합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## 전제 조건\n",
        "\n",
        "이 예제를 실행하려면 [GitHub 리포지토리](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)의 Model Maker 패키지를 포함하여 필요한 패키지를 설치해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "outputs": [],
      "source": [
        "!pip install -q tflite-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "필요한 패키지를 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import question_answer\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.question_answer import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l65ctmtW7_FF"
      },
      "source": [
        "\"엔드 투 엔드 개요\"는 간단한 엔드 투 엔드 예제를 보여줍니다. 다음 섹션에서는 예제를 단계별로 진행하면서 자세히 설명합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ_B8fMDOhMR"
      },
      "source": [
        "## 질문 답변 모델을 나타내는 model_spec 선택하기\n",
        "\n",
        "각 `model_spec` 객체는 질문 답변에 대한 특정 모델을 나타냅니다. Model Maker는 현재 MobileBERT 및 BERT-Base 모델을 지원합니다.\n",
        "\n",
        "지원 모델 | model_spec의 이름 | 모델 설명\n",
        "--- | --- | ---\n",
        "[MobileBERT](https://arxiv.org/pdf/2004.02984.pdf) | 'mobilebert_qa' | BERT-Base보다 4.3배 더 작고 5.5배 더 빠르면서도 기기 내 시나리오에 적합한 경쟁력 있는 결과를 제공합니다.\n",
        "[MobileBERT-SQuAD](https://arxiv.org/pdf/2004.02984.pdf) | 'mobilebert_qa_squad' | MobileBERT 모델과 동일한 모델 아키텍처이며 초기 모델은 이미 [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/)에서 재훈련되었습니다.\n",
        "[BERT-Base](https://arxiv.org/pdf/1810.04805.pdf) | 'bert_qa' | NLP 작업에서 널리 사용되는 표준 BERT 모델입니다.\n",
        "\n",
        "이 튜토리얼에서는 [MobileBERT-SQuAD](https://arxiv.org/pdf/2004.02984.pdf)가 예로 사용됩니다. 모델이 이미 [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/)에서 재훈련되었으므로 질문 답변 작업을 더 빨리 처리할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEAWuZQ1PFiX"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('mobilebert_qa_squad')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEncJxtl-nQ"
      },
      "source": [
        "## 기기 내 ML 앱에 특정한 입력 데이터 로드 및 데이터 전처리하기\n",
        "\n",
        "[TriviaQA](https://nlp.cs.washington.edu/triviaqa/)는 650K 이상의 질문-답변-증거 삼중 요소를 포함한 독해 데이터세트입니다. 이 튜토리얼에서는 이 데이터세트의 일부를 사용하여 Model Maker 라이브러리의 사용 방법을 학습합니다.\n",
        "\n",
        "데이터를 로드하려면 `--sample_size=8000` 및 `web` 데이터세트로 [변환기 Python 스크립트](https://github.com/mandarjoshi90/triviaqa#miscellaneous)를 실행하여 TriviaQA 데이터세트를 [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/)로 변환합니다. 다음과 같이 변환 코드를 약간 수정합니다.\n",
        "\n",
        "- 컨텍스트 문서에서 답변을 찾을 수 없는 샘플은 건너뜁니다.\n",
        "- 대문자나 소문자 없이 컨텍스트에서 원래 답변을 얻습니다.\n",
        "\n",
        "이미 변환된 데이터세트의 보관된 버전을 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tOfUr2KlgpU"
      },
      "outputs": [],
      "source": [
        "train_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-web-train-8000.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json')\n",
        "validation_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-verified-web-dev.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfZk8GNr_1nc"
      },
      "source": [
        "고유한 데이터세트로 MobileBERT 모델을 훈련할 수도 있습니다. Colab에서 이 노트북을 실행하는 경우, 왼쪽 사이드바를 사용하여 데이터를 업로드합니다.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_question_answer.png\" alt=\"파일 업로드\" width=\"800\" hspace=\"100\">\n",
        "\n",
        "클라우드에 데이터를 업로드하지 않으려면 [가이드](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)에 따라 오프라인으로 라이브러리를 실행할 수도 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E051HBUM5owi"
      },
      "source": [
        "`DataLoader.from_squad` 메서드를 사용하여 특정 `model_spec`에 따라 [SQuAD 형식](https://rajpurkar.github.io/SQuAD-explorer/) 데이터를 로드하고 전처리합니다. SQuAD2.0 또는 SQuAD1.1 형식을 사용할 수 있습니다. 매개변수 `version_2_with_negative`를 `True`로 설정하면 형식이 SQuAD2.0임을 의미합니다. 그렇지 않으면 형식은 SQuAD1.1입니다. 기본적으로, `version_2_with_negative`는 `False`입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_fOlZsklmlL"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWuoensX4vDA"
      },
      "source": [
        "## TensorFlow 모델 사용자 정의하기\n",
        "\n",
        "로드된 데이터를 기반으로 사용자 정의 질문 답변 모델을 만듭니다. `create` 함수는 다음 단계로 구성됩니다.\n",
        "\n",
        "1. Creates the model for question answer according to `model_spec`.\n",
        "2. 질문 답변 모델을 훈련합니다. 기본 epoch 및 기본 배치 크기는 `model_spec` 객체의 두 변수 `default_training_epochs` 및 `default_batch_size`에 따라 설정됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvYSUuJY3QxR"
      },
      "outputs": [],
      "source": [
        "model = question_answer.create(train_data, model_spec=spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JKI-pNc8idH"
      },
      "source": [
        "자세한 모델 구조를 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd7Hs8TF8n3H"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5FPk_tOxoZ"
      },
      "source": [
        "## 사용자 정의 모델 평가하기\n",
        "\n",
        "검증 데이터에서 모델을 평가하고 `f1` 점수 및 `exact match` 등을 포함한 메트릭 사전을 가져옵니다. 메트릭은 SQuAD1.1 및 SQuAD2.0 간에 서로 다릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8c2ZQ0J3Riy"
      },
      "outputs": [],
      "source": [
        "model.evaluate(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHoGAceO2xV"
      },
      "source": [
        "## TensorFlow Lite 모델로 내보내기\n",
        "\n",
        "나중에 기기 내 ML 애플리케이션에서 사용할 수 있도록 훈련된 모델을 [메타데이터](https://www.tensorflow.org/lite/convert/metadata)가 포함된 TensorFlow Lite 모델 형식으로 변환합니다. 어휘 파일은 메타데이터에 포함됩니다. 기본 TFLite 파일 이름은 `model.tflite`입니다.\n",
        "\n",
        "많은 기기 내 ML 애플리케이션에서 모델 크기는 중요한 요소입니다. 따라서 모델을 더 작게 만들고 잠재적으로 더 빠르게 실행하려면 양자화를 적용하는 것이 좋습니다. 기본 훈련 후 양자화 기술은 BERT 및 MobileBERT 모델에 대한 동적 범위 양자화입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6wA9lK3TQB"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w12kvDdHJIGH"
      },
      "source": [
        "[TensorFlow Lite 작업 라이브러리](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview)의 [BertQuestionAnswerer API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_question_answerer)를 사용하여 [bert_qa](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android) 참조 앱에서 TensorFlow Lite 모델 파일을 사용할 수 있습니다. Colab 왼쪽 사이드바에서 이를 다운로드할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFnJPvq3VGh3"
      },
      "source": [
        "The allowed export formats can be one or a list of the following:\n",
        "\n",
        "- `ExportFormat.TFLITE`\n",
        "- `ExportFormat.VOCAB`\n",
        "- `ExportFormat.SAVED_MODEL`\n",
        "\n",
        "기본적으로, 메타데이터와 함께 TensorFlow Lite 모델만 내보냅니다. 다른 파일을 선택적으로 내보낼 수도 있습니다. 예를 들어, 다음과 같이 vocab 파일만 내보냅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro2hz4kXVImY"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', export_format=ExportFormat.VOCAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZKYthlVrTos"
      },
      "source": [
        "You can also evaluate the tflite model with the `evaluate_tflite` method. This step is expected to take a long time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ochbq95ZrVFX"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('model.tflite', validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWiA_zX8rxE"
      },
      "source": [
        "## 고급 사용\n",
        "\n",
        "`create` 함수는 이 라이브러리의 중요한 부분으로, `model_spec` 매개변수가 모델 사양을 정의합니다. `BertQASpec` 클래스는 현재 지원됩니다. MobileBERT 모델과 BERT-Base 모델의 두 가지 모델이 있습니다. `create` 함수는 다음 단계로 구성됩니다.\n",
        "\n",
        "1. `model_spec`에 따라 질문 답변 모델을 생성합니다.\n",
        "2. 질문 답변 모델을 훈련합니다.\n",
        "\n",
        "이 섹션에서는 모델 조정, 훈련 하이퍼 매개변수 조정 등의 몇 가지 고급 주제를 설명합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtiksguDfhl"
      },
      "source": [
        "### 모델 조정하기\n",
        "\n",
        "`BertQASpec` 클래스에서 매개변수 `seq_len` 및 `query_len`과 같은 모델 인프라를 조정할 수 있습니다.\n",
        "\n",
        "모델의 조정 가능한 매개변수:\n",
        "\n",
        "- `seq_len`: 모델에 공급할 구절의 길이입니다.\n",
        "- `query_len`: 모델에 공급할 질문의 길이입니다.\n",
        "- `doc_stride`: 슬라이딩 윈도우 방식으로 문서 청크를 가져올 때의 폭입니다.\n",
        "- `initializer_range`: 모든 가중치 행렬을 초기화하기 위한 truncated_normal_initializer의 표준 편차입니다.\n",
        "- `trainable`: 사전 훈련된 레이어가 훈련 가능한지 여부를 지정하는 부울 값입니다.\n",
        "\n",
        "훈련 파이프라인의 조정 가능한 매개변수:\n",
        "\n",
        "- `model_dir`: 모델 체크포인트 파일의 위치입니다. 설정하지 않으면 임시 디렉토리가 사용됩니다.\n",
        "- `dropout_rate`: 드롭아웃 비율입니다.\n",
        "- `learning_rate`: Adam의 초기 학습률입니다.\n",
        "- `predict_batch_size`: 예측을 위한 배치 크기입니다.\n",
        "- `tpu`: 연결할 TPU 주소입니다. tpu를 사용하는 경우에만 사용됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAOd5_bzH9AQ"
      },
      "source": [
        "예를 들어, 더 긴 시퀀스 길이로 모델을 훈련할 수 있습니다. 모델을 변경하는 경우, 먼저 새 `model_spec`을 구성해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9WBN0UTQoMN"
      },
      "outputs": [],
      "source": [
        "new_spec = model_spec.get('mobilebert_qa')\n",
        "new_spec.seq_len = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LSTdghTP0Cv"
      },
      "source": [
        "나머지 단계는 같습니다. 모델 사양에 따라 전처리 단계가 다를 수 있으므로 `dataloader` 및 `create` 부분 모두 다시 실행해야 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQuy7RSDir3"
      },
      "source": [
        "### 훈련 하이퍼 매개변수 조정하기\n",
        "\n",
        "모델 성능에 영향을 미치는 `epochs` 및 `batch_size`와 같은 훈련 하이퍼 매개변수를 조정할 수도 있습니다. 예를 들면 다음과 같습니다.\n",
        "\n",
        "- `epochs`: epoch가 많을수록 정확성이 향상될 수 있지만, 과대적합으로 이어질 수 있습니다.\n",
        "- `batch_size`: 하나의 훈련 단계에서 사용할 샘플의 수입니다.\n",
        "\n",
        "예를 들어, 다음과 같이 더 많은 epoch와 더 큰 배치 크기로 훈련할 수 있습니다.\n",
        "\n",
        "```python\n",
        "model = question_answer.create(train_data, model_spec=spec, epochs=5, batch_size=64)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq6B9lKMfhS6"
      },
      "source": [
        "### 모델 아키텍처 변경하기\n",
        "\n",
        "`model_spec`을 변경하여 데이터 훈련의 기본 모델을 변경할 수 있습니다. 예를 들어, BERT-Base 모델로 변경하려면 다음을 실행합니다.\n",
        "\n",
        "```python\n",
        "spec = model_spec.get('bert_qa')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2d7yycrgu6L"
      },
      "source": [
        "나머지 단계는 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFQrDMXzOVoB"
      },
      "source": [
        "### TensorFlow Lite 모델에서 훈련 후 양자화 사용자 정의하기\n",
        "\n",
        "[훈련 후 양자화](https://www.tensorflow.org/lite/performance/post_training_quantization)는 모델 정확도를 약간만 떨어트리면서 모델 크기와 추론 지연 시간을 줄이는 동시에 CPU 및 하드웨어 가속기의 추론 속도도 개선할 수 있는 변환 기술입니다. 따라서 모델을 최적화하는 데 널리 사용됩니다.\n",
        "\n",
        "Model Maker 라이브러리는 모델을 내보낼 때 기본 훈련 후 양자화 기술을 적용합니다. 훈련 후 양자화를 사용자 지정하려는 경우 Model Maker는 [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig)를 사용하여 여러 훈련 후 양자화 옵션도 지원합니다. float16 양자화를 예로 들어보겠습니다. 먼저 양자화 구성을 정의합니다.\n",
        "\n",
        "```python\n",
        "config = QuantizationConfig.for_float16()\n",
        "```\n",
        "\n",
        "그런 다음 이러한 구성을 가진 TensorFlow Lite 모델을 내보냅니다.\n",
        "\n",
        "```python\n",
        "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPVopCeB6LV6"
      },
      "source": [
        "# 더 읽어보기\n",
        "\n",
        "기술적인 세부 사항을 배우려면 [BERT 질문 및 답변](https://www.tensorflow.org/lite/examples/bert_qa/overview) 예제를 읽을 수 있습니다. 자세한 내용은 다음을 참조하세요.\n",
        "\n",
        "- TensorFlow Lite Model Maker [가이드](https://www.tensorflow.org/lite/guide/model_maker) 및 [API 참조](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker)\n",
        "- 작업 라이브러리: 배포용 [BertQuestionAnswerer](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_question_answerer)\n",
        "- 엔드 투 엔드 참조 앱: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android) 및 [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/ios)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "model_maker_question_answer.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
