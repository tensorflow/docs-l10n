{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XX46cTrh6iD"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sKrlWr6Kh-mF"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hST65kOHXpiL"
      },
      "source": [
        "# TensorFlow Lite Model Maker를 이용한 오디오 도메인 전이 학습\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/audio_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/lite/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/lite/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/lite/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF 허브 모델 보기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB5k6xNKJ5Xe"
      },
      "source": [
        "이 colab 노트북에서는 [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker)를 사용하여 맞춤 오디오 분류 모델을 학습하는 방법을 배웁니다.\n",
        "\n",
        "Model Maker 라이브러리는 전이 학습을 사용하여 사용자 지정 데이터세트를 사용하여 TensorFlow Lite 모델을 훈련하는 프로세스를 단순화합니다. 고유한 사용자 지정 데이터세트로 TensorFlow Lite 모델을 재훈련하면 필요한 훈련 데이터의 양과 시간이 줄어듭니다.\n",
        "\n",
        "본 내용은 [오디오 모델을 사용자 지정하고 Android에 배포하기 위한 Codelab](https://codelabs.developers.google.com/codelabs/tflite-audio-classification-custom-model-android)의 일부입니다.\n",
        "\n",
        "사용자 지정 새 데이터세트를 사용하고 전화기에서 사용할 수 있는 TFLite 모델, 브라우저에서 추론에 사용할 수 있는 TensorFlow.JS 모델 및 제공에 사용할 수 있는 SavedModel 버전을 내보냅니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeZZ_cSsZfPx"
      },
      "source": [
        "## 종속성 설치하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbMc4vHjaYdQ"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install tflite-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2ck_Ghdcgt9"
      },
      "source": [
        "## TensorFlow, Model Maker 및 기타 라이브러리 가져오기\n",
        "\n",
        "필요한 종속성 중에서 TensorFlow 및 Model Maker를 사용합니다. 그 외에 다른 것들은 오디오 조작, 재생 및 시각화를 위한 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwUA9u4oWoCR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tflite_model_maker as mm\n",
        "from tflite_model_maker import audio_classifier\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import glob\n",
        "import random\n",
        "\n",
        "from IPython.display import Audio, Image\n",
        "from scipy.io import wavfile\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Model Maker Version: {mm.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIfm2TxKZAuA"
      },
      "source": [
        "## 새 데이터세트\n",
        "\n",
        "새 데이터세트는 5가지 유형의 새 노래로 구성된 교육 컬렉션입니다.\n",
        "\n",
        "- 흰가슴 우드렌\n",
        "- 참새\n",
        "- 솔잣새\n",
        "- 밤왕관 안피타\n",
        "- 아자라 가시꼬리\n",
        "\n",
        "원본 오디오는 전 세계의 새 소리를 공유하는 전용 웹사이트인 [Xeno-canto](https://www.xeno-canto.org/)에서 가져왔습니다.\n",
        "\n",
        "데이터 다운로드부터 시작하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upNRfilkNSmr"
      },
      "outputs": [],
      "source": [
        "birds_dataset_folder = tf.keras.utils.get_file('birds_dataset.zip',\n",
        "                                                'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset.zip',\n",
        "                                                cache_dir='./',\n",
        "                                                cache_subdir='dataset',\n",
        "                                                extract=True)\n",
        "                                                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441bbzZ5d6oq"
      },
      "source": [
        "## 데이터 살펴보기\n",
        "\n",
        "오디오는 이미 train 및 test 폴더로 분할되어 있습니다. 각 분할 폴더 안에는 `bird_code`를 이름으로 사용하는 각 새에 대해 하나의 폴더가 있습니다.\n",
        "\n",
        "오디오는 모두 모노이며 16kHz 샘플 레이트입니다.\n",
        "\n",
        "각 파일에 대한 자세한 내용은 `metadata.csv` 파일을 참조하세요. 여기에는 모든 파일 작성자, 라이선스 및 추가 정보가 포함되어 있습니다. 이 튜토리얼에서 직접 읽을 필요는 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayd7UqCfQQFU"
      },
      "outputs": [],
      "source": [
        "# @title [Run this] Util functions and data structures.\n",
        "\n",
        "data_dir = './dataset/small_birds_dataset'\n",
        "\n",
        "bird_code_to_name = {\n",
        "  'wbwwre1': 'White-breasted Wood-Wren',\n",
        "  'houspa': 'House Sparrow',\n",
        "  'redcro': 'Red Crossbill',  \n",
        "  'chcant2': 'Chestnut-crowned Antpitta',\n",
        "  'azaspi1': \"Azara's Spinetail\",   \n",
        "}\n",
        "\n",
        "birds_images = {\n",
        "  'wbwwre1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg/640px-Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg', # \tAlejandro Bayer Tamayo from Armenia, Colombia \n",
        "  'houspa': 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/House_Sparrow%2C_England_-_May_09.jpg/571px-House_Sparrow%2C_England_-_May_09.jpg', # \tDiliff\n",
        "  'redcro': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Red_Crossbills_%28Male%29.jpg/640px-Red_Crossbills_%28Male%29.jpg', #  Elaine R. Wilson, www.naturespicsonline.com\n",
        "  'chcant2': 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Chestnut-crowned_antpitta_%2846933264335%29.jpg/640px-Chestnut-crowned_antpitta_%2846933264335%29.jpg', # \tMike's Birds from Riverside, CA, US\n",
        "  'azaspi1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Synallaxis_azarae_76608368.jpg/640px-Synallaxis_azarae_76608368.jpg', # https://www.inaturalist.org/photos/76608368\n",
        "}\n",
        "\n",
        "test_files = os.path.abspath(os.path.join(data_dir, 'test/*/*.wav'))\n",
        "\n",
        "def get_random_audio_file():\n",
        "  test_list = glob.glob(test_files)\n",
        "  random_audio_path = random.choice(test_list)\n",
        "  return random_audio_path\n",
        "\n",
        "\n",
        "def show_bird_data(audio_path):\n",
        "  sample_rate, audio_data = wavfile.read(audio_path, 'rb')\n",
        "\n",
        "  bird_code = audio_path.split('/')[-2]\n",
        "  print(f'Bird name: {bird_code_to_name[bird_code]}')\n",
        "  print(f'Bird code: {bird_code}')\n",
        "  display(Image(birds_images[bird_code]))\n",
        "\n",
        "  plttitle = f'{bird_code_to_name[bird_code]} ({bird_code})'\n",
        "  plt.title(plttitle)\n",
        "  plt.plot(audio_data)\n",
        "  display(Audio(audio_data, rate=sample_rate))\n",
        "\n",
        "print('functions and data structures created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrv0uD7aXYl4"
      },
      "source": [
        "### 일부 오디오 재생하기\n",
        "\n",
        "데이터를 더 잘 이해하기 위해 테스트 분할에서 임의의 오디오 파일을 들어보겠습니다.\n",
        "\n",
        "참고: 이 노트북의 뒷부분에서 테스트를 위해 이 오디오에 대한 추론을 실행할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEeMZh-VQy97"
      },
      "outputs": [],
      "source": [
        "random_audio = get_random_audio_file()\n",
        "show_bird_data(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQj1Mf7YZELS"
      },
      "source": [
        "## 모델 훈련하기\n",
        "\n",
        "오디오용 Model Maker를 사용할 때는 모델 사양으로 시작해야 합니다. 이것은 새 모델이 새 클래스에 대해 학습하기 위해 정보를 추출하는 기본 모델입니다. 또한 이는 샘플 레이트, 채널 수와 같은 모델 사양 매개변수를 준수하도록 데이터세트가 변환되는 방식에도 영향을 줍니다.\n",
        "\n",
        "[YAMNet](https://tfhub.dev/google/yamnet/1)은 AudioSet 온톨로지에서 오디오 이벤트를 예측하기 위해 AudioSet 데이터세트에서 훈련된 오디오 이벤트 분류기입니다.\n",
        "\n",
        "입력은 16kHz 및 1채널로 예상됩니다.\n",
        "\n",
        "Model Maker가 알아서 처리하므로 직접 리샘플링을 수행할 필요가 없습니다.\n",
        "\n",
        "- `frame_length`는 각 훈련 샘플의 길이를 결정합니다. 이 경우 EXPECTED_WAVEFORM_LENGTH * 3초입니다.\n",
        "\n",
        "- `frame_steps`는 훈련 샘플이 얼마나 멀리 떨어져 있는지 결정합니다. 이 경우 i번째 샘플은 (i-1)번째 샘플 이후 EXPECTED_WAVEFORM_LENGTH * 6초에서 시작됩니다.\n",
        "\n",
        "이러한 값을 설정하는 이유는 실제 데이터세트의 몇 가지 제한을 해결하기 위한 것입니다.\n",
        "\n",
        "예를 들어, 새 데이터세트에서 새는 항상 노래하지 않습니다. 새는 노래하고, 쉬고, 노래하고, 그 사이에 소음도 있습니다. 프레임이 길면 노래를 캡처하는 데 도움이 되지만 너무 길게 설정하면 훈련용 샘플 수가 줄어듭니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUcxtfHXY7XS"
      },
      "outputs": [],
      "source": [
        "spec = audio_classifier.YamNetSpec(\n",
        "    keep_yamnet_and_custom_heads=True,\n",
        "    frame_step=3 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,\n",
        "    frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF185yZ_M7zu"
      },
      "source": [
        "## 데이터 로드하기\n",
        "\n",
        "Model Maker에는 폴더에서 데이터를 로드하고 모델 사양에 대해 예상되는 형식으로 가져오는 API가 있습니다.\n",
        "\n",
        "훈련 및 테스트 분할은 폴더를 기반으로 합니다. 유효성 검사 데이터세트는 훈련 분할의 20%로 생성됩니다.\n",
        "\n",
        "참고: `cache=True`는 나중에 훈련을 더 빠르게 하는 데 중요하지만 데이터를 저장하려면 더 많은 RAM이 필요합니다. 새 데이터세트의 경우 300MB에 불과하기 때문에 문제가 되지 않지만, 자신의 데이터를 사용하는 경우에는 주의를 기울여야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX0RqETqZgzo"
      },
      "outputs": [],
      "source": [
        "train_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, os.path.join(data_dir, 'train'), cache=True)\n",
        "train_data, validation_data = train_data.split(0.8)\n",
        "test_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, os.path.join(data_dir, 'test'), cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMghju-Rts2"
      },
      "source": [
        "## 모델 훈련하기\n",
        "\n",
        "audio_classifier에는 모델을 생성하고 이미 훈련을 시작하는 [`create`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/create) 메서드가 있습니다.\n",
        "\n",
        "많은 매개변수를 사용자 지정할 수 있습니다. 자세한 내용은 설명서를 참조하세요.\n",
        "\n",
        "이 첫 번째 시도에서는 모든 기본 구성을 사용하고 100 epoch에 대해 학습합니다.\n",
        "\n",
        "참고: 첫 번째 epoch는 캐시가 생성될 때이기 때문에 다른 모든 epoch보다 시간이 오래 걸립니다. 그 후 각 epoch는 1초에 가깝습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r6Awvl4ZkIv"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "print('Training the model')\n",
        "model = audio_classifier.create(\n",
        "    train_data,\n",
        "    spec,\n",
        "    validation_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXMEHZkAxJTl"
      },
      "source": [
        "정확도는 좋아 보이지만 테스트 데이터에 대해 평가 단계를 실행하고 모델이 시드되지 않은 데이터에서 좋은 결과를 얻었는지 확인하는 것이 중요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDoQACMrZnOx"
      },
      "outputs": [],
      "source": [
        "print('Evaluating the model')\n",
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QRRAM39aOxS"
      },
      "source": [
        "## 모델 이해하기\n",
        "\n",
        "분류자를 훈련할 때 [혼동 행렬](https://en.wikipedia.org/wiki/Confusion_matrix)을 확인하는 것이 유용합니다. 혼동 행렬을 통해 분류자가 테스트 데이터에서 어떤 성능을 나타내는지 자세히 알아볼 수 있습니다.\n",
        "\n",
        "Model Maker는 이미 혼동 행렬을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqB3c0368iH3"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion, test_labels):\n",
        "  \"\"\"Compute confusion matrix and normalize.\"\"\"\n",
        "  confusion_normalized = confusion.astype(\"float\") / confusion.sum(axis=1)\n",
        "  axis_labels = test_labels\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.2f', square=True)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "confusion_matrix = model.confusion_matrix(test_data)\n",
        "show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gr1s7juBy7H"
      },
      "source": [
        "## 모델 테스트하기 [선택 사항]\n",
        "\n",
        "결과를 보기 위해 테스트 데이터세트의 샘플 오디오에 대한 모델을 시도할 수 있습니다.\n",
        "\n",
        "먼저 제공 모델을 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmlmTl42Bq_u"
      },
      "outputs": [],
      "source": [
        "serving_model = model.create_serving_model()\n",
        "\n",
        "print(f'Model\\'s input shape and type: {serving_model.inputs}')\n",
        "print(f'Model\\'s output shape and type: {serving_model.outputs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQsZFO2mrYhx"
      },
      "source": [
        "이전에 로드한 임의의 오디오로 돌아갑니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dv5ViK0reXc"
      },
      "outputs": [],
      "source": [
        "# if you want to try another file just uncoment the line below\n",
        "random_audio = get_random_audio_file()\n",
        "show_bird_data(random_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uixOfKSUj_9m"
      },
      "source": [
        "생성된 모델에는 고정 입력 창이 있습니다.\n",
        "\n",
        "주어진 오디오 파일의 경우, 예상 크기의 데이터 창으로 분할해야 합니다. 마지막 창은 0으로 채워야 할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAvGKQL0lNty"
      },
      "outputs": [],
      "source": [
        "sample_rate, audio_data = wavfile.read(random_audio, 'rb')\n",
        "\n",
        "audio_data = np.array(audio_data) / tf.int16.max\n",
        "input_size = serving_model.input_shape[1]\n",
        "\n",
        "splitted_audio_data = tf.signal.frame(audio_data, input_size, input_size, pad_end=True, pad_value=0)\n",
        "\n",
        "print(f'Test audio path: {random_audio}')\n",
        "print(f'Original size of the audio data: {len(audio_data)}')\n",
        "print(f'Number of windows for inference: {len(splitted_audio_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLxKd0eFkMcR"
      },
      "source": [
        "분할된 모든 오디오를 반복하고 각각에 대해 모델을 적용합니다.\n",
        "\n",
        "방금 훈련한 모델에는 원본 YAMNet의 출력과 방금 훈련한 출력의 2가지 출력이 있습니다. 이는 실제 환경이 새 소리 이상으로 더 복잡하기 때문에 중요합니다. 예를 들어 새 사용 사례에서 YAMNet의 출력을 사용하여 관련 없는 오디오를 필터링할 수 있습니다. YAMNet이 새 또는 동물을 분류하지 않는 경우 모델의 출력이 관련 없는 분류를 가질 수 있음을 나타낼 수 있습니다.\n",
        "\n",
        "관계를 쉽게 이해할 수 있도록 아래에 두 출력이 프린트되어 있습니다. 모델에서 저지르는 대부분의 실수는 YAMNet의 예측이 도메인(예: 새)과 관련이 없을 때입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-8fJLrxGwYT"
      },
      "outputs": [],
      "source": [
        "print(random_audio)\n",
        "\n",
        "results = []\n",
        "print('Result of the window ith:  your model class -> score,  (spec class -> score)')\n",
        "for i, data in enumerate(splitted_audio_data):\n",
        "  yamnet_output, inference = serving_model(data)\n",
        "  results.append(inference[0].numpy())\n",
        "  result_index = tf.argmax(inference[0])\n",
        "  spec_result_index = tf.argmax(yamnet_output[0])\n",
        "  t = spec._yamnet_labels()[spec_result_index]\n",
        "  result_str = f'Result of the window {i}: ' \\\n",
        "  f'\\t{test_data.index_to_label[result_index]} -> {inference[0][result_index].numpy():.3f}, ' \\\n",
        "  f'\\t({spec._yamnet_labels()[spec_result_index]} -> {yamnet_output[0][spec_result_index]:.3f})'\n",
        "  print(result_str)\n",
        "\n",
        "\n",
        "results_np = np.array(results)\n",
        "mean_results = results_np.mean(axis=0)\n",
        "result_index = mean_results.argmax()\n",
        "print(f'Mean result: {test_data.index_to_label[result_index]} -> {mean_results[result_index]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yASrikBgZ9ZO"
      },
      "source": [
        "## 모델 내보내기\n",
        "\n",
        "마지막 단계는 임베디드 장치 또는 브라우저에서 사용할 모델을 내보내는 것입니다.\n",
        "\n",
        "`export` 메서드는 두 형식을 모두 내보냅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw_ehPxAdQlz"
      },
      "outputs": [],
      "source": [
        "models_path = './birds_models'\n",
        "print(f'Exporing the TFLite model to {models_path}')\n",
        "\n",
        "model.export(models_path, tflite_filename='my_birds_model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjZRKmurA3y_"
      },
      "source": [
        "Python 환경에서 제공하거나 사용하기 위해 SavedModel 버전을 내보낼 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veBwppOsA-kn"
      },
      "outputs": [],
      "source": [
        "model.export(models_path, export_format=[mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xr0idac6xfi"
      },
      "source": [
        "## 다음 단계\n",
        "\n",
        "잘 했습니다.\n",
        "\n",
        "이제 [TFLite AudioClassifier Task API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier)를 사용하여 새 모델을 모바일 장치에 배포할 수 있습니다.\n",
        "\n",
        "다른 클래스를 사용하여 자신의 데이터로 동일한 프로세스를 시도할 수도 있습니다. [오디오 분류를 위한 Model Maker](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier) 문서를 참조하세요.\n",
        "\n",
        "또한 엔드 투 엔드 참조 앱인 [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android/), [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios)을 통해 배워보세요."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "audio_classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
