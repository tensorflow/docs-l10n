{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a930wM_fqUNH"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Federated Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VxVUPYkahDa6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naLrzWx6FbQQ"
      },
      "source": [
        "# `federated_select` 및 희소 집계를 통한 클라이언트 효율적 대형 모델 페더레이션 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jM4S9YFXamd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/sparse_federated_learning\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.36.0/docs/tutorials/sparse_federated_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.36.0/docs/tutorials/sparse_federated_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/federated/docs/tutorials/sparse_federated_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saL2ruE25Cx1"
      },
      "source": [
        "이 튜토리얼은 각 클라이언트 장치가 `tff.federated_select` 및 희소 집계를 사용하여 모델의 작은 부분만 다운로드하고 업데이트하는 매우 큰 모델을 훈련하는 데 TFF를 사용할 수 있는 방법을 보여줍니다. 이 튜토리얼은 상당히 독립적이지만, <a href=\"https://www.tensorflow.org/federated/tutorials/federated_select\" data-md-type=\"link\">`tff.federated_select` 튜토리얼</a>과 [사용자 정의 FL 알고리즘 튜토리얼](https://www.tensorflow.org/federated/tutorials/building_your_own_federated_learning_algorithm)은 여기에 사용된 몇 가지 기술에 대한 좋은 소개 정보를 제공합니다.\n",
        "\n",
        "구체적으로, 이 튜토리얼에서는 bag-of-words 기능 표현을 기반으로 텍스트 문자열과 연결된 \"태그\"를 예측하는 다중 레이블 분류를 위해 로지스틱 회귀를 고려합니다. 중요한 것은 통신 및 클라이언트 측 계산 비용이 고정 상수(`MAX_TOKENS_SELECTED_PER_CLIENT`)에 의해 제어되고 실제 환경에서 매우 클 수 있는 전체 어휘 크기로 확장되지 *않는다는* 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg3OmMVWp_9D"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89APlnIQCvmp"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "from typing import Callable, List, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "tff.backends.native.set_local_python_execution_context()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92wW8RDBvE9F"
      },
      "source": [
        "각 클라이언트는 기껏해야 이 정도로 많은 고유 토큰에 대한 모델 가중치 행을 `federated_select` 처리합니다. 이것은 클라이언트의 로컬 모델 크기와 수행된 서버 -&gt; 클라이언트(`federated_select`) 및 클라이언트 - &gt; 서버`(federated_aggregate`) 통신 양에 대한 상한을 결정합니다.\n",
        "\n",
        "이 튜토리얼은 이를 1만큼 작게 설정하거나(각 클라이언트의 모든 토큰이 선택되지 않도록 함) 큰 값으로 설정하더라도 올바르게 실행되어야 하지만 모델 수렴이 영향을 받을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkDumr_6bDtY"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS_SELECTED_PER_CLIENT = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th2PI1FpvaSL"
      },
      "source": [
        "또한 다양한 유형에 대해 몇 가지 상수를 정의합니다. 이 colab의 경우 **토큰**은 데이터세트를 구문 분석한 후 특정 단어에 대한 정수 식별자입니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyQ9xmIyvZFO"
      },
      "outputs": [],
      "source": [
        "# There are some constraints on types\n",
        "# here that will require some explicit type conversions:\n",
        "#    - `tff.federated_select` requires int32\n",
        "#    - `tf.SparseTensor` requires int64 indices.\n",
        "TOKEN_DTYPE = tf.int64\n",
        "SELECT_KEY_DTYPE = tf.int32\n",
        "\n",
        "# Type for counts of token occurences.\n",
        "TOKEN_COUNT_DTYPE = tf.int32\n",
        "\n",
        "# A sparse feature vector can be thought of as a map\n",
        "# from TOKEN_DTYPE to FEATURE_DTYPE. \n",
        "# Our features are {0, 1} indicators, so we could potentially\n",
        "# use tf.int8 as an optimization.\n",
        "FEATURE_DTYPE = tf.int32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ej2eY4-zk_a"
      },
      "source": [
        "# 문제 설정: 데이터세트 및 모델\n",
        "\n",
        "이 튜토리얼에서는 쉽게 실험할 수 있도록 작은 장난감 데이터세트를 구성합니다. 그러나 데이터세트의 형식은 [Federated StackOverflow](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data)와 호환되며 [전처리](https://github.com/google-research/federated/blob/0a558bac8a724fc38175ff4f0ce46c7af3d24be2/utils/datasets/stackoverflow_tag_prediction.py) 및 [모델 아키텍처](https://github.com/google-research/federated/blob/49a43456aa5eaee3e1749855eed89c0087983541/utils/models/stackoverflow_lr_models.py)는 [*적응형 페더레이션 최적화*](https://arxiv.org/abs/2003.00295)의 StackOverflow 태그 예측 문제에서 채택되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ofeMf1phYY"
      },
      "source": [
        "## 데이터세트 구문 분석 및 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM9Fae0PXSfa"
      },
      "outputs": [],
      "source": [
        "NUM_OOV_BUCKETS = 1\n",
        "\n",
        "BatchType = collections.namedtuple('BatchType', ['tokens', 'tags'])\n",
        "\n",
        "def build_to_ids_fn(word_vocab: List[str],\n",
        "                    tag_vocab: List[str]) -> Callable[[tf.Tensor], tf.Tensor]:\n",
        "  \"\"\"Constructs a function mapping examples to sequences of token indices.\"\"\"\n",
        "  word_table_values = np.arange(len(word_vocab), dtype=np.int64)\n",
        "  word_table = tf.lookup.StaticVocabularyTable(\n",
        "      tf.lookup.KeyValueTensorInitializer(word_vocab, word_table_values),\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS)\n",
        "\n",
        "  tag_table_values = np.arange(len(tag_vocab), dtype=np.int64)\n",
        "  tag_table = tf.lookup.StaticVocabularyTable(\n",
        "      tf.lookup.KeyValueTensorInitializer(tag_vocab, tag_table_values),\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS)\n",
        "\n",
        "  def to_ids(example):\n",
        "    \"\"\"Converts a Stack Overflow example to a bag-of-words/tags format.\"\"\"\n",
        "    sentence = tf.strings.join([example['tokens'], example['title']],\n",
        "                               separator=' ')\n",
        "\n",
        "    # We represent that label (output tags) densely.\n",
        "    raw_tags = example['tags']\n",
        "    tags = tf.strings.split(raw_tags, sep='|')\n",
        "    tags = tag_table.lookup(tags)\n",
        "    tags, _ = tf.unique(tags)\n",
        "    tags = tf.one_hot(tags, len(tag_vocab) + NUM_OOV_BUCKETS)\n",
        "    tags = tf.reduce_max(tags, axis=0)\n",
        "\n",
        "    # We represent the features as a SparseTensor of {0, 1}s.\n",
        "    words = tf.strings.split(sentence)\n",
        "    tokens = word_table.lookup(words)\n",
        "    tokens, _ = tf.unique(tokens)\n",
        "    # Note:  We could choose to use the word counts as the feature vector\n",
        "    # instead of just {0, 1} values (see tf.unique_with_counts).\n",
        "    tokens = tf.reshape(tokens, shape=(tf.size(tokens), 1))\n",
        "    tokens_st = tf.SparseTensor(\n",
        "        tokens,\n",
        "        tf.ones(tf.size(tokens), dtype=FEATURE_DTYPE),\n",
        "        dense_shape=(len(word_vocab) + NUM_OOV_BUCKETS,))\n",
        "    tokens_st = tf.sparse.reorder(tokens_st)\n",
        "\n",
        "    return BatchType(tokens_st, tags)\n",
        "\n",
        "  return to_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv9NMKgbYki9"
      },
      "outputs": [],
      "source": [
        "def build_preprocess_fn(word_vocab, tag_vocab):\n",
        "\n",
        "  @tf.function\n",
        "  def preprocess_fn(dataset):\n",
        "    to_ids = build_to_ids_fn(word_vocab, tag_vocab)\n",
        "    # We *don't* shuffle in order to make this colab deterministic for\n",
        "    # easier testing and reproducibility.\n",
        "    # But real-world training should use `.shuffle()`.\n",
        "    return dataset.map(to_ids, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return preprocess_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxji1x30tZCi"
      },
      "source": [
        "## 작은 장난감 데이터세트\n",
        "\n",
        "12개의 단어로 구성된 글로벌 어휘와 3명의 클라이언트가 있는 작은 장난감 데이터세트를 구성합니다. 이 작은 예제는 엣지 케이스를 테스트하고(예를 들어, `MAX_TOKENS_SELECTED_PER_CLIENT = 6` 미만의 고유 토큰을 가진 두 클라이언트와 그 이상을 가진 클라이언트가 있는 경우) 코드를 개발하는 데 유용합니다.\n",
        "\n",
        "그러나 이 접근 방식의 실제 사용 사례는 수천만 개 이상의 글로벌 어휘가 될 것이며 각 클라이언트에 아마도 수천 개의 고유한 토큰이 나타날 것입니다. 데이터 형식이 동일하기 때문에 `tff.simulation.datasets.stackoverflow.load_data()` 데이터세트와 같은 보다 현실적인 테스트베드 문제로 확장하는 과정은 간단합니다.\n",
        "\n",
        "먼저, 단어와 태그 어휘를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gtatbx4tdPo"
      },
      "outputs": [],
      "source": [
        "# Features\n",
        "FRUIT_WORDS = ['apple', 'orange', 'pear', 'kiwi']\n",
        "VEGETABLE_WORDS = ['carrot', 'broccoli', 'arugula', 'peas']\n",
        "FISH_WORDS = ['trout', 'tuna', 'cod', 'salmon']\n",
        "WORD_VOCAB = FRUIT_WORDS + VEGETABLE_WORDS + FISH_WORDS\n",
        "\n",
        "# Labels\n",
        "TAG_VOCAB = ['FRUIT', 'VEGETABLE', 'FISH']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQLblngP7gmJ"
      },
      "source": [
        "이제, 작은 로컬 데이터세트로 3개의 클라이언트를 생성합니다. colab에서 이 튜토리얼을 실행하는 경우 아래 개발된 함수의 출력을 해석/확인하기 위해 \"탭에서 셀 미러링\" 기능을 사용하여 이 셀과 해당 출력을 고정하는 것이 유용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr74BLPM1Mxa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word vocab\n",
            " 0 apple\n",
            " 1 orange\n",
            " 2 pear\n",
            " 3 kiwi\n",
            " 4 carrot\n",
            " 5 broccoli\n",
            " 6 arugula\n",
            " 7 peas\n",
            " 8 trout\n",
            " 9 tuna\n",
            "10 cod\n",
            "11 salmon\n",
            "\n",
            "Tag vocab\n",
            " 0 FRUIT\n",
            " 1 VEGETABLE\n",
            " 2 FISH\n"
          ]
        }
      ],
      "source": [
        "preprocess_fn = build_preprocess_fn(WORD_VOCAB, TAG_VOCAB)\n",
        "\n",
        "\n",
        "def make_dataset(raw):\n",
        "  d = tf.data.Dataset.from_tensor_slices(\n",
        "      # Matches the StackOverflow formatting\n",
        "      collections.OrderedDict(\n",
        "          tokens=tf.constant([t[0] for t in raw]),\n",
        "          tags=tf.constant([t[1] for t in raw]),\n",
        "          title=['' for _ in raw]))\n",
        "  d = preprocess_fn(d)\n",
        "  return d\n",
        "\n",
        "\n",
        "# 4 distinct tokens\n",
        "CLIENT1_DATASET = make_dataset([\n",
        "    ('apple orange apple orange', 'FRUIT'),\n",
        "    ('carrot trout', 'VEGETABLE|FISH'),\n",
        "    ('orange apple', 'FRUIT'),\n",
        "    ('orange', 'ORANGE|CITRUS')  # 2 OOV tag\n",
        "])\n",
        "\n",
        "# 6 distinct tokens\n",
        "CLIENT2_DATASET = make_dataset([\n",
        "    ('pear cod', 'FRUIT|FISH'),\n",
        "    ('arugula peas', 'VEGETABLE'),\n",
        "    ('kiwi pear', 'FRUIT'),\n",
        "    ('sturgeon', 'FISH'),  # OOV word\n",
        "    ('sturgeon bass', 'FISH')  # 2 OOV words\n",
        "])\n",
        "\n",
        "# A client with all possible words & tags (13 distinct tokens).\n",
        "# With MAX_TOKENS_SELECTED_PER_CLIENT = 6, we won't download the model\n",
        "# slices for all tokens that occur on this client.\n",
        "CLIENT3_DATASET = make_dataset([\n",
        "    (' '.join(WORD_VOCAB + ['oovword']), '|'.join(TAG_VOCAB)),\n",
        "    # Mathe the OOV token and 'salmon' occur in the largest number\n",
        "    # of examples on this client:\n",
        "    ('salmon oovword', 'FISH|OOVTAG')\n",
        "])\n",
        "\n",
        "print('Word vocab')\n",
        "for i, word in enumerate(WORD_VOCAB):\n",
        "  print(f'{i:2d} {word}')\n",
        "\n",
        "print('\\nTag vocab')\n",
        "for i, tag in enumerate(TAG_VOCAB):\n",
        "  print(f'{i:2d} {tag}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH3T6M0f8Bh2"
      },
      "source": [
        "입력 요소(토큰/단어) 및 레이블(사후 태그)의 원시 수에 대한 상수를 정의합니다. 실제 입력/출력 공간은 OOV 토큰/태그를 추가하므로 `NUM_OOV_BUCKETS = 1` 더 큽니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBULpAepxloO"
      },
      "outputs": [],
      "source": [
        "NUM_WORDS = len(WORD_VOCAB) \n",
        "NUM_TAGS = len(TAG_VOCAB)\n",
        "\n",
        "WORD_VOCAB_SIZE = NUM_WORDS + NUM_OOV_BUCKETS\n",
        "TAG_VOCAB_SIZE = NUM_TAGS + NUM_OOV_BUCKETS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zF6NF_f8PQR"
      },
      "source": [
        "데이터세트의 배치 버전과 개별 배치를 생성하면 코드를 테스트하는 데 유용할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uxRQAYdJISP"
      },
      "outputs": [],
      "source": [
        "batched_dataset1 = CLIENT1_DATASET.batch(2)\n",
        "batched_dataset2 = CLIENT2_DATASET.batch(3)\n",
        "batched_dataset3 = CLIENT3_DATASET.batch(2)\n",
        "\n",
        "batch1 = next(iter(batched_dataset1))\n",
        "batch2 = next(iter(batched_dataset2))\n",
        "batch3 = next(iter(batched_dataset3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBcObjtlaq8q"
      },
      "source": [
        "## 희소 입력으로 모델 정의하기\n",
        "\n",
        "각 태그에 대해 간단한 독립 로지스틱 회귀 모델을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTtIkxCKao2Y"
      },
      "outputs": [],
      "source": [
        "def create_logistic_model(word_vocab_size: int, vocab_tags_size: int):\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(word_vocab_size,), sparse=True),\n",
        "      tf.keras.layers.Dense(\n",
        "          vocab_tags_size,\n",
        "          activation='sigmoid',\n",
        "          kernel_initializer=tf.keras.initializers.zeros,\n",
        "          # For simplicity, don't use a bias vector; this means the model\n",
        "          # is a single tensor, and we only need sparse aggregation of\n",
        "          # the per-token slices of the model. Generalizing to also handle\n",
        "          # other model weights that are fully updated \n",
        "          # (non-dense broadcast and aggregate) would be a good exercise.\n",
        "          use_bias=False),\n",
        "  ])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCMScjuWn9t-"
      },
      "source": [
        "먼저 예측을 수행하여 제대로 작동하는지 확인합시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGQOnvbfa3w4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]]\n"
          ]
        }
      ],
      "source": [
        "model = create_logistic_model(WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\n",
        "p = model.predict(batch1.tokens)\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOT2veElT-ij"
      },
      "source": [
        "그리고 몇 가지 간단한 중앙 집중식 훈련:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn3lB84WgRgV"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy())\n",
        "model.train_on_batch(batch1.tokens, batch1.tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1aahpVomC4G"
      },
      "source": [
        "# 페더레이션 계산을 위한 빌딩 블록\n",
        "\n",
        "각 장치가 모델의 관련 하위 집합만 다운로드하고 해당 하위 집합에 대한 업데이트만 제공한다는 주된 차이를 두고 간단한 버전의 [페더레이션 평균화](https://arxiv.org/abs/1602.05629) 알고리즘을 구현하겠습니다.\n",
        "\n",
        "`MAX_TOKENS_SELECTED_PER_CLIENT`의 단축형으로 `M`을 사용합니다. 높은 수준에서 한 라운드의 훈련에는 다음과 같은 단계가 포함됩니다:\n",
        "\n",
        "1. 참여하는 각 클라이언트는 로컬 데이터세트를 검색하여 입력 문자열을 구문 분석하고 이를 올바른 토큰(int 인덱스)에 매핑합니다. 이를 위해서는 전역(대형) 사전에 대한 액세스가 필요합니다(이는 [기능 해싱](https://en.wikipedia.org/wiki/Feature_hashing) 기술을 사용하여 잠재적으로 피할 수 있음). 그런 다음 각 토큰이 발생하는 횟수를 드물게 계산합니다. 장치에서 `U`의 고유한 토큰이 발생하는 경우 `num_actual_tokens = min(U, M)`개의 가장 빈번한 토큰의 훈련을 선택합니다.\n",
        "\n",
        "2. 클라이언트는 `federated_select`를 사용하여 서버에서 `num_actual_tokens`개의 선택된 토큰에 대한 모델 계수를 가져옵니다. 각 모델 슬라이스는 `(TAG_VOCAB_SIZE, )` 모양의 텐서이므로 클라이언트에 전송되는 총 데이터의 크기는 최대 `TAG_VOCAB_SIZE * M` 입니다(아래 참고 참조).\n",
        "\n",
        "3. 클라이언트는 매핑 `global_token -> local_token`을 구성합니다. 여기서 로컬 토큰(int 인덱스)은 선택한 토큰 목록에 있는 글로벌 토큰의 인덱스입니다.\n",
        "\n",
        "4. 클라이언트는 `[0, num_actual_tokens)` 범위에서 최대 `M`개의 토큰에 대한 계수만 있는 글로벌 모델의 \"소규모\" 버전을 사용합니다. `global -> local` 매핑은 선택한 모델 조각에서 이 모델의 밀집 매개변수를 초기화하는 데 사용됩니다.\n",
        "\n",
        "5. 클라이언트는 `global -> local` 매핑으로 사전 처리된 데이터에서 SGD를 사용하여 로컬 모델을 훈련합니다.\n",
        "\n",
        "6. 클라이언트는 행을 인덱싱하기 위해 `local -> global` 매핑을 사용하여 로컬 모델의 매개변수를 `IndexedSlices` 업데이트로 바꿉니다. 서버는 희소 합계 집계를 사용하여 이러한 업데이트를 집계합니다.\n",
        "\n",
        "7. 서버는 위 집계의 (밀집) 결과를 가져와서 참여하는 클라이언트 수로 나누고 결과적인 평균 업데이트를 글로벌 모델에 적용합니다.\n",
        "\n",
        "이 섹션에서는 이러한 단계를 위한 빌딩 블록을 구성한 다음, 한 번의 전체 훈련 라운드 논리를 캡처하는 최종 `federated_computation`에서 결합합니다.\n",
        "\n",
        "> 참고: 위의 설명에는 한 가지 기술적인 정보가 숨겨져 있습니다. `federated_select`와 로컬 모델 구성 모두에 정적으로 알려진 형상이 필요하므로 동적 클라이언트별 `num_actual_tokens` 크기를 사용할 수 없습니다. 대신, 정적 값 `M`을 사용하여 필요한 경우 패딩을 추가합니다. 이것은 알고리즘의 의미에 영향을 미치지 않습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrB3RWrCYHd_"
      },
      "source": [
        "### 클라이언트 토큰을 계산하고 `federated_select`할 모델 조각 결정하기\n",
        "\n",
        "각 장치는 모델의 어떤 \"슬라이스\"가 로컬 훈련 데이터세트와 관련이 있는지 결정해야 합니다. 본 문제의 경우 클라이언트 훈련 데이터세트의 각 토큰이 포함된 예제의 수를 (희소하게!) 계산하여 이 작업을 수행합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRr0ip0ja31O"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def token_count_fn(token_counts, batch):\n",
        "  \"\"\"Adds counts from `batch` to the running `token_counts` sum.\"\"\"\n",
        "  # Sum across the batch dimension.\n",
        "  flat_tokens = tf.sparse.reduce_sum(\n",
        "      batch.tokens, axis=0, output_is_sparse=True)\n",
        "  flat_tokens = tf.cast(flat_tokens, dtype=TOKEN_COUNT_DTYPE)\n",
        "  return tf.sparse.add(token_counts, flat_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTZdZZhFgeYz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: [0 1 4 8]\n",
            "counts: [2 3 1 1]\n"
          ]
        }
      ],
      "source": [
        "# Simple tests\n",
        "# Create the initial zero token counts using empty tensors.\n",
        "initial_token_counts = tf.SparseTensor(\n",
        "    indices=tf.zeros(shape=(0, 1), dtype=TOKEN_DTYPE),\n",
        "    values=tf.zeros(shape=(0,), dtype=TOKEN_COUNT_DTYPE),\n",
        "    dense_shape=(WORD_VOCAB_SIZE,))\n",
        "\n",
        "client_token_counts = batched_dataset1.reduce(initial_token_counts,\n",
        "                                              token_count_fn)\n",
        "tokens = tf.reshape(client_token_counts.indices, (-1,)).numpy()\n",
        "print('tokens:', tokens)\n",
        "np.testing.assert_array_equal(tokens, [0, 1, 4, 8])\n",
        "# The count is the number of *examples* in which the token/word\n",
        "# occurs, not the total number of occurences, since we still featurize\n",
        "# multiple occurences in the same example as a \"1\".\n",
        "counts = client_token_counts.values.numpy()\n",
        "print('counts:', counts)\n",
        "np.testing.assert_array_equal(counts, [2, 3, 1, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ghfTtZgce7"
      },
      "source": [
        "장치에서 가장 자주 발생하는 `MAX_TOKENS_SELECTED_PER_CLIENT` 토큰에 해당하는 모델 매개변수를 선택합니다. 장치에서 이보다 적은 수의 토큰이 발생하면 `federated_select`를 사용할 수 있도록 목록을 패딩합니다.\n",
        "\n",
        "예를 들어 토큰을 무작위로 선택하는 것과 같은 다른 전략이 더 나을 수 있다는 점에 유의하십시오(발생 확률에 따라). 이렇게 하면 클라이언트에 데이터가 있는 모델의 모든 조각이 업데이트될 가능성이 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45YOfL8fh5K8"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def keys_for_client(client_dataset, max_tokens_per_client):\n",
        "  \"\"\"Computes a set of max_tokens_per_client keys.\"\"\"\n",
        "  initial_token_counts = tf.SparseTensor(\n",
        "      indices=tf.zeros((0, 1), dtype=TOKEN_DTYPE),\n",
        "      values=tf.zeros((0,), dtype=TOKEN_COUNT_DTYPE),\n",
        "      dense_shape=(WORD_VOCAB_SIZE,))\n",
        "  client_token_counts = client_dataset.reduce(initial_token_counts,\n",
        "                                              token_count_fn)\n",
        "  # Find the most-frequently occuring tokens\n",
        "  tokens = tf.reshape(client_token_counts.indices, shape=(-1,))\n",
        "  counts = client_token_counts.values\n",
        "  perm = tf.argsort(counts, direction='DESCENDING')\n",
        "  tokens = tf.gather(tokens, perm)\n",
        "  counts = tf.gather(counts, perm)\n",
        "  num_raw_tokens = tf.shape(tokens)[0]\n",
        "  actual_num_tokens = tf.minimum(max_tokens_per_client, num_raw_tokens)\n",
        "  selected_tokens = tokens[:actual_num_tokens]\n",
        "  paddings = [[0, max_tokens_per_client - tf.shape(selected_tokens)[0]]]\n",
        "  padded_tokens = tf.pad(selected_tokens, paddings=paddings)\n",
        "  # Make sure the type is statically determined\n",
        "  padded_tokens = tf.reshape(padded_tokens, shape=(max_tokens_per_client,))\n",
        "\n",
        "  # We will pass these tokens as keys into `federated_select`, which\n",
        "  # requires SELECT_KEY_DTYPE=tf.int32 keys.\n",
        "  padded_tokens = tf.cast(padded_tokens, dtype=SELECT_KEY_DTYPE)\n",
        "  return padded_tokens, actual_num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTyJVDqkjGyG"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "\n",
        "# Case 1: actual_num_tokens > max_tokens_per_client\n",
        "selected_tokens, actual_num_tokens = keys_for_client(batched_dataset1, 3)\n",
        "assert tf.size(selected_tokens) == 3\n",
        "assert actual_num_tokens == 3\n",
        "\n",
        "# Case 2: actual_num_tokens < max_tokens_per_client\n",
        "selected_tokens, actual_num_tokens = keys_for_client(batched_dataset1, 10)\n",
        "assert tf.size(selected_tokens) == 10\n",
        "assert actual_num_tokens == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO0pJ8D-dzp7"
      },
      "source": [
        "### 글로벌 토큰을 로컬 토큰에 매핑하기\n",
        "\n",
        "위의 선택은 온디바이스 모델에 사용할 `[0, actual_num_tokens)` 범위의 밀집된 토큰 세트를 제공합니다. 그러나 우리가 읽은 데이터세트에는 훨씬 더 큰 글로벌 어휘 범위인 `[0, WORD_VOCAB_SIZE)`의 토큰이 있습니다.\n",
        "\n",
        "따라서 글로벌 토큰을 해당 로컬 토큰에 매핑해야 합니다. 로컬 토큰 ID는 이전 단계에서 계산된 `selected_tokens` 텐서에 대한 인덱스에 의해 간단히 제공됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG3uLlqJohXg"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def map_to_local_token_ids(client_data, client_keys):\n",
        "  global_to_local = tf.lookup.StaticHashTable(\n",
        "      # Note int32 -> int64 maps are not supported\n",
        "      tf.lookup.KeyValueTensorInitializer(\n",
        "          keys=tf.cast(client_keys, dtype=TOKEN_DTYPE),\n",
        "          # Note we need to use tf.shape, not the static \n",
        "          # shape client_keys.shape[0]\n",
        "          values=tf.range(0, limit=tf.shape(client_keys)[0],\n",
        "                          dtype=TOKEN_DTYPE)),\n",
        "      # We use -1 for tokens that were not selected, which can occur for clients\n",
        "      # with more than MAX_TOKENS_SELECTED_PER_CLIENT distinct tokens.\n",
        "      # We will simply remove these invalid indices from the batch below.\n",
        "      default_value=-1)\n",
        "\n",
        "  def to_local_ids(sparse_tokens):\n",
        "    indices_t = tf.transpose(sparse_tokens.indices)\n",
        "    batch_indices = indices_t[0]  # First column\n",
        "    tokens = indices_t[1]  # Second column\n",
        "    tokens = tf.map_fn(\n",
        "        lambda global_token_id: global_to_local.lookup(global_token_id), tokens)\n",
        "    # Remove tokens that aren't actually available (looked up as -1):\n",
        "    available_tokens = tokens >= 0\n",
        "    tokens = tokens[available_tokens]\n",
        "    batch_indices = batch_indices[available_tokens]\n",
        "\n",
        "    updated_indices = tf.transpose(\n",
        "        tf.concat([[batch_indices], [tokens]], axis=0))\n",
        "    st = tf.sparse.SparseTensor(\n",
        "        updated_indices,\n",
        "        tf.ones(tf.size(tokens), dtype=FEATURE_DTYPE),\n",
        "        dense_shape=sparse_tokens.dense_shape)\n",
        "    st = tf.sparse.reorder(st)\n",
        "    return st\n",
        "\n",
        "  return client_data.map(lambda b: BatchType(to_local_ids(b.tokens), b.tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Cn60NIBUWf"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "client_keys, actual_num_tokens = keys_for_client(\n",
        "    batched_dataset3, MAX_TOKENS_SELECTED_PER_CLIENT)\n",
        "client_keys = client_keys[:actual_num_tokens]\n",
        "\n",
        "d = map_to_local_token_ids(batched_dataset3, client_keys)\n",
        "batch  = next(iter(d))\n",
        "all_tokens = tf.gather(batch.tokens.indices, indices=1, axis=1)\n",
        "# Confirm we have local indices in the range [0, MAX):\n",
        "assert tf.math.reduce_max(all_tokens) < MAX_TOKENS_SELECTED_PER_CLIENT\n",
        "assert tf.math.reduce_max(all_tokens) >= 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nvZYfzga1yN"
      },
      "source": [
        "### 각 클라이언트에서 로컬(하위) 모델 훈련하기\n",
        "\n",
        "`federated_select`는 선택한 슬라이스를 선택 키와 동일한 순서로 `tf.data.Dataset`로 반환합니다. 따라서 먼저 이러한 데이터세트를 가져와서 클라이언트 모델의 모델 가중치로 사용할 수 있는 단일 밀도 텐서로 변환하는 유틸리티 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFpC_nFjgbtI"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def slices_dataset_to_tensor(slices_dataset):\n",
        "  \"\"\"Convert a dataset of slices to a tensor.\"\"\"\n",
        "  # Use batching to gather all of the slices into a single tensor.\n",
        "  d = slices_dataset.batch(MAX_TOKENS_SELECTED_PER_CLIENT,\n",
        "                           drop_remainder=False)\n",
        "  iter_d = iter(d)\n",
        "  tensor = next(iter_d)\n",
        "  # Make sure we have consumed everything\n",
        "  opt = iter_d.get_next_as_optional()\n",
        "  tf.Assert(tf.logical_not(opt.has_value()), data=[''], name='CHECK_EMPTY')\n",
        "  return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX0gtz5Ylvqf"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "weights = np.random.random(\n",
        "    size=(MAX_TOKENS_SELECTED_PER_CLIENT, TAG_VOCAB_SIZE)).astype(np.float32)\n",
        "model_slices_as_dataset = tf.data.Dataset.from_tensor_slices(weights)\n",
        "weights2 = slices_dataset_to_tensor(model_slices_as_dataset)\n",
        "np.testing.assert_array_equal(weights, weights2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft0ROTYl0laV"
      },
      "source": [
        "이제 각 클라이언트에서 실행할 간단한 로컬 훈련 루프를 정의하는 데 필요한 모든 구성 요소가 준비되었습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG66d8UR_9Gm"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def client_train_fn(model, client_optimizer,\n",
        "                    model_slices_as_dataset, client_data,\n",
        "                    client_keys, actual_num_tokens):\n",
        "  \n",
        "  initial_model_weights = slices_dataset_to_tensor(model_slices_as_dataset)\n",
        "  assert len(model.trainable_variables) == 1\n",
        "  model.trainable_variables[0].assign(initial_model_weights)\n",
        "\n",
        "  # Only keep the \"real\" (unpadded) keys.\n",
        "  client_keys = client_keys[:actual_num_tokens]\n",
        " \n",
        "  client_data = map_to_local_token_ids(client_data, client_keys)\n",
        "\n",
        "  loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "  for features, labels in client_data:\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(features)\n",
        "      loss = loss_fn(labels, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    client_optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  model_weights_delta = model.trainable_weights[0] - initial_model_weights\n",
        "  model_weights_delta = tf.slice(model_weights_delta, begin=[0, 0], \n",
        "                           size=[actual_num_tokens, -1])\n",
        "  return client_keys, model_weights_delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XOe0thb2TQr"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "# Note if you execute this cell a second time, you need to also re-execute\n",
        "# the preceeding cell to avoid \"tf.function-decorated function tried to \n",
        "# create variables on non-first call\" errors.\n",
        "on_device_model = create_logistic_model(MAX_TOKENS_SELECTED_PER_CLIENT,\n",
        "                                        TAG_VOCAB_SIZE)\n",
        "client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "client_keys, actual_num_tokens = keys_for_client(\n",
        "    batched_dataset2, MAX_TOKENS_SELECTED_PER_CLIENT)\n",
        "\n",
        "model_slices_as_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    np.zeros((MAX_TOKENS_SELECTED_PER_CLIENT, TAG_VOCAB_SIZE),\n",
        "             dtype=np.float32))\n",
        "\n",
        "keys, delta = client_train_fn(\n",
        "    on_device_model,\n",
        "    client_optimizer,\n",
        "    model_slices_as_dataset,\n",
        "    client_data=batched_dataset3,\n",
        "    client_keys=client_keys,\n",
        "    actual_num_tokens=actual_num_tokens)\n",
        "\n",
        "print(delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJFTBczKSYH"
      },
      "source": [
        "### IndexedSlice 집계하기\n",
        "\n",
        "`IndexedSlices`에 대한 페더레이션 희소 합계를 구성하기 위해 `tff.federated_aggregate`를 사용합니다. 이 간단한 구현에는 `dense_shape`가 사전에 정적으로 알려진다는 제약이 있습니다. 또한 이 합계는 클라이언트 -&gt; 서버 통신이 희소하지만 서버는 <code>accumulate</code> 및 `merge`에서 합계의 밀집된 표현을 유지관리하고 이 밀집된 표현을 출력한다는 점에서 단지 <em>반희소</em>입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIG_XYd2OYeO"
      },
      "outputs": [],
      "source": [
        "def federated_indexed_slices_sum(slice_indices, slice_values, dense_shape):\n",
        "  \"\"\"\n",
        "  Sumes IndexedSlices@CLIENTS to a dense @SERVER Tensor.\n",
        "\n",
        "  Intermediate aggregation is performed by converting to a dense representation,\n",
        "  which may not be suitable for all applications.\n",
        "\n",
        "  Args:\n",
        "    slice_indices: An IndexedSlices.indices tensor @CLIENTS.\n",
        "    slice_values: An IndexedSlices.values tensor @CLIENTS.\n",
        "    dense_shape: A statically known dense shape.\n",
        "\n",
        "  Returns:\n",
        "    A dense tensor placed @SERVER representing the sum of the client's\n",
        "    IndexedSclies.\n",
        "  \"\"\"\n",
        "  slices_dtype = slice_values.type_signature.member.dtype\n",
        "  zero = tff.tf_computation(\n",
        "      lambda: tf.zeros(dense_shape, dtype=slices_dtype))()\n",
        "\n",
        "  @tf.function\n",
        "  def accumulate_slices(dense, client_value):\n",
        "    indices, slices = client_value\n",
        "    # There is no built-in way to add `IndexedSlices`, but \n",
        "    # tf.convert_to_tensor is a quick way to convert to a dense representation\n",
        "    # so we can add them.\n",
        "    return dense + tf.convert_to_tensor(\n",
        "        tf.IndexedSlices(slices, indices, dense_shape))\n",
        "\n",
        "\n",
        "  return tff.federated_aggregate(\n",
        "      (slice_indices, slice_values),\n",
        "      zero=zero,\n",
        "      accumulate=tff.tf_computation(accumulate_slices),\n",
        "      merge=tff.tf_computation(lambda d1, d2: tf.add(d1, d2, name='merge')),\n",
        "      report=tff.tf_computation(lambda d: d))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "769qwlLhKFA9"
      },
      "source": [
        "테스트로 최소 `federated_computation`을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_FIvwj3UIAq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({<int64[?],float32[?,2]>}@CLIENTS -> float32[6,2]@SERVER)\n"
          ]
        }
      ],
      "source": [
        "dense_shape = (6, 2)\n",
        "indices_type = tff.TensorType(tf.int64, (None,))\n",
        "values_type = tff.TensorType(tf.float32, (None, 2))\n",
        "client_slice_type = tff.type_at_clients(\n",
        "    (indices_type, values_type))\n",
        "\n",
        "@tff.federated_computation(client_slice_type)\n",
        "def test_sum_indexed_slices(indices_values_at_client):\n",
        "  indices, values = indices_values_at_client\n",
        "  return federated_indexed_slices_sum(indices, values, dense_shape)\n",
        "\n",
        "print(test_sum_indexed_slices.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dulcCZg2S_9f"
      },
      "outputs": [],
      "source": [
        "x = tf.IndexedSlices(\n",
        "    values=np.array([[2., 2.1], [0., 0.1], [1., 1.1], [5., 5.1]],\n",
        "                    dtype=np.float32),\n",
        "    indices=[2, 0, 1, 5],\n",
        "    dense_shape=dense_shape)\n",
        "y = tf.IndexedSlices(\n",
        "    values=np.array([[0., 0.3], [3.1, 3.2]], dtype=np.float32),\n",
        "    indices=[1, 3],\n",
        "    dense_shape=dense_shape)\n",
        "\n",
        "# Sum one.\n",
        "result = test_sum_indexed_slices([(x.indices, x.values)])\n",
        "np.testing.assert_array_equal(tf.convert_to_tensor(x), result)\n",
        "\n",
        "# Sum two.\n",
        "expected = [[0., 0.1], [1., 1.4], [2., 2.1], [3.1, 3.2], [0., 0.], [5., 5.1]]\n",
        "result = test_sum_indexed_slices([(x.indices, x.values), (y.indices, y.values)])\n",
        "np.testing.assert_array_almost_equal(expected, result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sXg8fDQhwB7"
      },
      "source": [
        "# `federated_computation`에 모두 결합하기\n",
        "\n",
        "이제 TFF를 사용하여 구성 요소를 `tff.federated_computation`으로 결합합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yRpGJr_i3XK"
      },
      "outputs": [],
      "source": [
        "DENSE_MODEL_SHAPE = (WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\n",
        "client_data_type = tff.SequenceType(batched_dataset1.element_spec)\n",
        "model_type = tff.TensorType(tf.float32, shape=DENSE_MODEL_SHAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITawWBbodTdE"
      },
      "source": [
        "페더레이션 평균화 기반의 기본 서버 훈련 함수를 사용하여 서버 학습률 1.0으로 업데이트를 적용합니다. 클라이언트가 제공한 모델을 단순히 평균화하는 대신 모델에 업데이트(델타)를 적용하는 것이 중요합니다. 그렇지 않으면 주어진 모델 슬라이스가 주어진 라운드에 클라이언트에 의해 훈련되지 않으면 계수가 0이 될 수 있기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGgKTpnscdZI"
      },
      "outputs": [],
      "source": [
        "@tff.tf_computation\n",
        "def server_update(current_model_weights, update_sum, num_clients):\n",
        "  average_update = update_sum / num_clients\n",
        "  return current_model_weights + average_update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSFAmgzqBRHA"
      },
      "source": [
        "몇 가지 `tff.tf_computation` 구성 요소가 더 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soFQAmktUtYp"
      },
      "outputs": [],
      "source": [
        "# Function to select slices from the model weights in federated_select:\n",
        "select_fn = tff.tf_computation(\n",
        "    lambda model_weights, index: tf.gather(model_weights, index))\n",
        "\n",
        "\n",
        "# We need to wrap `client_train_fn` as a `tff.tf_computation`, making\n",
        "# sure we do any operations that might construct `tf.Variable`s outside\n",
        "# of the `tf.function` we are wrapping.\n",
        "@tff.tf_computation\n",
        "def client_train_fn_tff(model_slices_as_dataset, client_data, client_keys,\n",
        "                        actual_num_tokens):\n",
        "  # Note this is amaller than the global model, using\n",
        "  # MAX_TOKENS_SELECTED_PER_CLIENT which is much smaller than WORD_VOCAB_SIZE.\n",
        "  # W7e would like a model of size `actual_num_tokens`, but we\n",
        "  # can't build the model dynamically, so we will slice off the padded\n",
        "  # weights at the end.\n",
        "  client_model = create_logistic_model(MAX_TOKENS_SELECTED_PER_CLIENT,\n",
        "                                       TAG_VOCAB_SIZE)\n",
        "  client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "  return client_train_fn(client_model, client_optimizer,\n",
        "                         model_slices_as_dataset, client_data, client_keys,\n",
        "                         actual_num_tokens)\n",
        "\n",
        "@tff.tf_computation\n",
        "def keys_for_client_tff(client_data):\n",
        "  return keys_for_client(client_data, MAX_TOKENS_SELECTED_PER_CLIENT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftI2fCGNBJQ-"
      },
      "source": [
        "이제 모든 조각을 결합할 준비가 되었습니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM_2mzkgBOjl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<server_model=float32[13,4]@SERVER,client_data={<tokens=<indices=int64[?,2],values=int32[?],dense_shape=int64[2]>,tags=float32[?,4]>*}@CLIENTS> -> float32[13,4]@SERVER)\n"
          ]
        }
      ],
      "source": [
        "@tff.federated_computation(\n",
        "    tff.type_at_server(model_type), tff.type_at_clients(client_data_type))\n",
        "def sparse_model_update(server_model, client_data):\n",
        "  max_tokens = tff.federated_value(MAX_TOKENS_SELECTED_PER_CLIENT, tff.SERVER)\n",
        "  keys_at_clients, actual_num_tokens = tff.federated_map(\n",
        "      keys_for_client_tff, client_data)\n",
        "\n",
        "  model_slices = tff.federated_select(keys_at_clients, max_tokens, server_model,\n",
        "                                      select_fn)\n",
        "\n",
        "  update_keys, update_slices = tff.federated_map(\n",
        "      client_train_fn_tff,\n",
        "      (model_slices, client_data, keys_at_clients, actual_num_tokens))\n",
        "\n",
        "  dense_update_sum = federated_indexed_slices_sum(update_keys, update_slices,\n",
        "                                                  DENSE_MODEL_SHAPE)\n",
        "  num_clients = tff.federated_sum(tff.federated_value(1.0, tff.CLIENTS))\n",
        "\n",
        "  updated_server_model = tff.federated_map(\n",
        "      server_update, (server_model, dense_update_sum, num_clients))\n",
        "\n",
        "  return updated_server_model\n",
        "\n",
        "\n",
        "print(sparse_model_update.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSvveGwdZFCO"
      },
      "source": [
        "# 모델을 훈련시켜 보겠습니다!\n",
        "\n",
        "이제 훈련 함수가 생겼으니 시도해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn_V-E3FdrLO"
      },
      "outputs": [],
      "source": [
        "server_model = create_logistic_model(WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\n",
        "server_model.compile(  # Compile to make evaluation easy.\n",
        "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.0),  # Unused\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[ \n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.AUC(name='auc'),\n",
        "      tf.keras.metrics.Recall(top_k=2, name='recall_at_2'),\n",
        "  ])\n",
        "\n",
        "def evaluate(model, dataset, name):\n",
        "  metrics = model.evaluate(dataset, verbose=0)\n",
        "  metrics_str = ', '.join([f'{k}={v:.2f}' for k, v in \n",
        "                          (zip(server_model.metrics_names, metrics))])\n",
        "  print(f'{name}: {metrics_str}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw7S9PmfZ3Bw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before training\n",
            "Client 1: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.60\n",
            "Client 2: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.50\n",
            "Client 3: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.40\n",
            "Training on clients [0 1]\n",
            "Training on clients [0 2 1]\n",
            "Training on clients [2 0]\n",
            "Training on clients [1 0 2]\n",
            "Training on clients [2]\n",
            "Training on clients [2 0]\n",
            "Training on clients [1 2 0]\n",
            "Training on clients [0]\n",
            "Training on clients [2]\n",
            "Training on clients [1 2]\n",
            "After training\n",
            "Client 1: loss=0.67, precision=0.80, auc=0.91, recall_at_2=0.80\n",
            "Client 2: loss=0.68, precision=0.67, auc=0.96, recall_at_2=1.00\n",
            "Client 3: loss=0.65, precision=1.00, auc=0.93, recall_at_2=0.80\n"
          ]
        }
      ],
      "source": [
        "print('Before training')\n",
        "evaluate(server_model, batched_dataset1, 'Client 1')\n",
        "evaluate(server_model, batched_dataset2, 'Client 2')\n",
        "evaluate(server_model, batched_dataset3, 'Client 3')\n",
        "\n",
        "model_weights = server_model.trainable_weights[0]\n",
        "\n",
        "client_datasets = [batched_dataset1, batched_dataset2, batched_dataset3]\n",
        "for _ in range(10):  # Run 10 rounds of FedAvg\n",
        "  # We train on 1, 2, or 3 clients per round, selecting\n",
        "  # randomly.\n",
        "  cohort_size = np.random.randint(1, 4)\n",
        "  clients = np.random.choice([0, 1, 2], cohort_size, replace=False)\n",
        "  print('Training on clients', clients)\n",
        "  model_weights = sparse_model_update(\n",
        "      model_weights, [client_datasets[i] for i in clients])\n",
        "server_model.set_weights([model_weights])\n",
        "\n",
        "print('After training')\n",
        "evaluate(server_model, batched_dataset1, 'Client 1')\n",
        "evaluate(server_model, batched_dataset2, 'Client 2')\n",
        "evaluate(server_model, batched_dataset3, 'Client 3')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sparse_federated_learning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
