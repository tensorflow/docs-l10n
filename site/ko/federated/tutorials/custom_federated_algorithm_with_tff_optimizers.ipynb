{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkdnLiKk71g-"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0asMuNro71hA"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPFgLeZIsZ3Q"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/custom_federated_algorithm_with_tff_optimizers\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/federated/tutorials/custom_federated_algorithm_with_tff_optimizers.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/federated/tutorials/custom_federated_algorithm_with_tff_optimizers.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소그 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/federated/tutorials/custom_federated_algorithm_with_tff_optimizers.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zv28F7QLo8O"
      },
      "source": [
        "# 맞춤형 반복 프로세스에서 TFF 옵티마이저 사용\n",
        "\n",
        "이것은 [고유한 페더레이션 학습 알고리즘 빌드](building_your_own_federated_learning_algorithm.ipynb) 튜토리얼과 [페더레이션 평균화](https://github.com/tensorflow/federated/tree/main/tensorflow_federated/examples/simple_fedavg) 알고리즘에 대한 맞춤형 반복 프로세스를 빌드하기 위한 [simple_fedavg](https://arxiv.org/abs/1602.05629) 예제를 대체합니다. 이 튜토리얼에서는 Keras 옵티마이저 대신 [TFF 옵티마이저](https://github.com/tensorflow/federated/tree/main/tensorflow_federated/python/learning/optimizers)를 사용합니다. TFF 옵티마이저 추상화는 TFF 반복 프로세스에 더 쉽게 도입하도록 state-in-state-out으로 설계되었습니다. `tff.learning` API는 또한 TFF 옵티마이저를 입력 인수로 허용합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnUwFbCAKB2r"
      },
      "source": [
        "## 시작하기 전에\n",
        "\n",
        "시작하기 전에 다음을 실행하여 환경이 올바르게 설정되었는지 확인하세요. 인사말이 표시되지 않으면 [설치](../install.md) 가이드에서 지침을 참조하세요. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrGitA_KnRO0"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGTM6tWOLo8M"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "import functools\n",
        "import attrs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ_N9XbULo8P"
      },
      "source": [
        "## 데이터 및 모델 준비하기\n",
        "\n",
        "EMNIST 데이터 처리 및 모델은 [simple_fedavg](https://github.com/tensorflow/federated/tree/main/tensorflow_federated/examples/simple_fedavg) 예제와 매우 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Blrh8zJgLo8R"
      },
      "outputs": [],
      "source": [
        "only_digits=True\n",
        "\n",
        "# Load dataset.\n",
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(only_digits)\n",
        "\n",
        "# Define preprocessing functions.\n",
        "def preprocess_fn(dataset, batch_size=16):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "    return (tf.expand_dims(element['pixels'], -1), element['label'])\n",
        "\n",
        "  return dataset.batch(batch_size).map(batch_format_fn)\n",
        "\n",
        "# Preprocess and sample clients for prototyping.\n",
        "train_client_ids = sorted(emnist_train.client_ids)\n",
        "train_data = emnist_train.preprocess(preprocess_fn)\n",
        "central_test_data = preprocess_fn(\n",
        "    emnist_train.create_tf_dataset_for_client(train_client_ids[0]))\n",
        "\n",
        "# Define model.\n",
        "def create_keras_model():\n",
        "  \"\"\"The CNN model used in https://arxiv.org/abs/1602.05629.\"\"\"\n",
        "  data_format = 'channels_last'\n",
        "  input_shape = [28, 28, 1]\n",
        "\n",
        "  max_pool = functools.partial(\n",
        "      tf.keras.layers.MaxPooling2D,\n",
        "      pool_size=(2, 2),\n",
        "      padding='same',\n",
        "      data_format=data_format)\n",
        "  conv2d = functools.partial(\n",
        "      tf.keras.layers.Conv2D,\n",
        "      kernel_size=5,\n",
        "      padding='same',\n",
        "      data_format=data_format,\n",
        "      activation=tf.nn.relu)\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "      conv2d(filters=32, input_shape=input_shape),\n",
        "      max_pool(),\n",
        "      conv2d(filters=64),\n",
        "      max_pool(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "      tf.keras.layers.Dense(10 if only_digits else 62),\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Wrap as `tff.learning.models.VariableModel`.\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=central_test_data.element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPOWP2JjsfTk"
      },
      "source": [
        "## 맞춤형 반복 프로세스\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50N36Zz8qyY-"
      },
      "source": [
        "많은 경우 페더레이션 알고리즘에는 4가지 주요 구성 요소가 있습니다.\n",
        "\n",
        "1. 서버-클라이언트 브로드 캐스트 단계.\n",
        "2. 로컬 클라이언트 업데이트 단계.\n",
        "3. 클라이언트-서버 업로드 단계.\n",
        "4. 서버 업데이트 단계.\n",
        "\n",
        "TFF에서는 일반적으로 페더레이션 알고리즘을 [`tff.templates.IterativeProcess`](https://www.tensorflow.org/federated/api_docs/python/tff/templates/IterativeProcess)(나머지 부분에서 `IterativeProcess`라고 함)로 나타냅니다. 이것은 `initialize` 및 `next` 함수를 포함하는 클래스입니다. 여기서 `initialize`는 서버를 초기화하는 데 사용되며 `next`는 페더레이션 알고리즘의 한 통신 라운드를 수행합니다.\n",
        "\n",
        "클라이언트 업데이트 단계에서 옵티마이저를 사용하고 서버 업데이트 단계에서 또 다른 옵티마이저를 사용하는 페더레이션 평균화(FedAvg) 알고리즘을 빌드하기 위해 다양한 구성 요소를 도입합니다. 클라이언트 및 서버 업데이트의 핵심 로직은 순수 TF 블록으로 표현될 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxpNYucgLo8g"
      },
      "source": [
        "### TF 블록: 클라이언트 및 서버 업데이트\n",
        "\n",
        "각 클라이언트에서 로컬 `client_optimizer`가 초기화되고 클라이언트 모델 가중치를 업데이트하는 데 사용됩니다. 서버에서 `server_optimizer`는 *이전* 라운드의 상태를 사용하고 다음 라운드의 상태를 업데이트합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5rHPKreLo8g"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def client_update(model, dataset, server_weights, client_optimizer):\n",
        "  \"\"\"Performs local training on the client's dataset.\"\"\"\n",
        "  # Initialize the client model with the current server weights.\n",
        "  client_weights = model.trainable_variables\n",
        "  # Assign the server weights to the client model.\n",
        "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
        "                        client_weights, server_weights)\n",
        "  # Initialize the client optimizer.\n",
        "  trainable_tensor_specs = tf.nest.map_structure(\n",
        "          lambda v: tf.TensorSpec(v.shape, v.dtype), client_weights)\n",
        "  optimizer_state = client_optimizer.initialize(trainable_tensor_specs)\n",
        "  # Use the client_optimizer to update the local model.\n",
        "  for batch in iter(dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Compute a forward pass on the batch of data.\n",
        "      outputs = model.forward_pass(batch)\n",
        "    # Compute the corresponding gradient.\n",
        "    grads = tape.gradient(outputs.loss, client_weights)\n",
        "    # Apply the gradient using a client optimizer.\n",
        "    optimizer_state, updated_weights = client_optimizer.next(\n",
        "        optimizer_state, client_weights, grads)\n",
        "    tf.nest.map_structure(lambda a, b: a.assign(b), \n",
        "                          client_weights, updated_weights)\n",
        "  # Return model deltas.\n",
        "  return tf.nest.map_structure(tf.subtract, client_weights, server_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYxErLvHLo8i"
      },
      "outputs": [],
      "source": [
        "@attrs.define(eq=False, frozen=True)\n",
        "class ServerState(object):\n",
        "  trainable_weights: Any\n",
        "  optimizer_state: Any\n",
        "\n",
        "@tf.function\n",
        "def server_update(server_state, mean_model_delta, server_optimizer):\n",
        "  \"\"\"Updates the server model weights.\"\"\"\n",
        "  # Use aggregated negative model delta as pseudo gradient. \n",
        "  negative_weights_delta = tf.nest.map_structure(\n",
        "      lambda w: -1.0 * w, mean_model_delta)\n",
        "  new_optimizer_state, updated_weights = server_optimizer.next(\n",
        "      server_state.optimizer_state, server_state.trainable_weights, \n",
        "      negative_weights_delta)\n",
        "  return tff.structure.update_struct(\n",
        "      server_state,\n",
        "      trainable_weights=updated_weights,\n",
        "      optimizer_state=new_optimizer_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0zNTO7LLo84"
      },
      "source": [
        "### TFF 블록: `tff.tf_computation` 및 `tff.federated_computation`\n",
        "\n",
        "이제 오케스트레이션에 TFF를 사용하고 FedAvg에 대한 반복 프로세스를 빌드합니다. 위에서 정의한 TF 블록을 `tff.tf_computation`으로 래핑하고 `tff.federated_computation` 함수에서 TFF 메서드 `tff.federated_broadcast`, `tff.federated_map`, `tff.federated_mean`을 사용해야 합니다. 맞춤형 반복 프로세스를 정의할 때 `initialize` 및 `next` 함수와 함께 `tff.learning.optimizers.Optimizer` API를 쉽게 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJY9xUBZLo84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "server_state_type:\n",
            " <\n",
            "  trainable_weights=<\n",
            "    float32[5,5,1,32],\n",
            "    float32[32],\n",
            "    float32[5,5,32,64],\n",
            "    float32[64],\n",
            "    float32[3136,512],\n",
            "    float32[512],\n",
            "    float32[512,10],\n",
            "    float32[10]\n",
            "  >,\n",
            "  optimizer_state=<\n",
            "    float32[5,5,1,32],\n",
            "    float32[32],\n",
            "    float32[5,5,32,64],\n",
            "    float32[64],\n",
            "    float32[3136,512],\n",
            "    float32[512],\n",
            "    float32[512,10],\n",
            "    float32[10]\n",
            "  >\n",
            ">\n",
            "trainable_weights_type:\n",
            " <\n",
            "  float32[5,5,1,32],\n",
            "  float32[32],\n",
            "  float32[5,5,32,64],\n",
            "  float32[64],\n",
            "  float32[3136,512],\n",
            "  float32[512],\n",
            "  float32[512,10],\n",
            "  float32[10]\n",
            ">\n",
            "tf_dataset_type:\n",
            " <\n",
            "  float32[?,28,28,1],\n",
            "  int32[?]\n",
            ">*\n",
            "type signature of `initialize`:\n",
            " ( -> <\n",
            "  trainable_weights=<\n",
            "    float32[5,5,1,32],\n",
            "    float32[32],\n",
            "    float32[5,5,32,64],\n",
            "    float32[64],\n",
            "    float32[3136,512],\n",
            "    float32[512],\n",
            "    float32[512,10],\n",
            "    float32[10]\n",
            "  >,\n",
            "  optimizer_state=<\n",
            "    float32[5,5,1,32],\n",
            "    float32[32],\n",
            "    float32[5,5,32,64],\n",
            "    float32[64],\n",
            "    float32[3136,512],\n",
            "    float32[512],\n",
            "    float32[512,10],\n",
            "    float32[10]\n",
            "  >\n",
            ">@SERVER)\n",
            "type signature of `next`:\n",
            " (<\n",
            "  server_state=<\n",
            "    trainable_weights=<\n",
            "      float32[5,5,1,32],\n",
            "      float32[32],\n",
            "      float32[5,5,32,64],\n",
            "      float32[64],\n",
            "      float32[3136,512],\n",
            "      float32[512],\n",
            "      float32[512,10],\n",
            "      float32[10]\n",
            "    >,\n",
            "    optimizer_state=<\n",
            "      float32[5,5,1,32],\n",
            "      float32[32],\n",
            "      float32[5,5,32,64],\n",
            "      float32[64],\n",
            "      float32[3136,512],\n",
            "      float32[512],\n",
            "      float32[512,10],\n",
            "      float32[10]\n",
            "    >\n",
            "  >@SERVER,\n",
            "  federated_dataset={<\n",
            "    float32[?,28,28,1],\n",
            "    int32[?]\n",
            "  >*}@CLIENTS\n",
            "> -> <\n",
            "  trainable_weights=<\n",
            "    float32[5,5,1,32],\n",
            "    float32[32],\n",
            "    float32[5,5,32,64],\n",
            "    float32[64],\n",
            "    float32[3136,512],\n",
            "    float32[512],\n",
            "    float32[512,10],\n",
            "    float32[10]\n",
            "  >,\n",
            "  optimizer_state=<\n",
            "    float32[5,5,1,32],\n",
            "    float32[32],\n",
            "    float32[5,5,32,64],\n",
            "    float32[64],\n",
            "    float32[3136,512],\n",
            "    float32[512],\n",
            "    float32[512,10],\n",
            "    float32[10]\n",
            "  >\n",
            ">@SERVER)\n"
          ]
        }
      ],
      "source": [
        "# 1. Server and client optimizer to be used.\n",
        "server_optimizer = tff.learning.optimizers.build_sgdm(\n",
        "    learning_rate=0.05, momentum=0.9)\n",
        "client_optimizer = tff.learning.optimizers.build_sgdm(\n",
        "    learning_rate=0.01)\n",
        "\n",
        "# 2. Functions return initial state on server. \n",
        "@tff.tf_computation\n",
        "def server_init():\n",
        "  model = model_fn()\n",
        "  trainable_tensor_specs = tf.nest.map_structure(\n",
        "        lambda v: tf.TensorSpec(v.shape, v.dtype), model.trainable_variables)\n",
        "  optimizer_state = server_optimizer.initialize(trainable_tensor_specs)\n",
        "  return ServerState(\n",
        "      trainable_weights=model.trainable_variables,\n",
        "      optimizer_state=optimizer_state)\n",
        "\n",
        "@tff.federated_computation\n",
        "def server_init_tff():\n",
        "  return tff.federated_value(server_init(), tff.SERVER)\n",
        "\n",
        "# 3. One round of computation and communication.\n",
        "server_state_type = server_init.type_signature.result\n",
        "print('server_state_type:\\n', \n",
        "      server_state_type.formatted_representation())\n",
        "trainable_weights_type = server_state_type.trainable_weights\n",
        "print('trainable_weights_type:\\n', \n",
        "      trainable_weights_type.formatted_representation())\n",
        "\n",
        "# 3-1. Wrap server and client TF blocks with `tff.tf_computation`.\n",
        "@tff.tf_computation(server_state_type, trainable_weights_type)\n",
        "def server_update_fn(server_state, model_delta):\n",
        "  return server_update(server_state, model_delta, server_optimizer)\n",
        "\n",
        "whimsy_model = model_fn()\n",
        "tf_dataset_type = tff.SequenceType(whimsy_model.input_spec)\n",
        "print('tf_dataset_type:\\n', \n",
        "      tf_dataset_type.formatted_representation())\n",
        "@tff.tf_computation(tf_dataset_type, trainable_weights_type)\n",
        "def client_update_fn(dataset, server_weights):\n",
        "  model = model_fn()\n",
        "  return client_update(model, dataset, server_weights, client_optimizer)\n",
        "\n",
        "# 3-2. Orchestration with `tff.federated_computation`.\n",
        "federated_server_type = tff.FederatedType(server_state_type, tff.SERVER)\n",
        "federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
        "@tff.federated_computation(federated_server_type, federated_dataset_type)\n",
        "def run_one_round(server_state, federated_dataset):\n",
        "  # Server-to-client broadcast.\n",
        "  server_weights_at_client = tff.federated_broadcast(\n",
        "      server_state.trainable_weights)\n",
        "  # Local client update.\n",
        "  model_deltas = tff.federated_map(\n",
        "      client_update_fn, (federated_dataset, server_weights_at_client))\n",
        "  # Client-to-server upload and aggregation.\n",
        "  mean_model_delta = tff.federated_mean(model_deltas)\n",
        "  # Server update.\n",
        "  server_state = tff.federated_map(\n",
        "      server_update_fn, (server_state, mean_model_delta))\n",
        "  return server_state\n",
        "\n",
        "# 4. Build the iterative process for FedAvg.\n",
        "fedavg_process = tff.templates.IterativeProcess(\n",
        "    initialize_fn=server_init_tff, next_fn=run_one_round)\n",
        "print('type signature of `initialize`:\\n', \n",
        "      fedavg_process.initialize.type_signature.formatted_representation())\n",
        "print('type signature of `next`:\\n', \n",
        "      fedavg_process.next.type_signature.formatted_representation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UYZ3qeMLo9N"
      },
      "source": [
        "## 알고리즘 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwd9Gs0ULo9O"
      },
      "source": [
        "중앙 집중식 평가 데이터세트에서 성능을 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdNgYoIwLo9P"
      },
      "outputs": [],
      "source": [
        "def evaluate(server_state):\n",
        "  keras_model = create_keras_model()\n",
        "  tf.nest.map_structure(\n",
        "      lambda var, t: var.assign(t),\n",
        "      keras_model.trainable_weights, server_state.trainable_weights)\n",
        "  metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "  for batch in iter(central_test_data):\n",
        "    preds = keras_model(batch[0], training=False)\n",
        "    metric.update_state(y_true=batch[1], y_pred=preds)\n",
        "  return metric.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDarZn71G2mH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial test accuracy 0.09677419\n",
            "Test accuracy 0.13978495\n"
          ]
        }
      ],
      "source": [
        "server_state = fedavg_process.initialize()\n",
        "acc = evaluate(server_state)\n",
        "print('Initial test accuracy', acc)\n",
        "\n",
        "# Evaluate after a few rounds\n",
        "CLIENTS_PER_ROUND=2\n",
        "sampled_clients = train_client_ids[:CLIENTS_PER_ROUND]\n",
        "sampled_train_data = [\n",
        "    train_data.create_tf_dataset_for_client(client)\n",
        "    for client in sampled_clients]\n",
        "for round in range(20):\n",
        "  server_state = fedavg_process.next(server_state, sampled_train_data)\n",
        "acc = evaluate(server_state)\n",
        "print('Test accuracy', acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "custom_federated_algorithm_with_tff_optimizers.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
