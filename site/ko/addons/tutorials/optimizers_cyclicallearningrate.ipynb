{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# TensorFlow 애드온 최적화 도구: CyclicalLearningRate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/optimizers_cyclicallearningrate\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/addons/tutorials/optimizers_cyclicallearningrate.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/addons/tutorials/optimizers_cyclicallearningrate.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "      <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/addons/tutorials/optimizers_cyclicallearningrate.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## 개요\n",
        "\n",
        "이 튜토리얼은 Addons 패키지에서 Cyclical Learning Rate를 사용하는 방법을 보여줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqEImEhBJWFv"
      },
      "source": [
        "## 주기적 학습률\n",
        "\n",
        "신경망에 대한 훈련이 진행됨에 따라 학습률을 조정하는 것이 유익한 것으로 나타났습니다. 안장점 복구에서 역전파 동안 발생할 수 있는 수치적 불안정성 방지에 이르기까지 다양한 이점이 있습니다. 그러나 특정 훈련 타임스탬프와 관련하여 얼마나 조정해야 하는지 어떻게 알 수 있습니까? 2015년에 Leslie Smith는 손실 환경에서 더 빠르게 탐색하기 위해 학습률을 높이고 싶지만 수렴에 접근할 때 학습률을 낮추고 싶어한다는 사실을 알아냈습니다. 이 아이디어를 실현하기 위해 그는 함수의 주기에 대해 학습률을 조정하는 CLR( [Cyclical Learning Rates)을 제안했습니다.](https://arxiv.org/abs/1506.01186) 시각적 데모를 보려면 [이 블로그를](https://www.jeremyjordan.me/nn-learning-rate/) 확인하세요. 이제 CLR을 TensorFlow API로 사용할 수 있습니다. [자세한 내용은 여기](https://arxiv.org/abs/1506.01186) 에서 원본 문서를 확인하십시오. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-p545dluzjI"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPF3aDZZu8le"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLOnLLrlR-ti"
      },
      "source": [
        "## 데이터세트 로드 및 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAHLo_Ffvie3"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfUS_s-uSBvx"
      },
      "source": [
        "## 초매개변수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qumJ7KpwvvwE"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "INIT_LR = 1e-4\n",
        "MAX_LR = 1e-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-x3E7RWSXWc"
      },
      "source": [
        "## 모델 구축 및 모델 교육 유틸리티 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vni6Gz3Dv9Db"
      },
      "outputs": [],
      "source": [
        "def get_training_model():\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            layers.InputLayer((28, 28, 1)),\n",
        "            layers.experimental.preprocessing.Rescaling(scale=1./255),\n",
        "            layers.Conv2D(16, (5, 5), activation=\"relu\"),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Conv2D(32, (5, 5), activation=\"relu\"),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.SpatialDropout2D(0.2),\n",
        "            layers.GlobalAvgPool2D(),\n",
        "            layers.Dense(128, activation=\"relu\"),\n",
        "            layers.Dense(10, activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train_model(model, optimizer):\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "                       metrics=[\"accuracy\"])\n",
        "    history = model.fit(x_train,\n",
        "        y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(x_test, y_test),\n",
        "        epochs=EPOCHS)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlRKWRWrSk_t"
      },
      "source": [
        "재현성을 위해 실험을 수행하는 데 사용할 초기 모델 가중치가 직렬화됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JxnpsIzwCgj"
      },
      "outputs": [],
      "source": [
        "initial_model = get_training_model()\n",
        "initial_model.save(\"initial_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNF33-tBSuFG"
      },
      "source": [
        "## CLR 없이 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4dEJtQzwjei"
      },
      "outputs": [],
      "source": [
        "standard_model = tf.keras.models.load_model(\"initial_model\")\n",
        "no_clr_history = train_model(standard_model, optimizer=\"sgd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK0PAN-Sy6l"
      },
      "source": [
        "## CLR 일정 정의\n",
        "\n",
        "`tfa.optimizers.CyclicalLearningRate` 모듈은 최적화 프로그램에 전달할 수 있는 직접 일정을 반환합니다. 일정은 입력으로 단계를 취하고 논문에 제시된 대로 CLR 공식을 사용하여 계산된 값을 출력합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne0b8aGNyc3v"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = len(x_train) // BATCH_SIZE\n",
        "clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=INIT_LR,\n",
        "    maximal_learning_rate=MAX_LR,\n",
        "    scale_fn=lambda x: 1/(2.**(x-1)),\n",
        "    step_size=2 * steps_per_epoch\n",
        ")\n",
        "optimizer = tf.keras.optimizers.SGD(clr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icVL3hsUTwXV"
      },
      "source": [
        "여기에서 학습률의 하한과 상한을 지정하면 일정이 해당 범위(이 경우 [1e-4, 1e-2]) 사이에서 *진동합니다.* `scale_fn` 은 주어진 주기 내에서 학습률을 확장 및 축소하는 함수를 정의하는 데 사용됩니다. `step_size` 는 단일 주기의 지속 시간을 정의합니다. 2의 `step_size` 는 한 주기를 완료하기 위해 총 4번의 반복이 필요함을 의미합니다. `step_size` 의 권장 값은 다음과 같습니다.\n",
        "\n",
        "`factor * steps_per_epoch` 여기서 factor는 [2, 8] 범위 내에 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JV_ESYqUb4d"
      },
      "source": [
        "같은 [CLR 논문](https://arxiv.org/abs/1506.01186) 에서 Leslie는 학습률의 경계를 선택하는 간단하고 우아한 방법도 제시했습니다. 당신도 그것을 확인하는 것이 좋습니다. [이 블로그 게시물](https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/) 은 방법에 대한 좋은 소개를 제공합니다.\n",
        "\n",
        "`clr` 일정이 어떻게 보이는지 시각화합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_WRGfDx4Wwc"
      },
      "outputs": [],
      "source": [
        "step = np.arange(0, EPOCHS * steps_per_epoch)\n",
        "lr = clr(step)\n",
        "plt.plot(step, lr)\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBlKaAqNjHP1"
      },
      "source": [
        "CLR의 효과를 더 잘 시각화하기 위해 증가된 단계 수로 일정을 그릴 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjhoyk-Li368"
      },
      "outputs": [],
      "source": [
        "step = np.arange(0, 100 * steps_per_epoch)\n",
        "lr = clr(step)\n",
        "plt.plot(step, lr)\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObYcy5NRkF4V"
      },
      "source": [
        "이 자습서에서 사용하는 함수는 CLR 문서에서 `triangular2` `triangular` 과 `exp` (지수의 줄임말)라는 다른 두 가지 기능이 탐색되었습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OV_8QVIe5m_"
      },
      "source": [
        "## CLR을 사용하여 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRSglElvy_fF"
      },
      "outputs": [],
      "source": [
        "clr_model = tf.keras.models.load_model(\"initial_model\")\n",
        "clr_history = train_model(clr_model, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rhTLQJdnGfP"
      },
      "source": [
        "예상대로 손실은 평소보다 높게 시작한 다음 사이클이 진행됨에 따라 안정화됩니다. 아래 도표를 통해 이를 시각적으로 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyHEgnv6e8lX"
      },
      "source": [
        "## 손실 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg0JjLwH2RTl"
      },
      "outputs": [],
      "source": [
        "(fig, ax) = plt.subplots(2, 1, figsize=(10, 8))\n",
        "\n",
        "ax[0].plot(no_clr_history.history[\"loss\"], label=\"train_loss\")\n",
        "ax[0].plot(no_clr_history.history[\"val_loss\"], label=\"val_loss\")\n",
        "ax[0].set_title(\"No CLR\")\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].set_ylim([0, 2.5])\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(clr_history.history[\"loss\"], label=\"train_loss\")\n",
        "ax[1].plot(clr_history.history[\"val_loss\"], label=\"val_loss\")\n",
        "ax[1].set_title(\"CLR\")\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].set_ylim([0, 2.5])\n",
        "ax[1].legend()\n",
        "\n",
        "fig.tight_layout(pad=3.0)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EwZuz_pkqLM"
      },
      "source": [
        "이 장난감 예제에서는 CLR의 효과를 많이 보지 못했지만 [Super Convergence](https://arxiv.org/abs/1708.07120) 의 주요 구성 요소 중 하나이며 대규모 설정에서 훈련할 때 [정말 좋은 영향을 미칠](https://www.fast.ai/2018/08/10/fastai-diu-imagenet/) 수 있다는 점에 주목했습니다. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "optimizers_cyclicallearningrate.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
