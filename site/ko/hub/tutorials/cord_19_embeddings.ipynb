{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wFF5JFyD2Ki"
      },
      "source": [
        "#### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf6NouXxDqGk"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORy-KvWXGXBo"
      },
      "source": [
        "# TF-Hub CORD-19 Swivel 임베딩 살펴보기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/cord_19_embeddings\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/cord_19_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/cord_19_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/hub/tutorials/cord_19_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Download notebook</a>\n",
        "  </td>\n",
        "  <td><a href=\"https://tfhub.dev/tensorflow/cord-19/swivel-128d/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub 모델보기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VusdTAH0isl"
      },
      "source": [
        "TF-Hub(https://tfhub.dev/tensorflow/cord-19/swivel-128d/1)의 CORD-19 Swivel 텍스트 임베딩 모듈은 연구원들이 코로나바이러스감염증-19와 관련된 자연어 텍스트를 분석할 수 있도록 빌드되었습니다. 이러한 임베딩은 [CORD-19 데이터세트](https://pages.semanticscholar.org/coronavirus-research)에 있는 기사의 제목, 저자, 요약문, 본문 텍스트 및 참조 제목에 대해 훈련되었습니다.\n",
        "\n",
        "이 colab에서는 다음을 수행합니다.\n",
        "\n",
        "- 임베딩 공간에서 의미론적으로 유사한 단어를 분석합니다.\n",
        "- CORD-19 임베딩을 사용하여 SciCite 데이터세트에서 분류자를 훈련합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L69VQv2Z0isl"
      },
      "source": [
        "## 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym2nXOPuPV__"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "tf.logging.set_verbosity('ERROR')\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "try:\n",
        "  from google.colab import data_table\n",
        "  def display_df(df):\n",
        "    return data_table.DataTable(df, include_index=False)\n",
        "except ModuleNotFoundError:\n",
        "  # If google-colab is not available, just display the raw DataFrame\n",
        "  def display_df(df):\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VgRRf2I7tER"
      },
      "source": [
        "# 임베딩 분석하기\n",
        "\n",
        "서로 다른 용어 간의 상관 행렬을 계산하고 플롯하여 임베딩을 분석하는 것으로 시작하겠습니다. 임베딩이 여러 단어의 의미를 성공적으로 포착하는 방법을 학습한 경우, 의미론적으로 유사한 단어의 임베딩 벡터는 서로 가까워야 합니다. 코로나바이러스감염증-19와 관련된 일부 용어를 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNN_9bBKSLHU"
      },
      "outputs": [],
      "source": [
        "# Use the inner product between two embedding vectors as the similarity measure\n",
        "def plot_correlation(labels, features):\n",
        "  corr = np.inner(features, features)\n",
        "  corr /= np.max(corr)\n",
        "  sns.heatmap(corr, xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  # Load the module\n",
        "  query_input = tf.placeholder(tf.string)\n",
        "  module = hub.Module('https://tfhub.dev/tensorflow/cord-19/swivel-128d/1')\n",
        "  embeddings = module(query_input)\n",
        "\n",
        "  with tf.train.MonitoredTrainingSession() as sess:\n",
        "\n",
        "    # Generate embeddings for some terms\n",
        "    queries = [\n",
        "        # Related viruses\n",
        "        \"coronavirus\", \"SARS\", \"MERS\",\n",
        "        # Regions\n",
        "        \"Italy\", \"Spain\", \"Europe\",\n",
        "        # Symptoms\n",
        "        \"cough\", \"fever\", \"throat\"\n",
        "    ]\n",
        "\n",
        "    features = sess.run(embeddings, feed_dict={query_input: queries})\n",
        "    plot_correlation(queries, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg-PGqtm8B7K"
      },
      "source": [
        "임베딩이 여러 용어의 의미를 성공적으로 포착했음을 알 수 있습니다. 각 단어는 해당 클러스터의 다른 단어와 유사하지만(즉, \"coronavirus\"는 \"SARS\" 및 \"MERS\"와 높은 상관 관계가 있음) 다른 클러스터의 용어와는 다릅니다(즉, \"SARS\"와 \"Spain\" 사이의 유사성은 0에 가까움).\n",
        "\n",
        "이제 이러한 임베딩을 사용하여 특정 작업을 해결하는 방법을 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idJ1jFmH7xMa"
      },
      "source": [
        "## SciCite: 인용 의도 분류\n",
        "\n",
        "이 섹션에서는 텍스트 분류와 같은 다운스트림 작업에 임베딩을 사용하는 방법을 보여줍니다. TensorFlow 데이터세트의 [SciCite 데이터세트](https://www.tensorflow.org/datasets/catalog/scicite)를 사용하여 학술 논문에서 인용 의도를 분류합니다. 학술 논문의 인용이 포함된 문장이 주어지면 인용의 주요 의도가 배경 정보, 방법 사용 또는 결과 비교인지 여부를 분류합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-FB19HLfVp2V"
      },
      "outputs": [],
      "source": [
        "#@title Set up the dataset from TFDS\n",
        "\n",
        "class Dataset:\n",
        "  \"\"\"Build a dataset from a TFDS dataset.\"\"\"\n",
        "  def __init__(self, tfds_name, feature_name, label_name):\n",
        "    self.dataset_builder = tfds.builder(tfds_name)\n",
        "    self.dataset_builder.download_and_prepare()\n",
        "    self.feature_name = feature_name\n",
        "    self.label_name = label_name\n",
        "  \n",
        "  def get_data(self, for_eval):\n",
        "    splits = THE_DATASET.dataset_builder.info.splits\n",
        "    if tfds.Split.TEST in splits:\n",
        "      split = tfds.Split.TEST if for_eval else tfds.Split.TRAIN\n",
        "    else:\n",
        "      SPLIT_PERCENT = 80\n",
        "      split = \"train[{}%:]\".format(SPLIT_PERCENT) if for_eval else \"train[:{}%]\".format(SPLIT_PERCENT)\n",
        "    return self.dataset_builder.as_dataset(split=split)\n",
        "\n",
        "  def num_classes(self):\n",
        "    return self.dataset_builder.info.features[self.label_name].num_classes\n",
        "\n",
        "  def class_names(self):\n",
        "    return self.dataset_builder.info.features[self.label_name].names\n",
        "\n",
        "  def preprocess_fn(self, data):\n",
        "    return data[self.feature_name], data[self.label_name]\n",
        "\n",
        "  def example_fn(self, data):\n",
        "    feature, label = self.preprocess_fn(data)\n",
        "    return {'feature': feature, 'label': label}, label\n",
        "\n",
        "\n",
        "def get_example_data(dataset, num_examples, **data_kw):\n",
        "  \"\"\"Show example data\"\"\"\n",
        "  with tf.Session() as sess:\n",
        "    batched_ds = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples)\n",
        "    it = tf.data.make_one_shot_iterator(batched_ds).get_next()\n",
        "    data = sess.run(it)\n",
        "  return data\n",
        "\n",
        "\n",
        "TFDS_NAME = 'scicite' #@param {type: \"string\"}\n",
        "TEXT_FEATURE_NAME = 'string' #@param {type: \"string\"}\n",
        "LABEL_NAME = 'label' #@param {type: \"string\"}\n",
        "THE_DATASET = Dataset(TFDS_NAME, TEXT_FEATURE_NAME, LABEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CVjyBD0ZPh4Z"
      },
      "outputs": [],
      "source": [
        "#@title Let's take a look at a few labeled examples from the training set\n",
        "NUM_EXAMPLES = 20  #@param {type:\"integer\"}\n",
        "data = get_example_data(THE_DATASET, NUM_EXAMPLES, for_eval=False)\n",
        "display_df(\n",
        "    pd.DataFrame({\n",
        "        TEXT_FEATURE_NAME: [ex.decode('utf8') for ex in data[0]],\n",
        "        LABEL_NAME: [THE_DATASET.class_names()[x] for x in data[1]]\n",
        "    }))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65s9UpYJ_1ct"
      },
      "source": [
        "## 인용 의도 분류자 훈련하기\n",
        "\n",
        "Estimator를 사용하여 [SciCite 데이터세트](https://www.tensorflow.org/datasets/catalog/scicite)에 대한 분류자를 훈련합니다. 데이터세트를 모델로 읽어들이도록 input_fns를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "OldapWmKSGsW"
      },
      "outputs": [],
      "source": [
        "def preprocessed_input_fn(for_eval):\n",
        "  data = THE_DATASET.get_data(for_eval=for_eval)\n",
        "  data = data.map(THE_DATASET.example_fn, num_parallel_calls=1)\n",
        "  return data\n",
        "\n",
        "\n",
        "def input_fn_train(params):\n",
        "  data = preprocessed_input_fn(for_eval=False)\n",
        "  data = data.repeat(None)\n",
        "  data = data.shuffle(1024)\n",
        "  data = data.batch(batch_size=params['batch_size'])\n",
        "  return data\n",
        "\n",
        "\n",
        "def input_fn_eval(params):\n",
        "  data = preprocessed_input_fn(for_eval=True)\n",
        "  data = data.repeat(1)\n",
        "  data = data.batch(batch_size=params['batch_size'])\n",
        "  return data\n",
        "\n",
        "\n",
        "def input_fn_predict(params):\n",
        "  data = preprocessed_input_fn(for_eval=True)\n",
        "  data = data.batch(batch_size=params['batch_size'])\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcrmWUkVKg2u"
      },
      "source": [
        "분류 레이어가 상위에 놓인 CORD-19 임베딩을 사용하는 모델을 빌드하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff0uKqJCA9zh"
      },
      "outputs": [],
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "  # Embed the text\n",
        "  embed = hub.Module(params['module_name'], trainable=params['trainable_module'])\n",
        "  embeddings = embed(features['feature'])\n",
        "\n",
        "  # Add a linear layer on top\n",
        "  logits = tf.layers.dense(\n",
        "      embeddings, units=THE_DATASET.num_classes(), activation=None)\n",
        "  predictions = tf.argmax(input=logits, axis=1)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions={\n",
        "            'logits': logits,\n",
        "            'predictions': predictions,\n",
        "            'features': features['feature'],\n",
        "            'labels': features['label']\n",
        "        })\n",
        "  \n",
        "  # Set up a multi-class classification head\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      labels=labels, logits=logits)\n",
        "  loss = tf.reduce_mean(loss)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=params['learning_rate'])\n",
        "    train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "  elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
        "    precision = tf.metrics.precision(labels=labels, predictions=predictions)\n",
        "    recall = tf.metrics.recall(labels=labels, predictions=predictions)\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        loss=loss,\n",
        "        eval_metric_ops={\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yZUclu8xBYlj"
      },
      "outputs": [],
      "source": [
        "#@title Hyperparmeters { run: \"auto\" }\n",
        "\n",
        "EMBEDDING = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/1'  #@param {type: \"string\"}\n",
        "TRAINABLE_MODULE = False  #@param {type: \"boolean\"}\n",
        "STEPS =   8000#@param {type: \"integer\"}\n",
        "EVAL_EVERY = 200  #@param {type: \"integer\"}\n",
        "BATCH_SIZE = 10  #@param {type: \"integer\"}\n",
        "LEARNING_RATE = 0.01  #@param {type: \"number\"}\n",
        "\n",
        "params = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'module_name': EMBEDDING,\n",
        "    'trainable_module': TRAINABLE_MODULE\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weZKWK-pLBll"
      },
      "source": [
        "## 모델 훈련 및 평가하기\n",
        "\n",
        "SciCite 작업의 성능을 확인하기 위해 모델을 훈련하고 평가하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO1FWkZW2WS9"
      },
      "outputs": [],
      "source": [
        "estimator = tf.estimator.Estimator(functools.partial(model_fn, params=params))\n",
        "metrics = []\n",
        "\n",
        "for step in range(0, STEPS, EVAL_EVERY):\n",
        "  estimator.train(input_fn=functools.partial(input_fn_train, params=params), steps=EVAL_EVERY)\n",
        "  step_metrics = estimator.evaluate(input_fn=functools.partial(input_fn_eval, params=params))\n",
        "  print('Global step {}: loss {:.3f}, accuracy {:.3f}'.format(step, step_metrics['loss'], step_metrics['accuracy']))\n",
        "  metrics.append(step_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUNGAeyf1ygC"
      },
      "outputs": [],
      "source": [
        "global_steps = [x['global_step'] for x in metrics]\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(20,8))\n",
        "\n",
        "for axes_index, metric_names in enumerate([['accuracy', 'precision', 'recall'],\n",
        "                                            ['loss']]):\n",
        "  for metric_name in metric_names:\n",
        "    axes[axes_index].plot(global_steps, [x[metric_name] for x in metrics], label=metric_name)\n",
        "  axes[axes_index].legend()\n",
        "  axes[axes_index].set_xlabel(\"Global Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1biWylvB6ayg"
      },
      "source": [
        "특히 정확성이 빠르게 증가하는 동안 손실이 빠르게 감소하는 것을 볼 수 있습니다. 예측이 실제 레이블과 어떻게 관련되는지 확인하기 위해 몇 가지 예를 플롯해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK_NJXtoyG2o"
      },
      "outputs": [],
      "source": [
        "predictions = estimator.predict(functools.partial(input_fn_predict, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlxFER_Oriam"
      },
      "outputs": [],
      "source": [
        "first_10_predictions = list(itertools.islice(predictions, 10))\n",
        "\n",
        "display_df(\n",
        "  pd.DataFrame({\n",
        "      TEXT_FEATURE_NAME: [pred['features'].decode('utf8') for pred in first_10_predictions],\n",
        "      LABEL_NAME: [THE_DATASET.class_names()[pred['labels']] for pred in first_10_predictions],\n",
        "      'prediction': [THE_DATASET.class_names()[pred['predictions']] for pred in first_10_predictions]\n",
        "  }))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSGcrkE069_Q"
      },
      "source": [
        "이 무작위 샘플의 경우 모델이 대부분 올바른 레이블을 예측하여 과학적 문장을 상당히 잘 포함할 수 있음을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLE0kCfO5CIA"
      },
      "source": [
        "# 다음 단계\n",
        "\n",
        "이제 TF-Hub의 CORD-19 Swivel 임베딩에 대해 조금 더 알게 되었으므로 CORD-19 Kaggle 대회에 참여하여 코로나바이러스감염증-19 관련 학술 텍스트에서 과학적 통찰력을 얻는 데 기여해 보세요.\n",
        "\n",
        "- [CORD-19 Kaggle 챌린지](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)에 참여하세요.\n",
        "- [코로나바이러스감염증-19 공개 연구 데이터세트(CORD-19)](https://pages.semanticscholar.org/coronavirus-research)에 대해 자세히 알아보세요.\n",
        "- https://tfhub.dev/tensorflow/cord-19/swivel-128d/1에서 설명서를 참조하고 TF-Hub 임베딩에 대해 자세히 알아보세요.\n",
        "- [TensorFlow 임베딩 프로젝터](http://projector.tensorflow.org/?config=https://storage.googleapis.com/tfhub-examples/tensorflow/cord-19/swivel-128d/1/tensorboard/full_projector_config.json)로 CORD-19 임베딩 공간을 탐색해 보세요."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5wFF5JFyD2Ki"
      ],
      "name": "cord_19_embeddings.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
