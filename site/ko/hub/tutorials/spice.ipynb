{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXehiGc3Kr2I"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-6LKjmi8Ktoh"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/spice\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/spice.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/spice.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"> GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/hub/tutorials/spice.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/google/spice/2\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub 모델보기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPQKw4x4bL8w"
      },
      "source": [
        "# SPICE를 사용한 피치 감지\n",
        "\n",
        "이 colab에서는 TensorFlow Hub에서 다운로드한 SPICE 모델을 사용하는 방법을 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfKwZlPnPwD1"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -q -y timidity libsndfile1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYrIdOS8SW3b"
      },
      "outputs": [],
      "source": [
        "# All the imports to deal with sound data\n",
        "!pip install pydub librosa music21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p09o78LGYdnz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "from librosa import display as librosadisplay\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import statistics\n",
        "import sys\n",
        "\n",
        "from IPython.display import Audio, Javascript\n",
        "from scipy.io import wavfile\n",
        "\n",
        "from base64 import b64decode\n",
        "\n",
        "import music21\n",
        "from pydub import AudioSegment\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "\n",
        "print(\"tensorflow: %s\" % tf.__version__)\n",
        "#print(\"librosa: %s\" % librosa.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHxox8hXc3w1"
      },
      "source": [
        "# 오디오 입력 파일\n",
        "\n",
        "이제 가장 어려운 부분입니다. 여러분의 노래를 녹음하세요! :)\n",
        "\n",
        "오디오 파일을 얻기 위한 4가지 방법이 있습니다.\n",
        "\n",
        "1. Colab에서 직접 오디오를 녹음합니다.\n",
        "2. 컴퓨터에서 업로드합니다.\n",
        "3. Google 드라이브에 저장된 파일을 사용합니다.\n",
        "4. 웹에서 파일을 다운로드합니다.\n",
        "\n",
        "아래 네 가지 메서드 중 하나를 선택합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HaCAHOqiVu5B"
      },
      "outputs": [],
      "source": [
        "#@title [Run this] Definition of the JS code to record audio straight from the browser\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(sec=5):\n",
        "  try:\n",
        "    from google.colab import output\n",
        "  except ImportError:\n",
        "    print('No possible to import output from google.colab')\n",
        "    return ''\n",
        "  else:\n",
        "    print('Recording')\n",
        "    display(Javascript(RECORD))\n",
        "    s = output.eval_js('record(%d)' % (sec*1000))\n",
        "    fname = 'recorded_audio.wav'\n",
        "    print('Saving to', fname)\n",
        "    b = b64decode(s.split(',')[1])\n",
        "    with open(fname, 'wb') as f:\n",
        "      f.write(b)\n",
        "    return fname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "sBpWWkTzfUYR"
      },
      "outputs": [],
      "source": [
        "#@title Select how to input your audio  { run: \"auto\" }\n",
        "INPUT_SOURCE = 'https://storage.googleapis.com/download.tensorflow.org/data/c-scale-metronome.wav' #@param [\"https://storage.googleapis.com/download.tensorflow.org/data/c-scale-metronome.wav\", \"RECORD\", \"UPLOAD\", \"./drive/My Drive/YOUR_MUSIC_FILE.wav\"] {allow-input: true}\n",
        "\n",
        "print('You selected', INPUT_SOURCE)\n",
        "\n",
        "if INPUT_SOURCE == 'RECORD':\n",
        "  uploaded_file_name = record(5)\n",
        "elif INPUT_SOURCE == 'UPLOAD':\n",
        "  try:\n",
        "    from google.colab import files\n",
        "  except ImportError:\n",
        "    print(\"ImportError: files from google.colab seems to not be available\")\n",
        "  else:\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "      print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "          name=fn, length=len(uploaded[fn])))\n",
        "    uploaded_file_name = next(iter(uploaded))\n",
        "    print('Uploaded file: ' + uploaded_file_name)\n",
        "elif INPUT_SOURCE.startswith('./drive/'):\n",
        "  try:\n",
        "    from google.colab import drive\n",
        "  except ImportError:\n",
        "    print(\"ImportError: files from google.colab seems to not be available\")\n",
        "  else:\n",
        "    drive.mount('/content/drive')\n",
        "    # don't forget to change the name of the file you\n",
        "    # will you here!\n",
        "    gdrive_audio_file = 'YOUR_MUSIC_FILE.wav'\n",
        "    uploaded_file_name = INPUT_SOURCE\n",
        "elif INPUT_SOURCE.startswith('http'):\n",
        "  !wget --no-check-certificate 'https://storage.googleapis.com/download.tensorflow.org/data/c-scale-metronome.wav' -O c-scale.wav\n",
        "  uploaded_file_name = 'c-scale.wav'\n",
        "else:\n",
        "  print('Unrecognized input format!')\n",
        "  print('Please select \"RECORD\", \"UPLOAD\", or specify a file hosted on Google Drive or a file from the web to download file to download')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S2BvIoDf9nf"
      },
      "source": [
        "# 오디오 데이터 준비하기\n",
        "\n",
        "이제 오디오가 준비되었으니 예상 형식으로 변환한 다음 들어 보겠습니다!\n",
        "\n",
        "SPICE 모델은 16kHz의 샘플링 속도와 단 하나의 채널(모노)로 오디오 파일을 입력 받아야 합니다.\n",
        "\n",
        "이 부분에서 도움을 주기 위해 wav 파일을 모델의 예상 형식으로 변환하는 함수(`convert_audio_for_model`)를 만들었습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ1362i-JoFI"
      },
      "outputs": [],
      "source": [
        "# Function that converts the user-created audio to the format that the model \n",
        "# expects: bitrate 16kHz and only one channel (mono).\n",
        "\n",
        "EXPECTED_SAMPLE_RATE = 16000\n",
        "\n",
        "def convert_audio_for_model(user_file, output_file='converted_audio_file.wav'):\n",
        "  audio = AudioSegment.from_file(user_file)\n",
        "  audio = audio.set_frame_rate(EXPECTED_SAMPLE_RATE).set_channels(1)\n",
        "  audio.export(output_file, format=\"wav\")\n",
        "  return output_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL9pftZ2nPm9"
      },
      "outputs": [],
      "source": [
        "# Converting to the expected format for the model\n",
        "# in all the input 4 input method before, the uploaded file name is at\n",
        "# the variable uploaded_file_name\n",
        "converted_audio_file = convert_audio_for_model(uploaded_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TslkX2AOZN0p"
      },
      "outputs": [],
      "source": [
        "# Loading audio samples from the wav file:\n",
        "sample_rate, audio_samples = wavfile.read(converted_audio_file, 'rb')\n",
        "\n",
        "# Show some basic information about the audio.\n",
        "duration = len(audio_samples)/sample_rate\n",
        "print(f'Sample rate: {sample_rate} Hz')\n",
        "print(f'Total duration: {duration:.2f}s')\n",
        "print(f'Size of the input: {len(audio_samples)}')\n",
        "\n",
        "# Let's listen to the wav file.\n",
        "Audio(audio_samples, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBicZu5AgcpR"
      },
      "source": [
        "먼저 만들어 놓은 노래의 파형을 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAa2M3CLZcWW"
      },
      "outputs": [],
      "source": [
        "# We can visualize the audio as a waveform.\n",
        "_ = plt.plot(audio_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1eI0b8qgn08"
      },
      "source": [
        "보다 유익한 시각화 방법은 시간에 따른 주파수를 보여주는 [스펙트로그램](https://en.wikipedia.org/wiki/Spectrogram)입니다.\n",
        "\n",
        "여기서는 노래를 더 명확하게 나타내기 위해 로그 주파수 스케일을 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGR4UZtpZvWI"
      },
      "outputs": [],
      "source": [
        "MAX_ABS_INT16 = 32768.0\n",
        "\n",
        "def plot_stft(x, sample_rate, show_black_and_white=False):\n",
        "  x_stft = np.abs(librosa.stft(x, n_fft=2048))\n",
        "  fig, ax = plt.subplots()\n",
        "  fig.set_size_inches(20, 10)\n",
        "  x_stft_db = librosa.amplitude_to_db(x_stft, ref=np.max)\n",
        "  if(show_black_and_white):\n",
        "    librosadisplay.specshow(data=x_stft_db, y_axis='log', \n",
        "                             sr=sample_rate, cmap='gray_r')\n",
        "  else:\n",
        "    librosadisplay.specshow(data=x_stft_db, y_axis='log', sr=sample_rate)\n",
        "\n",
        "  plt.colorbar(format='%+2.0f dB')\n",
        "\n",
        "plot_stft(audio_samples / MAX_ABS_INT16 , sample_rate=EXPECTED_SAMPLE_RATE)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGCzo_cjjH-7"
      },
      "source": [
        "여기서 마지막 변환이 필요합니다. 오디오 샘플은 int16 형식이며 -1과 1 사이의 부동 소수점으로 정규화해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv4H4O1Xb8T8"
      },
      "outputs": [],
      "source": [
        "audio_samples = audio_samples / float(MAX_ABS_INT16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTdo_TwljVUV"
      },
      "source": [
        "# 모델 실행하기\n",
        "\n",
        "이제 쉬운 부분입니다. **TensorFlow 허브**로 모델을 로드하고 여기에 오디오를 입력합니다. SPICE는 피치와 불확실성의 두 가지 출력을 제공합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUptYSTAbc3I"
      },
      "source": [
        "**TensorFlow 허브**는 머신러닝 모델의 재사용 가능한 부분을 게시, 검색 및 소비하기 위한 라이브러리입니다. 이러한 라이브러리를 이용하면 머신러닝으로 문제를 해결하기가 쉬워집니다.\n",
        "\n",
        "모델을 로드하기 위해 허브 모듈과 모델을 가리키는 URL만 있으면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri0A0DSXY_Yd"
      },
      "outputs": [],
      "source": [
        "# Loading the SPICE model is easy:\n",
        "model = hub.load(\"https://tfhub.dev/google/spice/2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQV5H6J4suMT"
      },
      "source": [
        "**참고:** 여기서 흥미로운 점은 허브의 모든 모델 URL을 다운로드와 문서 읽기에 모두 사용할 수 있다는 것입니다. 따라서 브라우저에서 해당 링크를 가리키면 모델 사용 방법에 대한 설명서를 읽고 훈련 방법에 대해 자세히 알아볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUVICjIps9hI"
      },
      "source": [
        "모델이 로드되고 데이터가 준비되면 결과를 얻기 위해 3개의 줄이 필요합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP55fXBYcBhb"
      },
      "outputs": [],
      "source": [
        "# We now feed the audio to the SPICE tf.hub model to obtain pitch and uncertainty outputs as tensors.\n",
        "model_output = model.signatures[\"serving_default\"](tf.constant(audio_samples, tf.float32))\n",
        "\n",
        "pitch_outputs = model_output[\"pitch\"]\n",
        "uncertainty_outputs = model_output[\"uncertainty\"]\n",
        "\n",
        "# 'Uncertainty' basically means the inverse of confidence.\n",
        "confidence_outputs = 1.0 - uncertainty_outputs\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(20, 10)\n",
        "plt.plot(pitch_outputs, label='pitch')\n",
        "plt.plot(confidence_outputs, label='confidence')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blJwFWR4kMul"
      },
      "source": [
        "신뢰도가 낮은(신뢰도 &lt;0.9) 모든 피치 추정치를 제거하고 나머지 값을 플롯하여 결과를 이해하기 쉽게 만들겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1MRmcm2cEkM"
      },
      "outputs": [],
      "source": [
        "confidence_outputs = list(confidence_outputs)\n",
        "pitch_outputs = [ float(x) for x in pitch_outputs]\n",
        "\n",
        "indices = range(len (pitch_outputs))\n",
        "confident_pitch_outputs = [ (i,p)  \n",
        "  for i, p, c in zip(indices, pitch_outputs, confidence_outputs) if  c >= 0.9  ]\n",
        "confident_pitch_outputs_x, confident_pitch_outputs_y = zip(*confident_pitch_outputs)\n",
        " \n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(20, 10)\n",
        "ax.set_ylim([0, 1])\n",
        "plt.scatter(confident_pitch_outputs_x, confident_pitch_outputs_y, )\n",
        "plt.scatter(confident_pitch_outputs_x, confident_pitch_outputs_y, c=\"r\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNBZ7ZblkxOm"
      },
      "source": [
        "SPICE가 반환하는 피치 값은 0에서 1 사이의 범위에 있습니다. 절대 피치 값(Hz)으로 변환하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-CnpKzmcQi9"
      },
      "outputs": [],
      "source": [
        "def output2hz(pitch_output):\n",
        "  # Constants taken from https://tfhub.dev/google/spice/2\n",
        "  PT_OFFSET = 25.58\n",
        "  PT_SLOPE = 63.07\n",
        "  FMIN = 10.0;\n",
        "  BINS_PER_OCTAVE = 12.0;\n",
        "  cqt_bin = pitch_output * PT_SLOPE + PT_OFFSET;\n",
        "  return FMIN * 2.0 ** (1.0 * cqt_bin / BINS_PER_OCTAVE)\n",
        "    \n",
        "confident_pitch_values_hz = [ output2hz(p) for p in confident_pitch_outputs_y ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24yK0a6HjCSZ"
      },
      "source": [
        "이제 예측의 성능이 얼마나 좋은지 살펴보겠습니다. 예측된 피치를 원래 스펙트로그램 위에 중첩시킵니다. 피치 예측이 더 잘 보이도록 스펙트로그램을 흑백으로 변경했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1kaAcX9rrDo"
      },
      "outputs": [],
      "source": [
        "plot_stft(audio_samples / MAX_ABS_INT16 , \n",
        "          sample_rate=EXPECTED_SAMPLE_RATE, show_black_and_white=True)\n",
        "# Note: conveniently, since the plot is in log scale, the pitch outputs \n",
        "# also get converted to the log scale automatically by matplotlib.\n",
        "plt.scatter(confident_pitch_outputs_x, confident_pitch_values_hz, c=\"r\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NskqpiHLxq6V"
      },
      "source": [
        "# 음표로 변환하기\n",
        "\n",
        "이제 피치 값을 얻었으므로 이를 음표로 변환하겠습니다! 이 부분은 그 자체로 어렵습니다. 다음 두 가지를 고려해야 합니다.\n",
        "\n",
        "1. 나머지(노래가 없을 때)\n",
        "2. 각 음표의 크기(오프셋) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDOlm9PLTTjt"
      },
      "source": [
        "### 1: 노래가 없을 때를 나타내기 위해 출력에 0 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uSQ3bJmTZmo"
      },
      "outputs": [],
      "source": [
        "pitch_outputs_and_rests = [\n",
        "    output2hz(p) if c >= 0.9 else 0\n",
        "    for i, p, c in zip(indices, pitch_outputs, confidence_outputs)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fM0UwlsTt4w"
      },
      "source": [
        "### 2: 음표 오프셋 추가\n",
        "\n",
        "사람이 자유롭게 노래할 때 멜로디는 음표가 표현할 수 있는 절대 패치 값을 이동시킬 수 있습니다. 따라서 예측을 음표로 변환하려면 이 가능한 오프셋을 수정해야 합니다. 다음 코드가 이 부분을 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsJu-P5ksdFW"
      },
      "outputs": [],
      "source": [
        "A4 = 440\n",
        "C0 = A4 * pow(2, -4.75)\n",
        "note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
        "\n",
        "def hz2offset(freq):\n",
        "  # This measures the quantization error for a single note.\n",
        "  if freq == 0:  # Rests always have zero error.\n",
        "    return None\n",
        "  # Quantized note.\n",
        "  h = round(12 * math.log2(freq / C0))\n",
        "  return 12 * math.log2(freq / C0) - h\n",
        "\n",
        "\n",
        "# The ideal offset is the mean quantization error for all the notes\n",
        "# (excluding rests):\n",
        "offsets = [hz2offset(p) for p in pitch_outputs_and_rests if p != 0]\n",
        "print(\"offsets: \", offsets)\n",
        "\n",
        "ideal_offset = statistics.mean(offsets)\n",
        "print(\"ideal offset: \", ideal_offset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K17It_qT2DtE"
      },
      "source": [
        "이제 일부 휴리스틱을 사용하여 가장 가능성이 높은 노래의 음표 시퀀스를 시도하고 추정할 수 있습니다. 위에서 계산된 이상적인 오프셋이 하나의 요소이지만, 양자화를 시작하기 위해 속도(8분의 1과 같이 수행되는 예상의 수)와 시간 오프셋도 알아야 합니다. 간단하게 하기 위해 서로 다른 속도와 시간 오프셋을 시도하고 양자화 오류를 측정하여 최종적으로 이 오류를 최소화하는 값을 사용할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMUTI4L52ZHA"
      },
      "outputs": [],
      "source": [
        "def quantize_predictions(group, ideal_offset):\n",
        "  # Group values are either 0, or a pitch in Hz.\n",
        "  non_zero_values = [v for v in group if v != 0]\n",
        "  zero_values_count = len(group) - len(non_zero_values)\n",
        "\n",
        "  # Create a rest if 80% is silent, otherwise create a note.\n",
        "  if zero_values_count > 0.8 * len(group):\n",
        "    # Interpret as a rest. Count each dropped note as an error, weighted a bit\n",
        "    # worse than a badly sung note (which would 'cost' 0.5).\n",
        "    return 0.51 * len(non_zero_values), \"Rest\"\n",
        "  else:\n",
        "    # Interpret as note, estimating as mean of non-rest predictions.\n",
        "    h = round(\n",
        "        statistics.mean([\n",
        "            12 * math.log2(freq / C0) - ideal_offset for freq in non_zero_values\n",
        "        ]))\n",
        "    octave = h // 12\n",
        "    n = h % 12\n",
        "    note = note_names[n] + str(octave)\n",
        "    # Quantization error is the total difference from the quantized note.\n",
        "    error = sum([\n",
        "        abs(12 * math.log2(freq / C0) - ideal_offset - h)\n",
        "        for freq in non_zero_values\n",
        "    ])\n",
        "    return error, note\n",
        "\n",
        "\n",
        "def get_quantization_and_error(pitch_outputs_and_rests, predictions_per_eighth,\n",
        "                               prediction_start_offset, ideal_offset):\n",
        "  # Apply the start offset - we can just add the offset as rests.\n",
        "  pitch_outputs_and_rests = [0] * prediction_start_offset + \\\n",
        "                            pitch_outputs_and_rests\n",
        "  # Collect the predictions for each note (or rest).\n",
        "  groups = [\n",
        "      pitch_outputs_and_rests[i:i + predictions_per_eighth]\n",
        "      for i in range(0, len(pitch_outputs_and_rests), predictions_per_eighth)\n",
        "  ]\n",
        "\n",
        "  quantization_error = 0\n",
        "\n",
        "  notes_and_rests = []\n",
        "  for group in groups:\n",
        "    error, note_or_rest = quantize_predictions(group, ideal_offset)\n",
        "    quantization_error += error\n",
        "    notes_and_rests.append(note_or_rest)\n",
        "\n",
        "  return quantization_error, notes_and_rests\n",
        "\n",
        "\n",
        "best_error = float(\"inf\")\n",
        "best_notes_and_rests = None\n",
        "best_predictions_per_note = None\n",
        "\n",
        "for predictions_per_note in range(20, 65, 1):\n",
        "  for prediction_start_offset in range(predictions_per_note):\n",
        "\n",
        "    error, notes_and_rests = get_quantization_and_error(\n",
        "        pitch_outputs_and_rests, predictions_per_note,\n",
        "        prediction_start_offset, ideal_offset)\n",
        "\n",
        "    if error < best_error:      \n",
        "      best_error = error\n",
        "      best_notes_and_rests = notes_and_rests\n",
        "      best_predictions_per_note = predictions_per_note\n",
        "\n",
        "# At this point, best_notes_and_rests contains the best quantization.\n",
        "# Since we don't need to have rests at the beginning, let's remove these:\n",
        "while best_notes_and_rests[0] == 'Rest':\n",
        "  best_notes_and_rests = best_notes_and_rests[1:]\n",
        "# Also remove silence at the end.\n",
        "while best_notes_and_rests[-1] == 'Rest':\n",
        "  best_notes_and_rests = best_notes_and_rests[:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMZbWA3aVqee"
      },
      "source": [
        "이제 양자화된 음표를 악보로 작성해 보겠습니다!\n",
        "\n",
        "이 작업을 위해 [music21](http://web.mit.edu/music21/) 및 [Open Sheet Music Display](https://github.com/opensheetmusicdisplay/opensheetmusicdisplay)의 두 가지 라이브러리를 사용합니다.\n",
        "\n",
        "**참고:** 간단하게 하기 위해 여기서는 모든 음표의 지속 시간이 동일하다고 가정합니다(반음표)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVrk_IOIzpQR"
      },
      "outputs": [],
      "source": [
        "# Creating the sheet music score.\n",
        "sc = music21.stream.Score()\n",
        "# Adjust the speed to match the actual singing.\n",
        "bpm = 60 * 60 / best_predictions_per_note\n",
        "print ('bpm: ', bpm)\n",
        "a = music21.tempo.MetronomeMark(number=bpm)\n",
        "sc.insert(0,a)\n",
        "\n",
        "for snote in best_notes_and_rests:   \n",
        "    d = 'half'\n",
        "    if snote == 'Rest':      \n",
        "      sc.append(music21.note.Rest(type=d))\n",
        "    else:\n",
        "      sc.append(music21.note.Note(snote, type=d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "CEleCWHtG2s4"
      },
      "outputs": [],
      "source": [
        "#@title [Run this] Helper function to use Open Sheet Music Display (JS code) to show a music score\n",
        "\n",
        "from IPython.core.display import display, HTML, Javascript\n",
        "import json, random\n",
        "\n",
        "def showScore(score):\n",
        "    xml = open(score.write('musicxml')).read()\n",
        "    showMusicXML(xml)\n",
        "    \n",
        "def showMusicXML(xml):\n",
        "    DIV_ID = \"OSMD_div\"\n",
        "    display(HTML('<div id=\"'+DIV_ID+'\">loading OpenSheetMusicDisplay</div>'))\n",
        "    script = \"\"\"\n",
        "    var div_id = %%DIV_ID%%;\n",
        "    function loadOSMD() { \n",
        "        return new Promise(function(resolve, reject){\n",
        "            if (window.opensheetmusicdisplay) {\n",
        "                return resolve(window.opensheetmusicdisplay)\n",
        "            }\n",
        "            // OSMD script has a 'define' call which conflicts with requirejs\n",
        "            var _define = window.define // save the define object \n",
        "            window.define = undefined // now the loaded script will ignore requirejs\n",
        "            var s = document.createElement( 'script' );\n",
        "            s.setAttribute( 'src', \"https://cdn.jsdelivr.net/npm/opensheetmusicdisplay@0.7.6/build/opensheetmusicdisplay.min.js\" );\n",
        "            //s.setAttribute( 'src', \"/custom/opensheetmusicdisplay.js\" );\n",
        "            s.onload=function(){\n",
        "                window.define = _define\n",
        "                resolve(opensheetmusicdisplay);\n",
        "            };\n",
        "            document.body.appendChild( s ); // browser will try to load the new script tag\n",
        "        }) \n",
        "    }\n",
        "    loadOSMD().then((OSMD)=>{\n",
        "        window.openSheetMusicDisplay = new OSMD.OpenSheetMusicDisplay(div_id, {\n",
        "          drawingParameters: \"compacttight\"\n",
        "        });\n",
        "        openSheetMusicDisplay\n",
        "            .load(%%data%%)\n",
        "            .then(\n",
        "              function() {\n",
        "                openSheetMusicDisplay.render();\n",
        "              }\n",
        "            );\n",
        "    })\n",
        "    \"\"\".replace('%%DIV_ID%%',DIV_ID).replace('%%data%%',json.dumps(xml))\n",
        "    display(Javascript(script))\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTu4phq4WeAI"
      },
      "outputs": [],
      "source": [
        "# rendering the music score\n",
        "showScore(sc)\n",
        "print(best_notes_and_rests)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGPXm6Z83U2g"
      },
      "source": [
        "음표를 MIDI 파일로 변환하고 들어보겠습니다.\n",
        "\n",
        "이 파일을 만들기 위해 이전에 만든 스트림을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klYoWjgmPaod"
      },
      "outputs": [],
      "source": [
        "# Saving the recognized musical notes as a MIDI file\n",
        "converted_audio_file_as_midi = converted_audio_file[:-4] + '.mid'\n",
        "fp = sc.write('midi', fp=converted_audio_file_as_midi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz7Mj3Qx1lpR"
      },
      "outputs": [],
      "source": [
        "wav_from_created_midi = converted_audio_file_as_midi.replace(' ', '_') + \"_midioutput.wav\"\n",
        "print(wav_from_created_midi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahss5EOiWDDp"
      },
      "source": [
        "colab에서 노래를 들으려면 다시 wav로 변환해야 합니다. Timidity를 사용하면 간단히 해결됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmeJ-UITV2nq"
      },
      "outputs": [],
      "source": [
        "!timidity $converted_audio_file_as_midi -Ow -o $wav_from_created_midi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnvwmyNj7kCC"
      },
      "source": [
        "마지막으로, 음표로부터 생성되고 예상된 피치로부터 MIDI를 통해 만들어져 모델에 의해 추론된 오디오를 들어봅니다!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNLBB0zJV6vN"
      },
      "outputs": [],
      "source": [
        "Audio(wav_from_created_midi)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "spice.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
