{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNLUPuRpkFv_"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DQcWZm0FkPk-"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2022 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exbxve1rHlrF"
      },
      "source": [
        "# FILM 모델을 사용한 프레임 보간\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMWFVTlbrQ8m"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf_hub_film_example\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/tf_hub_film_example.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/tf_hub_film_example.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/hub/tutorials/tf_hub_film_example.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a>\n",
        "</td>\n",
        "  <td><a href=\"https://tfhub.dev/google/film/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub 모델 보기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61H28S7ArUAZ"
      },
      "source": [
        "프레임 보간은 주어진 이미지 세트에서 많은 중간 이미지를 합성하는 작업입니다. 이 기술은 프레임 속도 업샘플링 또는 슬로우 모션 비디오 효과 생성에 자주 사용됩니다.\n",
        "\n",
        "이 Colab에서는 FILM 모델을 사용하여 프레임 보간을 수행합니다. Colab은 또한 보간된 중간 이미지에서 비디오를 생성하는 코드 조각을 제공합니다.\n",
        "\n",
        "FILM 연구에 대한 자세한 내용은 다음 자료를 참조하세요.\n",
        "\n",
        "- Google AI 블로그: [대규모 모션 프레임 보간](https://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html)\n",
        "- 프로젝트 페이지: FILM: [큰 동작에 대한 프레임 보간](https://film-net.github.io/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVX7s6zMulsu"
      },
      "source": [
        "## 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi5t2OEJsGBW"
      },
      "outputs": [],
      "source": [
        "!pip install mediapy\n",
        "!sudo apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA1tq39MjOiF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "from typing import Generator, Iterable, List, Optional\n",
        "import mediapy as media"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTgXmeYGnT7q"
      },
      "source": [
        "## TFHub에서 모델 로드\n",
        "\n",
        "TensorFlow Hub에서 모델을 로드하려면 tfhub 라이브러리와 문서 URL인 모델 처리가 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GojhvyAtjUt0"
      },
      "outputs": [],
      "source": [
        "model = hub.load(\"https://tfhub.dev/google/film/1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOQJPsu2CwPk"
      },
      "source": [
        "## URL 또는 로컬에서 이미지를 로드하는 Util 함수\n",
        "\n",
        "이 함수는 이미지를 로드하고 나중에 모델에서 사용할 수 있도록 준비합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPnh5uhQvFln"
      },
      "outputs": [],
      "source": [
        "_UINT8_MAX_F = float(np.iinfo(np.uint8).max)\n",
        "\n",
        "def load_image(img_url: str):\n",
        "  \"\"\"Returns an image with shape [height, width, num_channels], with pixels in [0..1] range, and type np.float32.\"\"\"\n",
        "\n",
        "  if (img_url.startswith(\"https\")):\n",
        "    user_agent = {'User-agent': 'Colab Sample (https://tensorflow.org)'}\n",
        "    response = requests.get(img_url, headers=user_agent)\n",
        "    image_data = response.content\n",
        "  else:\n",
        "    image_data = tf.io.read_file(img_url)\n",
        "\n",
        "  image = tf.io.decode_image(image_data, channels=3)\n",
        "  image_numpy = tf.cast(image, dtype=tf.float32).numpy()\n",
        "  return image_numpy / _UINT8_MAX_F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjDFns1zp5y6"
      },
      "source": [
        "FILM의 모델 입력은 `time`, `x0`, `x1` 키가 있는 사전입니다.\n",
        "\n",
        "- `time`: 보간된 프레임의 위치이며 중간은 `0.5` 입니다.\n",
        "- `x0`: 초기 프레임입니다.\n",
        "- `x1`: 최종 프레임입니다.\n",
        "\n",
        "두 프레임 모두 정규화해야 합니다(위의 `load_image` 함수에서 수행). 여기서 각 픽셀은 `[0..1]` 범위에 있습니다.\n",
        "\n",
        "`time`은 `[0..1]` 사이의 값이며 생성된 이미지가 있어야 하는 위치를 나타냅니다. 0.5는 입력 이미지의 중간입니다.\n",
        "\n",
        "세 값 모두에 배치 차원도 있어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEQNQlHGsWSM"
      },
      "outputs": [],
      "source": [
        "# using images from the FILM repository (https://github.com/google-research/frame-interpolation/)\n",
        "\n",
        "image_1_url = \"https://github.com/google-research/frame-interpolation/blob/main/photos/one.png?raw=true\"\n",
        "image_2_url = \"https://github.com/google-research/frame-interpolation/blob/main/photos/two.png?raw=true\"\n",
        "\n",
        "time = np.array([0.5], dtype=np.float32)\n",
        "\n",
        "image1 = load_image(image_1_url)\n",
        "image2 = load_image(image_2_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6_MQE9EuF_K"
      },
      "outputs": [],
      "source": [
        "input = {\n",
        "    'time': np.expand_dims(time, axis=0), # adding the batch dimension to the time\n",
        "     'x0': np.expand_dims(image1, axis=0), # adding the batch dimension to the image\n",
        "     'x1': np.expand_dims(image2, axis=0)  # adding the batch dimension to the image\n",
        "}\n",
        "mid_frame = model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZkzYE2bptfD"
      },
      "source": [
        "모델은 몇 가지 결과를 출력하지만 여기서 사용할 것은 `image` 키이며 값은 보간된 프레임입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eClVbNFhA5Py"
      },
      "outputs": [],
      "source": [
        "print(mid_frame.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE2csH3u8ePe"
      },
      "outputs": [],
      "source": [
        "frames = [image1, mid_frame['image'][0].numpy(), image2]\n",
        "\n",
        "media.show_images(frames, titles=['input image one', 'generated image', 'input image two'], height=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS1AT8kn-f_l"
      },
      "source": [
        "생성된 프레임에서 비디오를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFc53B3p37SH"
      },
      "outputs": [],
      "source": [
        "media.show_video(frames, fps=3, title='FILM interpolated video')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5AOFNkj-lfO"
      },
      "source": [
        "## 프레임 보간기 라이브러리 정의\n",
        "\n",
        "보시다시피 전환이 너무 부드럽지 않습니다.\n",
        "\n",
        "이를 개선하려면 더 많은 보간 프레임이 필요합니다.\n",
        "\n",
        "중간 이미지를 사용하여 모델을 여러 번 계속 실행할 수 있지만 더 나은 솔루션이 있습니다.\n",
        "\n",
        "많은 보간된 이미지를 생성하고 더 매끄러운 비디오를 얻으려면 보간기 라이브러리를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsoDv_9geoZn"
      },
      "outputs": [],
      "source": [
        "\"\"\"A wrapper class for running a frame interpolation based on the FILM model on TFHub\n",
        "\n",
        "Usage:\n",
        "  interpolator = Interpolator()\n",
        "  result_batch = interpolator(image_batch_0, image_batch_1, batch_dt)\n",
        "  Where image_batch_1 and image_batch_2 are numpy tensors with TF standard\n",
        "  (B,H,W,C) layout, batch_dt is the sub-frame time in range [0..1], (B,) layout.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def _pad_to_align(x, align):\n",
        "  \"\"\"Pads image batch x so width and height divide by align.\n",
        "\n",
        "  Args:\n",
        "    x: Image batch to align.\n",
        "    align: Number to align to.\n",
        "\n",
        "  Returns:\n",
        "    1) An image padded so width % align == 0 and height % align == 0.\n",
        "    2) A bounding box that can be fed readily to tf.image.crop_to_bounding_box\n",
        "      to undo the padding.\n",
        "  \"\"\"\n",
        "  # Input checking.\n",
        "  assert np.ndim(x) == 4\n",
        "  assert align > 0, 'align must be a positive number.'\n",
        "\n",
        "  height, width = x.shape[-3:-1]\n",
        "  height_to_pad = (align - height % align) if height % align != 0 else 0\n",
        "  width_to_pad = (align - width % align) if width % align != 0 else 0\n",
        "\n",
        "  bbox_to_pad = {\n",
        "      'offset_height': height_to_pad // 2,\n",
        "      'offset_width': width_to_pad // 2,\n",
        "      'target_height': height + height_to_pad,\n",
        "      'target_width': width + width_to_pad\n",
        "  }\n",
        "  padded_x = tf.image.pad_to_bounding_box(x, **bbox_to_pad)\n",
        "  bbox_to_crop = {\n",
        "      'offset_height': height_to_pad // 2,\n",
        "      'offset_width': width_to_pad // 2,\n",
        "      'target_height': height,\n",
        "      'target_width': width\n",
        "  }\n",
        "  return padded_x, bbox_to_crop\n",
        "\n",
        "\n",
        "class Interpolator:\n",
        "  \"\"\"A class for generating interpolated frames between two input frames.\n",
        "\n",
        "  Uses the Film model from TFHub\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, align: int = 64) -> None:\n",
        "    \"\"\"Loads a saved model.\n",
        "\n",
        "    Args:\n",
        "      align: 'If >1, pad the input size so it divides with this before\n",
        "        inference.'\n",
        "    \"\"\"\n",
        "    self._model = hub.load(\"https://tfhub.dev/google/film/1\")\n",
        "    self._align = align\n",
        "\n",
        "  def __call__(self, x0: np.ndarray, x1: np.ndarray,\n",
        "               dt: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Generates an interpolated frame between given two batches of frames.\n",
        "\n",
        "    All inputs should be np.float32 datatype.\n",
        "\n",
        "    Args:\n",
        "      x0: First image batch. Dimensions: (batch_size, height, width, channels)\n",
        "      x1: Second image batch. Dimensions: (batch_size, height, width, channels)\n",
        "      dt: Sub-frame time. Range [0,1]. Dimensions: (batch_size,)\n",
        "\n",
        "    Returns:\n",
        "      The result with dimensions (batch_size, height, width, channels).\n",
        "    \"\"\"\n",
        "    if self._align is not None:\n",
        "      x0, bbox_to_crop = _pad_to_align(x0, self._align)\n",
        "      x1, _ = _pad_to_align(x1, self._align)\n",
        "\n",
        "    inputs = {'x0': x0, 'x1': x1, 'time': dt[..., np.newaxis]}\n",
        "    result = self._model(inputs, training=False)\n",
        "    image = result['image']\n",
        "\n",
        "    if self._align is not None:\n",
        "      image = tf.image.crop_to_bounding_box(image, **bbox_to_crop)\n",
        "    return image.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeGYaNBd_7a5"
      },
      "source": [
        "## 프레임 및 비디오 생성 유틸리티 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOJxup6s_1DP"
      },
      "outputs": [],
      "source": [
        "def _recursive_generator(\n",
        "    frame1: np.ndarray, frame2: np.ndarray, num_recursions: int,\n",
        "    interpolator: Interpolator) -> Generator[np.ndarray, None, None]:\n",
        "  \"\"\"Splits halfway to repeatedly generate more frames.\n",
        "\n",
        "  Args:\n",
        "    frame1: Input image 1.\n",
        "    frame2: Input image 2.\n",
        "    num_recursions: How many times to interpolate the consecutive image pairs.\n",
        "    interpolator: The frame interpolator instance.\n",
        "\n",
        "  Yields:\n",
        "    The interpolated frames, including the first frame (frame1), but excluding\n",
        "    the final frame2.\n",
        "  \"\"\"\n",
        "  if num_recursions == 0:\n",
        "    yield frame1\n",
        "  else:\n",
        "    # Adds the batch dimension to all inputs before calling the interpolator,\n",
        "    # and remove it afterwards.\n",
        "    time = np.full(shape=(1,), fill_value=0.5, dtype=np.float32)\n",
        "    mid_frame = interpolator(\n",
        "        np.expand_dims(frame1, axis=0), np.expand_dims(frame2, axis=0), time)[0]\n",
        "    yield from _recursive_generator(frame1, mid_frame, num_recursions - 1,\n",
        "                                    interpolator)\n",
        "    yield from _recursive_generator(mid_frame, frame2, num_recursions - 1,\n",
        "                                    interpolator)\n",
        "\n",
        "\n",
        "def interpolate_recursively(\n",
        "    frames: List[np.ndarray], num_recursions: int,\n",
        "    interpolator: Interpolator) -> Iterable[np.ndarray]:\n",
        "  \"\"\"Generates interpolated frames by repeatedly interpolating the midpoint.\n",
        "\n",
        "  Args:\n",
        "    frames: List of input frames. Expected shape (H, W, 3). The colors should be\n",
        "      in the range[0, 1] and in gamma space.\n",
        "    num_recursions: Number of times to do recursive midpoint\n",
        "      interpolation.\n",
        "    interpolator: The frame interpolation model to use.\n",
        "\n",
        "  Yields:\n",
        "    The interpolated frames (including the inputs).\n",
        "  \"\"\"\n",
        "  n = len(frames)\n",
        "  for i in range(1, n):\n",
        "    yield from _recursive_generator(frames[i - 1], frames[i],\n",
        "                                    times_to_interpolate, interpolator)\n",
        "  # Separately yield the final frame.\n",
        "  yield frames[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1R2KjhEAHu0"
      },
      "outputs": [],
      "source": [
        "times_to_interpolate = 6\n",
        "interpolator = Interpolator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZUo8tg1AYvZ"
      },
      "source": [
        "## 보간기 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMMNjs7sAWTG"
      },
      "outputs": [],
      "source": [
        "input_frames = [image1, image2]\n",
        "frames = list(\n",
        "    interpolate_recursively(input_frames, times_to_interpolate,\n",
        "                                        interpolator))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9mHHyCAAhrM"
      },
      "outputs": [],
      "source": [
        "print(f'video with {len(frames)} frames')\n",
        "media.show_video(frames, fps=30, title='FILM interpolated video')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0AZKeMVFwAc"
      },
      "source": [
        "자세한 내용은 [FILM의 모델 리포지토리](https://github.com/google-research/frame-interpolation)를 방문하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8764ry3SGDks"
      },
      "source": [
        "## 인용\n",
        "\n",
        "이 모델과 코드가 작업에 유용하다고 생각되면 다음을 인용하여 적절하게 인정해 주세요.\n",
        "\n",
        "```\n",
        "@inproceedings{reda2022film,\n",
        " title = {FILM: Frame Interpolation for Large Motion},\n",
        " author = {Fitsum Reda and Janne Kontkanen and Eric Tabellion and Deqing Sun and Caroline Pantofaru and Brian Curless},\n",
        " booktitle = {The European Conference on Computer Vision (ECCV)},\n",
        " year = {2022}\n",
        "}\n",
        "```\n",
        "\n",
        "```\n",
        "@misc{film-tf,\n",
        "  title = {Tensorflow 2 Implementation of \"FILM: Frame Interpolation for Large Motion\"},\n",
        "  author = {Fitsum Reda and Janne Kontkanen and Eric Tabellion and Deqing Sun and Caroline Pantofaru and Brian Curless},\n",
        "  year = {2022},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  howpublished = {\\url{https://github.com/google-research/frame-interpolation}}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tf_hub_film_example.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
