{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toCy3v03Dwx7"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKe-ubNcDvgv"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# 스트리밍 동작 인식을 위한 MoViNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/movinet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/google/collections/movinet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub 모델 보기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vxk2Kbc_KSP"
      },
      "source": [
        "이 튜토리얼은 사전 훈련된 비디오 분류 모델을 실행하여 주어진 비디오에서 활동(댄스, 수영, 자전거 타기 등)을 분류합니다.\n",
        "\n",
        "이 튜토리얼에서 사용된 모델 아키텍처는 [MoViNet](https://arxiv.org/pdf/2103.11511.pdf)(Mobile Video Networks)이라고 합니다. MoVieNets는 거대한 데이터세트([Kinetics 600](https://deepmind.com/research/open-source/kinetics))에서 훈련된 효율적인 비디오 분류 모델 집합입니다.\n",
        "\n",
        "TF Hub에서 사용 가능한 [i3d 모델](https://tfhub.dev/s?q=i3d-kinetics)과 달리 MoViNet은 스트리밍 비디오에서 프레임별 추론도 지원합니다.\n",
        "\n",
        "사전 훈련된 모델은 [TF Hub](https://tfhub.dev/google/collections/movinet/1)에서 사용할 수 있습니다. TF Hub 컬렉션에는 [TFLite](https://tensorflow.org/lite)에 최적화된 양자화된 모델도 포함되어 있습니다.\n",
        "\n",
        "이러한 모델의 소스는 [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/official/projects/movinet)에서 얻을 수 있습니다. 여기에는 MoViNet 모델의 구축 및 미세 조정도 포함하는 [이 튜토리얼의 상세 버전](https://colab.sandbox.google.com/github/tensorflow/models/blob/master/official/projects/movinet/movinet_tutorial.ipynb)이 포함됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E96e1UKQ8uR"
      },
      "source": [
        "![jumping jacks plot](https://storage.googleapis.com/tf_model_garden/vision/movinet/artifacts/jumpingjacks_plot.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_oLnvJy7kz5"
      },
      "source": [
        "## 설정\n",
        "\n",
        "더 작은 모델(A0-A2)에 대한 추론의 경우, 이 Colab에 대한 CPU가 충분합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUgUMGmY1yq-"
      },
      "outputs": [],
      "source": [
        "!sudo apt install -y ffmpeg\n",
        "!pip install -q mediapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3khsunT7kWa"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -q -y opencv-python-headless\n",
        "!pip install -q \"opencv-python-headless<4.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI_1csl6Q-gH"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pathlib\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tqdm\n",
        "\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 10,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn8K9oWbmREi"
      },
      "source": [
        "kinetics 600 레이블 목록을 가져오고 처음 몇 개의 레이블을 인쇄합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VJUAcjhkfb3"
      },
      "outputs": [],
      "source": [
        "labels_path = tf.keras.utils.get_file(\n",
        "    fname='labels.txt',\n",
        "    origin='https://raw.githubusercontent.com/tensorflow/models/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/kinetics_600_labels.txt'\n",
        ")\n",
        "labels_path = pathlib.Path(labels_path)\n",
        "\n",
        "lines = labels_path.read_text().splitlines()\n",
        "KINETICS_600_LABELS = np.array([line.strip() for line in lines])\n",
        "KINETICS_600_LABELS[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9BU5XsOmaq3"
      },
      "source": [
        "분류를 위한 간단한 예제 비디오를 제공하기 위해 팔 벌려 뛰기를 수행 중인 짧은 gif를 로드할 수 있습니다.\n",
        "\n",
        "![jumping jacks](https://github.com/tensorflow/models/raw/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/jumpingjack.gif)\n",
        "\n",
        "저작자 표시: CC-BY 라이선스에 따라 [Coach Bobby Bluford](https://www.youtube.com/watch?v=-AxHpj-EuPg)가 YouTube에서 공유한 영상입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aFKMbr4mfSg"
      },
      "source": [
        "gif를 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w62jqXhaSb15"
      },
      "outputs": [],
      "source": [
        "jumpingjack_url = 'https://github.com/tensorflow/models/raw/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/jumpingjack.gif'\n",
        "jumpingjack_path = tf.keras.utils.get_file(\n",
        "    fname='jumpingjack.gif',\n",
        "    origin=jumpingjack_url,\n",
        "    cache_dir='.', cache_subdir='.',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdRS_22PebfB"
      },
      "source": [
        "gif 파일을 `tf.Tensor`로 읽는 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPhmCu6oSi5f"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Read and process a video\n",
        "def load_gif(file_path, image_size=(224, 224)):\n",
        "  \"\"\"Loads a gif file into a TF tensor.\n",
        "\n",
        "  Use images resized to match what's expected by your model.\n",
        "  The model pages say the \"A2\" models expect 224 x 224 images at 5 fps\n",
        "\n",
        "  Args:\n",
        "    file_path: path to the location of a gif file.\n",
        "    image_size: a tuple of target size.\n",
        "\n",
        "  Returns:\n",
        "    a video of the gif file\n",
        "  \"\"\"\n",
        "  # Load a gif file, convert it to a TF tensor\n",
        "  raw = tf.io.read_file(file_path)\n",
        "  video = tf.io.decode_gif(raw)\n",
        "  # Resize the video\n",
        "  video = tf.image.resize(video, image_size)\n",
        "  # change dtype to a float32\n",
        "  # Hub models always want images normalized to [0,1]\n",
        "  # ref: https://www.tensorflow.org/hub/common_signatures/images#input\n",
        "  video = tf.cast(video, tf.float32) / 255.\n",
        "  return video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx7cZm8vpDJm"
      },
      "source": [
        "비디오의 형상은 `(frames, height, width, colors)`입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7k_PmbFSkHv"
      },
      "outputs": [],
      "source": [
        "jumpingjack=load_gif(jumpingjack_path)\n",
        "jumpingjack.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcKFy3oedBvF"
      },
      "source": [
        "## 모델을 사용하는 방법\n",
        "\n",
        "이 섹션에는 [TensorFlow Hub의 모델](https://tfhub.dev/google/collections/movinet/1)을 사용하는 방법을 보여주는 연습이 포함되어 있습니다. 작동 중인 모델만 보고 싶다면 다음 섹션으로 건너뛰세요.\n",
        "\n",
        "각 모델에는 `base` 및 `streaming`의 두 가지 버전이 있습니다.\n",
        "\n",
        "- `base` 버전은 비디오를 입력으로 사용하고 프레임에 대한 평균 확률을 반환합니다.\n",
        "- `streaming` 버전은 비디오 프레임과 RNN 상태를 입력으로 사용하고 해당 프레임과 새 RNN 상태에 대한 예측을 반환합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQO6Zb8Hm-9q"
      },
      "source": [
        "### 기본 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfnYU20JnPqp"
      },
      "source": [
        "[TensorFlow Hub에서 사전 훈련된 모델](https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/3)을 다운로드합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnpPo6HSR7qv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "id = 'a2'\n",
        "mode = 'base'\n",
        "version = '3'\n",
        "hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'\n",
        "model = hub.load(hub_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvaFwKhxndmb"
      },
      "source": [
        "이 버전의 모델에는 하나의 `signature`가 있습니다. 형상이 `(batch, frames, height, width, colors)`인 `tf.float32`의 `image` 인수를 취합니다. 그리고 하나의 출력, 즉 형상이 `(batch, classes)`인 로짓의 `tf.float32` 텐서를 포함하는 사전을 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GzZ4Y03T_gH"
      },
      "outputs": [],
      "source": [
        "sig = model.signatures['serving_default']\n",
        "print(sig.pretty_printed_signature())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Xny1ANomi4"
      },
      "source": [
        "비디오에서 이 서명을 실행하려면 먼저 비디오에 외부 `batch` 차원을 추가해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBOFEDG1XvZE"
      },
      "outputs": [],
      "source": [
        "#warmup\n",
        "sig(image = jumpingjack[tf.newaxis, :1]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCeW3KycVbGn"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "logits = sig(image = jumpingjack[tf.newaxis, ...])\n",
        "logits = logits['classifier_head'][0]\n",
        "\n",
        "print(logits.shape)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE8doqkPpxED"
      },
      "source": [
        "위의 출력 처리를 나중에 패키징하는 `get_top_k` 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OozPNO6LvZ00"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Get top_k labels and probabilities\n",
        "def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
        "  \"\"\"Outputs the top k model labels and probabilities on the given video.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    k: the number of top predictions to select.\n",
        "    label_map: a list of labels to map logit indices to label strings.\n",
        "\n",
        "  Returns:\n",
        "    a tuple of the top-k labels and probabilities.\n",
        "  \"\"\"\n",
        "  # Sort predictions to find top_k\n",
        "  top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
        "  # collect the labels of top_k predictions\n",
        "  top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
        "  # decode lablels\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "  # top_k probabilities of the predictions\n",
        "  top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
        "  return tuple(zip(top_labels, top_probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfKMT29pP_Z"
      },
      "source": [
        "`logits`을 확률로 변환하고 비디오의 상위 5개 클래스를 찾습니다. 모델은 비디오가 `jumping jacks`일 가능성이 있음을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-SrNGsGV5Mt"
      },
      "outputs": [],
      "source": [
        "probs = tf.nn.softmax(logits, axis=-1)\n",
        "for label, p in get_top_k(probs):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltdijoQpqjxZ"
      },
      "source": [
        "### 스트리밍 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dqdUPQXq45b"
      },
      "source": [
        "이전 섹션에서는 전체 비디오를 실행하는 모델을 사용했습니다. 종종 비디오를 처리할 때 마지막에 하나의 예측을 원하지 않고 프레임별로 예측을 업데이트하고 싶을 수 있습니다. 모델의 `stream` 버전을 사용하면 가능합니다.\n",
        "\n",
        "모델의 `stream` 버전을 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxt0hRXFZkAM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "id = 'a2'\n",
        "mode = 'stream'\n",
        "version = '3'\n",
        "hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'\n",
        "model = hub.load(hub_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDswtsGgsYGS"
      },
      "source": [
        "이 모델을 사용하는 것은 `base` 모델보다 약간 더 복잡합니다. 모델의 RNN에 대한 내부 상태를 추적해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fM_Vb1VsbDm"
      },
      "outputs": [],
      "source": [
        "list(model.signatures.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojr1_iYCtPvp"
      },
      "source": [
        "`init_states` 서명은 비디오의 **형상** `(batch, frames, height, width, colors)`를 입력으로 사용하고 초기 RNN 상태를 포함하는 텐서의 방대한 사전을 반환합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67loYFGpo_RP"
      },
      "outputs": [],
      "source": [
        "lines = model.signatures['init_states'].pretty_printed_signature().splitlines()\n",
        "lines = lines[:10]\n",
        "lines.append('      ...')\n",
        "print('.\\n'.join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5lG3vejn5df"
      },
      "outputs": [],
      "source": [
        "initial_state = model.init_states(jumpingjack[tf.newaxis, ...].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3DwmyHnuhH_"
      },
      "outputs": [],
      "source": [
        "type(initial_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8SyiEU6tB-e"
      },
      "outputs": [],
      "source": [
        "list(sorted(initial_state.keys()))[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeMCzJMBvwRF"
      },
      "source": [
        "RNN의 초기 상태가 확보되면 상태와 비디오 프레임을 입력으로 전달할 수 있습니다(비디오 프레임의 `(batch, frames, height, width, colors)` 형상 유지). 모델은 `(logits, state)` 쌍을 반환합니다.\n",
        "\n",
        "첫 번째 프레임을 본 후 모델은 비디오가 \"팔 벌려 뛰기\"인지 확신하지 못합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McSLdIgtsI3d"
      },
      "outputs": [],
      "source": [
        "inputs = initial_state.copy()\n",
        "\n",
        "# Add the batch axis, take the first frme, but keep the frame-axis.\n",
        "inputs['image'] = jumpingjack[tf.newaxis, 0:1, ...] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlH7PqLPX664"
      },
      "outputs": [],
      "source": [
        "# warmup\n",
        "model(inputs);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uzNXtu7X5sr"
      },
      "outputs": [],
      "source": [
        "logits, new_state = model(inputs)\n",
        "logits = logits[0]\n",
        "probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "for label, p in get_top_k(probs):\n",
        "  print(f'{label:20s}: {p:.3f}')\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLU644FQwXSb"
      },
      "source": [
        "루프에서 모델을 실행하여 각 프레임과 함께 업데이트된 상태를 전달하면 모델이 올바른 결과로 빠르게 수렴됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzm7T4ImmIEg"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "state = initial_state.copy()\n",
        "all_logits = []\n",
        "\n",
        "for n in range(len(jumpingjack)):\n",
        "  inputs = state\n",
        "  inputs['image'] = jumpingjack[tf.newaxis, n:n+1, ...]\n",
        "  result, state = model(inputs)\n",
        "  all_logits.append(logits)\n",
        "\n",
        "probabilities = tf.nn.softmax(all_logits, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7UtHoSWcOT2"
      },
      "outputs": [],
      "source": [
        "for label, p in get_top_k(probabilities[-1]):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ffV3NhZcsrv"
      },
      "outputs": [],
      "source": [
        "id = tf.argmax(probabilities[-1])\n",
        "plt.plot(probabilities[:, id])\n",
        "plt.xlabel('Frame #')\n",
        "plt.ylabel(f\"p('{KINETICS_600_LABELS[id]}')\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7MZ_AfRW845"
      },
      "source": [
        "`base` 모델을 실행한 이전 섹션보다 최종 확률이 훨씬 더 확실하다는 것을 알 수 있습니다. `base` 모델은 전체 프레임에 대한 예측의 평균을 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wij4tsyW8dR"
      },
      "outputs": [],
      "source": [
        "for label, p in get_top_k(tf.reduce_mean(probabilities, axis=0)):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLUoC9ejggGo"
      },
      "source": [
        "## 시간 경과에 따른 예측 애니메이션\n",
        "\n",
        "이전 섹션에서는 이러한 모델을 사용하는 방법에 대해 자세히 설명했습니다. 이 섹션은 이를 바탕으로 멋진 추론 애니메이션을 생성합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnFqOXazoWgy"
      },
      "source": [
        "아래의 숨겨진 셀은 이 섹션에서 사용되는 도우미 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dx55NK3ZoZeh"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Get top_k labels and probabilities predicted using MoViNets streaming model\n",
        "def get_top_k_streaming_labels(probs, k=5, label_map=KINETICS_600_LABELS):\n",
        "  \"\"\"Returns the top-k labels over an entire video sequence.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    k: the number of top predictions to select.\n",
        "    label_map: a list of labels to map logit indices to label strings.\n",
        "\n",
        "  Returns:\n",
        "    a tuple of the top-k probabilities, labels, and logit indices\n",
        "  \"\"\"\n",
        "  top_categories_last = tf.argsort(probs, -1, 'DESCENDING')[-1, :1]\n",
        "  # Sort predictions to find top_k\n",
        "  categories = tf.argsort(probs, -1, 'DESCENDING')[:, :k]\n",
        "  categories = tf.reshape(categories, [-1])\n",
        "\n",
        "  counts = sorted([\n",
        "      (i.numpy(), tf.reduce_sum(tf.cast(categories == i, tf.int32)).numpy())\n",
        "      for i in tf.unique(categories)[0]\n",
        "  ], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  top_probs_idx = tf.constant([i for i, _ in counts[:k]])\n",
        "  top_probs_idx = tf.concat([top_categories_last, top_probs_idx], 0)\n",
        "  # find unique indices of categories\n",
        "  top_probs_idx = tf.unique(top_probs_idx)[0][:k+1]\n",
        "  # top_k probabilities of the predictions\n",
        "  top_probs = tf.gather(probs, top_probs_idx, axis=-1)\n",
        "  top_probs = tf.transpose(top_probs, perm=(1, 0))\n",
        "  # collect the labels of top_k predictions\n",
        "  top_labels = tf.gather(label_map, top_probs_idx, axis=0)\n",
        "  # decode the top_k labels\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "\n",
        "  return top_probs, top_labels, top_probs_idx\n",
        "\n",
        "# Plot top_k predictions at a given time step\n",
        "def plot_streaming_top_preds_at_step(\n",
        "    top_probs,\n",
        "    top_labels,\n",
        "    step=None,\n",
        "    image=None,\n",
        "    legend_loc='lower left',\n",
        "    duration_seconds=10,\n",
        "    figure_height=500,\n",
        "    playhead_scale=0.8,\n",
        "    grid_alpha=0.3):\n",
        "  \"\"\"Generates a plot of the top video model predictions at a given time step.\n",
        "\n",
        "  Args:\n",
        "    top_probs: a tensor of shape (k, num_frames) representing the top-k\n",
        "      probabilities over all frames.\n",
        "    top_labels: a list of length k that represents the top-k label strings.\n",
        "    step: the current time step in the range [0, num_frames].\n",
        "    image: the image frame to display at the current time step.\n",
        "    legend_loc: the placement location of the legend.\n",
        "    duration_seconds: the total duration of the video.\n",
        "    figure_height: the output figure height.\n",
        "    playhead_scale: scale value for the playhead.\n",
        "    grid_alpha: alpha value for the gridlines.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of the output numpy image, figure, and axes.\n",
        "  \"\"\"\n",
        "  # find number of top_k labels and frames in the video\n",
        "  num_labels, num_frames = top_probs.shape\n",
        "  if step is None:\n",
        "    step = num_frames\n",
        "  # Visualize frames and top_k probabilities of streaming video\n",
        "  fig = plt.figure(figsize=(6.5, 7), dpi=300)\n",
        "  gs = mpl.gridspec.GridSpec(8, 1)\n",
        "  ax2 = plt.subplot(gs[:-3, :])\n",
        "  ax = plt.subplot(gs[-3:, :])\n",
        "  # display the frame\n",
        "  if image is not None:\n",
        "    ax2.imshow(image, interpolation='nearest')\n",
        "    ax2.axis('off')\n",
        "  # x-axis (frame number)\n",
        "  preview_line_x = tf.linspace(0., duration_seconds, num_frames)\n",
        "  # y-axis (top_k probabilities)\n",
        "  preview_line_y = top_probs\n",
        "\n",
        "  line_x = preview_line_x[:step+1]\n",
        "  line_y = preview_line_y[:, :step+1]\n",
        "\n",
        "  for i in range(num_labels):\n",
        "    ax.plot(preview_line_x, preview_line_y[i], label=None, linewidth='1.5',\n",
        "            linestyle=':', color='gray')\n",
        "    ax.plot(line_x, line_y[i], label=top_labels[i], linewidth='2.0')\n",
        "\n",
        "\n",
        "  ax.grid(which='major', linestyle=':', linewidth='1.0', alpha=grid_alpha)\n",
        "  ax.grid(which='minor', linestyle=':', linewidth='0.5', alpha=grid_alpha)\n",
        "\n",
        "  min_height = tf.reduce_min(top_probs) * playhead_scale\n",
        "  max_height = tf.reduce_max(top_probs)\n",
        "  ax.vlines(preview_line_x[step], min_height, max_height, colors='red')\n",
        "  ax.scatter(preview_line_x[step], max_height, color='red')\n",
        "\n",
        "  ax.legend(loc=legend_loc)\n",
        "\n",
        "  plt.xlim(0, duration_seconds)\n",
        "  plt.ylabel('Probability')\n",
        "  plt.xlabel('Time (s)')\n",
        "  plt.yscale('log')\n",
        "\n",
        "  fig.tight_layout()\n",
        "  fig.canvas.draw()\n",
        "\n",
        "  data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close()\n",
        "\n",
        "  figure_width = int(figure_height * data.shape[1] / data.shape[0])\n",
        "  image = PIL.Image.fromarray(data).resize([figure_width, figure_height])\n",
        "  image = np.array(image)\n",
        "\n",
        "  return image\n",
        "\n",
        "# Plotting top_k predictions from MoViNets streaming model\n",
        "def plot_streaming_top_preds(\n",
        "    probs,\n",
        "    video,\n",
        "    top_k=5,\n",
        "    video_fps=25.,\n",
        "    figure_height=500,\n",
        "    use_progbar=True):\n",
        "  \"\"\"Generates a video plot of the top video model predictions.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    video: the video to display in the plot.\n",
        "    top_k: the number of top predictions to select.\n",
        "    video_fps: the input video fps.\n",
        "    figure_fps: the output video fps.\n",
        "    figure_height: the height of the output video.\n",
        "    use_progbar: display a progress bar.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array representing the output video.\n",
        "  \"\"\"\n",
        "  # select number of frames per second\n",
        "  video_fps = 8.\n",
        "  # select height of the image\n",
        "  figure_height = 500\n",
        "  # number of time steps of the given video\n",
        "  steps = video.shape[0]\n",
        "  # estimate duration of the video (in seconds)\n",
        "  duration = steps / video_fps\n",
        "  # estiamte top_k probabilities and corresponding labels\n",
        "  top_probs, top_labels, _ = get_top_k_streaming_labels(probs, k=top_k)\n",
        "\n",
        "  images = []\n",
        "  step_generator = tqdm.trange(steps) if use_progbar else range(steps)\n",
        "  for i in step_generator:\n",
        "    image = plot_streaming_top_preds_at_step(\n",
        "        top_probs=top_probs,\n",
        "        top_labels=top_labels,\n",
        "        step=i,\n",
        "        image=video[i],\n",
        "        duration_seconds=duration,\n",
        "        figure_height=figure_height,\n",
        "    )\n",
        "    images.append(image)\n",
        "\n",
        "  return np.array(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLgFBslcZOQO"
      },
      "source": [
        "비디오 프레임에서 스트리밍 모델을 실행하고 로짓을 수집하여 시작합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXWR13wthnK5"
      },
      "outputs": [],
      "source": [
        "init_states = model.init_states(jumpingjack[tf.newaxis].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqSkt7l8ltwt"
      },
      "outputs": [],
      "source": [
        "# Insert your video clip here\n",
        "video = jumpingjack\n",
        "images = tf.split(video[tf.newaxis], video.shape[0], axis=1)\n",
        "\n",
        "all_logits = []\n",
        "\n",
        "# To run on a video, pass in one frame at a time\n",
        "states = init_states\n",
        "for image in tqdm.tqdm(images):\n",
        "  # predictions for each frame\n",
        "  logits, states = model({**states, 'image': image})\n",
        "  all_logits.append(logits)\n",
        "\n",
        "# concatinating all the logits\n",
        "logits = tf.concat(all_logits, 0)\n",
        "# estimating probabilities\n",
        "probs = tf.nn.softmax(logits, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOGcCMMJyuPl"
      },
      "outputs": [],
      "source": [
        "final_probs = probs[-1]\n",
        "print('Top_k predictions and their probablities\\n')\n",
        "for label, p in get_top_k(final_probs):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaybT0rbZct-"
      },
      "source": [
        "확률 시퀀스를 비디오로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdox556CtMRb"
      },
      "outputs": [],
      "source": [
        "# Generate a plot and output to a video tensor\n",
        "plot_video = plot_streaming_top_preds(probs, video, video_fps=8.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSStKE9klCs3"
      },
      "outputs": [],
      "source": [
        "# For gif format, set codec='gif'\n",
        "media.show_video(plot_video, fps=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCImgZ3OdJw7"
      },
      "source": [
        "## 리소스\n",
        "\n",
        "사전 훈련된 모델은 [TF Hub](https://tfhub.dev/google/collections/movinet/1)에서 사용할 수 있습니다. TF Hub 컬렉션에는 [TFLite](https://tensorflow.org/lite)에 최적화된 양자화된 모델도 포함되어 있습니다.\n",
        "\n",
        "이러한 모델의 소스는 [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/official/projects/movinet)에서 제공합니다. 여기에는 MoViNet 모델 구축 및 미세 조정에 대한 내용도 담긴 [이 튜토리얼의 상세 버전](https://colab.sandbox.google.com/github/tensorflow/models/blob/master/official/projects/movinet/movinet_tutorial.ipynb)이 포함됩니다. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "movinet.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
