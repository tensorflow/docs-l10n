{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJhWonqQN7u0"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MegtYH2UN8tT"
      },
      "outputs": [],
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlHqSdgSEwPE"
      },
      "source": [
        "# Universal Sentence Encoder-Lite 데모\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 보기</a> </td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub 모델 보기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0HuiScHQ3OK"
      },
      "source": [
        "이 Colab은 문장 유사성 작업에 Universal Sentence Encoder-Lite를 사용하는 방법을 보여줍니다. 이 모듈은 입력 문장에서 [SentencePiece](https://github.com/google/sentencepiece) 처리를 실행해야 한다는 점만 제외하면 [Universal Sentence Encoder](https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/2)와 매우 유사합니다.\n",
        "\n",
        "Universal Sentence Encoder를 사용하면 기존에 개별 단어에 대한 임베딩을 조회하는 것처럼 쉽게 문장 수준 임베딩을 얻을 수 있습니다. 그러면 문장 임베딩을 간단히 사용하여 문장 수준의 의미론적 유사성을 계산할 수 있을 뿐만 아니라 감독되지 않은 더 적은 훈련 데이터를 사용하여 다운스트림 분류 작업의 성능을 높일 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqCB2pyK-WSU"
      },
      "source": [
        "# 시작하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWeEjoO5M0Cx"
      },
      "source": [
        "## 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5_potQBMzcU"
      },
      "outputs": [],
      "source": [
        "# Install seaborn for pretty visualizations\n",
        "!pip3 install --quiet seaborn\n",
        "# Install SentencePiece package\n",
        "# SentencePiece package is needed for Universal Sentence Encoder Lite. We'll\n",
        "# use it for all the text processing and sentence feature ID lookup.\n",
        "!pip3 install --quiet sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMTa6V4a-cmf"
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import sentencepiece as spm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPXYQDBiFJHd"
      },
      "source": [
        "## TF-Hub에서 모듈 로드하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEWUT-lmAkxM"
      },
      "outputs": [],
      "source": [
        "module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5277Z-9qARYF"
      },
      "outputs": [],
      "source": [
        "input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
        "encodings = module(\n",
        "    inputs=dict(\n",
        "        values=input_placeholder.values,\n",
        "        indices=input_placeholder.indices,\n",
        "        dense_shape=input_placeholder.dense_shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yydbhuba_nek"
      },
      "source": [
        "## TF-Hub 모듈에서 SentencePiece 모델 로드하기\n",
        "\n",
        "SentencePiece 모델은 모듈의 자산 내에 편리하게 저장됩니다. 프로세서를 초기화하려면 이 모델을 로드해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CyUjKzE_tcJ"
      },
      "outputs": [],
      "source": [
        "with tf.Session() as sess:\n",
        "  spm_path = sess.run(module(signature=\"spm_path\"))\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "with tf.io.gfile.GFile(spm_path, mode=\"rb\") as f:\n",
        "  sp.LoadFromSerializedProto(f.read())\n",
        "print(\"SentencePiece model loaded at {}.\".format(spm_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y5kkN-l-5QV"
      },
      "outputs": [],
      "source": [
        "def process_to_IDs_in_sparse_format(sp, sentences):\n",
        "  # An utility method that processes sentences with the sentence piece processor\n",
        "  # 'sp' and returns the results in tf.SparseTensor-similar format:\n",
        "  # (values, indices, dense_shape)\n",
        "  ids = [sp.EncodeAsIds(x) for x in sentences]\n",
        "  max_len = max(len(x) for x in ids)\n",
        "  dense_shape=(len(ids), max_len)\n",
        "  values=[item for sublist in ids for item in sublist]\n",
        "  indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
        "  return (values, indices, dense_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpHEWrPAdxR"
      },
      "source": [
        "### 몇 가지 예를 통해 모듈 테스트하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSkjuGYoCBfU"
      },
      "outputs": [],
      "source": [
        "# Compute a representation for each message, showing various lengths supported.\n",
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the embedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)\n",
        "\n",
        "# Reduce logging output.\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  message_embeddings = session.run(\n",
        "      encodings,\n",
        "      feed_dict={input_placeholder.values: values,\n",
        "                input_placeholder.indices: indices,\n",
        "                input_placeholder.dense_shape: dense_shape})\n",
        "\n",
        "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "    print(\"Message: {}\".format(messages[i]))\n",
        "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "    message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46jrIgHyFDz9"
      },
      "source": [
        "# 의미론적 텍스트 유사성(STS) 작업 예제\n",
        "\n",
        "Universal Sentence Encoder에 의해 생성된 임베딩은 대략적으로 정규화됩니다. 두 문장의 의미론적 유사성은 인코딩의 내적으로 간편하게 계산될 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIQudHgWBGSk"
      },
      "outputs": [],
      "source": [
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "\n",
        "def run_and_plot(session, input_placeholder, messages):\n",
        "  values, indices, dense_shape = process_to_IDs_in_sparse_format(sp,messages)\n",
        "\n",
        "  message_embeddings = session.run(\n",
        "      encodings,\n",
        "      feed_dict={input_placeholder.values: values,\n",
        "                input_placeholder.indices: indices,\n",
        "                input_placeholder.dense_shape: dense_shape})\n",
        "  \n",
        "  plot_similarity(messages, message_embeddings, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlDqttNcE0Bx"
      },
      "source": [
        "## 시각화된 유사성\n",
        "\n",
        "여기서는 히트 맵으로 유사성을 나타냅니다. 최종 그래프는 9x9 행렬이며, 각 항 `[i, j]`는 문장 `i` 및 `j`에 대한 인코딩의 내적을 바탕으로 색상이 지정됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GSCW5QIBKVe"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    # Smartphones\n",
        "    \"I like my phone\",\n",
        "    \"My phone is not good.\",\n",
        "    \"Your cellphone looks great.\",\n",
        "\n",
        "    # Weather\n",
        "    \"Will it snow tomorrow?\",\n",
        "    \"Recently a lot of hurricanes have hit the US\",\n",
        "    \"Global warming is real\",\n",
        "\n",
        "    # Food and health\n",
        "    \"An apple a day, keeps the doctors away\",\n",
        "    \"Eating strawberries is healthy\",\n",
        "    \"Is paleo better than keto?\",\n",
        "\n",
        "    # Asking about age\n",
        "    \"How old are you?\",\n",
        "    \"what is your age?\",\n",
        "]\n",
        "\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  run_and_plot(session, input_placeholder, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkZ4sRBYBnL8"
      },
      "source": [
        "## 평가: 의미론적 텍스트 유사성(STS) 벤치마크\n",
        "\n",
        "[**STS 벤치마크**](https://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark)는 문장 임베딩을 사용하여 계산된 유사성 점수가 사람의 판단과 일치하는 정도에 대한 내재적 평가를 제공합니다. 벤치마크를 위해 시스템이 다양한 문장 쌍 선택에 대한 유사성 점수를 반환해야 합니다. 그런 다음 [Pearson 상관 관계](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)를 사용하여 사람의 판단에 대한 머신 유사성 점수의 품질을 평가합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNMVfSelBsHW"
      },
      "source": [
        "### 데이터 다운로드하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zAWVzBMBptq"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import scipy\n",
        "import math\n",
        "\n",
        "\n",
        "def load_sts_dataset(filename):\n",
        "  # Loads a subset of the STS dataset into a DataFrame. In particular both\n",
        "  # sentences and their human rated similarity score.\n",
        "  sent_pairs = []\n",
        "  with tf.gfile.GFile(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      ts = line.strip().split(\"\\t\")\n",
        "      # (sent_1, sent_2, similarity_score)\n",
        "      sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
        "  return pandas.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
        "\n",
        "\n",
        "def download_and_load_sts_data():\n",
        "  sts_dataset = tf.keras.utils.get_file(\n",
        "      fname=\"Stsbenchmark.tar.gz\",\n",
        "      origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
        "      extract=True)\n",
        "\n",
        "  sts_dev = load_sts_dataset(\n",
        "      os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"))\n",
        "  sts_test = load_sts_dataset(\n",
        "      os.path.join(\n",
        "          os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"))\n",
        "\n",
        "  return sts_dev, sts_test\n",
        "\n",
        "\n",
        "sts_dev, sts_test = download_and_load_sts_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8lEawD6B4Fr"
      },
      "source": [
        "### 평가 그래프 빌드하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etiZUkP-B6bR"
      },
      "outputs": [],
      "source": [
        "sts_input1 = tf.sparse_placeholder(tf.int64, shape=(None, None))\n",
        "sts_input2 = tf.sparse_placeholder(tf.int64, shape=(None, None))\n",
        "\n",
        "# For evaluation we use exactly normalized rather than\n",
        "# approximately normalized.\n",
        "sts_encode1 = tf.nn.l2_normalize(\n",
        "    module(\n",
        "        inputs=dict(values=sts_input1.values,\n",
        "                    indices=sts_input1.indices,\n",
        "                    dense_shape=sts_input1.dense_shape)),\n",
        "    axis=1)\n",
        "sts_encode2 = tf.nn.l2_normalize(\n",
        "    module(\n",
        "        inputs=dict(values=sts_input2.values,\n",
        "                    indices=sts_input2.indices,\n",
        "                    dense_shape=sts_input2.dense_shape)),\n",
        "    axis=1)\n",
        "\n",
        "sim_scores = -tf.acos(tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Q34ssLB-rw"
      },
      "source": [
        "### 문장 임베딩 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-vRFEFPJPyeF"
      },
      "outputs": [],
      "source": [
        "#@title Choose dataset for benchmark\n",
        "dataset = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n",
        "\n",
        "values1, indices1, dense_shape1 = process_to_IDs_in_sparse_format(sp, dataset['sent_1'].tolist())\n",
        "values2, indices2, dense_shape2 = process_to_IDs_in_sparse_format(sp, dataset['sent_2'].tolist())\n",
        "similarity_scores = dataset['sim'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QJ2DI85CBDh"
      },
      "outputs": [],
      "source": [
        "def run_sts_benchmark(session):\n",
        "  \"\"\"Returns the similarity scores\"\"\"\n",
        "  scores = session.run(\n",
        "      sim_scores,\n",
        "      feed_dict={\n",
        "          sts_input1.values: values1,\n",
        "          sts_input1.indices:  indices1,\n",
        "          sts_input1.dense_shape:  dense_shape1,\n",
        "          sts_input2.values:  values2,\n",
        "          sts_input2.indices:  indices2,\n",
        "          sts_input2.dense_shape:  dense_shape2,\n",
        "      })\n",
        "  return scores\n",
        "\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  scores = run_sts_benchmark(session)\n",
        "\n",
        "pearson_correlation = scipy.stats.pearsonr(scores, similarity_scores)\n",
        "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
        "    pearson_correlation[0], pearson_correlation[1]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IJhWonqQN7u0"
      ],
      "name": "semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
