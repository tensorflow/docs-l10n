{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Compression Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# 학습된 데이터 압축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/data_compression\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/generative/data_compression.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/generative/data_compression.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/generative/data_compression.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## 개요\n",
        "\n",
        "이 노트북은 신경망과 [TensorFlow Compression](https://github.com/tensorflow/compression)을 사용하여 손실 데이터 압축을 수행하는 방법을 보여줍니다.\n",
        "\n",
        "손실 압축 시에는 샘플을 인코딩하는 데 필요한 예상 비트 수인 **rate**와 샘플 재구성에서 예상되는 오류인 **왜곡** 사이의 절충이 포함됩니다.\n",
        "\n",
        "아래 예에서는 자동 인코더와 유사한 모델을 사용하여 MNIST 데이터세트의 이미지를 압축합니다. 이 방법은 [End-to-end Optimized Image Compression](https://arxiv.org/abs/1611.01704) 논문을 기초로 합니다.\n",
        "\n",
        "학습된 데이터 압축에 대한 더 많은 배경 지식은 기존 데이터 압축에 익숙한 사람들을 대상으로 하는 [이 백서](https://arxiv.org/abs/2007.03034) 또는 머신 러닝에 관련된 사람들을 대상으로 하는 [이 설문조사](https://arxiv.org/abs/2202.06533)에서 찾을 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## 설정\n",
        "\n",
        "`pip`를 통해 Tensorflow Compression을 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K489KsEgxuLI"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Installs the latest version of TFC compatible with the installed TF version.\n",
        "\n",
        "read MAJOR MINOR <<< \"$(pip show tensorflow | perl -p -0777 -e 's/.*Version: (\\d+)\\.(\\d+).*/\\1 \\2/sg')\"\n",
        "pip install \"tensorflow-compression<$MAJOR.$(($MINOR+1))\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfVAmHCVxpTS"
      },
      "source": [
        "라이브러리 종속성을 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_compression as tfc\n",
        "import tensorflow_datasets as tfds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsncKT2iymgQ"
      },
      "source": [
        "## 트레이너 모델 정의하기\n",
        "\n",
        "모델이 자동 인코더와 유사하고 훈련 및 추론 중에 다른 기능들을 수행해야 하기 때문에 설정은 예를 들어 분류자와 약간 다릅니다.\n",
        "\n",
        "훈련 모델은 세 부분으로 구성됩니다.\n",
        "\n",
        "- **분석**(또는 인코더) 변환: 이미지에서 잠재 공간으로 변환\n",
        "- **합성**(또는 디코더) 변환: 잠재 공간에서 이미지 공간으로 다시 변환\n",
        "- **Prior** 및 엔트로피 모델: 잠재 공간의 한계 확률 모델링\n",
        "\n",
        "먼저 변환을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yZESLgW-vp1"
      },
      "outputs": [],
      "source": [
        "def make_analysis_transform(latent_dims):\n",
        "  \"\"\"Creates the analysis (encoder) transform.\"\"\"\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(\n",
        "          20, 5, use_bias=True, strides=2, padding=\"same\",\n",
        "          activation=\"leaky_relu\", name=\"conv_1\"),\n",
        "      tf.keras.layers.Conv2D(\n",
        "          50, 5, use_bias=True, strides=2, padding=\"same\",\n",
        "          activation=\"leaky_relu\", name=\"conv_2\"),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(\n",
        "          500, use_bias=True, activation=\"leaky_relu\", name=\"fc_1\"),\n",
        "      tf.keras.layers.Dense(\n",
        "          latent_dims, use_bias=True, activation=None, name=\"fc_2\"),\n",
        "  ], name=\"analysis_transform\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sHdYBzF2xcu"
      },
      "outputs": [],
      "source": [
        "def make_synthesis_transform():\n",
        "  \"\"\"Creates the synthesis (decoder) transform.\"\"\"\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          500, use_bias=True, activation=\"leaky_relu\", name=\"fc_1\"),\n",
        "      tf.keras.layers.Dense(\n",
        "          2450, use_bias=True, activation=\"leaky_relu\", name=\"fc_2\"),\n",
        "      tf.keras.layers.Reshape((7, 7, 50)),\n",
        "      tf.keras.layers.Conv2DTranspose(\n",
        "          20, 5, use_bias=True, strides=2, padding=\"same\",\n",
        "          activation=\"leaky_relu\", name=\"conv_1\"),\n",
        "      tf.keras.layers.Conv2DTranspose(\n",
        "          1, 5, use_bias=True, strides=2, padding=\"same\",\n",
        "          activation=\"leaky_relu\", name=\"conv_2\"),\n",
        "  ], name=\"synthesis_transform\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYC8tHhkxTlK"
      },
      "source": [
        "트레이너는 두 변환의 인스턴스와 이전의 매개변수를 모두 보유합니다.\n",
        "\n",
        "`call` 메서드는 다음을 계산하도록 설정됩니다.\n",
        "\n",
        "- **비율**: 숫자 배치를 나타내는 데 필요한 비트 수에 대한 추정치\n",
        "- **왜곡**: 원래 숫자의 픽셀과 재구성된 픽셀 간의 평균 절대 차이\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROn2DbzsBirI"
      },
      "outputs": [],
      "source": [
        "class MNISTCompressionTrainer(tf.keras.Model):\n",
        "  \"\"\"Model that trains a compressor/decompressor for MNIST.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_dims):\n",
        "    super().__init__()\n",
        "    self.analysis_transform = make_analysis_transform(latent_dims)\n",
        "    self.synthesis_transform = make_synthesis_transform()\n",
        "    self.prior_log_scales = tf.Variable(tf.zeros((latent_dims,)))\n",
        "\n",
        "  @property\n",
        "  def prior(self):\n",
        "    return tfc.NoisyLogistic(loc=0., scale=tf.exp(self.prior_log_scales))\n",
        "\n",
        "  def call(self, x, training):\n",
        "    \"\"\"Computes rate and distortion losses.\"\"\"\n",
        "    # Ensure inputs are floats in the range (0, 1).\n",
        "    x = tf.cast(x, self.compute_dtype) / 255.\n",
        "    x = tf.reshape(x, (-1, 28, 28, 1))\n",
        "\n",
        "    # Compute latent space representation y, perturb it and model its entropy,\n",
        "    # then compute the reconstructed pixel-level representation x_hat.\n",
        "    y = self.analysis_transform(x)\n",
        "    entropy_model = tfc.ContinuousBatchedEntropyModel(\n",
        "        self.prior, coding_rank=1, compression=False)\n",
        "    y_tilde, rate = entropy_model(y, training=training)\n",
        "    x_tilde = self.synthesis_transform(y_tilde)\n",
        "\n",
        "    # Average number of bits per MNIST digit.\n",
        "    rate = tf.reduce_mean(rate)\n",
        "\n",
        "    # Mean absolute difference across pixels.\n",
        "    distortion = tf.reduce_mean(abs(x - x_tilde))\n",
        "\n",
        "    return dict(rate=rate, distortion=distortion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEXbp9RV3kRX"
      },
      "source": [
        "### 비율과 왜곡 계산하기\n",
        "\n",
        "훈련 세트의 이미지 하나를 사용하여 이를 단계별로 살펴보겠습니다. 훈련 및 검증을 위해 MNIST 데이터세트를 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FV99WTrIBen"
      },
      "outputs": [],
      "source": [
        "training_dataset, validation_dataset = tfds.load(\n",
        "    \"mnist\",\n",
        "    split=[\"train\", \"test\"],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwKgNTg_QfjH"
      },
      "source": [
        "그리고 하나의 이미지 $x$를 추출합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-BSdeHcPBBf"
      },
      "outputs": [],
      "source": [
        "(x, _), = validation_dataset.take(1)\n",
        "\n",
        "plt.imshow(tf.squeeze(x))\n",
        "print(f\"Data type: {x.dtype}\")\n",
        "print(f\"Shape: {x.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8IvuFkrRJIa"
      },
      "source": [
        "잠재 표현 $y$를 얻으려면 `float32`로 캐스팅하고 배치 차원을 추가하고 분석 변환을 통해 전달해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA0DOWq23lEq"
      },
      "outputs": [],
      "source": [
        "x = tf.cast(x, tf.float32) / 255.\n",
        "x = tf.reshape(x, (-1, 28, 28, 1))\n",
        "y = make_analysis_transform(10)(x)\n",
        "\n",
        "print(\"y:\", y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTojJQvZT8SX"
      },
      "source": [
        "잠재 공간은 테스트 시간에 양자화됩니다. 훈련 중에 이를 미분 가능한 방식으로 모델링하기 위해 $(-.5, .5)$ 구간에 균일한 노이즈를 추가하고 결과를 $\\tilde y$라고 합니다. 이 용어는 [End-to-end Optimized Image Compression](https://arxiv.org/abs/1611.01704) 논문에서 사용된 것과 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spr3503OUOFQ"
      },
      "outputs": [],
      "source": [
        "y_tilde = y + tf.random.uniform(y.shape, -.5, .5)\n",
        "\n",
        "print(\"y_tilde:\", y_tilde)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hRN89R7SA3U"
      },
      "source": [
        "\"Prior\"는 노이즈가 많은 잠재 공간의 한계 분포를 모델링하기 위해 훈련하는 확률 밀도입니다. 예를 들어, 각 잠재 차원에 대해 다른 척도를 가진 독립적인 [로지스틱 분포](https://en.wikipedia.org/wiki/Logistic_distribution) 세트일 수 있습니다. `tfc.NoisyLogistic`은 잠재 공간에 부가적인 노이즈가 있다는 사실을 설명합니다. 척도가 0에 가까워지면 로지스틱 분포가 dirac 델타(스파이크)에 접근하지만 추가된 노이즈로 인해 \"노이즈가 많은\" 분포가 대신 균일 분포에 접근합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tmA1Bw7ReMY"
      },
      "outputs": [],
      "source": [
        "prior = tfc.NoisyLogistic(loc=0., scale=tf.linspace(.01, 2., 10))\n",
        "\n",
        "_ = tf.linspace(-6., 6., 501)[:, None]\n",
        "plt.plot(_, prior.prob(_));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NSWtBZmUvVY"
      },
      "source": [
        "훈련하는 동안 `tfc.ContinuousBatchedEntropyModel`은 균일한 노이즈를 추가하고 노이즈와 prior를 사용하여 비율(잠재 표현을 인코딩하는 데 필요한 평균 비트 수)에 대한 (미분 가능한) 상한을 계산합니다. 그 경계는 손실로 최소화할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFuGlyJuThBC"
      },
      "outputs": [],
      "source": [
        "entropy_model = tfc.ContinuousBatchedEntropyModel(\n",
        "    prior, coding_rank=1, compression=False)\n",
        "y_tilde, rate = entropy_model(y, training=True)\n",
        "\n",
        "print(\"rate:\", rate)\n",
        "print(\"y_tilde:\", y_tilde)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyr8DGgmWd32"
      },
      "source": [
        "마지막으로, 노이즈가 많은 잠재 공간은 합성 변환을 통해 다시 전달되어 이미지 재구성 $\\tilde x$를 생성합니다. 왜곡은 원본 이미지와 재구성 사이의 오차입니다. 훈련되지 않은 변환을 사용하면 재구성이 그다지 유용하지 않다는 것은 분명합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtmI0xGEVym0"
      },
      "outputs": [],
      "source": [
        "x_tilde = make_synthesis_transform()(y_tilde)\n",
        "\n",
        "# Mean absolute difference across pixels.\n",
        "distortion = tf.reduce_mean(abs(x - x_tilde))\n",
        "print(\"distortion:\", distortion)\n",
        "\n",
        "x_tilde = tf.saturate_cast(x_tilde[0] * 255, tf.uint8)\n",
        "plt.imshow(tf.squeeze(x_tilde))\n",
        "print(f\"Data type: {x_tilde.dtype}\")\n",
        "print(f\"Shape: {x_tilde.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVz3I7E8ecij"
      },
      "source": [
        "모든 숫자 배치에 대해 `MNISTCompressionTrainer`를 호출하면 해당 배치에 대한 평균으로 비율과 왜곡이 생성됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICJnjj1LeB8L"
      },
      "outputs": [],
      "source": [
        "(example_batch, _), = validation_dataset.batch(32).take(1)\n",
        "trainer = MNISTCompressionTrainer(10)\n",
        "example_output = trainer(example_batch)\n",
        "\n",
        "print(\"rate: \", example_output[\"rate\"])\n",
        "print(\"distortion: \", example_output[\"distortion\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgdfRtmee5Mn"
      },
      "source": [
        "다음 섹션에서는 이러한 두 손실에 대해 경사하강법을 수행하도록 모델을 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKGVwv5MAq6w"
      },
      "source": [
        "## 모델 훈련하기\n",
        "\n",
        "비율-왜곡 Lagrangian, 즉 비율과 왜곡의 합을 최적화하는 방식으로 트레이너를 컴파일합니다. 여기서 한 가지 항은 Lagrange 매개변수 $\\lambda$에 의해 가중됩니다.\n",
        "\n",
        "이 손실 함수는 모델의 다른 부분에 다르게 영향을 줍니다.\n",
        "\n",
        "- 분석 변환은 비율과 왜곡 사이의 원하는 절충을 실현하는 잠재 표현을 생성하도록 훈련됩니다.\n",
        "- 합성 변환은 잠재적 표현이 주어지면 왜곡을 최소화하도록 훈련됩니다.\n",
        "- Prior의 매개변수는 잠재 표현이 주어지면 비율을 최소화하도록 훈련됩니다. 이는 최대 가능성의 의미에서 잠재 공간의 한계 분포로 prior를 피팅하는 것과 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5mm1aDkcgAf"
      },
      "outputs": [],
      "source": [
        "def pass_through_loss(_, x):\n",
        "  # Since rate and distortion are unsupervised, the loss doesn't need a target.\n",
        "  return x\n",
        "\n",
        "def make_mnist_compression_trainer(lmbda, latent_dims=50):\n",
        "  trainer = MNISTCompressionTrainer(latent_dims)\n",
        "  trainer.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    # Just pass through rate and distortion as losses/metrics.\n",
        "    loss=dict(rate=pass_through_loss, distortion=pass_through_loss),\n",
        "    metrics=dict(rate=pass_through_loss, distortion=pass_through_loss),\n",
        "    loss_weights=dict(rate=1., distortion=lmbda),\n",
        "  )\n",
        "  return trainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPwd4DTs3Mfr"
      },
      "source": [
        "다음으로 모델을 훈련시킵니다. 사람의 주석은 여기에서 필요하지 않습니다. 우리는 단지 이미지를 압축하기를 원하기 때문에 `map`을 사용하여 이미지를 삭제하고 대신 비율과 왜곡에 대해 \"더미\" 타겟을 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNBpCTgzAV7M"
      },
      "outputs": [],
      "source": [
        "def add_rd_targets(image, label):\n",
        "  # Training is unsupervised, so labels aren't necessary here. However, we\n",
        "  # need to add \"dummy\" targets for rate and distortion.\n",
        "  return image, dict(rate=0., distortion=0.)\n",
        "\n",
        "def train_mnist_model(lmbda):\n",
        "  trainer = make_mnist_compression_trainer(lmbda)\n",
        "  trainer.fit(\n",
        "      training_dataset.map(add_rd_targets).batch(128).prefetch(8),\n",
        "      epochs=15,\n",
        "      validation_data=validation_dataset.map(add_rd_targets).batch(128).cache(),\n",
        "      validation_freq=1,\n",
        "      verbose=1,\n",
        "  )\n",
        "  return trainer\n",
        "\n",
        "trainer = train_mnist_model(lmbda=2000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td4xuttmCd7T"
      },
      "source": [
        "## 일부 MNIST 이미지 압축하기\n",
        "\n",
        "테스트 시간에 압축 및 압축 해제를 위해 훈련된 모델을 두 부분으로 나눕니다.\n",
        "\n",
        "- 인코더 쪽은 분석 변환과 엔트로피 모델로 구성됩니다.\n",
        "- 디코더 쪽은 합성 변환과 동일한 엔트로피 모델로 구성됩니다.\n",
        "\n",
        "테스트 시간에 잠재 공간에는 추가 노이즈가 없지만 양자화되고 손실 없이 압축되므로 새로운 이름을 지정합니다. 우리는 이것과 이미지 재구성을 각각 $\\hat x$ 및 $\\hat y$라고 부릅니다([End-to-end Optimized Image Compression](https://arxiv.org/abs/1611.01704)을 따름)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBRAPa5jksss"
      },
      "outputs": [],
      "source": [
        "class MNISTCompressor(tf.keras.Model):\n",
        "  \"\"\"Compresses MNIST images to strings.\"\"\"\n",
        "\n",
        "  def __init__(self, analysis_transform, entropy_model):\n",
        "    super().__init__()\n",
        "    self.analysis_transform = analysis_transform\n",
        "    self.entropy_model = entropy_model\n",
        "\n",
        "  def call(self, x):\n",
        "    # Ensure inputs are floats in the range (0, 1).\n",
        "    x = tf.cast(x, self.compute_dtype) / 255.\n",
        "    y = self.analysis_transform(x)\n",
        "    # Also return the exact information content of each digit.\n",
        "    _, bits = self.entropy_model(y, training=False)\n",
        "    return self.entropy_model.compress(y), bits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSZ0X2xPnkN-"
      },
      "outputs": [],
      "source": [
        "class MNISTDecompressor(tf.keras.Model):\n",
        "  \"\"\"Decompresses MNIST images from strings.\"\"\"\n",
        "\n",
        "  def __init__(self, entropy_model, synthesis_transform):\n",
        "    super().__init__()\n",
        "    self.entropy_model = entropy_model\n",
        "    self.synthesis_transform = synthesis_transform\n",
        "\n",
        "  def call(self, string):\n",
        "    y_hat = self.entropy_model.decompress(string, ())\n",
        "    x_hat = self.synthesis_transform(y_hat)\n",
        "    # Scale and cast back to 8-bit integer.\n",
        "    return tf.saturate_cast(tf.round(x_hat * 255.), tf.uint8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI7rxeOUDnaC"
      },
      "source": [
        "`compression=True`로 인스턴스화하면 엔트로피 모델은 훈련된 prior를 범위 코딩 알고리즘에 대한 테이블로 변환합니다. `compress()`를 호출할 때 이 알고리즘이 호출되어 잠재 공간 벡터를 비트 시퀀스로 변환합니다. 각 바이너리 문자열의 길이는 잠재 공간의 내용에 근사합니다(prior 아래 잠재 공간의 음의 로그 가능성).\n",
        "\n",
        "범위 코딩 테이블이 양쪽에서 정확히 동일해야 하기 때문에 압축 및 압축 해제를 위한 엔트로피 모델은 동일한 인스턴스여야 합니다. 그렇지 않으면 디코딩 오류가 발생할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dnm_p7mbnigo"
      },
      "outputs": [],
      "source": [
        "def make_mnist_codec(trainer, **kwargs):\n",
        "  # The entropy model must be created with `compression=True` and the same\n",
        "  # instance must be shared between compressor and decompressor.\n",
        "  entropy_model = tfc.ContinuousBatchedEntropyModel(\n",
        "      trainer.prior, coding_rank=1, compression=True, **kwargs)\n",
        "  compressor = MNISTCompressor(trainer.analysis_transform, entropy_model)\n",
        "  decompressor = MNISTDecompressor(entropy_model, trainer.synthesis_transform)\n",
        "  return compressor, decompressor\n",
        "\n",
        "compressor, decompressor = make_mnist_codec(trainer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYu5sVVH3YMv"
      },
      "source": [
        "검증 데이터세트에서 16개의 이미지를 가져옵니다. 인수를 `skip`으로 변경하여 다른 하위 집합을 선택할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAxArlU728K5"
      },
      "outputs": [],
      "source": [
        "(originals, _), = validation_dataset.batch(16).skip(3).take(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHeN_ny929YS"
      },
      "source": [
        "이를 문자열로 압축하고 각 내용을 비트 단위로 추적합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smOk42gQ3IXv"
      },
      "outputs": [],
      "source": [
        "strings, entropies = compressor(originals)\n",
        "\n",
        "print(f\"String representation of first digit in hexadecimal: 0x{strings[0].numpy().hex()}\")\n",
        "print(f\"Number of bits actually needed to represent it: {entropies[0]:0.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j9R4bTT3Qhl"
      },
      "source": [
        "문자열에서 이미지를 다시 압축 해제합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOP6pEqU3P0w"
      },
      "outputs": [],
      "source": [
        "reconstructions = decompressor(strings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWo0Q-vy23tt"
      },
      "source": [
        "압축된 바이너리 표현 및 재구성된 숫자와 함께 16개의 원래 숫자를 각각 표시합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jU5IqzZzeEpf"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "def display_digits(originals, strings, entropies, reconstructions):\n",
        "  \"\"\"Visualizes 16 digits together with their reconstructions.\"\"\"\n",
        "  fig, axes = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(12.5, 5))\n",
        "  axes = axes.ravel()\n",
        "  for i in range(len(axes)):\n",
        "    image = tf.concat([\n",
        "        tf.squeeze(originals[i]),\n",
        "        tf.zeros((28, 14), tf.uint8),\n",
        "        tf.squeeze(reconstructions[i]),\n",
        "    ], 1)\n",
        "    axes[i].imshow(image)\n",
        "    axes[i].text(\n",
        "        .5, .5, f\"→ 0x{strings[i].numpy().hex()} →\\n{entropies[i]:0.2f} bits\",\n",
        "        ha=\"center\", va=\"top\", color=\"white\", fontsize=\"small\",\n",
        "        transform=axes[i].transAxes)\n",
        "    axes[i].axis(\"off\")\n",
        "  plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km9PqVEtPJPc"
      },
      "outputs": [],
      "source": [
        "display_digits(originals, strings, entropies, reconstructions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzlrIOiYOzJc"
      },
      "source": [
        "인코딩된 문자열의 길이는 각 숫자의 내용과 다릅니다.\n",
        "\n",
        "이는 범위 코딩 프로세스가 이산 확률로 작동하고 약간의 오버헤드가 있기 때문입니다. 따라서 특히 짧은 문자열의 경우 해당 일치는 대략적인 것입니다. 그러나 범위 코딩은 **점근적으로 최적**입니다. 한도 내에서 예상 비트 수는 교차 엔트로피(예상되는 내용)에 접근하며, 이에 대한 훈련 모델의 비율 항은 상한입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78qIG8t8FvJW"
      },
      "source": [
        "## 비율-왜곡 절충\n",
        "\n",
        "위의 모델은 각 숫자를 나타내는 데 사용된 평균 비트 수와 재구성 시 발생한 오차 사이의 특정 절충(`lmbda=2000`으로 지정)을 찾도록 훈련되었습니다.\n",
        "\n",
        "다른 값으로 실험을 반복하면 어떻게 될까요?\n",
        "\n",
        "우선 $\\lambda$를 500으로 줄여보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iFcAD0WF78p"
      },
      "outputs": [],
      "source": [
        "def train_and_visualize_model(lmbda):\n",
        "  trainer = train_mnist_model(lmbda=lmbda)\n",
        "  compressor, decompressor = make_mnist_codec(trainer)\n",
        "  strings, entropies = compressor(originals)\n",
        "  reconstructions = decompressor(strings)\n",
        "  display_digits(originals, strings, entropies, reconstructions)\n",
        "\n",
        "train_and_visualize_model(lmbda=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5OkgJMObMc"
      },
      "source": [
        "숫자의 충실도와 마찬가지로 코드의 비트 전송률이 낮아집니다. 그러나 대부분의 숫자는 여전히 인식할 수 있습니다.\n",
        "\n",
        "$\\lambda$를 더 줄여보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQp9_9_5GcxH"
      },
      "outputs": [],
      "source": [
        "train_and_visualize_model(lmbda=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ELLMAN1OwMQ"
      },
      "source": [
        "문자열은 이제 숫자당 1바이트 정도로 훨씬 짧아지기 시작합니다. 그러나 여기에는 대가가 따릅니다. 더 많은 숫자가 인식할 수 없게 됩니다.\n",
        "\n",
        "이것은 이 모델이 오류에 대한 인간의 인식을 고려하지 않으며 픽셀 값의 관점에서 절대 편차만 측정한다는 것을 보여줍니다. 더 잘 인식되는 이미지 품질을 얻으려면 픽셀 손실을 인지 손실로 대체해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cWHtH0LP_r"
      },
      "source": [
        "## 디코더를 생성 모델로 사용하기\n",
        "\n",
        "디코더에 임의의 비트를 공급하면 모델이 숫자를 나타내도록 학습한 분포에서 효과적으로 샘플링됩니다.\n",
        "\n",
        "먼저, 입력 문자열이 완전히 디코딩되지 않았는지 감지하는 온전성 검사 없이 압축기/압축 해제기를 다시 인스턴스화합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnic8YsM0_ke"
      },
      "outputs": [],
      "source": [
        "compressor, decompressor = make_mnist_codec(trainer, decode_sanity_check=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86uc9_Is1eeo"
      },
      "source": [
        "이제 충분히 긴 임의의 문자열을 압축 해제기에 공급하여 숫자를 디코딩/샘플링할 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4fP7BkqKCHY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "strings = tf.constant([os.urandom(8) for _ in range(16)])\n",
        "samples = decompressor(strings)\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(5, 5))\n",
        "axes = axes.ravel()\n",
        "for i in range(len(axes)):\n",
        "  axes[i].imshow(tf.squeeze(samples[i]))\n",
        "  axes[i].axis(\"off\")\n",
        "plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "data_compression.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
