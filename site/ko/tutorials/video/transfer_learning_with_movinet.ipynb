{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOgDUDMAG6mn"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B3PsBDmGG_W8"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifkGYxdCHIof"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWxDDkRwLVMC"
      },
      "source": [
        "# MoViNet을 사용한 비디오 분류 전이 학습\n",
        "\n",
        "MoViNets(Mobile Video Networks)는 스트리밍 비디오에 대한 추론을 지원하는 효율적인 비디오 분류 모델군을 제공합니다. 이 튜토리얼에서는 사전 훈련된 MoViNet 모델을 사용하여 [UCF101 데이터세트](https://www.crcv.ucf.edu/data/UCF101.php)로부터 특히 동작 인식 작업을 위해 비디오를 분류합니다. 사전 훈련된 모델은 이전에 더 큰 데이터세트에서 훈련된 저장된 네트워크입니다. Kondratyuk, D. 등(2021)의 [MoViNets: 효율적인 비디오 인식을 위한 모바일 비디오 네트워크](https://arxiv.org/abs/2103.11511) 논문에서 MoViNets에 대한 자세한 내용을 확인할 수 있습니다. 이 튜토리얼에서는 다음을 수행합니다.\n",
        "\n",
        "- 사전 훈련된 MoViNet 모델을 다운로드하는 방법을 알아봅니다.\n",
        "- MoViNet 모델의 컨볼루션 기반을 동결하여 새로운 분류기가 있는 사전 훈련된 모델로 새 모델을 생성합니다.\n",
        "- 분류기 헤드를 새 데이터세트의 레이블 수로 교체합니다.\n",
        "- [UCF101 데이터세트](https://www.crcv.ucf.edu/data/UCF101.php)에서 전이 학습을 수행합니다.\n",
        "\n",
        "이 튜토리얼에서 다운로드한 모델은 [official/projects/movinet](https://github.com/tensorflow/models/tree/master/official/projects/movinet)에서 가져온 것입니다. 이 리포지토리에는 TF Hub가 TensorFlow 2 SavedModel 형식으로 사용하는 MoViNet 모델 컬렉션이 포함되어 있습니다.\n",
        "\n",
        "이 전이 학습 튜토리얼은 TensorFlow 비디오 튜토리얼 시리즈의 세 번째 부분입니다. 다른 세 개의 튜토리얼은 다음과 같습니다.\n",
        "\n",
        "- [비디오 데이터 로드](https://www.tensorflow.org/tutorials/load_data/video): 이 튜토리얼은 이 문서에서 사용된 많은 코드를 설명합니다. 특히 `FrameGenerator` 클래스를 통해 데이터를 전처리하고 로드하는 방법에 대해 자세히 설명합니다.\n",
        "- [비디오 분류를 위한 3D CNN 모델 구축](https://www.tensorflow.org/tutorials/video/video_classification): 이 튜토리얼에서는 3D 데이터의 공간적 및 시간적 측면을 분해하는 (2+1)D CNN을 사용합니다. MRI 스캔과 같은 체적 데이터를 사용하는 경우 (2+1)D CNN 대신 3D CNN을 사용하는 것이 좋습니다.\n",
        "- [스트리밍 동작 인식을 위한 MoViNet](https://www.tensorflow.org/hub/tutorials/movinet): TF Hub에서 사용할 수 있는 MoViNet 모델에 익숙해집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GidiisyXwK--"
      },
      "source": [
        "## 설정\n",
        "\n",
        "우선 ZIP 파일의 내용을 검사하기 위한 [remotezip](https://github.com/gtsystem/python-remotezip), 진행률 표시줄을 사용하기 위한 [tqdm](https://github.com/tqdm/tqdm), 비디오 파일을 처리하기 위한 [OpenCV](https://opencv.org/)(`opencv-python` 및 `opencv-python-headless`가 동일한 버전인지 확인) 및 사전 훈련된 MoViNet 모델을 다운로드하기 위한 TensorFlow 모델([`tf-models-official`](https://github.com/tensorflow/models/tree/master/official))을 포함하여 몇 가지 필요한 라이브러리를 설치하고 가져옵니다. TensorFlow 모델 패키지는 TensorFlow의 고급 API를 사용하는 모델 컬렉션입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nubWhqYdwEXD"
      },
      "outputs": [],
      "source": [
        "!pip install remotezip tqdm opencv-python==4.5.2.52 opencv-python-headless==4.5.2.52 tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QImPsudoK9JI"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w3H4dfOPfnm"
      },
      "source": [
        "## 데이터 로드하기\n",
        "\n",
        "아래 숨겨진 셀은 UCF-101 데이터세트에서 데이터 조각을 다운로드하고 `tf.data.Dataset`에 로드하는 헬퍼 함수를 정의합니다. [비디오 데이터 로드 튜토리얼](https://www.tensorflow.org/tutorials/load_data/video)은 이 코드에 대한 자세한 연습을 제공합니다.\n",
        "\n",
        "숨겨진 블록 끝에 있는 `FrameGenerator` 클래스는 여기에서 가장 중요한 유틸리티로, TensorFlow 데이터 파이프라인에 데이터를 공급할 수 있는 반복 가능한 객체를 생성합니다. 특히 이 클래스에는 인코딩된 레이블과 함께 비디오 프레임을 로드하는 Python 생성기가 포함되어 있습니다. 생성기(`__call__`) 함수는 `frames_from_video_file`에 의해 생성된 프레임 배열과 프레임 세트와 관련된 레이블의 원-핫 인코딩 벡터를 생성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fwEhJ13_PSy6"
      },
      "outputs": [],
      "source": [
        "#@title \n",
        "\n",
        "def list_files_per_class(zip_url):\n",
        "  \"\"\"\n",
        "    List the files in each class of the dataset given the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: URL from which the files can be unzipped. \n",
        "\n",
        "    Return:\n",
        "      files: List of files in each of the classes.\n",
        "  \"\"\"\n",
        "  files = []\n",
        "  with rz.RemoteZip(URL) as zip:\n",
        "    for zip_info in zip.infolist():\n",
        "      files.append(zip_info.filename)\n",
        "  return files\n",
        "\n",
        "def get_class(fname):\n",
        "  \"\"\"\n",
        "    Retrieve the name of the class given a filename.\n",
        "\n",
        "    Args:\n",
        "      fname: Name of the file in the UCF101 dataset.\n",
        "\n",
        "    Return:\n",
        "      Class that the file belongs to.\n",
        "  \"\"\"\n",
        "  return fname.split('_')[-3]\n",
        "\n",
        "def get_files_per_class(files):\n",
        "  \"\"\"\n",
        "    Retrieve the files that belong to each class. \n",
        "\n",
        "    Args:\n",
        "      files: List of files in the dataset.\n",
        "\n",
        "    Return:\n",
        "      Dictionary of class names (key) and files (values).\n",
        "  \"\"\"\n",
        "  files_for_class = collections.defaultdict(list)\n",
        "  for fname in files:\n",
        "    class_name = get_class(fname)\n",
        "    files_for_class[class_name].append(fname)\n",
        "  return files_for_class\n",
        "\n",
        "def download_from_zip(zip_url, to_dir, file_names):\n",
        "  \"\"\"\n",
        "    Download the contents of the zip file from the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: Zip URL containing data.\n",
        "      to_dir: Directory to download data to.\n",
        "      file_names: Names of files to download.\n",
        "  \"\"\"\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for fn in tqdm.tqdm(file_names):\n",
        "      class_name = get_class(fn)\n",
        "      zip.extract(fn, str(to_dir / class_name))\n",
        "      unzipped_file = to_dir / class_name / fn\n",
        "\n",
        "      fn = pathlib.Path(fn).parts[-1]\n",
        "      output_file = to_dir / class_name / fn\n",
        "      unzipped_file.rename(output_file,)\n",
        "\n",
        "def split_class_lists(files_for_class, count):\n",
        "  \"\"\"\n",
        "    Returns the list of files belonging to a subset of data as well as the remainder of\n",
        "    files that need to be downloaded.\n",
        "\n",
        "    Args:\n",
        "      files_for_class: Files belonging to a particular class of data.\n",
        "      count: Number of files to download.\n",
        "\n",
        "    Return:\n",
        "      split_files: Files belonging to the subset of data.\n",
        "      remainder: Dictionary of the remainder of files that need to be downloaded.\n",
        "  \"\"\"\n",
        "  split_files = []\n",
        "  remainder = {}\n",
        "  for cls in files_for_class:\n",
        "    split_files.extend(files_for_class[cls][:count])\n",
        "    remainder[cls] = files_for_class[cls][count:]\n",
        "  return split_files, remainder\n",
        "\n",
        "def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n",
        "  \"\"\"\n",
        "    Download a subset of the UFC101 dataset and split them into various parts, such as\n",
        "    training, validation, and test. \n",
        "\n",
        "    Args:\n",
        "      zip_url: Zip URL containing data.\n",
        "      num_classes: Number of labels.\n",
        "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n",
        "              (value is number of files per split).\n",
        "      download_dir: Directory to download data to.\n",
        "\n",
        "    Return:\n",
        "      dir: Posix path of the resulting directories containing the splits of data.\n",
        "  \"\"\"\n",
        "  files = list_files_per_class(zip_url)\n",
        "  for f in files:\n",
        "    tokens = f.split('/')\n",
        "    if len(tokens) <= 2:\n",
        "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
        "\n",
        "  files_for_class = get_files_per_class(files)\n",
        "\n",
        "  classes = list(files_for_class.keys())[:num_classes]\n",
        "\n",
        "  for cls in classes:\n",
        "    new_files_for_class = files_for_class[cls]\n",
        "    random.shuffle(new_files_for_class)\n",
        "    files_for_class[cls] = new_files_for_class\n",
        "\n",
        "  # Only use the number of classes you want in the dictionary\n",
        "  files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n",
        "\n",
        "  dirs = {}\n",
        "  for split_name, split_count in splits.items():\n",
        "    print(split_name, \":\")\n",
        "    split_dir = download_dir / split_name\n",
        "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
        "    download_from_zip(zip_url, split_dir, split_files)\n",
        "    dirs[split_name] = split_dir\n",
        "\n",
        "  return dirs\n",
        "\n",
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded. \n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame\n",
        "\n",
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))  \n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result\n",
        "\n",
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    \"\"\" Returns a set of frames with their associated label. \n",
        "\n",
        "      Args:\n",
        "        path: Video file paths.\n",
        "        n_frames: Number of frames. \n",
        "        training: Boolean to determine if training dataset is being created.\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.avi'))\n",
        "    classes = [p.parent.name for p in video_paths] \n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames) \n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDHrNLZkPSR9"
      },
      "outputs": [],
      "source": [
        "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\n",
        "download_dir = pathlib.Path('./UCF101_subset/')\n",
        "subset_paths = download_ufc_101_subset(URL, \n",
        "                        num_classes = 10, \n",
        "                        splits = {\"train\": 30, \"test\": 20}, \n",
        "                        download_dir = download_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYYShfhMx9DW"
      },
      "source": [
        "훈련 및 테스트 데이터세트를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-twTu3_Bx-iJ"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "num_frames = 8\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n",
        "                                          output_signature = output_signature)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n",
        "                                         output_signature = output_signature)\n",
        "test_ds = test_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7stgmuBCGQT"
      },
      "source": [
        "여기에서 생성된 레이블은 클래스의 인코딩을 나타냅니다. 예를 들어, 'ApplyEyeMakeup'은 정수에 매핑됩니다. 훈련 데이터의 레이블을 살펴보고 데이터세트가 충분히 섞였는지 확인하세요. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9L2-toXCOQq"
      },
      "outputs": [],
      "source": [
        "for frames, labels in train_ds.take(10):\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ3qwZnpfy9c"
      },
      "source": [
        "데이터의 모양을 살펴봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6MqP4m2fyQT"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxbhPqXGvc_F"
      },
      "source": [
        "## MoViNet이란?\n",
        "\n",
        "앞서 언급한 바와 같이 [MoViNets](https://arxiv.org/abs/2103.11511)는 동작 인식과 같은 작업에서 스트리밍 비디오 또는 온라인 추론에 사용되는 비디오 분류 모델입니다. MoViNets를 사용하여 동작 인식에 비디오 데이터를 분류하는 방법을 이용해 보세요.\n",
        "\n",
        "2D 프레임 기반 분류자는 전체 비디오에 걸쳐 실행하거나 한 번에 한 프레임씩 스트리밍하기에 효율적이고 간단합니다. 시간적 컨텍스트를 고려할 수 없기 때문에 정확도가 제한되고 프레임 간에 일관성 없는 출력을 제공할 수 있습니다.\n",
        "\n",
        "간단한 3D CNN은 정확도와 시간적 일관성을 높일 수 있는 양방향 시간 컨텍스트를 사용합니다. 이러한 네트워크는 더 많은 리소스가 필요할 수 있으며 미래를 내다보기 때문에 스트리밍 데이터에 사용할 수 없습니다.\n",
        "\n",
        "![표준 컨볼루션](https://www.tensorflow.org/images/tutorials/video/standard_convolution.png)\n",
        "\n",
        "MoViNet 아키텍처는 시간 축을 따라 \"인과적\"인 3D 컨볼루션을 사용합니다(예: `padding=\"causal\"`을 포함한 `layers.Conv1D`). 이것은 두 접근 방식의 장점 중 일부를 제공하며 주로 효율적인 스트리밍의 이점을 줍니다.\n",
        "\n",
        "![인과 컨볼루션](https://www.tensorflow.org/images/tutorials/video/causal_convolution.png)\n",
        "\n",
        "인과 컨볼루션은 시간 *t*의 출력이 시간 *t*까지의 입력만 사용하여 계산되도록 합니다. 이것이 어떻게 스트리밍을 보다 효율적으로 만들 수 있는지 보여주기 위해 친숙하고 더 간단한 예인 RNN으로 시작하겠습니다. RNN은 시간 흐름에 따라 상태를 전달합니다.\n",
        "\n",
        "![RNN 모델](https://www.tensorflow.org/images/tutorials/video/rnn_comparison.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMvDkgfFZC6a"
      },
      "outputs": [],
      "source": [
        "gru = layers.GRU(units=4, return_sequences=True, return_state=True)\n",
        "\n",
        "inputs = tf.random.normal(shape=[1, 10, 8]) # (batch, sequence, channels)\n",
        "\n",
        "result, state = gru(inputs) # Run it all at once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7xyb5C4bTs7"
      },
      "source": [
        "RNN의 `return_sequences=True` 인수를 설정하여 계산 종료 시 상태를 반환하도록 요청합니다. 그러면 일시 중지한 다음 중단한 위치에서 계속 진행하여 정확히 동일한 결과를 얻을 수 있습니다.\n",
        "\n",
        "![RNN에서 상태 전달](https://www.tensorflow.org/images/tutorials/video/rnn_state_passing.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI8FOPRRXXPa"
      },
      "outputs": [],
      "source": [
        "first_half, state = gru(inputs[:, :5, :])   # run the first half, and capture the state\n",
        "second_half, _ = gru(inputs[:,5:, :], initial_state=state)  # Use the state to continue where you left off.\n",
        "\n",
        "print(np.allclose(result[:, :5,:], first_half))\n",
        "print(np.allclose(result[:, 5:,:], second_half))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM3MArumY_Qk"
      },
      "source": [
        "인과 컨볼루션은 주의해서 다루면 같은 방식으로 사용할 수 있습니다. 이 기법은 Le Paine 등이 [Fast Wavenet Generation Algorithm](https://arxiv.org/abs/1611.09482)에 사용했습니다. [MoVinet 논문](https://arxiv.org/abs/2103.11511)에서는 `state`를 \"스트림 버퍼\"라고 합니다.\n",
        "\n",
        "![인과 컨볼루션에서 상태 전달](https://www.tensorflow.org/images/tutorials/video/causal_conv_states.png)\n",
        "\n",
        "이 약간의 상태를 앞으로 전달하면 위에 표시된 전체 수용 필드를 다시 계산하는 것을 피할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UsxiPs8yA2e"
      },
      "source": [
        "## 선행 훈련된 MoViNet 모델 다운로드\n",
        "\n",
        "이 섹션에서는 다음을 수행합니다.\n",
        "\n",
        "1. TensorFlow 모델에서 [`official/projects/movinet`](https://github.com/tensorflow/models/tree/master/official/projects/movinet)에 제공된 오픈 소스 코드를 사용하여 MoViNet 모델을 만들 수 있습니다.\n",
        "2. 사전 훈련된 가중치를 로드합니다.\n",
        "3. 미세 조정 속도를 높이기 위해 컨볼루션 베이스 또는 최종 분류자 헤드를 제외한 다른 모든 레이어를 고정합니다.\n",
        "\n",
        "모델을 구축하려면 `a0` 구성으로 시작할 수 있습니다. 다른 모델과 비교했을 때 이것이 가장 빠른 훈련 방법이기 때문입니다. 해당 사용 사례에 적합한 모델을 확인하려면 [TensorFlow Model Garden에서 사용 가능한 MoViNet 모델](https://github.com/tensorflow/models/blob/master/official/projects/movinet/configs/movinet.py)을 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhSCM6cee05F"
      },
      "outputs": [],
      "source": [
        "model_id = 'a0'\n",
        "resolution = 224\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(model_id=model_id)\n",
        "backbone.trainable = False\n",
        "\n",
        "# Set num_classes=600 to load the pre-trained weights from the original model\n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "model.build([None, None, None, None, 3])\n",
        "\n",
        "# Load pre-trained weights\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
        "!tar -xvf movinet_a0_base.tar.gz\n",
        "\n",
        "checkpoint_dir = f'movinet_{model_id}_base'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW23HVNtCXff"
      },
      "source": [
        "분류자를 구축하려면 백본과 데이터세트의 클래스 수를 사용하는 함수를 만듭니다. `build_classifier` 함수는 분류자를 구축하기 위해 백본과 데이터세트의 클래스 수를 가져옵니다. 이 경우 새 분류자는 `num_classes` 출력(UCF101의 이 하위 집합에 대한 10개 클래스)을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cfAelbU5Gi3"
      },
      "outputs": [],
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HWSk-u7oPUZ"
      },
      "outputs": [],
      "source": [
        "model = build_classifier(batch_size, num_frames, resolution, backbone, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhbX7qdTN8lc"
      },
      "source": [
        "이 튜토리얼에서는 `tf.keras.optimizers.Adam` 옵티마이저와 `tf.keras.losses.SparseCategoricalCrossentropy` 손실 함수를 선택합니다. 모든 단계에서 모델 성능의 정확도를 보려면 metrics 인수를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVqBLrn1tBsd"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2\n",
        "\n",
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VflEr_t6CuQu"
      },
      "source": [
        "모델을 훈련시킵니다. 두 epoch 후에 훈련 세트와 테스트 세트 모두에 대해 정확도가 높고 손실이 낮은 것에 주목하세요. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZeiYzI0tqQG"
      },
      "outputs": [],
      "source": [
        "results = model.fit(train_ds,\n",
        "                    validation_data=test_ds,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkLl2zF8G9W0"
      },
      "source": [
        "## 모델 평가하기\n",
        "\n",
        "이 모델은 훈련 데이터 세트에서 높은 정확도를 달성했습니다. 다음으로, `Model.evaluate`를 사용하여 테스트 세트에서 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqgbzOiKuxxT"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkFst2gsHBwD"
      },
      "source": [
        "모델 성능을 더 시각화하려면 [혼동 행렬](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix)을 사용합니다. 혼동 행렬을 사용하면 정확도를 넘어 분류 모델의 성능을 평가할 수 있습니다. 이 다중 클래스 분류 문제에 대한 혼동 행렬을 작성하기 위해 테스트세트의 실제 값과 예측 값을 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hssSdW9XHF_j"
      },
      "outputs": [],
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "  \"\"\"\n",
        "    Create a list of actual ground truth values and the predictions from the model.\n",
        "\n",
        "    Args:\n",
        "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
        "\n",
        "    Return:\n",
        "      Ground truth and predicted values for a particular dataset.\n",
        "  \"\"\"\n",
        "  actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TmTue6THGWO"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
        "  sns.set(rc={'figure.figsize':(12, 12)})\n",
        "  sns.set(font_scale=1.4)\n",
        "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
        "  ax.set_xlabel('Predicted Action')\n",
        "  ax.set_ylabel('Actual Action')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RK1A1C1HH6V"
      },
      "outputs": [],
      "source": [
        "fg = FrameGenerator(subset_paths['train'], num_frames, training = True)\n",
        "label_names = list(fg.class_ids_for_name.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4AFi2e5HKEO"
      },
      "outputs": [],
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)\n",
        "plot_confusion_matrix(actual, predicted, label_names, 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddQG9sYxa1Ib"
      },
      "source": [
        "## 다음 단계\n",
        "\n",
        "이제 MoViNet 모델과 다양한 TensorFlow API(전이 학습용 API 등)를 활용하는 방법에 익숙해졌으므로 이 튜토리얼의 코드를 자신의 데이터세트에 사용해 보세요. 데이터를 비디오 데이터에 국한시킬 필요는 없습니다. MRI 스캔과 같은 체적 데이터도 3D CNN에 사용할 수 있습니다. [조현병 분류 및 조절을 위한 뇌 MRI 기반 3차원 컨볼루션 신경망](https://arxiv.org/pdf/2003.08818.pdf)에 언급된 NUSDAT 및 IMH 데이터세트는 이러한 MRI 데이터의 두 가지 소스가 될 수 있습니다.\n",
        "\n",
        "특히, 이 튜토리얼에서 사용한 `FrameGenerator` 클래스와 다른 비디오 데이터 및 분류 튜토리얼을 사용하면 모델에 데이터를 로드하는 데 도움이 됩니다.\n",
        "\n",
        "TensorFlow에서 비디오 데이터 작업에 대해 자세히 알아보려면 다음 튜토리얼을 확인하세요.\n",
        "\n",
        "- [비디오 데이터 로드](https://www.tensorflow.org/tutorials/load_data/video)\n",
        "- [비디오 분류를 위한 3D CNN 모델 구축](https://www.tensorflow.org/tutorials/video/video_classification)\n",
        "- [스트리밍 동작 인식을 위한 MoViNet](https://www.tensorflow.org/hub/tutorials/movinet)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transfer_learning_with_movinet.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
