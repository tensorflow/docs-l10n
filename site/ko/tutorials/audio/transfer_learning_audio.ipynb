{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4espuLKJiA"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DjZQV2njKJ3U"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTL0TERThT6z"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/transfer_learning_audio\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF 허브 모델 보기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2madPFAGHb3"
      },
      "source": [
        "# 환경 소리 분류를 위한 YAMNet을 사용한 전이 학습\n",
        "\n",
        "[YAMNet](https://tfhub.dev/google/yamnet/1)은 웃음, 짖음 또는 사이렌과 같은 [521개 클래스](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv)의 오디오 이벤트를 예측할 수 있는 사전 훈련된 심층 신경망입니다.\n",
        "\n",
        "이 튜토리얼에서는 다음 방법을 배웁니다.\n",
        "\n",
        "- 추론을 위해 YAMNet 모델을 로드하고 사용합니다.\n",
        "- 고양이와 개 소리를 분류하기 위해 YAMNet 임베딩을 사용하여 새 모델을 빌드합니다.\n",
        "- 모델을 평가하고 내보냅니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mdp2TpBh96Y"
      },
      "source": [
        "## 텐서플로와 추가 라이브러리 임포트\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCcKYqu_hvKe"
      },
      "source": [
        "디스크에서 오디오 파일을 더 쉽게 로드할 수 있도록 우선 [TensorFlow I/O](https://www.tensorflow.org/io)를 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urBpRWDHTHHU"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l3nqdWVF-kC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_io as tfio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ZhybCnt_bM"
      },
      "source": [
        "## YAMNet 소개\n",
        "\n",
        "[YAMNet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet)은 [MobileNetV1](https://arxiv.org/abs/1704.04861) 깊이별 분리 가능한 콘볼루션 아키텍처를 사용하는 사전 훈련된 신경망으로, 오디오 파형을 입력으로 사용하고 [AudioSet](http://g.co/audioset) 말뭉치에서 521개의 오디오 이벤트 각각에 대해 독립적인 예측을 수행할 수 있습니다.\n",
        "\n",
        "내부적으로, 이 모델은 오디오 신호로부터 \"프레임\"을 추출하고 이러한 프레임을 일괄 처리합니다. 이 버전의 모델은 0.96초 길이의 프레임을 사용하고 0.48초마다 하나의 프레임을 추출합니다.\n",
        "\n",
        "이 모델은 `[-1.0, +1.0]` 범위의 단일 채널(모노) 16kHz 샘플로 나타내어지는 임의 길이의 파형을 포함하는 1차원 float32 Tensor 또는 NumPy 배열을 허용합니다. 이 튜토리얼에는 WAV 파일을 지원되는 형식으로 변환하는 데 도움이 되는 코드가 포함되어 있습니다.\n",
        "\n",
        "이 모델은 클래스 점수, 임베딩(전이 학습에 사용됨) 및 log mel [스펙트로그램](https://www.tensorflow.org/tutorials/audio/simple_audio#spectrogram)을 포함하여 3개의 출력을 반환합니다. 자세한 내용은 [여기](https://tfhub.dev/google/yamnet/1)에서 확인할 수 있습니다.\n",
        "\n",
        "YAMNet의 특정 용도 중 하나는 1,024차원 임베딩 출력인 고차원 특성 추출기입니다. 기본(YAMNet) 모델의 입력 특성을 사용하고 하나의 숨겨진 `tf.keras.layers.Dense` 레이어로 구성된 얕은 모델에 이를 입력합니다. 그런 다음 많은 레이블이 지정된 데이터와 전체 훈련 *없이* 오디오 분류를 위해 소량의 데이터로 네트워크를 훈련합니다. (자세한 내용은 [TensorFlow Hub를 사용한 이미지 분류를 위한 전이 학습](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)과 유사합니다.)\n",
        "\n",
        "먼저, 모델을 테스트하고 오디오 분류 결과를 확인합니다. 그런 다음 데이터 사전 처리 파이프라인을 구성합니다.\n",
        "\n",
        "### TensorFlow Hub에서 YAMNet 로드하기\n",
        "\n",
        "Tensorflow [Hub](https://tfhub.dev/)에서 사전 훈련된 YAMNet을 사용하여 사운드 파일에서 임베딩을 추출합니다.\n",
        "\n",
        "TensorFlow Hub에서 모델을 로드하는 것은 간단합니다. 모델을 선택하고 해당 URL을 복사한 다음 `load` 함수를 사용합니다.\n",
        "\n",
        "참고: 모델 설명서를 읽으려면 브라우저에서 모델 URL을 사용하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06CWkBV5v3gr"
      },
      "outputs": [],
      "source": [
        "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "yamnet_model = hub.load(yamnet_model_handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmrPJ0GHw9rr"
      },
      "source": [
        "모델이 로드되면 [YAMNet 기본 사용 튜토리얼](https://www.tensorflow.org/hub/tutorials/yamnet)을 따르고 샘플 WAV 파일을 다운로드하여 추론을 실행할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5i6xktEq00P"
      },
      "outputs": [],
      "source": [
        "testing_wav_file_name = tf.keras.utils.get_file('miaow_16k.wav',\n",
        "                                                'https://storage.googleapis.com/audioset/miaow_16k.wav',\n",
        "                                                cache_dir='./',\n",
        "                                                cache_subdir='test_data')\n",
        "\n",
        "print(testing_wav_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBm9y9iV2U_-"
      },
      "source": [
        "나중에 훈련 데이터로 작업할 때도 사용할 오디오 파일을 로드하는 함수가 필요합니다. [간단한 오디오 인식](https://www.tensorflow.org/tutorials/audio/simple_audio#reading_audio_files_and_their_labels)에서 오디오 파일 및 해당 레이블을 읽는 방법을 자세히 알아보세요.\n",
        "\n",
        "참고: `wav_data`에서 반환된 `load_wav_16k_mono`는 이미 `[-1.0, 1.0]` 범위의 값으로 정규화되었습니다(자세한 내용은 [TF Hub에서 YAMNet 설명서](https://tfhub.dev/google/yamnet/1) 참조)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwc9Wrdg2EtY"
      },
      "outputs": [],
      "source": [
        "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
        "\n",
        "@tf.function\n",
        "def load_wav_16k_mono(filename):\n",
        "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(\n",
        "          file_contents,\n",
        "          desired_channels=1)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRqpjkwB0Jjw"
      },
      "outputs": [],
      "source": [
        "testing_wav_data = load_wav_16k_mono(testing_wav_file_name)\n",
        "\n",
        "_ = plt.plot(testing_wav_data)\n",
        "\n",
        "# Play the audio file.\n",
        "display.Audio(testing_wav_data, rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z6rqlEz20YB"
      },
      "source": [
        "### 클래스 매핑 로드하기\n",
        "\n",
        "YAMNet이 인식할 수 있는 클래스 이름을 로드하는 것이 중요합니다. 매핑 파일은 CSV 형식으로 `yamnet_model.class_map_path()`에 들어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gyj23e_3Mgr"
      },
      "outputs": [],
      "source": [
        "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\n",
        "class_names =list(pd.read_csv(class_map_path)['display_name'])\n",
        "\n",
        "for name in class_names[:20]:\n",
        "  print(name)\n",
        "print('...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xbycDnT40u0"
      },
      "source": [
        "### 추론 실행\n",
        "\n",
        "YAMNet은 프레임 수준의 클래스 점수(즉, 모든 프레임에 대해 521점)를 제공합니다. 클립 레벨 예측을 결정하기 위해 전체 프레임에 걸쳐 클래스별로 점수를 집계할 수 있습니다(예: 평균 또는 최대 집계 사용). 이 작업을 위해 아래에서 `scores_np.mean(axis=0)`이 사용됩니다. 마지막으로, 클립 수준에서 최고 점수의 클래스를 찾으려면 521개의 집계 점수 중 최대값을 취합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT0otp-A4Y3u"
      },
      "outputs": [],
      "source": [
        "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n",
        "class_scores = tf.reduce_mean(scores, axis=0)\n",
        "top_class = tf.math.argmax(class_scores)\n",
        "inferred_class = class_names[top_class]\n",
        "\n",
        "print(f'The main sound is: {inferred_class}')\n",
        "print(f'The embeddings shape: {embeddings.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBaLNg5H5IWa"
      },
      "source": [
        "참고: 모델은 동물 소리를 올바르게 추론했습니다. 이 튜토리얼의 목표는 특정 클래스에 대한 모델의 정확도를 높이는 것입니다. 또한 모델은 프레임당 1개씩 13개의 임베딩을 생성했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmthELBg1A2-"
      },
      "source": [
        "## ESC-50 데이터세트\n",
        "\n",
        "[ESC-50 데이터세트](https://github.com/karolpiczak/ESC-50#repository-content)([Piczak, 2015](https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf))는 5초 길이의 환경 오디오 녹음 2,000개의 레이블이 지정된 컬렉션입니다. 데이터세트는 50개의 클래스로 구성되며 클래스당 40개의 예제가 있습니다.\n",
        "\n",
        "데이터세트를 다운로드하고 압축을 풉니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWobqK8JmZOU"
      },
      "outputs": [],
      "source": [
        "_ = tf.keras.utils.get_file('esc-50.zip',\n",
        "                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcruxiuX1cO5"
      },
      "source": [
        "### 데이터 살펴보기\n",
        "\n",
        "각 파일의 메타데이터는 `./datasets/ESC-50-master/meta/esc50.csv`의 csv 파일에 지정됩니다.\n",
        "\n",
        "그리고 모든 오디오 파일은 `./datasets/ESC-50-master/audio/`에 있습니다.\n",
        "\n",
        "매핑을 사용하여 pandas `DataFrame`을 만들고 이를 사용하여 데이터를 더 명확하게 볼 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwmLygPrMAbH"
      },
      "outputs": [],
      "source": [
        "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
        "base_data_path = './datasets/ESC-50-master/audio/'\n",
        "\n",
        "pd_data = pd.read_csv(esc50_csv)\n",
        "pd_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d4rHBEQ2QAU"
      },
      "source": [
        "### 데이터 필터링\n",
        "\n",
        "이제 데이터가 `DataFrame`에 저장되었으므로 몇 가지 변환을 적용합니다.\n",
        "\n",
        "- 행을 필터링하고 선택한 클래스(`dog` 및 `cat`)만 사용합니다. 다른 클래스를 사용하려면 여기에서 선택할 수 있습니다.\n",
        "- 전체 경로를 갖도록 파일 이름을 수정합니다. 이렇게 하면 나중에 더 쉽게 로드할 수 있습니다.\n",
        "- 대상을 특정 범위 내로 변경합니다. 이 예에서 `dog`는 `0`으로 유지되지만 `cat`은 원래 값 `5` 대신 `1`이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFnEoQjgs14I"
      },
      "outputs": [],
      "source": [
        "my_classes = ['dog', 'cat']\n",
        "map_class_to_id = {'dog':0, 'cat':1}\n",
        "\n",
        "filtered_pd = pd_data[pd_data.category.isin(my_classes)]\n",
        "\n",
        "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
        "filtered_pd = filtered_pd.assign(target=class_id)\n",
        "\n",
        "full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))\n",
        "filtered_pd = filtered_pd.assign(filename=full_path)\n",
        "\n",
        "filtered_pd.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkDcBS-aJdCz"
      },
      "source": [
        "### 오디오 파일 로드 및 임베딩 가져오기\n",
        "\n",
        "여기에서 `load_wav_16k_mono`를 적용하고 모델에 대한 WAV 데이터를 준비합니다.\n",
        "\n",
        "WAV 데이터에서 임베딩을 추출할 때 형상의 배열 `(N, 1024)`을 가져옵니다. 여기서 `N`은 YAMNet이 찾은 프레임 수입니다(오디오 0.48초마다 하나씩)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKDT5RomaDKO"
      },
      "source": [
        "모델은 각 프레임을 하나의 입력으로 사용합니다. 따라서 행당 하나의 프레임이 있는 새 열을 만들어야 합니다. 또한 이러한 새 행을 적절하게 반영하려면 레이블과 `fold` 열을 확장해야 합니다.\n",
        "\n",
        "확장된 `fold` 열은 원래 값을 유지합니다. 분할을 수행할 때 서로 다른 분할에 동일한 오디오의 일부가 있게 되어 유효성 검사 및 테스트 단계의 효율성이 떨어질 수 있으므로 프레임을 혼합할 수 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5Rq3_PyKLtU"
      },
      "outputs": [],
      "source": [
        "filenames = filtered_pd['filename']\n",
        "targets = filtered_pd['target']\n",
        "folds = filtered_pd['fold']\n",
        "\n",
        "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
        "main_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsEfovDVAHGY"
      },
      "outputs": [],
      "source": [
        "def load_wav_for_map(filename, label, fold):\n",
        "  return load_wav_16k_mono(filename), label, fold\n",
        "\n",
        "main_ds = main_ds.map(load_wav_for_map)\n",
        "main_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0tG8DBNAHcE"
      },
      "outputs": [],
      "source": [
        "# applies the embedding extraction model to a wav data\n",
        "def extract_embedding(wav_data, label, fold):\n",
        "  ''' run YAMNet to extract embedding from the wav data '''\n",
        "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
        "  num_embeddings = tf.shape(embeddings)[0]\n",
        "  return (embeddings,\n",
        "            tf.repeat(label, num_embeddings),\n",
        "            tf.repeat(fold, num_embeddings))\n",
        "\n",
        "# extract embedding\n",
        "main_ds = main_ds.map(extract_embedding).unbatch()\n",
        "main_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdfPIeD0Qedk"
      },
      "source": [
        "### 데이터 분할\n",
        "\n",
        "`fold` 열을 사용하여 데이터세트를 학습, 유효성 검증 및 테스트세트로 분할합니다.\n",
        "\n",
        "ESC-50은 5개의 균일한 크기의 교차 유효성 검증 `fold`로 배열되어 동일한 원본 소스의 클립이 항상 동일한 `fold`에 있도록 합니다. 자세한 내용은 [ESC: Dataset for Environmental Sound Classification](https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf) 문서를 참조하세요.\n",
        "\n",
        "마지막 단계는 훈련 중에 사용하지 않을 것이므로 데이터세트에서 `fold` 열을 제거하는 것입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZYvlFiVsffC"
      },
      "outputs": [],
      "source": [
        "cached_ds = main_ds.cache()\n",
        "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)\n",
        "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)\n",
        "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)\n",
        "\n",
        "# remove the folds column now that it's not needed anymore\n",
        "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
        "\n",
        "train_ds = train_ds.map(remove_fold_column)\n",
        "val_ds = val_ds.map(remove_fold_column)\n",
        "test_ds = test_ds.map(remove_fold_column)\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5PaMwvtcAIe"
      },
      "source": [
        "## 모델 생성하기\n",
        "\n",
        "대부분의 작업이 끝났습니다! 다음으로, 하나의 숨겨진 레이어와 두 개의 출력을 사용하여 소리에서 고양이와 개를 인식하는 매우 간단한 [순차](https://www.tensorflow.org/guide/keras/sequential_model) 모델을 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYCE0Fr1GpN3"
      },
      "outputs": [],
      "source": [
        "my_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
        "                          name='input_embedding'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(my_classes))\n",
        "], name='my_model')\n",
        "\n",
        "my_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1qgH35HY0SE"
      },
      "outputs": [],
      "source": [
        "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                            patience=3,\n",
        "                                            restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3sj84eOZ3pk"
      },
      "outputs": [],
      "source": [
        "history = my_model.fit(train_ds,\n",
        "                       epochs=20,\n",
        "                       validation_data=val_ds,\n",
        "                       callbacks=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbraYKYpdoE"
      },
      "source": [
        "과적합이 없는지 확인하기 위해 테스트 데이터에 대해 `evaluate` 메서드를 실행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4Nh5nec3Sky"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = my_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cid-qIrIpqHS"
      },
      "source": [
        "훌륭합니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCKZonrJcXab"
      },
      "source": [
        "## 모델 테스트하기\n",
        "\n",
        "다음으로, YAMNet만 사용하여 이전 테스트의 임베딩에 대해 모델을 시도합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79AFpA3_ctCF"
      },
      "outputs": [],
      "source": [
        "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n",
        "result = my_model(embeddings).numpy()\n",
        "\n",
        "inferred_class = my_classes[result.mean(axis=0).argmax()]\n",
        "print(f'The main sound is: {inferred_class}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2yleeev645r"
      },
      "source": [
        "## WAV 파일을 직접 입력으로 받을 수 있는 모델 저장하기\n",
        "\n",
        "임베딩을 입력으로 제공하면 모델이 작동합니다.\n",
        "\n",
        "실제 시나리오에서는 오디오 데이터를 직접 입력으로 사용해야 할 것입니다.\n",
        "\n",
        "이렇게 하려면 YAMNet과 모델을 다른 애플리케이션을 위해 내보낼 수 있는 단일 모델로 결합합니다.\n",
        "\n",
        "모델의 결과를 더 쉽게 사용할 수 있도록 최종 레이어는 `reduce_mean` 연산이 됩니다. 제공을 위해 이 모델을 사용할 때(튜토리얼 뒷부분에서 알아볼 예정) 최종 레이어의 이름이 필요합니다. 이를 정의하지 않으면 TensorFlow가 증분시키면서 이름을 자동으로 지정하는데, 그러면 모델을 훈련할 때마다 이름이 계속 달라지므로 테스트하기가 어려워집니다. 원시 TensorFlow 작업을 사용할 때는 이름을 할당할 수 없습니다. 이 문제를 해결하기 위해 `reduce_mean`을 적용하고 `'classifier'`라고 부르는 사용자 지정 레이어를 생성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUVCI2Suunpw"
      },
      "outputs": [],
      "source": [
        "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, axis=0, **kwargs):\n",
        "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.math.reduce_mean(input, axis=self.axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE_Npm0nzlwc"
      },
      "outputs": [],
      "source": [
        "saved_model_path = './dogs_and_cats_yamnet'\n",
        "\n",
        "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
        "embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
        "                                            trainable=False, name='yamnet')\n",
        "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
        "serving_outputs = my_model(embeddings_output)\n",
        "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
        "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
        "serving_model.save(saved_model_path, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-0bY5FMme1C"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(serving_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btHQDN9mqxM_"
      },
      "source": [
        "저장된 모델을 로드하여 예상대로 작동하는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkYVpJS72WWB"
      },
      "outputs": [],
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BkmvvNzq49l"
      },
      "source": [
        "그리고 최종 테스트로... 일부 사운드 데이터가 주어지면 모델이 올바른 결과를 반환합니까?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeXtD5HO28y-"
      },
      "outputs": [],
      "source": [
        "reloaded_results = reloaded_model(testing_wav_data)\n",
        "cat_or_dog = my_classes[tf.math.argmax(reloaded_results)]\n",
        "print(f'The main sound is: {cat_or_dog}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRrOcBYTUgwn"
      },
      "source": [
        "서비스 제공 설정에서 새 모델을 시도하려면 'serving_default' 서명을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycC8zzDSUG2s"
      },
      "outputs": [],
      "source": [
        "serving_results = reloaded_model.signatures['serving_default'](testing_wav_data)\n",
        "cat_or_dog = my_classes[tf.math.argmax(serving_results['classifier'])]\n",
        "print(f'The main sound is: {cat_or_dog}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da7blblCHs8c"
      },
      "source": [
        "## (선택 사항) 몇 가지 추가 테스트\n",
        "\n",
        "모델이 준비되었습니다.\n",
        "\n",
        "테스트 데이터세트에서 이를 YAMNet과 비교해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDf5MASIIN1z"
      },
      "outputs": [],
      "source": [
        "test_pd = filtered_pd.loc[filtered_pd['fold'] == 5]\n",
        "row = test_pd.sample(1)\n",
        "filename = row['filename'].item()\n",
        "print(filename)\n",
        "waveform = load_wav_16k_mono(filename)\n",
        "print(f'Waveform values: {waveform}')\n",
        "_ = plt.plot(waveform)\n",
        "\n",
        "display.Audio(waveform, rate=16000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYUzFxYJIcE1"
      },
      "outputs": [],
      "source": [
        "# Run the model, check the output.\n",
        "scores, embeddings, spectrogram = yamnet_model(waveform)\n",
        "class_scores = tf.reduce_mean(scores, axis=0)\n",
        "top_class = tf.math.argmax(class_scores)\n",
        "inferred_class = class_names[top_class]\n",
        "top_score = class_scores[top_class]\n",
        "print(f'[YAMNet] The main sound is: {inferred_class} ({top_score})')\n",
        "\n",
        "reloaded_results = reloaded_model(waveform)\n",
        "your_top_class = tf.math.argmax(reloaded_results)\n",
        "your_inferred_class = my_classes[your_top_class]\n",
        "class_probabilities = tf.nn.softmax(reloaded_results, axis=-1)\n",
        "your_top_score = class_probabilities[your_top_class]\n",
        "print(f'[Your model] The main sound is: {your_inferred_class} ({your_top_score})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8Tsym8Rq-0V"
      },
      "source": [
        "## 다음 단계\n",
        "\n",
        "개나 고양이의 소리를 분류할 수 있는 모델을 만들었습니다. 같은 아이디어와 다른 데이터세트를 사용하여 예를 들어, 새의 지저귐을 바탕으로 [새의 음향 식별자](https://www.kaggle.com/c/birdclef-2021/)를 구축해 볼 수 있습니다.\n",
        "\n",
        "소셜 미디어에서 TensorFlow 팀과 자신의 프로젝트를 공유하세요!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transfer_learning_audio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
