{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt9dL5dIir8X"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ufPx7EiCiqgR"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4StGz9ynOEL6"
      },
      "source": [
        "# 비디오 데이터 로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwQtSOz0VrVX"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/video\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/video.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/video.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/load_data/video.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-SqCosJ6-0H"
      },
      "source": [
        "이 튜토리얼은 [UCF101 인간 행동 데이터세트](https://en.wikipedia.org/wiki/Audio_Video_Interleave)를 사용하여 [AVI](https://www.tensorflow.org/datasets/catalog/ucf101) 비디오 데이터를 로드하고 전처리하는 방법을 보여줍니다. 데이터를 전처리하면 비디오 분류/인식, 캡션 또는 클러스터링과 같은 작업에 사용할 수 있습니다. 원본 데이터세트에는 첼로 연주, 양치질, 눈 화장 등 101개 범주로 YouTube에서 수집한 사실적인 동작 비디오가 포함되어 있습니다. 다음을 수행하는 방법을 배우게 됩니다.\n",
        "\n",
        "- zip 파일에서 데이터를 로드합니다.\n",
        "\n",
        "- 비디오 파일에서 프레임 시퀀스를 읽습니다.\n",
        "\n",
        "- 비디오 데이터를 시각화합니다.\n",
        "\n",
        "- 프레임 생성기 [`tf.data.Dataset`](https://www.tensorflow.org/guide/data)을 래핑합니다.\n",
        "\n",
        "이 비디오 로드 및 전처리 튜토리얼은 TensorFlow 비디오 튜토리얼 시리즈의 첫 번째 부분입니다. 다른 세 개의 튜토리얼은 다음과 같습니다.\n",
        "\n",
        "- [비디오 분류를 위한 3D CNN 모델 구축](https://www.tensorflow.org/tutorials/video/video_classification): 이 튜토리얼에서는 3D 데이터의 공간적 및 시간적 측면을 분해하는 (2+1)D CNN을 사용합니다. MRI 스캔과 같은 체적 데이터를 사용하는 경우 (2+1)D CNN 대신 3D CNN을 사용하는 것이 좋습니다.\n",
        "- [스트리밍 동작 인식을 위한 MoViNet](https://www.tensorflow.org/hub/tutorials/movinet): TF Hub에서 사용할 수 있는 MoViNet 모델에 익숙해집니다.\n",
        "- [MoViNet을 사용한 비디오 분류 전이 학습](https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet): 이 튜토리얼에서는 UCF-101 데이터세트와 함께 상이한 데이터세트에서 훈련된 사전 훈련 비디오 분류 모델을 사용하는 방법에 대해 설명합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnpPjKVD68eH"
      },
      "source": [
        "## 설정\n",
        "\n",
        "ZIP 파일의 내용을 검사하기 위한 [remotezip](https://github.com/gtsystem/python-remotezip), 진행률 표시줄을 사용하기 위한 [tqdm](https://github.com/tqdm/tqdm), 비디오 파일을 처리하기 위한 [OpenCV](https://opencv.org/), Jupyter 노트북에 데이터를 삽입하기 위한 [`tensorflow_docs`](https://github.com/tensorflow/docs/tree/master/tools/tensorflow_docs)를 포함하여 몇 가지 필요한 라이브러리를 설치하고 가져오는 것으로 시작합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjI3AaaO16bd"
      },
      "outputs": [],
      "source": [
        "# The way this tutorial uses the `TimeDistributed` layer requires TF>=2.10\n",
        "!pip install -U \"tensorflow>=2.10.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5SBasQcbwQA"
      },
      "outputs": [],
      "source": [
        "!pip install remotezip tqdm opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RYQIJ9C6BVH"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython import display\n",
        "from urllib import request\n",
        "from tensorflow_docs.vis import embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbhwWLLM7FXo"
      },
      "source": [
        "## UCF101 데이터세트의 하위 집합 다운로드\n",
        "\n",
        "[UCF101 데이터세트](https://www.tensorflow.org/datasets/catalog/ucf101)에는 주로 동작 인식에 사용되는 비디오의 다양한 동작이 101개의 범주로 포함되어 있습니다. 이 데모에서 이러한 범주 중 일부를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVIgj-jIA8U8"
      },
      "outputs": [],
      "source": [
        "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tm8aBzw6Md7"
      },
      "source": [
        "위 URL에는 UCF 101 데이터세트가 포함된 zip 파일이 포함되어 있습니다. `remotezip` 라이브러리를 사용하여 해당 URL에 있는 zip 파일의 내용을 검사하는 함수를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY-x7TaZlK6O"
      },
      "outputs": [],
      "source": [
        "def list_files_from_zip_url(zip_url):\n",
        "  \"\"\" List the files in each class of the dataset given a URL with the zip file.\n",
        "\n",
        "    Args:\n",
        "      zip_url: A URL from which the files can be extracted from.\n",
        "\n",
        "    Returns:\n",
        "      List of files in each of the classes.\n",
        "  \"\"\"\n",
        "  files = []\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for zip_info in zip.infolist():\n",
        "      files.append(zip_info.filename)\n",
        "  return files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYErXAdUr-rk"
      },
      "outputs": [],
      "source": [
        "files = list_files_from_zip_url(URL)\n",
        "files = [f for f in files if f.endswith('.avi')]\n",
        "files[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ4l8D9dFPS7"
      },
      "source": [
        "몇 개의 비디오와 제한된 수의 훈련용 클래스로 시작합니다. 위의 코드 블록을 실행한 후 각 비디오의 파일 이름에 클래스 이름이 포함되어 있음을 확인하세요.\n",
        "\n",
        "파일 이름에서 클래스 이름을 검색하는 `get_class` 함수를 정의합니다. 그런 다음 모든 파일(위의 `files`) 목록을 각 클래스에 대한 파일을 나열하는 사전으로 변환하는 `get_files_per_class`라는 함수를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyyivOX0sO19"
      },
      "outputs": [],
      "source": [
        "def get_class(fname):\n",
        "  \"\"\" Retrieve the name of the class given a filename.\n",
        "\n",
        "    Args:\n",
        "      fname: Name of the file in the UCF101 dataset.\n",
        "\n",
        "    Returns:\n",
        "      Class that the file belongs to.\n",
        "  \"\"\"\n",
        "  return fname.split('_')[-3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qnH0xKzlyw_"
      },
      "outputs": [],
      "source": [
        "def get_files_per_class(files):\n",
        "  \"\"\" Retrieve the files that belong to each class.\n",
        "\n",
        "    Args:\n",
        "      files: List of files in the dataset.\n",
        "\n",
        "    Returns:\n",
        "      Dictionary of class names (key) and files (values). \n",
        "  \"\"\"\n",
        "  files_for_class = collections.defaultdict(list)\n",
        "  for fname in files:\n",
        "    class_name = get_class(fname)\n",
        "    files_for_class[class_name].append(fname)\n",
        "  return files_for_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxSt5YgSGrWn"
      },
      "source": [
        "클래스별 파일 목록이 준비되면 데이터세트를 만들기 위해 사용할 클래스 수와 클래스당 원하는 비디오 수를 선택할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPdURg74uUTk"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "FILES_PER_CLASS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUs0xtXsr9i3"
      },
      "outputs": [],
      "source": [
        "files_for_class = get_files_per_class(files)\n",
        "classes = list(files_for_class.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YqFARvqwon9"
      },
      "outputs": [],
      "source": [
        "print('Num classes:', len(classes))\n",
        "print('Num videos for class[0]:', len(files_for_class[classes[0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFAFqKqE92bQ"
      },
      "source": [
        "데이터세트 내에 있는 클래스의 하위 집합과 클래스당 특정 수의 파일을 선택하는 `select_subset_of_classes`라는 새 함수를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3jek4QimIj-"
      },
      "outputs": [],
      "source": [
        "def select_subset_of_classes(files_for_class, classes, files_per_class):\n",
        "  \"\"\" Create a dictionary with the class name and a subset of the files in that class.\n",
        "\n",
        "    Args:\n",
        "      files_for_class: Dictionary of class names (key) and files (values).\n",
        "      classes: List of classes.\n",
        "      files_per_class: Number of files per class of interest.\n",
        "\n",
        "    Returns:\n",
        "      Dictionary with class as key and list of specified number of video files in that class.\n",
        "  \"\"\"\n",
        "  files_subset = dict()\n",
        "\n",
        "  for class_name in classes:\n",
        "    class_files = files_for_class[class_name]\n",
        "    files_subset[class_name] = class_files[:files_per_class]\n",
        "\n",
        "  return files_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cjcz6Gpcb-W"
      },
      "outputs": [],
      "source": [
        "files_subset = select_subset_of_classes(files_for_class, classes[:NUM_CLASSES], FILES_PER_CLASS)\n",
        "list(files_subset.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrlDS1lZx3E"
      },
      "source": [
        "비디오를 훈련, 검증 및 테스트 세트로 분할하는 헬퍼 함수를 정의합니다. 비디오는 zip 파일로 URL에서 다운로드되어 각각의 하위 디렉터리에 배치됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH9sWS_6nRz3"
      },
      "outputs": [],
      "source": [
        "def download_from_zip(zip_url, to_dir, file_names):\n",
        "  \"\"\" Download the contents of the zip file from the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: A URL with a zip file containing data.\n",
        "      to_dir: A directory to download data to.\n",
        "      file_names: Names of files to download.\n",
        "  \"\"\"\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for fn in tqdm.tqdm(file_names):\n",
        "      class_name = get_class(fn)\n",
        "      zip.extract(fn, str(to_dir / class_name))\n",
        "      unzipped_file = to_dir / class_name / fn\n",
        "\n",
        "      fn = pathlib.Path(fn).parts[-1]\n",
        "      output_file = to_dir / class_name / fn\n",
        "      unzipped_file.rename(output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pejRTChA6mrp"
      },
      "source": [
        "다음 함수는 데이터의 하위 집합에 아직 배치되지 않은 나머지 데이터를 반환합니다. 이 나머지 데이터를 지정된 다음 데이터 하위 집합에 배치할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ARYc-WLqqNF"
      },
      "outputs": [],
      "source": [
        "def split_class_lists(files_for_class, count):\n",
        "  \"\"\" Returns the list of files belonging to a subset of data as well as the remainder of\n",
        "    files that need to be downloaded.\n",
        "    \n",
        "    Args:\n",
        "      files_for_class: Files belonging to a particular class of data.\n",
        "      count: Number of files to download.\n",
        "\n",
        "    Returns:\n",
        "      Files belonging to the subset of data and dictionary of the remainder of files that need to be downloaded.\n",
        "  \"\"\"\n",
        "  split_files = []\n",
        "  remainder = {}\n",
        "  for cls in files_for_class:\n",
        "    split_files.extend(files_for_class[cls][:count])\n",
        "    remainder[cls] = files_for_class[cls][count:]\n",
        "  return split_files, remainder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlEQ_I0TLd1X"
      },
      "source": [
        "다음 `download_ucf_101_subset` 함수를 사용하면 UCF101 데이터세트의 하위 집합을 다운로드하고 훈련, 검증 및 테스트 세트로 분할할 수 있습니다. 사용하려는 클래스 수를 지정할 수 있습니다. `splits` 인수를 사용하면 키 값이 하위 집합의 이름(예: \"train\")과 클래스당 가지려는 비디오 수인 사전을 전달할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHH2Y1M06xoz"
      },
      "outputs": [],
      "source": [
        "def download_ucf_101_subset(zip_url, num_classes, splits, download_dir):\n",
        "  \"\"\" Download a subset of the UCF101 dataset and split them into various parts, such as\n",
        "    training, validation, and test.\n",
        "\n",
        "    Args:\n",
        "      zip_url: A URL with a ZIP file with the data.\n",
        "      num_classes: Number of labels.\n",
        "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n",
        "              (value is number of files per split).\n",
        "      download_dir: Directory to download data to.\n",
        "\n",
        "    Return:\n",
        "      Mapping of the directories containing the subsections of data.\n",
        "  \"\"\"\n",
        "  files = list_files_from_zip_url(zip_url)\n",
        "  for f in files:\n",
        "    path = os.path.normpath(f)\n",
        "    tokens = path.split(os.sep)\n",
        "    if len(tokens) <= 2:\n",
        "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
        "  \n",
        "  files_for_class = get_files_per_class(files)\n",
        "\n",
        "  classes = list(files_for_class.keys())[:num_classes]\n",
        "\n",
        "  for cls in classes:\n",
        "    random.shuffle(files_for_class[cls])\n",
        "    \n",
        "  # Only use the number of classes you want in the dictionary\n",
        "  files_for_class = {x: files_for_class[x] for x in classes}\n",
        "\n",
        "  dirs = {}\n",
        "  for split_name, split_count in splits.items():\n",
        "    print(split_name, \":\")\n",
        "    split_dir = download_dir / split_name\n",
        "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
        "    download_from_zip(zip_url, split_dir, split_files)\n",
        "    dirs[split_name] = split_dir\n",
        "\n",
        "  return dirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuD-xU8Q66Vm"
      },
      "outputs": [],
      "source": [
        "download_dir = pathlib.Path('./UCF101_subset/')\n",
        "subset_paths = download_ucf_101_subset(URL,\n",
        "                                       num_classes = NUM_CLASSES,\n",
        "                                       splits = {\"train\": 30, \"val\": 10, \"test\": 10},\n",
        "                                       download_dir = download_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBMRm9Ub3Zrk"
      },
      "source": [
        "데이터를 다운로드한 후에는 이제 UCF101 데이터세트의 하위 집합 복사본이 있어야 합니다. 다음 코드를 실행하여 모든 데이터 하위 집합 사이에 가지고 있는 총 비디오 수를 인쇄합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zupvOLYP4D4q"
      },
      "outputs": [],
      "source": [
        "video_count_train = len(list(download_dir.glob('train/*/*.avi')))\n",
        "video_count_val = len(list(download_dir.glob('val/*/*.avi')))\n",
        "video_count_test = len(list(download_dir.glob('test/*/*.avi')))\n",
        "video_total = video_count_train + video_count_val + video_count_test\n",
        "print(f\"Total videos: {video_total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmJG1SlXiOX8"
      },
      "source": [
        "이제 데이터 파일의 디렉터리를 미리 볼 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9be0WlDiNM0"
      },
      "outputs": [],
      "source": [
        "!find ./UCF101_subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4uslY4dScyu"
      },
      "source": [
        "## 각 비디오 파일에서 프레임 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1vvyT0F7JAZ"
      },
      "source": [
        "`frames_from_video_file` 함수는 비디오를 여러 프레임으로 분할하고 비디오 파일에서 무작위로 선택된 `n_frames` 범위를 읽은 다음, NumPy `array`로 반환합니다. 메모리와 계산 오버헤드를 줄이려면 **적은** 수의 프레임을 선택합니다. 또한 각 비디오에서 **동일한** 수의 프레임을 선택하면 데이터 배치 작업을 더 쉽게 수행할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNBCiV3bMzpD"
      },
      "outputs": [],
      "source": [
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "    \n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded. \n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ujLDC9G7JyE"
      },
      "outputs": [],
      "source": [
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))  \n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ENtlwhxwyTe"
      },
      "source": [
        "## 비디오 데이터 시각화하기\n",
        "\n",
        "프레임 세트를 NumPy 배열로 반환하는 `frames_from_video_file` 함수입니다. Patrick Gillett가 [Wikimedia](https://commons.wikimedia.org/wiki/Category:Videos_of_sports){:.external}에 올린 새 동영상에서 이 기능을 사용해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2hgSghlykzA"
      },
      "outputs": [],
      "source": [
        "!curl -O https://upload.wikimedia.org/wikipedia/commons/8/86/End_of_a_jam.ogv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdHvHw3hym-U"
      },
      "outputs": [],
      "source": [
        "video_path = \"End_of_a_jam.ogv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u845YODXyqo5"
      },
      "outputs": [],
      "source": [
        "sample_video = frames_from_video_file(video_path, n_frames = 10)\n",
        "sample_video.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFHGHiFgGjv2"
      },
      "outputs": [],
      "source": [
        "def to_gif(images):\n",
        "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
        "  imageio.mimsave('./animation.gif', converted_images, fps=10)\n",
        "  return embed.embed_file('./animation.gif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hiwUJenEN3p"
      },
      "outputs": [],
      "source": [
        "to_gif(sample_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dktTnDVG7xf"
      },
      "source": [
        "이 비디오를 검토하는 외에 UCF-101 데이터를 표시할 수도 있습니다. 이렇게 하려면 다음 코드를 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MghJzJsWme0t"
      },
      "outputs": [],
      "source": [
        "# docs-infra: no-execute\n",
        "ucf_sample_video = frames_from_video_file(next(subset_paths['train'].glob('*/*.avi')), 50)\n",
        "to_gif(ucf_sample_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlvuC5_E7XrF"
      },
      "source": [
        "다음으로, TensorFlow 데이터 파이프라인에 데이터를 공급할 수 있는 반복 가능한 객체를 생성하기 위해 `FrameGenerator` 클래스를 정의합니다. 생성기(`__call__`) 함수는 `frames_from_video_file`에 의해 생성된 프레임 배열과 프레임 세트와 연관된 레이블의 원-핫 인코딩된 벡터를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVmfLTlw7Ues"
      },
      "outputs": [],
      "source": [
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    \"\"\" Returns a set of frames with their associated label. \n",
        "\n",
        "      Args:\n",
        "        path: Video file paths.\n",
        "        n_frames: Number of frames. \n",
        "        training: Boolean to determine if training dataset is being created.\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.avi'))\n",
        "    classes = [p.parent.name for p in video_paths] \n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames) \n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsvhPIkpzx-r"
      },
      "source": [
        "TensorFlow Dataset 객체로 래핑하기 전에 `FrameGenerator` 객체를 테스트합니다. 또한 훈련 데이터세트의 경우 데이터가 섞이도록 훈련 모드를 활성화해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5jwagZxzxOf"
      },
      "outputs": [],
      "source": [
        "fg = FrameGenerator(subset_paths['train'], 10, training=True)\n",
        "\n",
        "frames, label = next(fg())\n",
        "\n",
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7MRRFSks7l1"
      },
      "source": [
        "마지막으로, TensorFlow 데이터 입력 파이프라인을 만듭니다. 생성기 객체에서 생성하는 이 파이프라인을 사용하면 딥 러닝 모델에 데이터를 공급할 수 있습니다. 이 비디오 파이프라인에서 각 요소는 단일 프레임 세트 및 관련 레이블입니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM4NboJr7ck4"
      },
      "outputs": [],
      "source": [
        "# Create the training set\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], 10, training=True),\n",
        "                                          output_signature = output_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oF_8m8IZvcY"
      },
      "source": [
        "레이블이 섞였는지 확인합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XYVmsgiZsJD"
      },
      "outputs": [],
      "source": [
        "for frames, labels in train_ds.take(10):\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi8-WkOkEXw5"
      },
      "outputs": [],
      "source": [
        "# Create the validation set\n",
        "val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], 10),\n",
        "                                        output_signature = output_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6qXc-6i7eyK"
      },
      "outputs": [],
      "source": [
        "# Print the shapes of the data\n",
        "train_frames, train_labels = next(iter(train_ds))\n",
        "print(f'Shape of training set of frames: {train_frames.shape}')\n",
        "print(f'Shape of training labels: {train_labels.shape}')\n",
        "\n",
        "val_frames, val_labels = next(iter(val_ds))\n",
        "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
        "print(f'Shape of validation labels: {val_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIrFpUIxvTLe"
      },
      "source": [
        "## 성능을 높이도록 데이터세트 구성하기\n",
        "\n",
        "I/O가 차단되지 않고 디스크에서 데이터를 생성할 수 있도록 버퍼링된 프리페치를 사용합니다. 데이터를 로드하는 동안 사용해야 하는 두 가지 중요한 함수는 다음과 같습니다.\n",
        "\n",
        "- `Dataset.cache`: 첫 번째 epoch 동안 디스크에서 로드된 후 프레임 세트를 메모리에 유지합니다. 이 함수는 모델을 훈련하는 동안 데이터세트가 병목을 일으키지 않도록 합니다. 데이터세트가 너무 커서 메모리에 맞지 않는 경우 이 방법을 사용하여 고성능 디스크 캐시를 생성할 수도 있습니다.\n",
        "\n",
        "- `Dataset.prefetch`: 훈련하는 동안 데이터 전처리와 모델 실행을 중첩시킵니다. 자세한 내용은 [`tf.data`로 성능 향상](https://www.tensorflow.org/guide/data_performance)을 참조하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSxjFtxAvY3_"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaY-hyr-Fbfr"
      },
      "source": [
        "모델에 공급할 데이터를 준비하려면 아래와 같이 배치 처리를 사용합니다. AVI 파일과 같은 비디오 데이터로 작업할 때 데이터는 5차원 객체로 형성되어야 합니다. 이러한 차원은 `[batch_size, number_of_frames, height, width, channels]`와 같습니다. 이에 비해 이미지에는 `[batch_size, height, width, channels]` 4가지 차원이 있습니다. 아래 이미지는 비디오 데이터의 형태가 표현되는 방식을 보여주는 그림입니다.\n",
        "\n",
        "![비디오 데이터 형태](https://www.tensorflow.org/images/tutorials/video/video_data_shape.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp2Qc6XSFmeB"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.batch(2)\n",
        "val_ds = val_ds.batch(2)\n",
        "\n",
        "train_frames, train_labels = next(iter(train_ds))\n",
        "print(f'Shape of training set of frames: {train_frames.shape}')\n",
        "print(f'Shape of training labels: {train_labels.shape}')\n",
        "\n",
        "val_frames, val_labels = next(iter(val_ds))\n",
        "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
        "print(f'Shape of validation labels: {val_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqjXn1FgsMqZ"
      },
      "source": [
        "## 다음 단계\n",
        "\n",
        "레이블이 있는 비디오 프레임의 TensorFlow `Dataset`을 만들었으므로 이제 딥 러닝 모델에 이를 사용할 수 있습니다. 사전 훈련된 [EfficientNet](https://arxiv.org/abs/1905.11946){:.external}을 사용하는 다음 분류 모델은 몇 분 안에 높은 정확도로 훈련됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzqgPBUuForj"
      },
      "outputs": [],
      "source": [
        "net = tf.keras.applications.EfficientNetB0(include_top = False)\n",
        "net.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(scale=255),\n",
        "    tf.keras.layers.TimeDistributed(net),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.GlobalAveragePooling3D()\n",
        "])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds, \n",
        "          epochs = 10,\n",
        "          validation_data = val_ds,\n",
        "          callbacks = tf.keras.callbacks.EarlyStopping(patience = 2, monitor = 'val_loss'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdJm7ojgGxtT"
      },
      "source": [
        "TensorFlow에서 비디오 데이터 작업에 대해 자세히 알아보려면 다음 튜토리얼을 확인하세요.\n",
        "\n",
        "- [비디오 분류를 위한 3D CNN 모델 구축](https://www.tensorflow.org/tutorials/video/video_classification)\n",
        "- [스트리밍 동작 인식을 위한 MoViNet](https://www.tensorflow.org/hub/tutorials/movinet)\n",
        "- [MoViNet을 사용한 비디오 분류 전이 학습](https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "video.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
