{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DweYe9FcbMK_"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AVV2e0XKbJeX"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtoed20cRJJ"
      },
      "source": [
        "# CSV 데이터 로드하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ap_W4aQcgNT"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/csv\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-3Xbt0FfGfs"
      },
      "source": [
        "이 튜토리얼은 TensorFlow에서 CSV 데이터를 사용하는 방법에 대한 예제를 제공합니다.\n",
        "\n",
        "다음과 같이 두 가지 주요 내용이 있습니다.\n",
        "\n",
        "1. **Loading the data off disk**\n",
        "2. **Pre-processing it into a form suitable for training.**\n",
        "\n",
        "이 튜토리얼은 로딩에 중점을 두며 전처리에 대한 몇 가지 빠른 예를 제공합니다. 전처리 측면에 대해 자세히 알아보려면 [전처리 레이어 작업](https://www.tensorflow.org/guide/keras/preprocessing_layers) 가이드 및 [Keras 전처리 레이어를 사용하여 구조화된 데이터 분류](../structured_data/preprocessing_layers.ipynb) 튜토리얼을 확인하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## 설정하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZhJYbJxHNGJ"
      },
      "source": [
        "## 인메모리 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny5TEgcmHjVx"
      },
      "source": [
        "작은 크기의 CSV 데이터세트를 사용하여 TensorFlow 모델을 훈련시키는 가장 간단한 방법은 이를 메모리에 pandas Dataframe 또는 NumPy 배열로 로드하는 것입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgpBOuU8PGFf"
      },
      "source": [
        "비교적 간단한 예는 [전복 데이터세트](https://archive.ics.uci.edu/ml/datasets/abalone)입니다.\n",
        "\n",
        "- 데이터세트의 크기가 작습니다.\n",
        "- 모든 입력 특성은 모두 제한된 범위의 부동 소수점 값입니다.\n",
        "\n",
        "다음은 [Pandas `DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)에 데이터를 다운로드하는 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZVExo9DKoNz"
      },
      "outputs": [],
      "source": [
        "abalone_train = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
        "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
        "\n",
        "abalone_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP22mdyPQ1_t"
      },
      "source": [
        "이 데이터세트에는 바다 고등류의 일종인 [전복](https://en.wikipedia.org/wiki/Abalone) 측정값 세트가 포함되어 있습니다.\n",
        "\n",
        "![an abalone shell](https://tensorflow.org/images/abalone_shell.jpg)\n",
        "\n",
        "[“전복 껍질”](https://www.flickr.com/photos/thenickster/16641048623/) ([Nicki Dugan Pogue](https://www.flickr.com/photos/thenickster/) 제공, CC BY-SA 2.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlfGrk_9N-wf"
      },
      "source": [
        "이 데이터 세트의 명목상 작업은 다른 측정값으로부터 나이를 예측하는 것이므로 다음과 같이 훈련을 위해 특성과 레이블을 분리합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udOnDJOxNi7p"
      },
      "outputs": [],
      "source": [
        "abalone_features = abalone_train.copy()\n",
        "abalone_labels = abalone_features.pop('Age')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seK9n71-UBfT"
      },
      "source": [
        "이 데이터세트에서는 모든 특성을 동일하게 취급합니다. 다음과 같이 특성을 단일 NumPy 배열로 묶습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp3N5McbUMwb"
      },
      "outputs": [],
      "source": [
        "abalone_features = np.array(abalone_features)\n",
        "abalone_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1yFOxLOdxh"
      },
      "source": [
        "다음으로, 회귀 모델로 나이를 예측합니다. 입력 텐서가 하나만 있으므로 여기에서는 `keras.Sequential` 모델이면 충분합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8zzNrZqOmfB"
      },
      "outputs": [],
      "source": [
        "abalone_model = tf.keras.Sequential([\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
        "                      optimizer = tf.keras.optimizers.Adam())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6IWeP78O2wE"
      },
      "source": [
        "해당 모델을 훈련하려면 특성과 레이블을 `Model.fit`로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZdpCD92SN3Z"
      },
      "outputs": [],
      "source": [
        "abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GapLOj1OOTQH"
      },
      "source": [
        "지금까지 CSV 데이터를 사용하여 모델을 훈련하는 가장 기본적인 방법을 보았습니다. 다음으로 숫자 열을 정규화하기 위해 전처리를 적용하는 방법을 배웁니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B87Rd1SOUv02"
      },
      "source": [
        "## 기본 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrB2Jd-U0Vt"
      },
      "source": [
        "모델에 대한 입력을 정규화하면 좋습니다. Keras 전처리 레이어는 이 정규화를 모델에 빌드하는 편리한 방법을 제공합니다.\n",
        "\n",
        "`tf.keras.layers.Normalization` 레이어는 각 열의 평균과 분산을 미리 계산하고 이를 사용하여 데이터를 정규화합니다.\n",
        "\n",
        "먼저 레이어를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2WQpDU5VRk7"
      },
      "outputs": [],
      "source": [
        "normalize = layers.Normalization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGgEZE-7Vpt6"
      },
      "source": [
        "그런 다음 `Normalization.adapt` 메서드를 사용하여 정규화 레이어를 데이터에 맞게 조정합니다.\n",
        "\n",
        "참고: `PreprocessingLayer.adapt` 메서드와 함께 훈련 데이터만 사용하고 검증 또는 테스트 데이터는 사용하지 마세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WgOPIiOVpLg"
      },
      "outputs": [],
      "source": [
        "normalize.adapt(abalone_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE6vh0byV7cE"
      },
      "source": [
        "그런 다음 모델에서 정규화 레이어를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quPcZ9dTWA9A"
      },
      "outputs": [],
      "source": [
        "norm_abalone_model = tf.keras.Sequential([\n",
        "  normalize,\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "norm_abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
        "                           optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "norm_abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuqj601Qw0Ml"
      },
      "source": [
        "## 혼합 데이터 유형\n",
        "\n",
        "The \"Titanic\" dataset contains information about the passengers on the Titanic. The nominal task on this dataset is to predict who survived.\n",
        "\n",
        "![The Titanic](images/csv/Titanic.jpg)\n",
        "\n",
        "Image [from Wikimedia](https://commons.wikimedia.org/wiki/File:RMS_Titanic_3.jpg)\n",
        "\n",
        "The raw data can easily be loaded as a Pandas `DataFrame`, but is not immediately usable as input to a TensorFlow model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS-dBMpuYMnz"
      },
      "outputs": [],
      "source": [
        "titanic = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8rCGIK1ZzKx"
      },
      "outputs": [],
      "source": [
        "titanic_features = titanic.copy()\n",
        "titanic_labels = titanic_features.pop('survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urHOwpCDYtcI"
      },
      "source": [
        "데이터 유형과 범위가 다르기 때문에 단순히 특성을 NumPy 배열에 쌓아서 `keras.Sequential` 모델로 전달할 수 없습니다. 각 열을 개별적으로 처리해야 합니다.\n",
        "\n",
        "한 가지 옵션으로 데이터를 오프라인으로 전처리(원하는 도구 사용)하여 범주형 열을 숫자 열로 변환한 다음 처리된 출력을 TensorFlow 모델에 전달할 수 있습니다. 이 접근 방식의 단점은 모델을 저장하고 내보내는 경우 전처리가 함께 저장되지 않는다는 것입니다. Keras 전처리 레이어는 모델의 일부이기 때문에 이 문제를 피할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bta4Sx0Zau5v"
      },
      "source": [
        "이 예제에서는 [Keras 함수형 API](https://www.tensorflow.org/guide/keras/functional)를 사용하여 전처리 로직을 구현하는 모델을 빌드합니다. [하위 클래스화](https://www.tensorflow.org/guide/keras/custom_layers_and_models)하여 같은 작업을 수행할 수도 있습니다.\n",
        "\n",
        "함수형 API는 \"기호화된\" 텐서에서 작동합니다. 정상적인 \"즉시(eager)\" 텐서에는 값이 있습니다. 대조적으로 이러한 \"기호화된\" 텐서에는 값이 없습니다. 대신에 실행되는 작업을 추적하고 나중에 실행할 수 있는 계산 표현을 작성합니다. 다음은 간단한 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "730F16_97D-3"
      },
      "outputs": [],
      "source": [
        "# Create a symbolic input\n",
        "input = tf.keras.Input(shape=(), dtype=tf.float32)\n",
        "\n",
        "# Perform a calculation using the input\n",
        "result = 2*input + 1\n",
        "\n",
        "# the result doesn't have a value\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtcNXWB18kMJ"
      },
      "outputs": [],
      "source": [
        "calc = tf.keras.Model(inputs=input, outputs=result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUGQOUqZ8sa-"
      },
      "outputs": [],
      "source": [
        "print(calc(1).numpy())\n",
        "print(calc(2).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNS9lT7f6_U2"
      },
      "source": [
        "전처리 모델을 빌드하려면 먼저 CSV 열의 이름 및 데이터 유형과 일치하는 기호화된 `tf.keras.Input` 객체 세트를 빌드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WODe_1da3yw"
      },
      "outputs": [],
      "source": [
        "inputs = {}\n",
        "\n",
        "for name, column in titanic_features.items():\n",
        "  dtype = column.dtype\n",
        "  if dtype == object:\n",
        "    dtype = tf.string\n",
        "  else:\n",
        "    dtype = tf.float32\n",
        "\n",
        "  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
        "\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaheJFmymq8l"
      },
      "source": [
        "전처리 논리의 첫 번째 단계는 숫자 입력을 함께 연결하고 이를 정규화 레이어를 통해 실행하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPRC_E6rkp8D"
      },
      "outputs": [],
      "source": [
        "numeric_inputs = {name:input for name,input in inputs.items()\n",
        "                  if input.dtype==tf.float32}\n",
        "\n",
        "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
        "norm = layers.Normalization()\n",
        "norm.adapt(np.array(titanic[numeric_inputs.keys()]))\n",
        "all_numeric_inputs = norm(x)\n",
        "\n",
        "all_numeric_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JoR45Uj712l"
      },
      "source": [
        "나중에 연결할 수 있도록 모든 기호화된 전처리 결과를 수집합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7jIJw5XntdN"
      },
      "outputs": [],
      "source": [
        "preprocessed_inputs = [all_numeric_inputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Hryylyosfm"
      },
      "source": [
        "문자열 입력의 경우 `tf.keras.layers.StringLookup` 함수를 사용하여 문자열로부터 어휘의 정수 인덱스로 매핑합니다. 그런 다음 `tf.keras.layers.CategoryEncoding`을 사용하여 인덱스를 모델에 적합한 `float32` 데이터로 변환합니다.\n",
        "\n",
        "`tf.keras.layers.CategoryEncoding` 레이어의 기본 설정은 각 입력에 대해 원-핫 벡터를 생성하는 것입니다. `tf.keras.layers.Embedding`도 작동합니다. 이 주제에 대한 자세한 내용은 [전처리 레이어 작업](https://www.tensorflow.org/guide/keras/preprocessing_layers) 가이드 및 [Keras 전처리 레이어를 사용하여 구조화된 데이터 분류](../structured_data/preprocessing_layers.ipynb) 튜토리얼을 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79fi1Cgan2YV"
      },
      "outputs": [],
      "source": [
        "for name, input in inputs.items():\n",
        "  if input.dtype == tf.float32:\n",
        "    continue\n",
        "  \n",
        "  lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))\n",
        "  one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
        "\n",
        "  x = lookup(input)\n",
        "  x = one_hot(x)\n",
        "  preprocessed_inputs.append(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnhv0T7itnc7"
      },
      "source": [
        "`inputs` 및 `preprocessed_inputs` 모음을 사용하여 전처리된 모든 입력을 함께 연결하고 전처리를 처리하는 모델을 빌드할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJRzUTe8ukXc"
      },
      "outputs": [],
      "source": [
        "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
        "\n",
        "titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
        "\n",
        "tf.keras.utils.plot_model(model = titanic_preprocessing , rankdir=\"LR\", dpi=72, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNHxrNW8vdda"
      },
      "source": [
        "이 모델은 입력 전처리만 포함합니다. 이를 실행하여 데이터에 어떤 영향을 미치는지 확인할 수 있습니다. Keras 모델은 Pandas <code>DataFrames</code>를 자동으로 변환하지 않습니다. 왜냐하면 하나의 텐서로 변환해야 하는지 아니면 텐서 사전으로 변환해야 하는지가 명확하지 않기 때문입니다. 따라서 이를 텐서 사전으로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YjdYyMEacwQ"
      },
      "outputs": [],
      "source": [
        "titanic_features_dict = {name: np.array(value) \n",
        "                         for name, value in titanic_features.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nKJYoPByada"
      },
      "source": [
        "첫 번째 훈련 예제를 잘라서 이 전처리 모델로 전달하면 숫자 특성과 문자열 원-핫이 모두 함께 연결된 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjnmU8PSv8T3"
      },
      "outputs": [],
      "source": [
        "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
        "titanic_preprocessing(features_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkBf4LvmzMDp"
      },
      "source": [
        "이제 이 위에 모델을 빌드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coIPtGaCzUV7"
      },
      "outputs": [],
      "source": [
        "def titanic_model(preprocessing_head, inputs):\n",
        "  body = tf.keras.Sequential([\n",
        "    layers.Dense(64),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  preprocessed_inputs = preprocessing_head(inputs)\n",
        "  result = body(preprocessed_inputs)\n",
        "  model = tf.keras.Model(inputs, result)\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                optimizer=tf.keras.optimizers.Adam())\n",
        "  return model\n",
        "\n",
        "titanic_model = titanic_model(titanic_preprocessing, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK5uBQQF2KbZ"
      },
      "source": [
        "모델을 훈련할 때 특성 사전을 `x`로, 레이블을 `y`로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1gVfwJ61ejz"
      },
      "outputs": [],
      "source": [
        "titanic_model.fit(x=titanic_features_dict, y=titanic_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxgJarZk3bfH"
      },
      "source": [
        "전처리는 모델의 일부이므로 모델을 저장하고 다른 곳에 다시 로드하여도 동일한 결과를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay-8ymNA2ZCh"
      },
      "outputs": [],
      "source": [
        "titanic_model.save('test')\n",
        "reloaded = tf.keras.models.load_model('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm6jMTpD20lK"
      },
      "outputs": [],
      "source": [
        "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
        "\n",
        "before = titanic_model(features_dict)\n",
        "after = reloaded(features_dict)\n",
        "assert (before-after)<1e-3\n",
        "print(before)\n",
        "print(after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VsPlxIRZpXf"
      },
      "source": [
        "## tf.data 사용하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyVDCwGzR5HW"
      },
      "source": [
        "이전 섹션에서는 모델을 훈련하는 동안 모델의 내장 데이터 셔플링 및 배치에 의존했습니다.\n",
        "\n",
        "입력 데이터 파이프라인을 더 많이 제어해야 하거나 메모리에 쉽게 맞출 수 없는 데이터를 사용해야 하는 경우 `tf.data`를 사용합니다.\n",
        "\n",
        "더 많은 예를 보려면 [`tf.data`: TensorFlow 입력 파이프라인 빌드](../../guide/data.ipynb) 가이드를 참조하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP5Y1jM2Sor0"
      },
      "source": [
        "### 인메모리 데이터에서\n",
        "\n",
        "CSV 데이터에 `tf.data`를 적용하는 첫 번째 예제로 다음 코드를 고려할 경우 이전 섹션의 특성 사전을 수동으로 분할합니다. 각 인덱스에는 각 특성에 대해 이러한 인덱스를 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8wE-MVuVu7_"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def slices(features):\n",
        "  for i in itertools.count():\n",
        "    # For each feature take index `i`\n",
        "    example = {name:values[i] for name, values in features.items()}\n",
        "    yield example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ3RTbS9YEal"
      },
      "source": [
        "이것을 실행하고 첫 번째 예제를 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwq8XK88WwFk"
      },
      "outputs": [],
      "source": [
        "for example in slices(titanic_features_dict):\n",
        "  for name, value in example.items():\n",
        "    print(f\"{name:19s}: {value}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvp8Dct6YOIE"
      },
      "source": [
        "메모리 데이터 로더에서 가장 기본적인 `tf.data.Dataset`은 `Dataset.from_tensor_slices` 생성자입니다. 이것은 TensorFlow에서 위의 `slices` 함수의 일반화된 버전을 구현하는 `tf.data.Dataset`를 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gEJthslYxeV"
      },
      "outputs": [],
      "source": [
        "features_ds = tf.data.Dataset.from_tensor_slices(titanic_features_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZC0rTpMZMZK"
      },
      "source": [
        "다른 Python iterable과 마찬가지로 `tf.data.Dataset`에 대해 반복할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOHbiefaY4ag"
      },
      "outputs": [],
      "source": [
        "for example in features_ds:\n",
        "  for name, value in example.items():\n",
        "    print(f\"{name:19s}: {value}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwcFoVJWZY5F"
      },
      "source": [
        "`from_tensor_slices` 함수는 모든 구조의 중첩된 사전 또는 튜플을 처리할 수 있습니다. 다음 코드는 `(features_dict, labels)` 쌍의 데이터세트를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIHGBy76Zcrx"
      },
      "outputs": [],
      "source": [
        "titanic_ds = tf.data.Dataset.from_tensor_slices((titanic_features_dict, titanic_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQwxitt8c2GK"
      },
      "source": [
        "이 `Dataset`를 사용하여 모델을 훈련하려면 최소한 데이터를 `shuffle`하고 `batch` 처리해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbJcbldhddeC"
      },
      "outputs": [],
      "source": [
        "titanic_batches = titanic_ds.shuffle(len(titanic_labels)).batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4FRqhRFuoJx"
      },
      "source": [
        "`features` 및 `labels`를 `Model.fit`에 전달하는 대신 데이터세트를 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yXkNPumdBtB"
      },
      "outputs": [],
      "source": [
        "titanic_model.fit(titanic_batches, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXuibiv9exT7"
      },
      "source": [
        "### 단일 파일로부터\n",
        "\n",
        "지금까지 이 튜토리얼은 인메모리 데이터로 작업했습니다. `tf.data`는 데이터 파이프라인을 빌드하기 위한 확장성이 뛰어난 툴킷이며 CSV 파일 로드를 처리하는 몇 가지 기능을 제공합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncf5t6tgL5ZI"
      },
      "outputs": [],
      "source": [
        "titanic_file_path = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4N-plO4tDXd"
      },
      "source": [
        "이제 파일에서 CSV 데이터를 읽고 `tf.data.Dataset`를 작성합니다.\n",
        "\n",
        "(전체 설명서는 `tf.data.experimental.make_csv_dataset`를 참조하세요.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIbUscB9sqha"
      },
      "outputs": [],
      "source": [
        "titanic_csv_ds = tf.data.experimental.make_csv_dataset(\n",
        "    titanic_file_path,\n",
        "    batch_size=5, # Artificially small to make examples easier to show.\n",
        "    label_name='survived',\n",
        "    num_epochs=1,\n",
        "    ignore_errors=True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf3v3BKgy4AG"
      },
      "source": [
        "이 함수에는 많은 편리한 특성이 포함되어 있어 데이터 작업이 용이합니다. 여기에는 다음이 포함되어 있습니다.\n",
        "\n",
        "- 열 헤더를 사전 키로 사용.\n",
        "- 각 열의 유형을 자동으로 결정.\n",
        "\n",
        "주의: `tf.data.experimental.make_csv_dataset`에서 `num_epochs` 인수를 설정해야 합니다. 그렇지 않으면 `tf.data.Dataset`의 기본 동작은 루프를 무한히 반복하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4oMO9MIxgTG"
      },
      "outputs": [],
      "source": [
        "for batch, label in titanic_csv_ds.take(1):\n",
        "  for key, value in batch.items():\n",
        "    print(f\"{key:20s}: {value}\")\n",
        "  print()\n",
        "  print(f\"{'label':20s}: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-TgA6o2Ja6U"
      },
      "source": [
        "참고: 위의 셀을 두 번 실행하면 다른 결과가 생성됩니다. `tf.data.experimental.make_csv_dataset`의 기본 설정에는 `shuffle_buffer_size=1000`이 포함되며, 이는 이 작은 데이터세트에는 충분하지만 실제 데이터세트에는 그렇지 않을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6uviU_KCCWD"
      },
      "source": [
        "즉시 데이터 압축을 풀 수도 있습니다. 다음은 [대도시 주간 교통량 데이터세트](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume)가 포함된 gzip으로 압축된 CSV 파일입니다.\n",
        "\n",
        "![교통 정체.](images/csv/traffic.jpg)\n",
        "\n",
        "이미지 출처: [Wikimedia](https://commons.wikimedia.org/wiki/File:Trafficjam.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT7oZI2E46Q8"
      },
      "outputs": [],
      "source": [
        "traffic_volume_csv_gz = tf.keras.utils.get_file(\n",
        "    'Metro_Interstate_Traffic_Volume.csv.gz', \n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\",\n",
        "    cache_dir='.', cache_subdir='traffic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-IOsFHbCw0i"
      },
      "source": [
        "압축된 파일로부터 직접 읽도록 `compression_type` 인수를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar0MPEVJ5NeA"
      },
      "outputs": [],
      "source": [
        "traffic_volume_csv_gz_ds = tf.data.experimental.make_csv_dataset(\n",
        "    traffic_volume_csv_gz,\n",
        "    batch_size=256,\n",
        "    label_name='traffic_volume',\n",
        "    num_epochs=1,\n",
        "    compression_type=\"GZIP\")\n",
        "\n",
        "for batch, label in traffic_volume_csv_gz_ds.take(1):\n",
        "  for key, value in batch.items():\n",
        "    print(f\"{key:20s}: {value[:5]}\")\n",
        "  print()\n",
        "  print(f\"{'label':20s}: {label[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p12Y6tGq8D6M"
      },
      "source": [
        "참고: `tf.data` 파이프라인에서 해당 날짜-시간 문자열을 파싱해야 하는 경우 `tfa.text.parse_time`을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrAXzYGP3l0"
      },
      "source": [
        "### 캐싱"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN2dL_LRP83r"
      },
      "source": [
        "CSV 데이터를 파싱하는 데 약간의 오버헤드가 있습니다. 작은 크기의 모델의 경우 이때 훈련 병목 현상이 발생할 수 있습니다.\n",
        "\n",
        "사용 사례에 따라 CSV 데이터가 첫 epoch에서만 구문 분석되도록 `Dataset.cache` 또는 `tf.data.Dataset.snapshot`을 사용하는 것이 좋습니다.\n",
        "\n",
        "`cache`와 `snapshot` 메서드의 주요 차이점은 `cache` 파일은 이를 생성한 TensorFlow 프로세스에서만 사용할 수 있다는 것입니다. 다만, `snapshot` 파일은 다른 프로세스에서 읽을 수 있습니다.\n",
        "\n",
        "예를 들어, `traffic_volume_csv_gz_ds`를 20번 반복하는 데 캐싱 없이는 약 15초, 캐싱이 있으면 약 2초가 걸릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk38Sw4MO4eh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i, (batch, label) in enumerate(traffic_volume_csv_gz_ds.repeat(20)):\n",
        "  if i % 40 == 0:\n",
        "    print('.', end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN3HtDONh5TX"
      },
      "source": [
        "참고: `Dataset.cache`는 첫 번째 epoch의 데이터를 저장하고 순서대로 재생합니다. 따라서 `cache` 메서드를 사용하면 파이프라인의 초기에 모든 셔플이 비활성화됩니다. 아래에서 `Dataset.shuffle`은 `Dataset.cache` 뒤에 다시 추가됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5Jj72MrPbnh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "caching = traffic_volume_csv_gz_ds.cache().shuffle(1000)\n",
        "\n",
        "for i, (batch, label) in enumerate(caching.shuffle(1000).repeat(20)):\n",
        "  if i % 40 == 0:\n",
        "    print('.', end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN7uUBjmgNZ9"
      },
      "source": [
        "참고: `tf.data.Dataset.snapshot` 파일은 사용 중인 데이터세트를 *임시* 저장할 경우 사용합니다. 이것은 장기 저장을 위한 형식이 *아닙니다*. 파일 형식은 내부 세부 정보로 간주되며 TensorFlow 버전 간의 호환은 보장되지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHGD1E8ktUvW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "snapshotting = traffic_volume_csv_gz_ds.snapshot('titanic.tfsnap').shuffle(1000)\n",
        "\n",
        "for i, (batch, label) in enumerate(snapshotting.shuffle(1000).repeat(20)):\n",
        "  if i % 40 == 0:\n",
        "    print('.', end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUSSegnMCGRz"
      },
      "source": [
        "CSV 파일 로드로 인해 데이터 로드가 느려지고 `Dataset.cache` 및 `tf.data.Dataset.snapshot`이 사용 사례에 충분하지 않은 경우, 데이터를 보다 간소화된 형식으로 다시 인코딩하는 것이 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0iGXv9pC5kr"
      },
      "source": [
        "### 여러 파일"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FFzHQrCDH4w"
      },
      "source": [
        "지금까지 이 섹션의 모든 예제는 `tf.data` 없이 쉽게 수행할 수 있었습니다. `tf.data`를 사용하여 실제로 작업을 단순화할 수 있는 한 예는 파일 모음을 처리할 경우입니다.\n",
        "\n",
        "예를 들어 [문자 글꼴 이미지](https://archive.ics.uci.edu/ml/datasets/Character+Font+Images) 데이터세트는 글꼴당 하나씩, csv 파일 모음으로 배포됩니다.\n",
        "\n",
        "![글꼴](images/csv/fonts.jpg)\n",
        "\n",
        "<a href=\"https://pixabay.com/users/wilhei-883152/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=705667\">Pixabay</a>에서 <a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=705667\">Willi Heidelbach</a>가 제공한 이미지\n",
        "\n",
        "데이터세트를 다운로드하고 내부 파일을 검토합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmVknMdJh5ks"
      },
      "outputs": [],
      "source": [
        "fonts_zip = tf.keras.utils.get_file(\n",
        "    'fonts.zip',  \"https://archive.ics.uci.edu/ml/machine-learning-databases/00417/fonts.zip\",\n",
        "    cache_dir='.', cache_subdir='fonts',\n",
        "    extract=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsDlMCnyi55e"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "font_csvs =  sorted(str(p) for p in pathlib.Path('fonts').glob(\"*.csv\"))\n",
        "\n",
        "font_csvs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRAEJx9ROAGl"
      },
      "outputs": [],
      "source": [
        "len(font_csvs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Udrw9iG-FS"
      },
      "source": [
        "많은 파일을 처리할 경우 glob 스타일의 `file_pattern`을 `tf.data.experimental.make_csv_dataset` 함수에 전달할 수 있습니다. 파일의 순서는 각 반복마다 뒤섞입니다.\n",
        "\n",
        "`num_parallel_reads` 인수를 사용하여 병렬로 읽고 함께 인터리브 처리되는 파일의 수를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TSUNdT6iG58"
      },
      "outputs": [],
      "source": [
        "fonts_ds = tf.data.experimental.make_csv_dataset(\n",
        "    file_pattern = \"fonts/*.csv\",\n",
        "    batch_size=10, num_epochs=1,\n",
        "    num_parallel_reads=20,\n",
        "    shuffle_buffer_size=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMoexinLHYFa"
      },
      "source": [
        "이러한 CSV 파일의 이미지는 단일 행으로 평면화되어 있습니다. 열 이름의 형식은 `r{row}c{column}`입니다. 다음은 첫 번째 배치입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmFvBWxxi3pq"
      },
      "outputs": [],
      "source": [
        "for features in fonts_ds.take(1):\n",
        "  for i, (name, value) in enumerate(features.items()):\n",
        "    if i>15:\n",
        "      break\n",
        "    print(f\"{name:20s}: {value}\")\n",
        "print('...')\n",
        "print(f\"[total: {len(features)} features]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrC3sKdeOhb5"
      },
      "source": [
        "#### 선택 사항: 패킹 필드\n",
        "\n",
        "여러분은 아마도 이와 같이 별도의 열에 있는 각 픽셀로 작업하고 싶지는 않을 것입니다. 이 데이터세트를 사용하기 전에 픽셀을 이미지 텐서로 패킹해야 합니다.\n",
        "\n",
        "다음은 각 예제의 이미지를 빌드하기 위해 열 이름을 파싱하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hct5EMEWNyfH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def make_images(features):\n",
        "  image = [None]*400\n",
        "  new_feats = {}\n",
        "\n",
        "  for name, value in features.items():\n",
        "    match = re.match('r(\\d+)c(\\d+)', name)\n",
        "    if match:\n",
        "      image[int(match.group(1))*20+int(match.group(2))] = value\n",
        "    else:\n",
        "      new_feats[name] = value\n",
        "\n",
        "  image = tf.stack(image, axis=0)\n",
        "  image = tf.reshape(image, [20, 20, -1])\n",
        "  new_feats['image'] = image\n",
        "\n",
        "  return new_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qy8utAwARP"
      },
      "source": [
        "데이터세트의 각 배치에 해당 함수를 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJnnfIW9baE4"
      },
      "outputs": [],
      "source": [
        "fonts_image_ds = fonts_ds.map(make_images)\n",
        "\n",
        "for features in fonts_image_ds.take(1):\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ThqrthGwHSm"
      },
      "source": [
        "결과 이미지를 플롯합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5dcey31T_tk"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,6), dpi=120)\n",
        "\n",
        "for n in range(9):\n",
        "  plt.subplot(3,3,n+1)\n",
        "  plt.imshow(features['image'][..., n])\n",
        "  plt.title(chr(features['m_label'][n]))\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-nNR0Nncdd1"
      },
      "source": [
        "## 하위 수준 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jiGZeUijJNd"
      },
      "source": [
        "지금까지 이 튜토리얼은 csv 데이터를 읽기 위한 가장 높은 수준의 유틸리티에 중점을 두었습니다. 사용 사례가 이 기본 패턴에 맞지 않는 경우 고급 사용자에게 도움이 될 수 있는 다른 두 가지 API가 있습니다.\n",
        "\n",
        "- `tf.io.decode_csv`: 텍스트 줄을 CSV 열 텐서 목록으로 파싱하는 함수입니다.\n",
        "- `tf.data.experimental.CsvDataset`: 하위 수준 CSV 데이터세트 생성자입니다.\n",
        "\n",
        "이 섹션에서는 `tf.data.experimental.make_csv_dataset`에서 제공하는 기능을 다시 만들어 이 하위 수준 기능을 사용하는 방법을 보여줍니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL_ixywomOHW"
      },
      "source": [
        "### `tf.io.decode_csv`\n",
        "\n",
        "이 함수는 문자열 또는 문자열 목록을 열 목록으로 디코딩합니다.\n",
        "\n",
        "`tf.data.experimental.make_csv_dataset`과 달리 이 함수는 열 데이터 유형을 추측하지 않습니다. 각 열에 대해 올바른 유형의 값이 포함된 `record_defaults` 목록을 제공하여 열 유형을 지정합니다.\n",
        "\n",
        "`tf.io.decode_csv`를 사용하여 타이타닉 데이터를 **문자열로** 읽기 위해 다음과 같이 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1D2C-qdlqeW"
      },
      "outputs": [],
      "source": [
        "text = pathlib.Path(titanic_file_path).read_text()\n",
        "lines = text.split('\\n')[1:-1]\n",
        "\n",
        "all_strings = [str()]*10\n",
        "all_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W4UeJYyHPx5"
      },
      "outputs": [],
      "source": [
        "features = tf.io.decode_csv(lines, record_defaults=all_strings) \n",
        "\n",
        "for f in features:\n",
        "  print(f\"type: {f.dtype.name}, shape: {f.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8TaHSQFoQL4"
      },
      "source": [
        "실제 유형으로 파싱하려면 해당 유형의 `record_defaults` 목록을 만듭니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzUjR59yoUe1"
      },
      "outputs": [],
      "source": [
        "print(lines[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sPTunxwoeWU"
      },
      "outputs": [],
      "source": [
        "titanic_types = [int(), str(), float(), int(), int(), float(), str(), str(), str(), str()]\n",
        "titanic_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3NlViCzoB7F"
      },
      "outputs": [],
      "source": [
        "features = tf.io.decode_csv(lines, record_defaults=titanic_types) \n",
        "\n",
        "for f in features:\n",
        "  print(f\"type: {f.dtype.name}, shape: {f.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-LkTUTnpn2P"
      },
      "source": [
        "참고: CSV 텍스트의 개별 라인보다 대형 라인 배치에서 `tf.io.decode_csv`를 호출하는 것이 더 효율적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp1UItJmqGqw"
      },
      "source": [
        "### `tf.data.experimental.CsvDataset`\n",
        "\n",
        "`tf.data.experimental.CsvDataset` 클래스는 `tf.data.experimental.make_csv_dataset` 함수의 편리한 특성인 열 헤더 파싱, 열 유형 추론, 자동 셔플링, 파일 인터리빙 없이 최소한의 CSV `Dataset` 인터페이스를 제공합니다.\n",
        "\n",
        "이 생성자는 `tf.io.decode_csv`와 같은 방식으로 `record_defaults`를 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OzZLp3krP-t"
      },
      "outputs": [],
      "source": [
        "simple_titanic = tf.data.experimental.CsvDataset(titanic_file_path, record_defaults=titanic_types, header=True)\n",
        "\n",
        "for example in simple_titanic.take(1):\n",
        "  print([e.numpy() for e in example])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HBmfI-Ks7dw"
      },
      "source": [
        "위의 코드는 기본적으로 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5O5d69Yq7gG"
      },
      "outputs": [],
      "source": [
        "def decode_titanic_line(line):\n",
        "  return tf.io.decode_csv(line, titanic_types)\n",
        "\n",
        "manual_titanic = (\n",
        "    # Load the lines of text\n",
        "    tf.data.TextLineDataset(titanic_file_path)\n",
        "    # Skip the header row.\n",
        "    .skip(1)\n",
        "    # Decode the line.\n",
        "    .map(decode_titanic_line)\n",
        ")\n",
        "\n",
        "for example in manual_titanic.take(1):\n",
        "  print([e.numpy() for e in example])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R3ralsnt2AC"
      },
      "source": [
        "#### 여러 파일\n",
        "\n",
        "`tf.data.experimental.CsvDataset`을 사용하여 글꼴 데이터세트를 파싱하려면 먼저 `record_defaults`에 대한 열 유형을 결정해야 합니다. 우선 한 파일의 첫 번째 행을 검사합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tlFOTjCvAI5"
      },
      "outputs": [],
      "source": [
        "font_line = pathlib.Path(font_csvs[0]).read_text().splitlines()[1]\n",
        "print(font_line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etyGu8K_ySRz"
      },
      "source": [
        "처음 두 개의 필드만 문자열이고 나머지는 정수 또는 부동 소수점이며 쉼표를 계산하여 총 특성 수를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crgZZn0BzkSB"
      },
      "outputs": [],
      "source": [
        "num_font_features = font_line.count(',')+1\n",
        "font_column_types = [str(), str()] + [float()]*(num_font_features-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeK2Pw540RNj"
      },
      "source": [
        "`tf.data.experimental.CsvDataset` 생성자는 입력 파일 목록을 가져올 수 있지만 순차적으로 읽습니다. CSV 목록의 첫 번째 파일은 `AGENCY.csv`입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SvL5Uvl0r0N"
      },
      "outputs": [],
      "source": [
        "font_csvs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfAX3G8Xywy6"
      },
      "source": [
        "따라서 파일 목록을 `CsvDataset`에 전달할 때 `AGENCY.csv`의 레코드를 먼저 읽습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtr1E66VmBqj"
      },
      "outputs": [],
      "source": [
        "simple_font_ds = tf.data.experimental.CsvDataset(\n",
        "    font_csvs, \n",
        "    record_defaults=font_column_types, \n",
        "    header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k750Mgq4yt_o"
      },
      "outputs": [],
      "source": [
        "for row in simple_font_ds.take(10):\n",
        "  print(row[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiqWKQV21FrE"
      },
      "source": [
        "여러 파일을 인터리브하려면 `Dataset.interleave`를 사용합니다.\n",
        "\n",
        "CSV 파일 이름이 포함된 초기 데이터세트는 다음과 같습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9dS3SNb23W8"
      },
      "outputs": [],
      "source": [
        "font_files = tf.data.Dataset.list_files(\"fonts/*.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNiLHMXpzHy5"
      },
      "source": [
        "이렇게 하면 각 epoch마다 파일 이름을 셔플합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNd-TYyNzIgg"
      },
      "outputs": [],
      "source": [
        "print('Epoch 1:')\n",
        "for f in list(font_files)[:5]:\n",
        "  print(\"    \", f.numpy())\n",
        "print('    ...')\n",
        "print()\n",
        "\n",
        "print('Epoch 2:')\n",
        "for f in list(font_files)[:5]:\n",
        "  print(\"    \", f.numpy())\n",
        "print('    ...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0QB1PtU3WAN"
      },
      "source": [
        "`interleave` 메서드는 상위 `Dataset`의 각 요소마다 하위 `Dataset`를 생성하는 `map_func`를 사용합니다.\n",
        "\n",
        "여기에서 파일 데이터세트의 각 요소에서 `tf.data.experimental.CsvDataset`을 생성하려고 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWp4rH0Q4uPh"
      },
      "outputs": [],
      "source": [
        "def make_font_csv_ds(path):\n",
        "  return tf.data.experimental.CsvDataset(\n",
        "    path, \n",
        "    record_defaults=font_column_types, \n",
        "    header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxRGdLMB5nRF"
      },
      "source": [
        "인터리브로 반환한 `Dataset`는 여러 하위 `Dataset`를 순환하며 요소를 반환합니다. 아래에서 데이터세트가 `cycle_length=3` 세 가지 글꼴 파일을 순환하는 방식을 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OePMNF_x1_Cc"
      },
      "outputs": [],
      "source": [
        "font_rows = font_files.interleave(make_font_csv_ds,\n",
        "                                  cycle_length=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UORIGWLy54-E"
      },
      "outputs": [],
      "source": [
        "fonts_dict = {'font_name':[], 'character':[]}\n",
        "\n",
        "for row in font_rows.take(10):\n",
        "  fonts_dict['font_name'].append(row[0].numpy().decode())\n",
        "  fonts_dict['character'].append(chr(row[2].numpy()))\n",
        "\n",
        "pd.DataFrame(fonts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkKZa_HX8zAm"
      },
      "source": [
        "#### 공연\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BtGHraUApdJ"
      },
      "source": [
        "이전에 `tf.io.decode_csv`가 문자열 배치에서 실행될 때 더 효율적이라는 점을 언급했습니다.\n",
        "\n",
        "대형 배치 크기를 사용할 때 이 사실을 활용하여 CSV 로드 성능을 향상시킬 수 있습니다(단, 먼저 [캐싱](#caching)을 시도해야 함)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d35zWMH7MDL1"
      },
      "source": [
        "내장 로더 20을 사용할 경우 2048개의 예제 배치에 약 17초가 걸립니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieUVAPryjpJS"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=2048\n",
        "fonts_ds = tf.data.experimental.make_csv_dataset(\n",
        "    file_pattern = \"fonts/*.csv\",\n",
        "    batch_size=BATCH_SIZE, num_epochs=1,\n",
        "    num_parallel_reads=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUC2KW4LkQIz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i,batch in enumerate(fonts_ds.take(20)):\n",
        "  print('.',end='')\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lhnh6rZEDS2"
      },
      "source": [
        "**텍스트 줄 배치**를 `decode_csv`에 전달하면 더 빠르게 약 5초 만에 실행됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XbPZV1okVF9"
      },
      "outputs": [],
      "source": [
        "fonts_files = tf.data.Dataset.list_files(\"fonts/*.csv\")\n",
        "fonts_lines = fonts_files.interleave(\n",
        "    lambda fname:tf.data.TextLineDataset(fname).skip(1), \n",
        "    cycle_length=100).batch(BATCH_SIZE)\n",
        "\n",
        "fonts_fast = fonts_lines.map(lambda x: tf.io.decode_csv(x, record_defaults=font_column_types))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te9C2km-qO8W"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i,batch in enumerate(fonts_fast.take(20)):\n",
        "  print('.',end='')\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aebC1plsMeOi"
      },
      "source": [
        "대규모 배치를 사용하여 CSV 성능을 높이는 또 다른 예는 [과대적합과 과소적합 튜토리얼](../keras/overfit_and_underfit.ipynb)을 참조하세요.\n",
        "\n",
        "이러한 종류의 접근 방식이 효과가 있을 수 있지만 `Dataset.cache` 및 `tf.data.Dataset.snapshot`과 같은 다른 옵션과 함께 데이터를 보다 간소화된 형식으로 다시 인코딩하는 방법도 고려하세요."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "csv.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
