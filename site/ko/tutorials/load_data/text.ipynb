{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DweYe9FcbMK_"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AVV2e0XKbJeX"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZfSvVcDo6GQ"
      },
      "source": [
        "# 텍스트 로드하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giK0nMbZFnoR"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a>   </td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwlfPb11GH8J"
      },
      "source": [
        "이 튜토리얼은 텍스트를 로드하고 전처리하는 두 가지 방법을 보여 줍니다.\n",
        "\n",
        "- 먼저 Keras 유틸리티와 전처리 레이어를 사용합니다. 여기에는 데이터 표준화, 토큰화 및 벡터화를 위해 데이터를 `tf.data.Dataset`와 `tf.keras.layers.TextVectorization`으로 변환하는 `tf.keras.utils.text_dataset_from_directory`가 포함되어 있습니다. TensorFlow가 처음이라면 여기서부터 시작해야 합니다.\n",
        "- 그런 다음 `tf.data.TextLineDataset`와 같은 하위 수준 유틸리티를 사용하여 텍스트 파일을 로드하고, 보다 세밀한 제어를 위해 `text.UnicodeScriptTokenizer` 및 `text.case_fold_utf8`과 같은 [TensorFlow Text](https://www.tensorflow.org/text) API를 사용하여 데이터를 처리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa6IKWvADqH7"
      },
      "outputs": [],
      "source": [
        "!pip install \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import pathlib\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az-d_K5_HQ5k"
      },
      "source": [
        "## 예제 1: 스택 오버플로 질문에 대한 태그 예측하기\n",
        "\n",
        "첫 번째 예제로 스택 오버플로에서 프로그래밍 질문 데이터세트를 다운로드합니다. 각 질문(*\"값으로 사전을 어떻게 정렬하나요?\"*)마다 정확히 하나의 태그(`Python`, `CSharp`, `JavaScript`, 또는 `Java`)가 레이블로 지정됩니다. 여러분의 작업은 질문에 대한 태그를 예측하는 모델을 개발하는 것입니다. 이것은 중요하고 널리 적용 가능한 머신러닝 문제인 다중 클래스 분류 예제입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjC3yLa5IjP7"
      },
      "source": [
        "### 데이터세트 다운로드 및 탐색하기\n",
        "\n",
        "`tf.keras.utils.get_file`을 사용하여 스택 오버플로 데이터세트를 다운로드하고, 디렉터리 구조를 탐색하여 시작하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ELgzA6SHTuV"
      },
      "outputs": [],
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "\n",
        "dataset_dir = utils.get_file(\n",
        "    origin=data_url,\n",
        "    untar=True,\n",
        "    cache_dir='stack_overflow',\n",
        "    cache_subdir='')\n",
        "\n",
        "dataset_dir = pathlib.Path(dataset_dir).parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIrPl5fUH2gb"
      },
      "outputs": [],
      "source": [
        "list(dataset_dir.iterdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEoV7YByJoWQ"
      },
      "outputs": [],
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "list(train_dir.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mxAN17MhEh0"
      },
      "source": [
        "`train/csharp`, `train/java`, `train/python` 및 `train/javascript` 디렉터리에는 많은 텍스트 파일이 포함되어 있으며, 각 텍스트 파일의 내용은 스택 오버플로 질문입니다.\n",
        "\n",
        "다음과 같이 예제 파일을 출력하고 데이터를 검사합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go1vTSGdJu08"
      },
      "outputs": [],
      "source": [
        "sample_file = train_dir/'python/1755.txt'\n",
        "\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deWBTkpJiO7D"
      },
      "source": [
        "### 데이터세트 로드하기\n",
        "\n",
        "다음으로 디스크로부터 데이터를 로드하고 데이터를 훈련에 적합한 형식으로 준비합니다. 이렇게 하기 위해 여러분은 `tf.keras.utils.text_dataset_from_directory` 유틸리티를 사용하여 레이블이 지정된 `tf.data.Dataset`를 생성합니다. `tf.data`는 입력 파이프라인을 구축하는 강력한 도구 모음입니다([tf.data: TensorFlow 입력 파이프라인 빌드](../../guide/data.ipynb) 가이드에서 자세히 알아보세요).\n",
        "\n",
        "`tf.keras.utils.text_dataset_from_directory` API는 다음과 같은 디렉터리 구조를 예상합니다.\n",
        "\n",
        "```\n",
        "train/\n",
        "...csharp/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...java/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...javascript/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...python/\n",
        "......1.txt\n",
        "......2.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyl6JTAjlbQV"
      },
      "source": [
        "머신러닝 실험을 실행할 때 데이터세트를 [훈련](https://developers.google.com/machine-learning/glossary#training_set), [검증](https://developers.google.com/machine-learning/glossary#validation_set) 및 [테스트](https://developers.google.com/machine-learning/glossary#test-set)의 세 부분으로 나누는 것이 가장 좋습니다.\n",
        "\n",
        "스택 오버플로 데이터세트는 이미 훈련 세트와 테스트 세트로 나누어져 있지만 여기에는 검증 세트가 없습니다.\n",
        "\n",
        "`validation_split`이 `0.2`(즉, 20%)로 설정된 `tf.keras.utils.text_dataset_from_directory`를 사용하여 훈련 데이터를 80:20의 비율로 분할하는 검증 세트를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqyliMw8N-az"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMI_gPLfloD7"
      },
      "source": [
        "이전 셀 출력을 통해 알 수 있듯이 훈련 폴더에는 8,000개의 예제가 있으며 여러분은 그 중 80%(또는 6,400개의 예제)를 훈련에 사용할 것입니다. 여러분은 `tf.data.Dataset`를 `Model.fit`에 직접 전달하여 모델을 훈련할 수 있다는 것을 곧 배우게 될 것입니다.\n",
        "\n",
        "먼저 데이터세트를 반복하고 몇 가지 예제를 인쇄하여 데이터에 대한 감각을 익히세요.\n",
        "\n",
        "참고: 분류 문제의 난이도를 높이기 위해 데이터세트 작성자는 프로그래밍 질문에서 *Python*, *CSharp*, *JavaScript* 또는 *Java*라는 단어를 *blank*로 대체했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JMTyZ6Glt_C"
      },
      "outputs": [],
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(10):\n",
        "    print(\"Question: \", text_batch.numpy()[i])\n",
        "    print(\"Label:\", label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCZGl4Q5l2sS"
      },
      "source": [
        "레이블은 `0`, `1`, `2` 또는 `3`입니다. 이들이 어떠한 문자열 레이블에 해당하는지 확인하려면 데이터세트의 `class_names` 속성을 검사하면 됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIpCS7YjmGkj"
      },
      "outputs": [],
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "  print(\"Label\", i, \"corresponds to\", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUsdn-37qol9"
      },
      "source": [
        "다음으로 `tf.keras.utils.text_dataset_from_directory`를 사용하여 검증 및 테스트 세트를 만듭니다. 검증을 위해 훈련 세트의 나머지 1,600개 리뷰를 사용합니다.\n",
        "\n",
        "참고: `tf.keras.utils.text_dataset_from_directory`의 `validation_split` 및 `subset` 인수를 사용할 때 검증 및 훈련 분할이 겹치지 않도록 임의 시드를 지정하거나 `shuffle=False`를 전달하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7m6sCWJQuYt"
      },
      "outputs": [],
      "source": [
        "# Create a validation set.\n",
        "raw_val_ds = utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXMZc7fMQwKE"
      },
      "outputs": [],
      "source": [
        "test_dir = dataset_dir/'test'\n",
        "\n",
        "# Create a test set.\n",
        "raw_test_ds = utils.text_dataset_from_directory(\n",
        "    test_dir,\n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdt-ATrGRGDL"
      },
      "source": [
        "### 훈련을 위한 데이터세트 준비하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6fRti45Rlj8"
      },
      "source": [
        "다음으로 `tf.keras.layers.TextVectorization` 레이어를 사용하여 데이터를 표준화, 토큰화 및 벡터화합니다.\n",
        "\n",
        "- *표준화*는 일반적으로 데이터세트를 단순화하기 위해 구두점이나 HTML 요소를 제거하도록 텍스트를 전처리하는 것을 일컫습니다.\n",
        "- *토큰화*는 문자열을 토큰으로 분할하는 것을 일컫습니다(예: 문장을 공백을 사용하여 개별 단어로 분할).\n",
        "- *벡터화*는 신경망에 제공할 수 있도록 토큰을 숫자로 변환하는 것을 일컫습니다.\n",
        "\n",
        "위의 모든 작업은 이 레이어로 수행할 수 있습니다(`tf.keras.layers.TextVectorization` API 문서에서 각 작업에 대해 자세히 알아볼 수 있습니다).\n",
        "\n",
        "참고 사항:\n",
        "\n",
        "- 기본 표준화는 텍스트를 소문자로 변환하고 구두점을 제거합니다(`standardize='lower_and_strip_punctuation'`).\n",
        "- 기본 토큰화는 공백으로 분할합니다(`split='whitespace'`).\n",
        "- 기본 벡터화 모드는 `'int'`(`output_mode='int'`)입니다. 이 모드는 정수 인덱스를 출력합니다(토큰당 하나). 이 모드는 단어 순서를 고려하는 모델을 빌드하는 데 사용할 수 있습니다. `'binary'`와 같은 다른 모드를 사용하여 [bag-of-words](https://developers.google.com/machine-learning/glossary#bag-of-words) 모델을 빌드할 수도 있습니다.\n",
        "\n",
        "`TextVectorization`을 사용하는 표준화, 토큰화 및 벡터화에 대해 자세히 알아보기 위해 다음 두 가지 모델을 빌드합니다.\n",
        "\n",
        "- 먼저 `'binary'` 벡터화 모드를 사용하여 bag-of-words 모델을 빌드합니다.\n",
        "- 그런 다음 1D ConvNet에서 `'int'` 모드를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voaC43rZR0jc"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "binary_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDPFxuf2Hfz"
      },
      "source": [
        "`'int'` 모드의 경우 최대 어휘 크기 외에 명시적인 최대 시퀀스 길이(`MAX_SEQUENCE_LENGTH`)를 설정해야 레이어가 패딩되거나 시퀀스를 정확히 `output_sequence_length` 값으로 자릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWsY01Zl2aRe"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "int_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts6h9b5atD-Y"
      },
      "source": [
        "다음으로 전처리 레이어의 상태를 데이터세트에 맞추기 위해 `TextVectorization.adapt`를 호출합니다. 그러면 모델이 문자열 인덱스를 정수로 빌드합니다.\n",
        "\n",
        "참고: 테스트세트를 사용하면 정보가 누출되므로 `TextVectorization.adapt`를 호출할 때 훈련 데이터만 사용하는 것이 중요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTXsdDEqSf9e"
      },
      "outputs": [],
      "source": [
        "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
        "train_text = raw_train_ds.map(lambda text, labels: text)\n",
        "binary_vectorize_layer.adapt(train_text)\n",
        "int_vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKVO6Jg7Sls0"
      },
      "source": [
        "이러한 레이어를 사용하여 데이터를 전처리한 결과를 인쇄합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RngfPyArSsvM"
      },
      "outputs": [],
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return binary_vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1W54wf0LhQ0"
      },
      "outputs": [],
      "source": [
        "def int_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return int_vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi_sElMiSmXe"
      },
      "outputs": [],
      "source": [
        "# Retrieve a batch (of 32 reviews and labels) from the dataset.\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_question, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Question\", first_question)\n",
        "print(\"Label\", first_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGukZoYv2v3v"
      },
      "outputs": [],
      "source": [
        "print(\"'binary' vectorized question:\",\n",
        "      binary_vectorize_text(first_question, first_label)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu07FsIw2yH5"
      },
      "outputs": [],
      "source": [
        "print(\"'int' vectorized question:\",\n",
        "      int_vectorize_text(first_question, first_label)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgjeF9PdS7tN"
      },
      "source": [
        "위에 표시된 것처럼 `TextVectorization`의 `'binary'` 모드는 입력에 한 번 이상 존재하는 토큰을 나타내는 배열을 반환하는 반면 `'int'` 모드는 각 토큰을 정수로 대체하기에 원래 순서를 유지합니다.\n",
        "\n",
        "레이어에서 `TextVectorization.get_vocabulary`를 호출하여 각 정수가 해당하는 토큰(문자열)을 조회할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpBnTZilS8wt"
      },
      "outputs": [],
      "source": [
        "print(\"1289 ---> \", int_vectorize_layer.get_vocabulary()[1289])\n",
        "print(\"313 ---> \", int_vectorize_layer.get_vocabulary()[313])\n",
        "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kHgPE_YwHvp"
      },
      "source": [
        "모델을 훈련할 준비가 거의 되었습니다.\n",
        "\n",
        "최종 전처리 단계로 이전에 생성한 `TextVectorization` 레이어를 훈련, 검증 및 테스트 세트에 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46LeHmnD55wJ"
      },
      "outputs": [],
      "source": [
        "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
        "\n",
        "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
        "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
        "int_test_ds = raw_test_ds.map(int_vectorize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHuAF8hYfP5Z"
      },
      "source": [
        "### 성능을 높이도록 데이터세트 구성하기\n",
        "\n",
        "다음은 I/O가 차단되지 않도록 데이터를 로드할 때 사용해야 하는 두 가지 중요한 메서드입니다.\n",
        "\n",
        "- `Dataset.cache`는 데이터가 디스크에서 로드된 후 메모리에 데이터를 보관합니다. 이렇게 하면 모델을 훈련하는 동안 데이터세트로 인한 병목 현상이 발생하지 않습니다. 데이터세트가 너무 커서 메모리에 맞지 않는 경우 이 메서드를 사용하여 성능이 뛰어난 온 디스크 캐시를 생성할 수도 있습니다. 다수의 작은 파일보다 읽기가 더 효율적입니다.\n",
        "- `Dataset.prefetch`는 훈련하는 동안 데이터 전처리 및 모델 실행을 중첩시킵니다.\n",
        "\n",
        "[tf.data API를 통한 성능 향상](../../guide/data_performance.ipynb) 가이드의 *프리페칭* 섹션에서 두 가지 메서드와 데이터를 디스크에 캐시하는 방법에 대해 자세히 알아볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PabA9DFIfSz7"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def configure_dataset(dataset):\n",
        "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8GcJLvb3JH0"
      },
      "outputs": [],
      "source": [
        "binary_train_ds = configure_dataset(binary_train_ds)\n",
        "binary_val_ds = configure_dataset(binary_val_ds)\n",
        "binary_test_ds = configure_dataset(binary_test_ds)\n",
        "\n",
        "int_train_ds = configure_dataset(int_train_ds)\n",
        "int_val_ds = configure_dataset(int_val_ds)\n",
        "int_test_ds = configure_dataset(int_test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYGb7z_bfpGm"
      },
      "source": [
        "### 모델 훈련하기\n",
        "\n",
        "이제 신경망을 만들 차례입니다.\n",
        "\n",
        "`'binary'` 벡터화된 데이터의 경우 간단한 bag-of-words 선형 모델을 정의한 다음 데이터를 구성하고 훈련합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q8iAU-VMzaN"
      },
      "outputs": [],
      "source": [
        "binary_model = tf.keras.Sequential([layers.Dense(4)])\n",
        "\n",
        "binary_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = binary_model.fit(\n",
        "    binary_train_ds, validation_data=binary_val_ds, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwidD-SwNIkz"
      },
      "source": [
        "그런 다음 `'int'` 벡터화된 레이어를 사용하여 1D ConvNet을 빌드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ztw2XH_LbVz"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, num_labels):\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
        "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
        "      layers.GlobalMaxPooling1D(),\n",
        "      layers.Dense(num_labels)\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9rG1cFRL31Z"
      },
      "outputs": [],
      "source": [
        "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n",
        "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n",
        "int_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3J9Eeuv97zE"
      },
      "source": [
        "두 모델을 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8ViDXw99v_u"
      },
      "outputs": [],
      "source": [
        "print(\"Linear model on binary vectorized data:\")\n",
        "print(binary_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9BOeoCwborD"
      },
      "outputs": [],
      "source": [
        "print(\"ConvNet model on int vectorized data:\")\n",
        "print(int_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYYW9tUdCtTy"
      },
      "source": [
        "테스트 데이터에서 두 모델을 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dTc4nZqf7fK"
      },
      "outputs": [],
      "source": [
        "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
        "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
        "\n",
        "print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))\n",
        "print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9dhj8Hey9DS"
      },
      "source": [
        "참고: 이 예제 데이터세트는 다소 단순한 분류 문제를 나타냅니다. 더 복잡한 데이터세트와 문제는 전처리 전략과 모델 아키텍처에서 미묘하지만 중요한 차이를 나타냅니다. 다양한 접근 방식을 비교하려면 다양한 하이퍼 매개 변수와 epochs를 시도해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GaXTsIgP-3"
      },
      "source": [
        "### 모델 내보내기\n",
        "\n",
        "위의 코드에서는 모델에 텍스트를 제공하기 전에 `tf.keras.layers.TextVectorization`을 데이터세트에 적용했습니다. 모델이 원시 문자열을 처리할 수 있도록 하려면(예: 배포를 단순화하기 위해) 모델 내부에 `TextVectorization` 레이어를 포함할 수 있습니다.\n",
        "\n",
        "이를 위해 방금 훈련한 가중치를 사용하여 새 모델을 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bRe3KX8gRCX"
      },
      "outputs": [],
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [binary_vectorize_layer, binary_model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(binary_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2eqTVBP4DUN"
      },
      "source": [
        "이제 여러분의 모델은 원시 문자열을 입력으로 사용하고 `Model.predict`를 사용하여 각 레이블의 점수를 예측할 수 있습니다. 다음과 같이 최대 점수를 가진 레이블을 찾는 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU53uRXz45iO"
      },
      "outputs": [],
      "source": [
        "def get_string_labels(predicted_scores_batch):\n",
        "  predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
        "  predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n",
        "  return predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqnWc7Nn5eou"
      },
      "source": [
        "### 새 데이터에 대한 추론 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOR2MupW1_zS"
      },
      "outputs": [],
      "source": [
        "inputs = [\n",
        "    \"how do I extract keys from a dict into a list?\",  # 'python'\n",
        "    \"debug public static void main(string[] args) {...}\",  # 'java'\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QDVfii_4slI"
      },
      "source": [
        "모델 내부에 텍스트 전처리 논리를 포함하면 배포를 단순화하고 [훈련/테스트 왜곡](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew) 가능성을 줄이는 프로덕션용 모델을 내보낼 수 있습니다.\n",
        "\n",
        "`tf.keras.layers.TextVectorization`를 적용할 위치를 선택할 때 염두에 두어야 할 성능 차이가 있습니다. 레이어를 모델 외부에서 사용하면 GPU에서 훈련할 때 비동기 CPU 처리 및 데이터 버퍼링을 수행할 수 있습니다. 따라서 GPU에서 모델을 훈련하는 경우 모델을 개발하는 동안 최상의 성능을 얻기 위해 이 옵션을 사용하고 배포 준비가 완료되면 모델 내부에 `TextVectorization` 레이어를 포함하도록 전환할 수 있습니다.\n",
        "\n",
        "모델 저장에 대해 자세히 알아보려면 [모델 저장과 복원](../keras/save_and_load.ipynb) 튜토리얼을 방문하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4cvuFzavTRy"
      },
      "source": [
        "## 예제 2: 일리아드(Iliad) 번역의 작성자 예측하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOlJ22508RIe"
      },
      "source": [
        "다음은 `tf.data.TextLineDataset`를 사용하여 텍스트 파일로부터 예제를 로드하고 [TensorFlow Text](https://www.tensorflow.org/text)를 사용하여 데이터를 전처리하는 예를 제공합니다. 호머의 일리아드 작품을 다르게 번역한 3개의 영어 번역문을 사용하게 되며, 한 줄의 텍스트가 제공되었을 때 번역가를 식별하는 모델을 훈련합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pCgKbOSk7kU"
      },
      "source": [
        "### 데이터세트 다운로드 및 탐색하기\n",
        "\n",
        "3가지 번역본은 다음과 같습니다.\n",
        "\n",
        "- [William Cowper](https://en.wikipedia.org/wiki/William_Cowper): [텍스트](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n",
        "- [Edward, Earl of Derby](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby): [텍스트](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n",
        "- [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29): [텍스트](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n",
        "\n",
        "이 튜토리얼에서 사용된 텍스트 파일은 문서 헤더와 바닥 글, 줄 번호 및 챕터 제목 등을 제거하는 일반적인 전처리 작업을 거쳤습니다.\n",
        "\n",
        "이 가볍게 손질한 파일을 로컬로 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YlKQthEYlFw"
      },
      "outputs": [],
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "  text_dir = utils.get_file(name, origin=DIRECTORY_URL + name)\n",
        "\n",
        "parent_dir = pathlib.Path(text_dir).parent\n",
        "list(parent_dir.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8PHK5J_cXE5"
      },
      "source": [
        "### 데이터세트 로드하기\n",
        "\n",
        "이전에는 `tf.keras.utils.text_dataset_from_directory`를 사용할 경우 파일의 모든 콘텐츠를 단일 예제로 취급했습니다. 여기에서는 텍스트 파일로부터 `tf.data.Dataset`를 생성하도록 설계된 `tf.data.TextLineDataset`을 사용합니다. 이때 각 예제는 원본 파일의 텍스트 줄입니다. `TextLineDataset`은 주로 줄 기반의 텍스트 데이터(예: 시 또는 오류 로그)에 유용합니다.\n",
        "\n",
        "이러한 파일을 반복하여 각 파일을 자체 데이터세트에 로드합니다. 각 예제는 개별적으로 레이블을 지정해야 하므로 `Dataset.map`을 사용하여 각 예제에 labeler 함수를 적용합니다. 이렇게 하면 데이터세트의 모든 예제를 반복하여 (`example, label`) 쌍을 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIIWIdPXgk7I"
      },
      "outputs": [],
      "source": [
        "def labeler(example, index):\n",
        "  return example, tf.cast(index, tf.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ajx7AmZnEg3"
      },
      "outputs": [],
      "source": [
        "labeled_data_sets = []\n",
        "\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  lines_dataset = tf.data.TextLineDataset(str(parent_dir/file_name))\n",
        "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
        "  labeled_data_sets.append(labeled_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPOsVK1e9NGM"
      },
      "source": [
        "다음으로 `Dataset.concatenate`를 사용하여 레이블이 지정된 데이터세트를 단일 데이터세트로 결합한 후 `Dataset.shuffle`을 사용하여 셔플합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jAeYkTIi9-2"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SIZE = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd544E-Sh63L"
      },
      "outputs": [],
      "source": [
        "all_labeled_data = labeled_data_sets[0]\n",
        "for labeled_dataset in labeled_data_sets[1:]:\n",
        "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
        "\n",
        "all_labeled_data = all_labeled_data.shuffle(\n",
        "    BUFFER_SIZE, reshuffle_each_iteration=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4JEHrJXeG5k"
      },
      "source": [
        "이전과 같이 몇 가지 예제를 출력합니다. 데이터세트가 아직 일괄 처리되지 않았으므로 `all_labeled_data`의 각 항목은 하나의 데이터 포인트에 해당합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gywKlN0xh6u5"
      },
      "outputs": [],
      "source": [
        "for text, label in all_labeled_data.take(10):\n",
        "  print(\"Sentence: \", text.numpy())\n",
        "  print(\"Label:\", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rrpU2_sfDh0"
      },
      "source": [
        "### 훈련을 위한 데이터세트 준비하기\n",
        "\n",
        "`tf.keras.layers.TextVectorization`을 사용하여 텍스트 데이터세트를 전처리하는 대신에 이제는 TensorFlow Text API를 사용하여 데이터를 표준화 및 토큰화하고 어휘를 빌드하고, `tf.lookup.StaticVocabularyTable`을 사용하여 토큰을 모델에 정수에 매핑한 후 모델에 공급합니다([TensorFlow 텍스트](https://www.tensorflow.org/text)에 대해 자세히 알아보기).\n",
        "\n",
        "다음과 같이 텍스트를 소문자로 변환하고 토큰화하는 함수를 정의합니다.\n",
        "\n",
        "- TensorFlow Text는 다양한 토크나이저를 제공합니다. 이 예제에서는 `text.UnicodeScriptTokenizer`를 사용하여 데이터세트를 토큰화합니다.\n",
        "- 여러분은 `Dataset.map`을 사용하여 데이터세트에 토큰화를 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4DpQW-Y12rm"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz8xEj0ugu51"
      },
      "outputs": [],
      "source": [
        "def tokenize(text, unused_label):\n",
        "  lower_case = tf_text.case_fold_utf8(text)\n",
        "  return tokenizer.tokenize(lower_case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzUrAzOq31QL"
      },
      "outputs": [],
      "source": [
        "tokenized_ds = all_labeled_data.map(tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx4Q2i8XLV7o"
      },
      "source": [
        "데이터세트를 반복하고 몇 가지 토큰화된 예제를 출력할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2mkWri7LiGq"
      },
      "outputs": [],
      "source": [
        "for text_batch in tokenized_ds.take(5):\n",
        "  print(\"Tokens: \", text_batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPd4PsskJ_Xt"
      },
      "source": [
        "다음으로 빈도별로 토큰을 정렬하고 상위 `VOCAB_SIZE` 토큰을 유지하여 어휘를 빌드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkHtbGnDh6mg"
      },
      "outputs": [],
      "source": [
        "tokenized_ds = configure_dataset(tokenized_ds)\n",
        "\n",
        "vocab_dict = collections.defaultdict(lambda: 0)\n",
        "for toks in tokenized_ds.as_numpy_iterator():\n",
        "  for tok in toks:\n",
        "    vocab_dict[tok] += 1\n",
        "\n",
        "vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "vocab = [token for token, count in vocab]\n",
        "vocab = vocab[:VOCAB_SIZE]\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size: \", vocab_size)\n",
        "print(\"First five vocab entries:\", vocab[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyKSsaNAKi17"
      },
      "source": [
        "토큰을 정수로 변환하려면 `vocab` 세트를 사용하여 `tf.lookup.StaticVocabularyTable`을 생성합니다. [`2`, `vocab_size + 2`] 범위의 정수에 토큰을 매핑합니다. `TextVectorization` 레이어와 마찬가지로 `0`은 패딩을 나타내기 위해 예약되어 있으며 `1`은 OOV(out-of-vocabulary) 토큰을 나타내기 위해 예약되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCBo2yFHD7y6"
      },
      "outputs": [],
      "source": [
        "keys = vocab\n",
        "values = range(2, len(vocab) + 2)  # Reserve `0` for padding, `1` for OOV tokens.\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(\n",
        "    keys, values, key_dtype=tf.string, value_dtype=tf.int64)\n",
        "\n",
        "num_oov_buckets = 1\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5F-EiBpOADE"
      },
      "source": [
        "마지막으로 토크나이저 및 조회 테이블을 사용하여 데이터세트를 표준화, 토큰화 및 벡터화하는 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcIQ7LOTh6eT"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, label):\n",
        "  standardized = tf_text.case_fold_utf8(text)\n",
        "  tokenized = tokenizer.tokenize(standardized)\n",
        "  vectorized = vocab_table.lookup(tokenized)\n",
        "  return vectorized, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6S5Qyabi-vo"
      },
      "source": [
        "결과를 출력하기 위해 단일 예제에서 다음을 시도할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgxPZaxUuTbk"
      },
      "outputs": [],
      "source": [
        "example_text, example_label = next(iter(all_labeled_data))\n",
        "print(\"Sentence: \", example_text.numpy())\n",
        "vectorized_text, example_label = preprocess_text(example_text, example_label)\n",
        "print(\"Vectorized sentence: \", vectorized_text.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9qHM0v8k_Mg"
      },
      "source": [
        "이제 `Dataset.map`을 사용하여 데이터세트에서 전처리 함수를 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmQVsAgJ-RM0"
      },
      "outputs": [],
      "source": [
        "all_encoded_data = all_labeled_data.map(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZToSXSm0qr"
      },
      "source": [
        "### 데이터세트를 훈련 및 검증 세트로 분할하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itxIJwkrUXgv"
      },
      "source": [
        "Keras `TextVectorization` 레이어는 벡터화된 데이터도 일괄 처리하고 패딩합니다. 배치 내부의 예제는 크기와 모양이 같아야 하기 때문에 패딩이 필요하지만 이러한 데이터세트의 예제는 모두 같은 크기가 아니며 각 텍스트 줄의 단어 수도 다릅니다.\n",
        "\n",
        "`tf.data.Dataset`은 데이터세트 분할 및 패딩 일괄 처리를 지원합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-rmbijQh6bf"
      },
      "outputs": [],
      "source": [
        "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
        "validation_data = all_encoded_data.take(VALIDATION_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTP0IwHBCn0Q"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "validation_data = validation_data.padded_batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-wmFq8uW1zS"
      },
      "source": [
        "이제 `validation_data` 및 `train_data`는 (`example, label`) 쌍의 모음이 아니라 배치의 모음입니다. 각 배치는 배열로 표시되는 한 쌍의 (*많은 예제*, *많은 레이블*)입니다.\n",
        "\n",
        "이는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMslWfuwoqpB"
      },
      "outputs": [],
      "source": [
        "sample_text, sample_labels = next(iter(validation_data))\n",
        "print(\"Text batch shape: \", sample_text.shape)\n",
        "print(\"Label batch shape: \", sample_labels.shape)\n",
        "print(\"First text example: \", sample_text[0])\n",
        "print(\"First label example: \", sample_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI4I6_Sa0vWu"
      },
      "source": [
        "패딩에는 `0`을 사용하고 OOV(out-of-vocabulary) 토큰에는 `1`을 사용하였기에 어휘 크기가 2배 증가했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u21LlkO8QGRX"
      },
      "outputs": [],
      "source": [
        "vocab_size += 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h44Ox11OYLP-"
      },
      "source": [
        "이전과 같은 더 나은 성능을 위한 데이터세트를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpT0b_7mYRXV"
      },
      "outputs": [],
      "source": [
        "train_data = configure_dataset(train_data)\n",
        "validation_data = configure_dataset(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8SUhGFNsmRi"
      },
      "source": [
        "### 모델 훈련하기\n",
        "\n",
        "이전과 같이 이 데이터세트에서 모델을 훈련할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJgI1pow2YR9"
      },
      "outputs": [],
      "source": [
        "model = create_model(vocab_size=vocab_size, num_labels=3)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data, validation_data=validation_data, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTPCYf_Jh6TH"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(validation_data)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_knIsO-r4pHb"
      },
      "source": [
        "### 모델 내보내기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEuMLJA_Xiwo"
      },
      "source": [
        "원시 문자열을 입력으로 사용할 수 있는 모델을 만들기 위해 사용자 정의 전처리 함수와 동일한 단계를 수행하는 Keras `TextVectorization` 레이어를 생성하게 됩니다. 이미 어휘를 훈련했으므로 `TextVectorization.adapt` 대신 `TextVectorization.set_vocabulary`를 사용하여 새 어휘를 훈련할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ODkRXbk6aHb"
      },
      "outputs": [],
      "source": [
        "preprocess_layer = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    standardize=tf_text.case_fold_utf8,\n",
        "    split=tokenizer.tokenize,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "preprocess_layer.set_vocabulary(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-Cvd27y4qwt"
      },
      "outputs": [],
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [preprocess_layer, model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyg0B4zsc-UD"
      },
      "outputs": [],
      "source": [
        "# Create a test dataset of raw strings.\n",
        "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
        "test_ds = configure_dataset(test_ds)\n",
        "\n",
        "loss, accuracy = export_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Mm0Y9QYQwE"
      },
      "source": [
        "인코딩된 검증 세트의 모델과 원시 검증 세트에 대해 내보내기를 수행한 모델의 손실 및 정확성은 예상대로 동일합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stk2BP8GE-qo"
      },
      "source": [
        "### 새 데이터에 대한 추론 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w1fQGJPD2Yh"
      },
      "outputs": [],
      "source": [
        "inputs = [\n",
        "    \"Join'd to th' Ionians with their flowing robes,\",  # Label: 1\n",
        "    \"the allies, and his armour flashed about him so that he seemed to all\",  # Label: 2\n",
        "    \"And with loud clangor of his arms he fell.\",  # Label: 0\n",
        "]\n",
        "\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = tf.argmax(predicted_scores, axis=1)\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eA8TVdnA-3L"
      },
      "source": [
        "## TensorFlow 데이터세트(TFDS)를 사용하여 더 많은 데이터세트 다운로드하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QFSxfZ3Vqsn"
      },
      "source": [
        "[TensorFlow 데이터세트](https://www.tensorflow.org/datasets/catalog/overview)로부터 더 많은 데이터세트를 다운로드할 수 있습니다.\n",
        "\n",
        "이 예제에서는 [IMDB 대형 영화 리뷰 데이터세트](https://www.tensorflow.org/datasets/catalog/imdb_reviews)를 사용하여 감정 분류용 모델을 훈련합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzC65LOaVw0B"
      },
      "outputs": [],
      "source": [
        "# Training set.\n",
        "train_ds = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='train[:80%]',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKGkgPBkFh0k"
      },
      "outputs": [],
      "source": [
        "# Validation set.\n",
        "val_ds = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='train[80%:]',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQjf3YZAb5Ne"
      },
      "source": [
        "몇 가지 예제를 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq1w8MnfWt2C"
      },
      "outputs": [],
      "source": [
        "for review_batch, label_batch in val_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(\"Review: \", review_batch[i].numpy())\n",
        "    print(\"Label: \", label_batch[i].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-lVaukyb75k"
      },
      "source": [
        "이제 이전과 같이 데이터를 전처리하고 모델을 훈련할 수 있습니다.\n",
        "\n",
        "참고: 이진 분류 문제이므로 여러분의 모델에 `tf.keras.losses.SparseCategoricalCrossentropy` 대신 `tf.keras.losses.BinaryCrossentropy`를 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciz2CxAsZw3Z"
      },
      "source": [
        "### 훈련을 위한 데이터세트 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzT_t9ihZLH4"
      },
      "outputs": [],
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
        "train_text = train_ds.map(lambda text, labels: text)\n",
        "vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz-Xrd_ZZ4tB"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycn0Itd6g5aF"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(vectorize_text)\n",
        "val_ds = val_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc11jQTlZ5lj"
      },
      "outputs": [],
      "source": [
        "# Configure datasets for performance as before.\n",
        "train_ds = configure_dataset(train_ds)\n",
        "val_ds = configure_dataset(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQzoYkaGZ82Z"
      },
      "source": [
        "### 모델 생성, 구성 및 훈련하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9IOTLkyZ-a7"
      },
      "outputs": [],
      "source": [
        "model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=1)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLnDs5dhaBAk"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq59QpNzaDMa"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_ds, validation_data=val_ds, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCMWCEtyaEbR"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGtqLXVnaaFy"
      },
      "source": [
        "### 모델 내보내기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE9WZARZaZr1"
      },
      "outputs": [],
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [vectorize_layer, model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhF8tDH-afoC"
      },
      "outputs": [],
      "source": [
        "# 0 --> negative review\n",
        "# 1 --> positive review\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "]\n",
        "\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = [int(round(x[0])) for x in predicted_scores]\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1KSXDFPWiPN"
      },
      "source": [
        "## 결론\n",
        "\n",
        "이 튜토리얼에서는 텍스트를 로드하고 전처리하는 여러 방법을 보여 드렸습니다. 다음 단계로 다음과 같은 추가 텍스트 전처리 [TensorFlow 텍스트](https://www.tensorflow.org/text) 튜토리얼을 탐색할 수 있습니다.\n",
        "\n",
        "- [TF 텍스트를 사용한 BERT 전처리](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)\n",
        "- [TF 텍스트로 토큰화](https://www.tensorflow.org/text/guide/tokenizers)\n",
        "- [하위 단어 토크나이저](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
        "\n",
        "[TensorFlow 데이터세트](https://www.tensorflow.org/datasets/catalog/overview)에서 새 데이터세트를 찾을 수도 있습니다. 그리고 `tf.data`에 대해 자세히 알아보려면 [입력 파이프라인 빌드](../../guide/data.ipynb) 가이드를 확인하세요."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
