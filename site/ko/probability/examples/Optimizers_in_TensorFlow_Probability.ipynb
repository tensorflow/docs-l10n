{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EheA5_j_cEwc"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Probability Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YCriMWd-pRTP"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvRwFTkqcp1e"
      },
      "source": [
        "# TensorFlow Probability에서 옵티마이저\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/probability/examples/Optimizers_in_TensorFlow_Probability\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> TensorFlow.org에서 보기</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/probability/examples/Optimizers_in_TensorFlow_Probability.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/probability/examples/Optimizers_in_TensorFlow_Probability.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/probability/examples/Optimizers_in_TensorFlow_Probability.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">notebook다운로드</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiBI9YkYoVBO"
      },
      "source": [
        "## 개요\n",
        "\n",
        "이 colab에서는 TensorFlow Probability에 구현된 다양한 옵티마이저의 사용 방법을 보여줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWQZUqnMf-3A"
      },
      "source": [
        "## 종속성 및 전제 조건"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nA2FSdTgcEM"
      },
      "outputs": [],
      "source": [
        "#@title Import { display-mode: \"form\" }\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import contextlib\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from six.moves import urllib\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ec_JW02g-Q"
      },
      "source": [
        "## BFGS 및 L-BFGS 옵티마이저\n",
        "\n",
        "Quasi Newton 방법은 널리 쓰이는 1차 최적화 알고리즘 클래스입니다. 이 방법은 정확한 헤세 행렬에 양의 정부호 근사치를 사용하여 검색 방향을 탐색합니다.\n",
        "\n",
        "Broyden-Fletcher-Goldfarb-Shanno 알고리즘([BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm))은 일반적인 이 아이디어를 특정하게 구현한 것입니다. 이는 기울기가 모든 곳에서 연속적인 중간 규모 문제에 적용 가능하므로 이러한 경우에 선택되는 방법입니다(예: $L_2$ 패널티가 있는 선형 회귀).\n",
        "\n",
        "[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)는 합리적인 비용으로 헤세 행렬을 계산할 수 없거나 희소하지 않은 더 큰 문제를 해결하는 데 유용한 BFGS의 제한된 메모리 버전입니다. 헤세 행렬의 완전 조밀한 $n \\times n$ 근사값을 저장하는 대신 이러한 근사값을 암시적으로 나타내는, 길이가 $n$인 몇 개의 벡터만을 저장합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Tm6BS93hQ9Ym"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions\n",
        "\n",
        "CACHE_DIR = os.path.join(os.sep, 'tmp', 'datasets')\n",
        "\n",
        "\n",
        "def make_val_and_grad_fn(value_fn):\n",
        "  @functools.wraps(value_fn)\n",
        "  def val_and_grad(x):\n",
        "    return tfp.math.value_and_gradient(value_fn, x)\n",
        "  return val_and_grad\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def timed_execution():\n",
        "  t0 = time.time()\n",
        "  yield\n",
        "  dt = time.time() - t0\n",
        "  print('Evaluation took: %f seconds' % dt)\n",
        "\n",
        "\n",
        "def np_value(tensor):\n",
        "  \"\"\"Get numpy value out of possibly nested tuple of tensors.\"\"\"\n",
        "  if isinstance(tensor, tuple):\n",
        "    return type(tensor)(*(np_value(t) for t in tensor))\n",
        "  else:\n",
        "    return tensor.numpy()\n",
        "\n",
        "def run(optimizer):\n",
        "  \"\"\"Run an optimizer and measure it's evaluation time.\"\"\"\n",
        "  optimizer()  # Warmup.\n",
        "  with timed_execution():\n",
        "    result = optimizer()\n",
        "  return np_value(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7iEPlp7FkN"
      },
      "source": [
        "### 단순 이차 함수에 대한 L-BFGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJdXP5E828aP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.014586 seconds\n",
            "L-BFGS Results\n",
            "Converged: True\n",
            "Location of the minimum: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Number of iterations: 10\n"
          ]
        }
      ],
      "source": [
        "# Fix numpy seed for reproducibility\n",
        "np.random.seed(12345)\n",
        "\n",
        "# The objective must be supplied as a function that takes a single\n",
        "# (Tensor) argument and returns a tuple. The first component of the\n",
        "# tuple is the value of the objective at the supplied point and the\n",
        "# second value is the gradient at the supplied point. The value must\n",
        "# be a scalar and the gradient must have the same shape as the\n",
        "# supplied argument.\n",
        "\n",
        "# The `make_val_and_grad_fn` decorator helps transforming a function\n",
        "# returning the objective value into one that returns both the gradient\n",
        "# and the value. It also works for both eager and graph mode.\n",
        "\n",
        "dim = 10\n",
        "minimum = np.ones([dim])\n",
        "scales = np.exp(np.random.randn(dim))\n",
        "\n",
        "@make_val_and_grad_fn\n",
        "def quadratic(x):\n",
        "  return tf.reduce_sum(scales * (x - minimum) ** 2, axis=-1)\n",
        "\n",
        "# The minimization routine also requires you to supply an initial\n",
        "# starting point for the search. For this example we choose a random\n",
        "# starting point.\n",
        "start = np.random.randn(dim)\n",
        "\n",
        "# Finally an optional argument called tolerance let's you choose the\n",
        "# stopping point of the search. The tolerance specifies the maximum\n",
        "# (supremum) norm of the gradient vector at which the algorithm terminates.\n",
        "# If you don't have a specific need for higher or lower accuracy, leaving\n",
        "# this parameter unspecified (and hence using the default value of 1e-8)\n",
        "# should be good enough.\n",
        "tolerance = 1e-10\n",
        "\n",
        "@tf.function\n",
        "def quadratic_with_lbfgs():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "    quadratic,\n",
        "    initial_position=tf.constant(start),\n",
        "    tolerance=tolerance)\n",
        "\n",
        "results = run(quadratic_with_lbfgs)\n",
        "\n",
        "# The optimization results contain multiple pieces of information. The most\n",
        "# important fields are: 'converged' and 'position'.\n",
        "# Converged is a boolean scalar tensor. As the name implies, it indicates\n",
        "# whether the norm of the gradient at the final point was within tolerance.\n",
        "# Position is the location of the minimum found. It is important to check\n",
        "# that converged is True before using the value of the position.\n",
        "\n",
        "print('L-BFGS Results')\n",
        "print('Converged:', results.converged)\n",
        "print('Location of the minimum:', results.position)\n",
        "print('Number of iterations:', results.num_iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dnp7Nm161KY"
      },
      "source": [
        "### BFGS와 동일한 문제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1k6n3_n4W2K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.010468 seconds\n",
            "BFGS Results\n",
            "Converged: True\n",
            "Location of the minimum: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Number of iterations: 10\n"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def quadratic_with_bfgs():\n",
        "  return tfp.optimizer.bfgs_minimize(\n",
        "    quadratic,\n",
        "    initial_position=tf.constant(start),\n",
        "    tolerance=tolerance)\n",
        "\n",
        "results = run(quadratic_with_bfgs)\n",
        "\n",
        "print('BFGS Results')\n",
        "print('Converged:', results.converged)\n",
        "print('Location of the minimum:', results.position)\n",
        "print('Number of iterations:', results.num_iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT1GJU3s1LAW"
      },
      "source": [
        "## L1 패널티가 있는 선형 회귀: 전립선암 데이터\n",
        "\n",
        "책의 사례: Trevor Hastie, Robert Tibshirani, Jerome Friedman의 *통계 학습, 데이터 마이닝, 추론 및 예측의 요소들*\n",
        "\n",
        "이는 L1 패널티가 있는 최적화 문제임에 유의하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RLvid3kqi4L"
      },
      "source": [
        "### 데이터세트 획득"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlMkFHomqyxu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data to /tmp/datasets/prostate.data.\n"
          ]
        }
      ],
      "source": [
        "def cache_or_download_file(cache_dir, url_base, filename):\n",
        "  \"\"\"Read a cached file or download it.\"\"\"\n",
        "  filepath = os.path.join(cache_dir, filename)\n",
        "  if tf.io.gfile.exists(filepath):\n",
        "    return filepath\n",
        "  if not tf.io.gfile.exists(cache_dir):\n",
        "    tf.io.gfile.makedirs(cache_dir)\n",
        "  url = url_base + filename\n",
        "  print(\"Downloading {url} to {filepath}.\".format(url=url, filepath=filepath))\n",
        "  urllib.request.urlretrieve(url, filepath)\n",
        "  return filepath\n",
        "\n",
        "def get_prostate_dataset(cache_dir=CACHE_DIR):\n",
        "  \"\"\"Download the prostate dataset and read as Pandas dataframe.\"\"\"\n",
        "  url_base = 'http://web.stanford.edu/~hastie/ElemStatLearn/datasets/'\n",
        "  return pd.read_csv(\n",
        "      cache_or_download_file(cache_dir, url_base, 'prostate.data'),\n",
        "      delim_whitespace=True, index_col=0)\n",
        "\n",
        "prostate_df = get_prostate_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY4JVbrZqpZ-"
      },
      "source": [
        "### 문제 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7d6oBnYFZwh"
      },
      "outputs": [],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "feature_names = ['lcavol', 'lweight',\t'age',\t'lbph',\t'svi', 'lcp',\t\n",
        "                 'gleason',\t'pgg45']\n",
        "\n",
        "# Normalize features\n",
        "scalar = preprocessing.StandardScaler()\n",
        "prostate_df[feature_names] = pd.DataFrame(\n",
        "    scalar.fit_transform(\n",
        "        prostate_df[feature_names].astype('float64')))\n",
        "\n",
        "# select training set\n",
        "prostate_df_train = prostate_df[prostate_df.train == 'T'] \n",
        "\n",
        "# Select features and labels \n",
        "features = prostate_df_train[feature_names]\n",
        "labels =  prostate_df_train[['lpsa']]\n",
        "\n",
        "# Create tensors\n",
        "feat = tf.constant(features.values, dtype=tf.float64)\n",
        "lab = tf.constant(labels.values, dtype=tf.float64)\n",
        "\n",
        "dtype = feat.dtype\n",
        "\n",
        "regularization = 0 # regularization parameter\n",
        "dim = 8 # number of features\n",
        "\n",
        "# We pick a random starting point for the search\n",
        "start = np.random.randn(dim + 1)\n",
        "\n",
        "def regression_loss(params):\n",
        "  \"\"\"Compute loss for linear regression model with L1 penalty\n",
        "\n",
        "  Args:\n",
        "    params: A real tensor of shape [dim + 1]. The zeroth component\n",
        "      is the intercept term and the rest of the components are the\n",
        "      beta coefficients.\n",
        "\n",
        "  Returns:\n",
        "    The mean square error loss including L1 penalty.\n",
        "  \"\"\"\n",
        "  params = tf.squeeze(params)\n",
        "  intercept, beta  = params[0], params[1:]\n",
        "  pred = tf.matmul(feat, tf.expand_dims(beta, axis=-1)) + intercept\n",
        "  mse_loss = tf.reduce_sum(\n",
        "      tf.cast(\n",
        "        tf.losses.mean_squared_error(y_true=lab, y_pred=pred), tf.float64))\n",
        "  l1_penalty = regularization * tf.reduce_sum(tf.abs(beta))\n",
        "  total_loss = mse_loss + l1_penalty\n",
        "  return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyRl2uksnAY0"
      },
      "source": [
        "### L-BFGS로 풀기\n",
        "\n",
        "L-BFGS를 사용하여 피팅합니다. L1 패널티가 미분 불연속을 도입하더라도, 실제로 L-BFGS는 여전히 잘 작동합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXkJbrYVqNSW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.017987 seconds\n",
            "L-BFGS Results\n",
            "Converged: True\n",
            "Intercept: Fitted (2.3879985744556484)\n",
            "Beta:      Fitted [ 0.68626215  0.28193532 -0.17030254  0.10799274  0.33634988 -0.24888523\n",
            "  0.11992237  0.08689026]\n"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def l1_regression_with_lbfgs():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "    make_val_and_grad_fn(regression_loss),\n",
        "    initial_position=tf.constant(start),\n",
        "    tolerance=1e-8)\n",
        "\n",
        "results = run(l1_regression_with_lbfgs)\n",
        "minimum = results.position\n",
        "fitted_intercept = minimum[0]\n",
        "fitted_beta = minimum[1:]\n",
        "\n",
        "print('L-BFGS Results')\n",
        "print('Converged:', results.converged)\n",
        "print('Intercept: Fitted ({})'.format(fitted_intercept))\n",
        "print('Beta:      Fitted {}'.format(fitted_beta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cieJV7D7gIpU"
      },
      "source": [
        "### Nelder Mead로 해결\n",
        "\n",
        "[Nelder Mead 방법](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)은 가장 널리 사용되는 미분없는 최소화 방법 중 하나입니다. 이 옵티마이저는 기울기 정보를 사용하지 않으며 대상 함수의 미분 가능성을 가정하지 않습니다. 따라서 L1 패널티가 있는 최적화 문제와 같이 비평활 목적 함수에 적합합니다.\n",
        "\n",
        "$n$ 차원 최적화 문제의 경우 non-degenerate 심플렉스에 걸쳐 $n+1$ 후보 솔루션 세트를 유지합니다. 각 정점의 함수 값을 사용하여 일련의 움직임(반사, 확장, 수축 및 축소)에 따라 심플렉스를 연속적으로 수정합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8Dg-siSdrsV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.325643 seconds\n",
            "Nelder Mead Results\n",
            "Converged: True\n",
            "Intercept: Fitted (2.387998456121595)\n",
            "Beta:      Fitted [ 0.68626266  0.28193456 -0.17030291  0.10799375  0.33635132 -0.24888703\n",
            "  0.11992244  0.08689023]\n"
          ]
        }
      ],
      "source": [
        "# Nelder mead expects an initial_vertex of shape [n + 1, 1].\n",
        "initial_vertex = tf.expand_dims(tf.constant(start, dtype=dtype), axis=-1)\n",
        "\n",
        "@tf.function\n",
        "def l1_regression_with_nelder_mead():\n",
        "  return tfp.optimizer.nelder_mead_minimize(\n",
        "      regression_loss,\n",
        "      initial_vertex=initial_vertex,\n",
        "      func_tolerance=1e-10,\n",
        "      position_tolerance=1e-10)\n",
        "\n",
        "results = run(l1_regression_with_nelder_mead)\n",
        "minimum = results.position.reshape([-1])\n",
        "fitted_intercept = minimum[0]\n",
        "fitted_beta = minimum[1:]\n",
        "\n",
        "print('Nelder Mead Results')\n",
        "print('Converged:', results.converged)\n",
        "print('Intercept: Fitted ({})'.format(fitted_intercept))\n",
        "print('Beta:      Fitted {}'.format(fitted_beta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntLeQCtFZizJ"
      },
      "source": [
        "## L2 패널티를 사용한 로지스틱 회귀\n",
        "\n",
        "본 사례에서는 분류를 위한 합성 데이터 세트를 만들고 L-BFGS 옵티마이저를 사용하여 매개변수에 맞춥니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_uCJjyDZiVM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.056751 seconds\n",
            "Converged: True\n",
            "Intercept: Fitted (1.4111415084244365), Actual (1.3934058329729904)\n",
            "Beta:\n",
            "\tFitted [-0.18016612  0.53121578 -0.56420632 -0.5336374   2.00499675],\n",
            "\tActual [-0.20470766  0.47894334 -0.51943872 -0.5557303   1.96578057]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "dim = 5  # The number of features\n",
        "n_obs = 10000  # The number of observations\n",
        "\n",
        "betas = np.random.randn(dim)  # The true beta\n",
        "intercept = np.random.randn()  # The true intercept\n",
        "\n",
        "features = np.random.randn(n_obs, dim)  # The feature matrix\n",
        "probs = sp.special.expit(\n",
        "    np.matmul(features, np.expand_dims(betas, -1)) + intercept)\n",
        "\n",
        "labels = sp.stats.bernoulli.rvs(probs)  # The true labels\n",
        "\n",
        "regularization = 0.8\n",
        "feat = tf.constant(features)\n",
        "lab = tf.constant(labels, dtype=feat.dtype)\n",
        "\n",
        "@make_val_and_grad_fn\n",
        "def negative_log_likelihood(params):\n",
        "  \"\"\"Negative log likelihood for logistic model with L2 penalty\n",
        "\n",
        "  Args:\n",
        "    params: A real tensor of shape [dim + 1]. The zeroth component\n",
        "      is the intercept term and the rest of the components are the\n",
        "      beta coefficients.\n",
        "\n",
        "  Returns:\n",
        "    The negative log likelihood plus the penalty term. \n",
        "  \"\"\"\n",
        "  intercept, beta  = params[0], params[1:]\n",
        "  logit = tf.matmul(feat, tf.expand_dims(beta, -1)) + intercept\n",
        "  log_likelihood = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "      labels=lab, logits=logit))\n",
        "  l2_penalty = regularization * tf.reduce_sum(beta ** 2)\n",
        "  total_loss = log_likelihood + l2_penalty\n",
        "  return total_loss\n",
        "\n",
        "start = np.random.randn(dim + 1)\n",
        "\n",
        "@tf.function\n",
        "def l2_regression_with_lbfgs():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "      negative_log_likelihood,\n",
        "      initial_position=tf.constant(start),\n",
        "      tolerance=1e-8)\n",
        "\n",
        "results = run(l2_regression_with_lbfgs)\n",
        "minimum = results.position\n",
        "fitted_intercept = minimum[0]\n",
        "fitted_beta = minimum[1:]\n",
        "\n",
        "print('Converged:', results.converged)\n",
        "print('Intercept: Fitted ({}), Actual ({})'.format(fitted_intercept, intercept))\n",
        "print('Beta:\\n\\tFitted {},\\n\\tActual {}'.format(fitted_beta, betas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVbdnfYu8cYh"
      },
      "source": [
        "## 배치 처리 지원\n",
        "\n",
        "예를 들어 다양한 시작 지점으로부터의 단일 함수의 최적화, 또는 단일 지점으로부터의 여러 매개변수 함수의 최적화를 위해 BFGS와 L-BFGS는 모두 배치 연산을 지원합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVAO1lit8zzK"
      },
      "source": [
        "### 단일 함수, 복수 시작 지점\n",
        "\n",
        "Himmelblau 함수는 최적화 테스트 케이스입니다. 함수는 다음과 같이 주어집니다.\n",
        "\n",
        "$$f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$\n",
        "\n",
        "함수는 다음 위치에서 4개의 minima를 가집니다.\n",
        "\n",
        "- (3, 2),\n",
        "- (-2.805118, 3.131312),\n",
        "- (-3.779310, -3.283186),\n",
        "- (3.584428, -1.848126).\n",
        "\n",
        "이러한 모든 minima는 적절한 시작 지점으로부터 도달할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RhP1VON7tO_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 0.019095 seconds\n",
            "Converged: [ True  True  True  True]\n",
            "Minima: [[ 3.          2.        ]\n",
            " [-2.80511809  3.13131252]\n",
            " [-3.77931025 -3.28318599]\n",
            " [ 3.58442834 -1.84812653]]\n"
          ]
        }
      ],
      "source": [
        "# The function to minimize must take as input a tensor of shape [..., n]. In\n",
        "# this n=2 is the size of the domain of the input and [...] are batching\n",
        "# dimensions. The return value must be of shape [...], i.e. a batch of scalars\n",
        "# with the objective value of the function evaluated at each input point.\n",
        "\n",
        "@make_val_and_grad_fn\n",
        "def himmelblau(coord):\n",
        "  x, y = coord[..., 0], coord[..., 1]\n",
        "  return (x * x + y - 11) ** 2 + (x + y * y - 7) ** 2\n",
        "\n",
        "starts = tf.constant([[1, 1],\n",
        "                      [-2, 2],\n",
        "                      [-1, -1],\n",
        "                      [1, -2]], dtype='float64')\n",
        "\n",
        "# The stopping_condition allows to further specify when should the search stop.\n",
        "# The default, tfp.optimizer.converged_all, will proceed until all points have\n",
        "# either converged or failed. There is also a tfp.optimizer.converged_any to\n",
        "# stop as soon as the first point converges, or all have failed.\n",
        "\n",
        "@tf.function\n",
        "def batch_multiple_starts():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "      himmelblau, initial_position=starts,\n",
        "      stopping_condition=tfp.optimizer.converged_all,\n",
        "      tolerance=1e-8)\n",
        "\n",
        "results = run(batch_multiple_starts)\n",
        "print('Converged:', results.converged)\n",
        "print('Minima:', results.position)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W73lxbpD_UEs"
      },
      "source": [
        "### 다중 함수\n",
        "\n",
        "데모를 위해 본 예제에서는 많은 수의 무작위 생성된 고차원 2차 보울을 동시에 최적화합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy9sPmO1_3w5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation took: 1.994132 seconds\n",
            "All converged: True\n",
            "Largest error: 4.4131473142527966e-08\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "dim = 100\n",
        "batches = 500\n",
        "minimum = np.random.randn(batches, dim)\n",
        "scales = np.exp(np.random.randn(batches, dim))\n",
        "\n",
        "@make_val_and_grad_fn\n",
        "def quadratic(x):\n",
        "  return tf.reduce_sum(input_tensor=scales * (x - minimum)**2, axis=-1)\n",
        "\n",
        "# Make all starting points (1, 1, ..., 1). Note not all starting points need\n",
        "# to be the same.\n",
        "start = tf.ones((batches, dim), dtype='float64')\n",
        "\n",
        "@tf.function\n",
        "def batch_multiple_functions():\n",
        "  return tfp.optimizer.lbfgs_minimize(\n",
        "      quadratic, initial_position=start,\n",
        "      stopping_condition=tfp.optimizer.converged_all,\n",
        "      max_iterations=100,\n",
        "      tolerance=1e-8)\n",
        "\n",
        "results = run(batch_multiple_functions)\n",
        "print('All converged:', np.all(results.converged))\n",
        "print('Largest error:', np.max(results.position - minimum))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Optimizers_in_TensorFlow_Probability.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
