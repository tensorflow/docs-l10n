{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2J3nB-ZrRv1"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Probability Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9qDhTJmprPnm"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfPtIQ3DdZ8r"
      },
      "source": [
        "# 일반화된 선형 모델\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/probability/examples/Generalized_Linear_Models\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/probability/tensorflow_probability/examples/jupyter_notebooks/Generalized_Linear_Models.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOfH1_F9YsOG"
      },
      "source": [
        "이 노트북에서는 작업 예제를 통해 일반화된 선형 모델을 소개합니다. TensorFlow Probability에서 GLM을 효율적으로 피팅하기 위한 두 가지 알고리즘을 사용하여 이 예제를 두 가지 다른 방법으로 해결합니다. 즉, 밀집 데이터에 대한 Fisher 스코어링과 희소 데이터에 대한 좌표별 근위 경사 하강법입니다. 피팅된 계수를 실제 계수와 비교하고, 좌표별 근위 경사 하강법의 경우 R의 유사한 `glmnet` 알고리즘 출력과 비교합니다. 마지막으로, GLM의 몇 가지 주요 속성에 대한 추가적인 수학 정보와 파생 내용을 제공합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsfQ6vLb5I0"
      },
      "source": [
        "# 배경 설명"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdMX-QKagFnY"
      },
      "source": [
        "일반화된 선형 모델(GLM)은 변환(연결 함수)으로 래핑되고 지수족의 응답 분포를 가지고 있는 선형 모델($\\eta = x^\\top \\beta$)입니다. 연결 함수와 응답 분포의 선택은 매우 유연하여 GLM에 뛰어난 표현력을 제공합니다. 모든 정의 및 결과의 순차적인 표시부터 명확한 표기법으로 GLM을 구성하는 내용까지, 전체 세부 사항은 아래의 \"GLM 사실의 파생\"에서 확인할 수 있습니다. 다음을 요약합니다.\n",
        "\n",
        "GLM에서 반응 변수 $Y$에 대한 예측 분포는 관측된 예측 변수 $x$의 벡터와 연관됩니다. 분포의 형식은 다음과 같습니다.\n",
        "\n",
        "$$ \\begin{align*} p(y , |, x) &amp;= m(y, \\phi) \\exp\\left(\\frac{\\theta, T(y) - A(\\theta)}{\\phi}\\right) \\ \\theta &amp;:= h(\\eta) \\ \\eta &amp;:= x^\\top \\beta \\end{align*} $$\n",
        "\n",
        "여기서 $\\beta$는 매개변수(\"가중치\")이고 $\\phi$는 분산(\"분산\")을 나타내는 하이퍼 매개변수이며 $m$, $h$, $T$, $A$는 사용자 지정된 모델군에 의해 그 특성이 부여됩니다.\n",
        "\n",
        "$Y$의 평균은 **선형 응답** $\\eta$와 (역) 연결 함수의 구성에 의해 $x$에 따라 달라집니다. 예:\n",
        "\n",
        "$$ \\mu := g^{-1}(\\eta) $$\n",
        "\n",
        "여기서 $g$는 소위 **연결 함수**입니다. TFP에서 연결 함수와 모델군의 선택은 `tfp.glm.ExponentialFamily` 하위 클래스에 의해 공동으로 지정됩니다. 예를 들면 다음과 같습니다.\n",
        "\n",
        "- `tfp.glm.Normal`, 일명 \"선형 회귀\"\n",
        "- `tfp.glm.Bernoulli`, 일명 \"로지스틱 회귀\"\n",
        "- `tfp.glm.Poisson`, 일명 \"푸아송 회귀\"\n",
        "- `tfp.glm.BernoulliNormalCDF`, 일명 \"프로빗 회귀\".\n",
        "\n",
        "`tfp.Distribution`이 이미 일급 객체이기 때문에 TFP는 연결 함수보다 `Y`에 대한 분포에 따라 모델군의 이름을 지정하는 것을 선호합니다. `tfp.glm.ExponentialFamily` 서브 클래스 이름에 두 번째 단어가 포함된 경우, 이는 [비표준 연결 함수](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function)를 나타냅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oGScpRnqH_b"
      },
      "source": [
        "GLM에는 최대 가능도 estimator를 효율적으로 구현할 수 있게 해주는 몇 가지 놀라운 속성이 있습니다. 이러한 속성 중 가장 중요한 것은 로그-가능도 $\\ell$의 기울기 및 Fisher 정보 행렬에 대한 간단한 공식입니다. 이는 동일한 예측 변수 아래 응답의 재표본 추출에서 음의 로그-가능도에 대한 Hessian 기대값입니다. 즉, 다음과 같습니다.\n",
        "\n",
        "$$ \\begin{align*} \\nabla_\\beta, \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{y}) &amp;= \\mathbf{x}^\\top ,\\text{diag}\\left(\\frac{ {\\textbf{Mean}_T}'(\\mathbf{x} \\beta) }{ {\\textbf{Var}*T}(\\mathbf{x} \\beta) }\\right) \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}*T}(\\mathbf{x} \\beta)\\right) \\ \\mathbb{E}*{Y_i \\sim \\text{GLM} | x_i} \\left[ \\nabla*\\beta^2, \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{Y}) \\right] &amp;= -\\mathbf{x}^\\top ,\\text{diag}\\left( \\frac{ \\phi, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)^2 }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta) }\\right), \\mathbf{x} \\end{align*} $$\n",
        "\n",
        "여기서 $\\mathbf{x}$는 $i$th 행이 $i$th 데이터 샘플에 대한 예측 벡터인 행렬이고 $\\mathbf{y}$는 $i$th 좌표가 $i$th 데이터 샘플에 대해 관찰된 응답인 벡터입니다. 여기서(대략적으로 말하면), ${\\text{Mean}_T}(\\eta) := \\mathbb{E}[T(Y),|,\\eta]$ 및 ${\\text{Var}_T}(\\ eta) := \\text{Var}[T(Y),|,\\eta]$, 및 굵은 글씨체는 이러한 함수의 벡터화를 나타냅니다. 이러한 기대치와 편차가 어떤 분포에 걸쳐져 있는지에 대한 자세한 내용은 아래의 \"GLM 사실 도출\"에서 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNDwfwBObKl"
      },
      "source": [
        "# 예제\n",
        "\n",
        "이 섹션에서는 TensorFlow Probability의 두 가지 기본 제공 GLM 피팅 알고리즘인 Fisher 점수(`tfp.glm.fit`) 및 좌표별 근위 경사 하강법(`tfp.glm.fit_sparse`)을 간략하게 설명하고 예시합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4phryMfsP4Sn"
      },
      "source": [
        "## 합성 데이터세트\n",
        "\n",
        "일부 훈련 데이터세트를 로드한다고 가정해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA2Rf9PPgMAD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEVnTz2hh9RN"
      },
      "outputs": [],
      "source": [
        "def make_dataset(n, d, link, scale=1., dtype=np.float32):\n",
        "  model_coefficients = tfd.Uniform(\n",
        "      low=-1., high=np.array(1, dtype)).sample(d, seed=42)\n",
        "  radius = np.sqrt(2.)\n",
        "  model_coefficients *= radius / tf.linalg.norm(model_coefficients)\n",
        "  mask = tf.random.shuffle(tf.range(d)) < int(0.5 * d)\n",
        "  model_coefficients = tf.where(\n",
        "      mask, model_coefficients, np.array(0., dtype))\n",
        "  model_matrix = tfd.Normal(\n",
        "      loc=0., scale=np.array(1, dtype)).sample([n, d], seed=43)\n",
        "  scale = tf.convert_to_tensor(scale, dtype)\n",
        "  linear_response = tf.linalg.matvec(model_matrix, model_coefficients)\n",
        "  \n",
        "  if link == 'linear':\n",
        "    response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)\n",
        "  elif link == 'probit':\n",
        "    response = tf.cast(\n",
        "        tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,\n",
        "                   dtype)\n",
        "  elif link == 'logit':\n",
        "    response = tfd.Bernoulli(logits=linear_response).sample(seed=44)\n",
        "  else:\n",
        "    raise ValueError('unrecognized true link: {}'.format(link))\n",
        "  return model_matrix, response, model_coefficients, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Fk5XZKbvi4"
      },
      "source": [
        "### 참고: 로컬 런타임에 연결하세요.\n",
        "\n",
        "이 노트북에서는 로컬 파일을 사용하여 Python과 R 커널 간에 데이터를 공유합니다. 이 공유를 활성화하려면 로컬 파일을 읽고 쓸 수 있는 권한이 있는 동일한 컴퓨터에서 런타임을 사용하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EAQjTrZJqKx"
      },
      "outputs": [],
      "source": [
        "x, y, model_coefficients_true, _ = [t.numpy() for t in make_dataset(\n",
        "    n=int(1e5), d=100, link='probit')]\n",
        "\n",
        "DATA_DIR = '/tmp/glm_example'\n",
        "tf.io.gfile.makedirs(DATA_DIR)\n",
        "with tf.io.gfile.GFile('{}/x.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, x, delimiter=',')\n",
        "with tf.io.gfile.GFile('{}/y.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, y.astype(np.int32) + 1, delimiter=',', fmt='%d')\n",
        "with tf.io.gfile.GFile(\n",
        "    '{}/model_coefficients_true.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, model_coefficients_true, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P5I-aJdN6GZ"
      },
      "source": [
        "## L1 정규화 미사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN6HfiH3bAb0"
      },
      "source": [
        "`tfp.glm.fit` 함수는 일부 인수를 취하는 Fisher 스코어링을 구현합니다.\n",
        "\n",
        "- `model_matrix` = $\\mathbf{x}$\n",
        "- `response` = $\\mathbf{y}$\n",
        "- `model` = 호출 가능하며 $\\boldsymbol{\\eta}$ 인수가 주어지면 삼중 $\\left( {\\textbf{Mean}_T}(\\boldsymbol{\\eta}), {\\textbf{Var}_T}(\\boldsymbol{\\eta}), {\\textbf{Mean}_T}'(\\boldsymbol{\\eta}) \\right)$를 반환합니다.\n",
        "\n",
        "`model`을 `tfp.glm.ExponentialFamily` 클래스의 인스턴스로 사용하는 것이 좋습니다. 몇 가지 미리 만들어진 구현을 사용할 수 있으므로 대부분의 일반적인 GLM에는 사용자 지정 코드가 필요하지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXkxVBSmesjn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_converged: True\n",
            "    num_iter: 6\n",
            "    accuracy: 0.75241\n",
            "    deviance: -0.992436110973\n",
            "||w0-w1||_2 / (1+||w0||_2): 0.0231555201462\n"
          ]
        }
      ],
      "source": [
        "@tf.function(autograph=False)\n",
        "def fit_model():\n",
        "  model_coefficients, linear_response, is_converged, num_iter = tfp.glm.fit(\n",
        "      model_matrix=x, response=y, model=tfp.glm.BernoulliNormalCDF())\n",
        "  log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(y, linear_response)\n",
        "  return (model_coefficients, linear_response, is_converged, num_iter,\n",
        "          log_likelihood)\n",
        " \n",
        "[model_coefficients, linear_response, is_converged, num_iter,\n",
        " log_likelihood] = [t.numpy() for t in fit_model()]\n",
        "\n",
        "print(('is_converged: {}\\n'\n",
        "       '    num_iter: {}\\n'\n",
        "       '    accuracy: {}\\n'\n",
        "       '    deviance: {}\\n'\n",
        "       '||w0-w1||_2 / (1+||w0||_2): {}'\n",
        "      ).format(\n",
        "    is_converged,\n",
        "    num_iter,\n",
        "    np.mean((linear_response > 0.) == y),\n",
        "    2. * np.mean(log_likelihood),\n",
        "    np.linalg.norm(model_coefficients_true - model_coefficients, ord=2) /\n",
        "        (1. + np.linalg.norm(model_coefficients_true, ord=2))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6qexoHAJzEF"
      },
      "source": [
        "### 수학적 세부 사항\n",
        "\n",
        "Fisher 스코어링은 최대 가능도 추정치를 찾기 위한 Newton의 방법을 변형한 것입니다.\n",
        "\n",
        "$$ \\hat\\beta := \\underset{\\beta}{\\text{arg max}}\\ \\ \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}). $$\n",
        "\n",
        "로그-가능도의 기울기 0을 찾는 Vanilla Newton의 방법은 업데이트 규칙을 따릅니다.\n",
        "\n",
        "## $$ \\beta^{(t+1)}_{\\text{Newton}} := \\beta^{(t)}\n",
        "\n",
        "\\alpha \\left( \\nabla^2_\\beta, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}) \\right)*{\\beta = \\beta^{(t)}}^{-1} \\left( \\nabla*\\beta, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}) \\right)_{\\beta = \\beta^{(t)}} $$\n",
        "\n",
        "여기서 $\\alpha \\in (0, 1]$은 단계 크기를 제어하는 데 사용되는 학습률입니다.\n",
        "\n",
        "Fisher 스코어링에서는 Hesian을 음의 Fisher 정보 행렬로 바꿉니다.\n",
        "\n",
        "## $$ \\begin{align*} \\beta^{(t+1)} &amp;:= \\beta^{(t)}\n",
        "\n",
        "\\alpha, \\mathbb{E}*{ Y_i \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta^{(t)}), \\phi) } \\left[ \\left( \\nabla^2_\\beta, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{Y}) \\right)*{\\beta = \\beta^{(t)}} \\right]^{-1} \\left( \\nabla*\\beta, \\ell(\\beta\\ ;\\ \\mathbf{x}, \\mathbf{y}) \\right)_{\\beta = \\beta^{(t)}} \\[3mm] \\end{align*} $$\n",
        "\n",
        "[여기서 $\\mathbf{Y} = (Y_i)_{i=1}^{n}$는 랜덤인 반면 $\\mathbf{y}$는 여전히 관찰된 응답의 벡터입니다.]\n",
        "\n",
        "아래 \"GLM 매개변수를 데이터에 피팅하기\" 공식을 사용하면 다음과 같이 단순화됩니다.\n",
        "\n",
        "$$ \\begin{align*} \\beta^{(t+1)} &amp;= \\beta^{(t)} + \\alpha \\left( \\mathbf{x}^\\top \\text{diag}\\left( \\frac{ \\phi, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t)})^2 }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t)}) }\\right), \\mathbf{x} \\right)^{-1} \\left( \\mathbf{x}^\\top \\text{diag}\\left(\\frac{ {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t)}) }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t)}) }\\right) \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta^{(t)})\\right) \\right). \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076quM7tN8_1"
      },
      "source": [
        "## L1 정규화 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnP3jeZOk7Y5"
      },
      "source": [
        "`tfp.glm.fit_sparse`는 [Yuan, Ho 및 Lin 2012](#1)의 알고리즘을 기반으로 희소 데이터세트에 더 적합한 GLM 피터를 구현합니다. 해당 요소에는 다음이 포함됩니다.\n",
        "\n",
        "- L1 정규화\n",
        "- 매트릭스 반전 없음\n",
        "- 기울기와 Hessian에 대한 평가가 거의 없음\n",
        "\n",
        "먼저 코드의 사용 예를 제시합니다. 알고리즘의 세부 사항은 아래 \"`tfp.glm.fit_sparse`에 대한 알고리즘 세부 사항\"에서 더 자세히 설명합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Oky1X4ijfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_converged: True\n",
            "    num_iter: 1\n",
            "\n",
            "Coefficients:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learned</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.216240</td>\n",
              "      <td>0.220758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.043702</td>\n",
              "      <td>0.063950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.145379</td>\n",
              "      <td>-0.153256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.024382</td>\n",
              "      <td>0.046572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.242985</td>\n",
              "      <td>-0.242609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.106168</td>\n",
              "      <td>-0.123367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.039745</td>\n",
              "      <td>-0.067560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.217717</td>\n",
              "      <td>-0.222169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.016553</td>\n",
              "      <td>-0.041692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.018959</td>\n",
              "      <td>0.049624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.057686</td>\n",
              "      <td>-0.078299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.234406</td>\n",
              "      <td>-0.240482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.232209</td>\n",
              "      <td>0.225448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.144485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>-0.178534</td>\n",
              "      <td>-0.186722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.218493</td>\n",
              "      <td>0.229656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.195579</td>\n",
              "      <td>0.200442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.031153</td>\n",
              "      <td>0.050457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.229065</td>\n",
              "      <td>0.231451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.006512</td>\n",
              "      <td>-0.039516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-0.107947</td>\n",
              "      <td>-0.119896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.149419</td>\n",
              "      <td>0.171693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.047955</td>\n",
              "      <td>0.063434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.083171</td>\n",
              "      <td>-0.107145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.084615</td>\n",
              "      <td>0.101221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-0.168431</td>\n",
              "      <td>-0.175473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.138411</td>\n",
              "      <td>0.152623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.061161</td>\n",
              "      <td>0.081945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-0.083348</td>\n",
              "      <td>-0.104929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.141154</td>\n",
              "      <td>-0.153871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Learned      True\n",
              "0   0.216240  0.220758\n",
              "1   0.000000  0.000000\n",
              "2   0.000000  0.000000\n",
              "3   0.000000  0.000000\n",
              "4   0.000000  0.000000\n",
              "5   0.043702  0.063950\n",
              "6  -0.145379 -0.153256\n",
              "7   0.000000  0.000000\n",
              "8   0.000000  0.000000\n",
              "9   0.000000  0.000000\n",
              "10  0.000000  0.000000\n",
              "11  0.000000  0.000000\n",
              "12  0.000000  0.000000\n",
              "13  0.024382  0.046572\n",
              "14 -0.242985 -0.242609\n",
              "15 -0.106168 -0.123367\n",
              "16  0.000000  0.000000\n",
              "17 -0.039745 -0.067560\n",
              "18 -0.217717 -0.222169\n",
              "19  0.000000  0.000000\n",
              "20  0.000000  0.000000\n",
              "21 -0.016553 -0.041692\n",
              "22  0.018959  0.049624\n",
              "23 -0.057686 -0.078299\n",
              "24  0.003642  0.035682\n",
              "25  0.000000  0.000000\n",
              "26  0.000000  0.000000\n",
              "27 -0.234406 -0.240482\n",
              "28  0.000000  0.000000\n",
              "29  0.232209  0.225448\n",
              "..       ...       ...\n",
              "70  0.000000  0.000000\n",
              "71  0.130166  0.144485\n",
              "72  0.000000  0.000000\n",
              "73  0.000000  0.000000\n",
              "74  0.000000  0.000000\n",
              "75 -0.178534 -0.186722\n",
              "76  0.000000  0.000000\n",
              "77  0.218493  0.229656\n",
              "78  0.000000  0.000000\n",
              "79  0.000000  0.000000\n",
              "80  0.195579  0.200442\n",
              "81  0.000000  0.000000\n",
              "82  0.000000  0.000000\n",
              "83  0.031153  0.050457\n",
              "84  0.229065  0.231451\n",
              "85 -0.006512 -0.039516\n",
              "86 -0.107947 -0.119896\n",
              "87  0.000000  0.000000\n",
              "88  0.149419  0.171693\n",
              "89  0.000000  0.000000\n",
              "90  0.047955  0.063434\n",
              "91  0.000000  0.003592\n",
              "92 -0.083171 -0.107145\n",
              "93  0.084615  0.101221\n",
              "94 -0.168431 -0.175473\n",
              "95  0.138411  0.152623\n",
              "96  0.000000  0.000000\n",
              "97  0.061161  0.081945\n",
              "98 -0.083348 -0.104929\n",
              "99 -0.141154 -0.153871\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = tfp.glm.Bernoulli()\n",
        "model_coefficients_start = tf.zeros(x.shape[-1], np.float32)\n",
        "@tf.function(autograph=False)\n",
        "def fit_model():\n",
        "  return tfp.glm.fit_sparse(\n",
        "    model_matrix=tf.convert_to_tensor(x),\n",
        "    response=tf.convert_to_tensor(y),\n",
        "    model=model,\n",
        "    model_coefficients_start=model_coefficients_start,\n",
        "    l1_regularizer=800.,\n",
        "    l2_regularizer=None,\n",
        "    maximum_iterations=10,\n",
        "    maximum_full_sweeps_per_iteration=10,\n",
        "    tolerance=1e-6,\n",
        "    learning_rate=None)\n",
        "\n",
        "model_coefficients, is_converged, num_iter = [t.numpy() for t in fit_model()]\n",
        "coefs_comparison = pd.DataFrame({\n",
        "  'Learned': model_coefficients,\n",
        "  'True': model_coefficients_true,\n",
        "})\n",
        "  \n",
        "print(('is_converged: {}\\n'\n",
        "       '    num_iter: {}\\n\\n'\n",
        "       'Coefficients:').format(\n",
        "    is_converged,\n",
        "    num_iter))\n",
        "coefs_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrJC2J1YbR5L"
      },
      "source": [
        "학습된 계수는 실제 계수와 동일한 희소성 패턴을 가집니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ7SzrPZMpke"
      },
      "outputs": [],
      "source": [
        "# Save the learned coefficients to a file.\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_prox.csv'.format(DATA_DIR), 'w') as f:\n",
        "  np.savetxt(f, model_coefficients, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW9NgB1Zisqh"
      },
      "source": [
        "### R의 `glmnet`와 비교하기\n",
        "\n",
        "좌표별 근위 경사 하강법의 출력을 유사한 알고리즘을 사용하는 R의 `glmnet` 출력과 비교합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aptz7SWwkd5v"
      },
      "source": [
        "#### 참고: 이 섹션을 실행하려면 R colab 런타임으로 전환해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS1H3n53h9qc"
      },
      "outputs": [],
      "source": [
        "suppressMessages({\n",
        "  library('glmnet')\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X6zKSaxie7I"
      },
      "outputs": [],
      "source": [
        "data_dir <- '/tmp/glm_example'\n",
        "x <- as.matrix(read.csv(paste(data_dir, '/x.csv', sep=''),\n",
        "                        header=FALSE))\n",
        "y <- as.matrix(read.csv(paste(data_dir, '/y.csv', sep=''),\n",
        "                        header=FALSE, colClasses='integer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb31LbhRjsSz"
      },
      "outputs": [],
      "source": [
        "fit <- glmnet(\n",
        "x = x,\n",
        "y = y,\n",
        "family = \"binomial\",  # Logistic regression\n",
        "alpha = 1,  # corresponds to l1_weight = 1, l2_weight = 0\n",
        "standardize = FALSE,\n",
        "intercept = FALSE,\n",
        "thresh = 1e-30,\n",
        "type.logistic = \"Newton\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTN4RKQbhlCm"
      },
      "outputs": [],
      "source": [
        "write.csv(as.matrix(coef(fit, 0.008)),\n",
        "          paste(data_dir, '/model_coefficients_glmnet.csv', sep=''),\n",
        "          row.names=FALSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsrEKgUGjGjf"
      },
      "source": [
        "#### R, TFP 및 실제 계수 비교(참고: Python 커널로 돌아가기)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCOlGo_4i2sb"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/tmp/glm_example'\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_glmnet.csv'.format(DATA_DIR),\n",
        "                       'r') as f:\n",
        "  model_coefficients_glmnet = np.loadtxt(f,\n",
        "                                   skiprows=2  # Skip column name and intercept\n",
        "                               )\n",
        "\n",
        "with tf.io.gfile.GFile('{}/model_coefficients_prox.csv'.format(DATA_DIR),\n",
        "                       'r') as f:\n",
        "  model_coefficients_prox = np.loadtxt(f)\n",
        "\n",
        "with tf.io.gfile.GFile(\n",
        "    '{}/model_coefficients_true.csv'.format(DATA_DIR), 'r') as f:\n",
        "  model_coefficients_true = np.loadtxt(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l-SZ85lnKg5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R</th>\n",
              "      <th>TFP</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.281080</td>\n",
              "      <td>0.216240</td>\n",
              "      <td>0.220758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.056625</td>\n",
              "      <td>0.043702</td>\n",
              "      <td>0.063950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.188771</td>\n",
              "      <td>-0.145379</td>\n",
              "      <td>-0.153256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.030112</td>\n",
              "      <td>0.024382</td>\n",
              "      <td>0.046572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.316488</td>\n",
              "      <td>-0.242985</td>\n",
              "      <td>-0.242609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.139214</td>\n",
              "      <td>-0.106168</td>\n",
              "      <td>-0.123367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.050239</td>\n",
              "      <td>-0.039745</td>\n",
              "      <td>-0.067560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.283372</td>\n",
              "      <td>-0.217717</td>\n",
              "      <td>-0.222169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.021815</td>\n",
              "      <td>-0.016553</td>\n",
              "      <td>-0.041692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.024070</td>\n",
              "      <td>0.018959</td>\n",
              "      <td>0.049624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.074039</td>\n",
              "      <td>-0.057686</td>\n",
              "      <td>-0.078299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.005321</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.035682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.304958</td>\n",
              "      <td>-0.234406</td>\n",
              "      <td>-0.240482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301562</td>\n",
              "      <td>0.232209</td>\n",
              "      <td>0.225448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.169291</td>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.144485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>-0.231294</td>\n",
              "      <td>-0.178534</td>\n",
              "      <td>-0.186722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.284215</td>\n",
              "      <td>0.218493</td>\n",
              "      <td>0.229656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.254524</td>\n",
              "      <td>0.195579</td>\n",
              "      <td>0.200442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.040716</td>\n",
              "      <td>0.031153</td>\n",
              "      <td>0.050457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.297475</td>\n",
              "      <td>0.229065</td>\n",
              "      <td>0.231451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.008569</td>\n",
              "      <td>-0.006512</td>\n",
              "      <td>-0.039516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-0.141028</td>\n",
              "      <td>-0.107947</td>\n",
              "      <td>-0.119896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.194130</td>\n",
              "      <td>0.149419</td>\n",
              "      <td>0.171693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.062601</td>\n",
              "      <td>0.047955</td>\n",
              "      <td>0.063434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.107693</td>\n",
              "      <td>-0.083171</td>\n",
              "      <td>-0.107145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.109381</td>\n",
              "      <td>0.084615</td>\n",
              "      <td>0.101221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-0.218831</td>\n",
              "      <td>-0.168431</td>\n",
              "      <td>-0.175473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.180662</td>\n",
              "      <td>0.138411</td>\n",
              "      <td>0.152623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.078815</td>\n",
              "      <td>0.061161</td>\n",
              "      <td>0.081945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-0.108332</td>\n",
              "      <td>-0.083348</td>\n",
              "      <td>-0.104929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.183284</td>\n",
              "      <td>-0.141154</td>\n",
              "      <td>-0.153871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           R       TFP      True\n",
              "0   0.281080  0.216240  0.220758\n",
              "1   0.000000  0.000000  0.000000\n",
              "2   0.000000  0.000000  0.000000\n",
              "3   0.000000  0.000000  0.000000\n",
              "4   0.000000  0.000000  0.000000\n",
              "5   0.056625  0.043702  0.063950\n",
              "6  -0.188771 -0.145379 -0.153256\n",
              "7   0.000000  0.000000  0.000000\n",
              "8   0.000000  0.000000  0.000000\n",
              "9   0.000000  0.000000  0.000000\n",
              "10  0.000000  0.000000  0.000000\n",
              "11  0.000000  0.000000  0.000000\n",
              "12  0.000000  0.000000  0.000000\n",
              "13  0.030112  0.024382  0.046572\n",
              "14 -0.316488 -0.242985 -0.242609\n",
              "15 -0.139214 -0.106168 -0.123367\n",
              "16  0.000000  0.000000  0.000000\n",
              "17 -0.050239 -0.039745 -0.067560\n",
              "18 -0.283372 -0.217717 -0.222169\n",
              "19  0.000000  0.000000  0.000000\n",
              "20  0.000000  0.000000  0.000000\n",
              "21 -0.021815 -0.016553 -0.041692\n",
              "22  0.024070  0.018959  0.049624\n",
              "23 -0.074039 -0.057686 -0.078299\n",
              "24  0.005321  0.003642  0.035682\n",
              "25  0.000000  0.000000  0.000000\n",
              "26  0.000000  0.000000  0.000000\n",
              "27 -0.304958 -0.234406 -0.240482\n",
              "28  0.000000  0.000000  0.000000\n",
              "29  0.301562  0.232209  0.225448\n",
              "..       ...       ...       ...\n",
              "70  0.000000  0.000000  0.000000\n",
              "71  0.169291  0.130166  0.144485\n",
              "72  0.000000  0.000000  0.000000\n",
              "73  0.000000  0.000000  0.000000\n",
              "74  0.000000  0.000000  0.000000\n",
              "75 -0.231294 -0.178534 -0.186722\n",
              "76  0.000000  0.000000  0.000000\n",
              "77  0.284215  0.218493  0.229656\n",
              "78  0.000000  0.000000  0.000000\n",
              "79  0.000000  0.000000  0.000000\n",
              "80  0.254524  0.195579  0.200442\n",
              "81  0.000000  0.000000  0.000000\n",
              "82  0.000000  0.000000  0.000000\n",
              "83  0.040716  0.031153  0.050457\n",
              "84  0.297475  0.229065  0.231451\n",
              "85 -0.008569 -0.006512 -0.039516\n",
              "86 -0.141028 -0.107947 -0.119896\n",
              "87  0.000000  0.000000  0.000000\n",
              "88  0.194130  0.149419  0.171693\n",
              "89  0.000000  0.000000  0.000000\n",
              "90  0.062601  0.047955  0.063434\n",
              "91  0.000000  0.000000  0.003592\n",
              "92 -0.107693 -0.083171 -0.107145\n",
              "93  0.109381  0.084615  0.101221\n",
              "94 -0.218831 -0.168431 -0.175473\n",
              "95  0.180662  0.138411  0.152623\n",
              "96  0.000000  0.000000  0.000000\n",
              "97  0.078815  0.061161  0.081945\n",
              "98 -0.108332 -0.083348 -0.104929\n",
              "99 -0.183284 -0.141154 -0.153871\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coefs_comparison = pd.DataFrame({\n",
        "    'TFP': model_coefficients_prox,\n",
        "    'R': model_coefficients_glmnet,\n",
        "    'True': model_coefficients_true,\n",
        "})\n",
        "coefs_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfv0GVXqY74Y"
      },
      "source": [
        "# `tfp.glm.fit_sparse`에 대한 알고리즘 세부 정보\n",
        "\n",
        "알고리즘을 Newton의 방법에 대한 세 가지 수정 시퀀스로 제시합니다. 각각에서 $\\beta$에 대한 업데이트 규칙은 벡터 $s$와 로그-가능도의 그래디언트 및 Hessian을 근사 계산하는 행렬 $H$를 기반으로 합니다. $t$ 단계에서 변경할 좌표 $j^{(t)}$를 선택하고 업데이트 규칙에 따라 $\\beta$를 업데이트합니다.\n",
        "\n",
        "## $$ \\begin{align*} u^{(t)} &amp;:= \\frac{ \\left( s^{(t)} \\right)*{j^{(t)}} }{ \\left( H^{(t)} \\right)*{j^{(t)},, j^{(t)}} } \\[3mm] \\beta^{(t+1)} &amp;:= \\beta^{(t)}\n",
        "\n",
        "\\alpha, u^{(t)} ,\\text{onehot}(j^{(t)}) \\end{align*} $$\n",
        "\n",
        "이 업데이트는 학습률이 $\\alpha$인 Newton과 유사한 단계입니다. 최종 조각(L1 정규화)을 제외하고 아래 수정 사항은 $s$ 및 $H$를 업데이트하는 방식만 다릅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH7C1xBWUV7_"
      },
      "source": [
        "## 출발점: 좌표별 Newton 방법\n",
        "\n",
        "좌표별 Newton 방법에서는 $s$ 및 $H$를 로그-가능도의 실제 그래디언트 및 Hessian으로 설정합니다.\n",
        "\n",
        "$$ \\begin{align*} s^{(t)}*{\\text{vanilla}} &amp;:= \\left( \\nabla*\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)*{\\beta = \\beta^{(t)}} \\ H^{(t)}*{\\text{vanilla}} &amp;:= \\left( \\nabla^2_\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)_{\\beta = \\beta^{(t)}} \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rJZD6iyUl0v"
      },
      "source": [
        "## 그래디언트 및 Hessian에 대한 더 적은 평가\n",
        "\n",
        "로그-가능도의 그래디언트와 Hessian은 종종 계산하는 데 비용이 많이 들기 때문에 대략적으로 계산하는 것이 좋습니다. 다음과 같이 할 수 있습니다.\n",
        "\n",
        "- 일반적으로 Hessian을 로컬 상수로 근사하고 (근사) Hessian을 사용하여 그래디언트를 1차로 근사합니다.\n",
        "\n",
        "$$ \\begin{align*} H_{\\text{approx}}^{(t+1)} &amp;:= H^{(t)} \\ s_{\\text{approx}}^{(t+1)} &amp;:= s^{(t)} + H^{(t)} \\left( \\beta^{(t+1)} - \\beta^{(t)} \\right) \\end{align*} $$\n",
        "\n",
        "- 때로 위와 같이 \"바닐라\" 업데이트 단계를 수행하여 $s^{(t+1)}$를 정확한 그래디언트로 설정하고 $H^{(t+1)}$를 로그-가능도의 정확한 Hessian으로 설정합니다. $\\beta^{(t+1)}$에서 평가됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvvyaVnUqIQ"
      },
      "source": [
        "## Hessian을 음의 Fisher 정보로 대체\n",
        "\n",
        "바닐라 업데이트 단계의 비용을 더 줄이기 위해 정확한 Hessian 대신 $H$를 음의 Fisher 정보 행렬(아래 \"GLM 매개변수를 데이터에 맞추기\"의 공식을 사용하여 효율적으로 계산할 수 있음)로 설정할 수 있습니다.\n",
        "\n",
        "$$ \\begin{align*} H_{\\text{Fisher}}^{(t+1)} &amp;:= \\mathbb{E}*{Y_i \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta^{(t+1)}), \\phi)} \\left[ \\left( \\nabla_\\beta^2, \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{Y}) \\right)_{\\beta = \\beta^{(t+1)}} \\right] \\ &amp;= -\\mathbf{x}^\\top ,\\text{diag}\\left( \\frac{ \\phi, {\\textbf{Mean}*T}'(\\mathbf{x} \\beta^{(t+1)})^2 }{ {\\textbf{Var}*T}(\\mathbf{x} \\beta^{(t+1)}) }\\right), \\mathbf{x} \\ s*{\\text{Fisher}}^{(t+1)} &amp;:= s*{\\text{vanilla}}^{(t+1)} \\ &amp;= \\left( \\mathbf{x}^\\top ,\\text{diag}\\left(\\frac{ {\\textbf{Mean}_T}'(\\mathbf{x} \\beta^{(t+1)}) }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta^{(t+1)}) }\\right) \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta^{(t+1)})\\right) \\right) \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTH07xYpWGcR"
      },
      "source": [
        "## 근위 경사 하강법을 통한 L1 정규화\n",
        "\n",
        "L1 정규화를 통합하기 위해 아래의 업데이트 규칙을\n",
        "\n",
        "## $$ \\beta^{(t+1)} := \\beta^{(t)}\n",
        "\n",
        "\\alpha, u^{(t)} ,\\text{onehot}(j^{(t)}) $$\n",
        "\n",
        "더 일반적인 다음 업데이트 규칙으로 대체합니다.\n",
        "\n",
        "$$ \\begin{align*} \\gamma^{(t)} &amp;:= -\\frac{\\alpha, r_{\\text<l>}}{\\left(H^{(t)}\\right)<em data-md-type=\"emphasis\">{j^{(t)},, j^{(t)}}} \\[2mm] \\left(\\beta</em>{\\text{reg}}^{(t+1)}\\right)_j &amp;:= \\begin{cases} \\beta^{(t+1)}_j &amp;\\text{if } j \\neq j^{(t)} \\ \\text{SoftThreshold} \\left( \\beta^{(t)}_j - \\alpha, u^{(t)} ,\\ \\gamma^{(t)} \\right) &amp;\\text{if } j = j^{(t)} \\end{cases} \\end{align*} $$</l>\n",
        "\n",
        "여기서 $r_{\\text<l>} &gt; 0$는 제공된 상수(L1 정규화 계수)이고 $\\text{SoftThreshold}$는 다음과 같이 정의된 소프트 임계값 연산자입니다.</l>\n",
        "\n",
        "$$ \\text{SoftThreshold}(\\beta, \\gamma) := \\begin{cases} \\beta + \\gamma &amp;\\text{if } \\beta &lt; -\\gamma \\ 0 &amp;\\text{if } -\\gamma \\leq \\beta \\leq \\gamma \\ \\beta - \\gamma &amp;\\text{if } \\beta &gt; \\gamma. \\end{cases} $$\n",
        "\n",
        "이 업데이트 규칙에는 다음과 같은 두 가지 영감을 주는 속성이 있으며 아래에서 설명합니다.\n",
        "\n",
        "1. $r_{\\text<l>} \\to 0$의 제한적인 경우(즉, L1 정규화 없음), 이 업데이트 규칙은 원래 업데이트 규칙과 동일합니다.</l>\n",
        "\n",
        "2. 이 업데이트 규칙은 고정점이 L1 정규화 최소화 문제에 대한 솔루션인 근접 연산자를 적용하는 것으로 해석될 수 있습니다.\n",
        "\n",
        "$$ \\underset{\\beta - \\beta^{(t)} \\in \\text{span}{ \\text{onehot}(j^{(t)}) }}{\\text{arg min}} \\left( -\\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y})\n",
        "\n",
        "- r_{\\text<l>} \\left\\lVert \\beta \\right\\rVert_1 \\right). $$</l>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSs7_osNPLVt"
      },
      "source": [
        "### 퇴화 사례 $r_{\\text<l>} = 0$는 원래 업데이트 규칙을 복구합니다.</l>\n",
        "\n",
        "(1)을 보려면 $r_{\\text<l>} = 0$이면 $\\gamma^{(t)} = 0$이므로</l>\n",
        "\n",
        "$$ \\begin{align*} \\left(\\beta_{\\text{reg}}^{(t+1)}\\right)*{j^{(t)}} &amp;= \\text{SoftThreshold} \\left( \\beta^{(t)}*{j^{(t)}} - \\alpha, u^{(t)} ,\\ 0 \\right) \\ &amp;= \\beta^{(t)}_{j^{(t)}} - \\alpha, u^{(t)}. \\end{align*} $$\n",
        "\n",
        "따라서\n",
        "\n",
        "$$ \\begin{align*} \\beta_{\\text{reg}}^{(t+1)} &amp;= \\beta^{(t)} - \\alpha, u^{(t)} ,\\text{onehot}(j^{(t)}) \\ &amp;= \\beta^{(t+1)}. \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiHy_0NIPT5f"
      },
      "source": [
        "### 고정점이 정규화된 MLE인 근접 연산자\n",
        "\n",
        "(2)를 보려면 먼저 모든 $\\gamma &gt; 0$에 대해 다음 업데이트 규칙([Wikipedia](#3) 참조)\n",
        "\n",
        "$$ \\left(\\beta_{\\text{exact-prox}, \\gamma}^{(t+1)}\\right)*{j^{(t)}} := \\text{prox}*{\\gamma \\lVert \\cdot \\rVert_1} \\left( \\beta^{(t)}*{j^{(t)}} + \\frac{\\gamma}{r*{\\text<l>}} \\left( \\left( \\nabla_\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)<em data-md-type=\"emphasis\">{\\beta = \\beta^{(t)}} \\right)</em>{j^{(t)}} \\right) $$</l>\n",
        "\n",
        "이 (2)를 충족한다는 점에 주목하세요. 여기서 $\\text{prox}$는 근접 연산자입니다(이 연산자가 $\\mathsf{P}$로 표기된 [Yu](#4) 참조). 위 수식의 우변은 [다음과 같이](#2) 계산됩니다.\n",
        "\n",
        "# $$ \\left(\\beta_{\\text{exact-prox}, \\gamma}^{(t+1)}\\right)_{j^{(t)}}\n",
        "\n",
        "\\text{SoftThreshold} \\left( \\beta^{(t)}*{j^{(t)}} + \\frac{\\gamma}{r*{\\text<l>}} \\left( \\left( \\nabla_\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)<em data-md-type=\"emphasis\">{\\beta = \\beta^{(t)}} \\right)</em>{j^{(t)}} ,\\ \\gamma \\right). $$</l>\n",
        "\n",
        "특히, $\\gamma = \\gamma^{(t)} = -\\frac{\\alpha, r_{\\text<l>}}{\\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}}}$를 설정하여 다음 업데이트 규칙을 얻습니다(음의 로그-가능도가 볼록이기만 하다면 $\\gamma^{(t)} &gt; 0$입니다).</l>\n",
        "\n",
        "# $$ \\left(\\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\\right)_{j^{(t)}}\n",
        "\n",
        "\\text{SoftThreshold} \\left( \\beta^{(t)}*{j^{(t)}} - \\alpha \\frac{ \\left( \\left( \\nabla*\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)*{\\beta = \\beta^{(t)}} \\right)*{j^{(t)}} }{ \\left(H^{(t)}\\right)_{j^{(t)}, j^{(t)}} } ,\\ \\gamma^{(t)} \\right). $$\n",
        "\n",
        "그런 다음 정확한 그래디언트 $\\left( \\nabla_\\beta, \\ell(\\beta ,;, \\mathbf{x}, \\mathbf{y}) \\right)_{\\beta = \\beta^{(t) }}$를 근사값 $s^{(t)}$로 대체하여 다음을 얻습니다.\n",
        "\n",
        "\\begin{align*} \\left(\\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)}\\right)*{j^{(t)}} &amp;\\approx \\text{SoftThreshold} \\left( \\beta^{(t)}*{j^{(t)}} - \\alpha \\frac{ \\left(s^{(t)}\\right)*{j^{(t)}} }{ \\left(H^{(t)}\\right)*{j^{(t)}, j^{(t)}} } ,\\ \\gamma^{(t)} \\right) \\ &amp;= \\text{SoftThreshold} \\left( \\beta^{(t)}_{j^{(t)}} - \\alpha, u^{(t)} ,\\ \\gamma^{(t)} \\right). \\end{align*}\n",
        "\n",
        "따라서\n",
        "\n",
        "$$ \\beta_{\\text{exact-prox}, \\gamma^{(t)}}^{(t+1)} \\approx \\beta_{\\text{reg}}^{(t+1)}. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7YOOrmI8j0L"
      },
      "source": [
        "# GLM 사실 도출\n",
        "\n",
        "이 섹션에서는 이전 섹션에서 사용된 GLM에 대해 자세히 설명하고 결과를 도출합니다. 그런 다음 TensorFlow의 `gradients`를 사용하여 로그-가능도와 Fisher 정보의 그래디언트에 대한 파생 공식을 수치적으로 검증합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkHZyhuAIW-p"
      },
      "source": [
        "## 점수 및 Fisher 정보"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbyYy0bE8pOK"
      },
      "source": [
        "확률 밀도가 $\\left{p(\\cdot | \\theta)\\right}_{\\theta \\in \\mathcal{T}}$인 매개변수 벡터 $\\theta$로 매개변수화된 확률 분포군을 고려하세요. 매개변수 벡터 $\\theta_0$에서 결과 $y$의 **점수**는 $y$($\\theta_0$에서 평가됨)의 로그-가능도의 그래디언트로 정의됩니다. 즉,\n",
        "\n",
        "$$ \\text{score}(y, \\theta_0) := \\left[\\nabla_\\theta, \\log p(y | \\theta)\\right]_{\\theta=\\theta_0}. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYGaMPIx8uOc"
      },
      "source": [
        "### 주장: 점수의 기대치는 0\n",
        "\n",
        "강하지 않은 규칙성 조건(적분에서 미분을 전달할 수 있음)에서는 다음과 같습니다.\n",
        "\n",
        "$$ \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\text{score}(Y, \\theta_0)\\right] = 0. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3H-wNmJ800R"
      },
      "source": [
        "#### 증명\n",
        "\n",
        "다음을 가지고 있습니다.\n",
        "\n",
        "$$ \\begin{align*} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\text{score}(Y, \\theta_0)\\right] &amp;:=\\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\left(\\nabla_\\theta \\log p(Y|\\theta)\\right)*{\\theta=\\theta_0}\\right] \\ &amp;\\stackrel{\\text{(1)}}{=} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[\\frac{\\left(\\nabla_\\theta p(Y|\\theta)\\right)*{\\theta=\\theta_0}}{p(Y|\\theta=\\theta_0)}\\right] \\ &amp;\\stackrel{\\text{(2)}}{=} \\int*{\\mathcal{Y}} \\left[\\frac{\\left(\\nabla_\\theta p(y|\\theta)\\right)*{\\theta=\\theta_0}}{p(y|\\theta=\\theta_0)}\\right] p(y | \\theta=\\theta_0), dy \\ &amp;= \\int*{\\mathcal{Y}} \\left(\\nabla_\\theta p(y|\\theta)\\right)*{\\theta=\\theta_0}, dy \\ &amp;\\stackrel{\\text{(3)}}{=} \\left[\\nabla*\\theta \\left(\\int_{\\mathcal{Y}} p(y|\\theta), dy\\right) \\right]*{\\theta=\\theta_0} \\ &amp;\\stackrel{\\text{(4)}}{=} \\left[\\nabla*\\theta, 1 \\right]_{\\theta=\\theta_0} \\ &amp;= 0, \\end{align*} $$\n",
        "\n",
        "여기서 우리는 다음을 사용했습니다: (1) 미분에 대한 연쇄 법칙, (2) 기대치의 정의, (3) 적분 기호로 미분 전달(규칙성 조건 사용), (4) 확률 밀도의 적분은 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1DPVOI9OT2"
      },
      "source": [
        "### 주장(Fisher 정보): 점수의 분산은 로그-가능도의 음수 기대 Hessian과 같습니다.\n",
        "\n",
        "강하지 않은 규칙성 조건(적분에서 미분을 전달할 수 있음)에서는 다음과 같습니다.\n",
        "\n",
        "# $$ \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\text{score}(Y, \\theta_0) \\text{score}(Y, \\theta_0)^\\top \\right]\n",
        "\n",
        "-\\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\left(\\nabla*\\theta^2 \\log p(Y | \\theta)\\right)_{\\theta=\\theta_0} \\right] $$\n",
        "\n",
        "여기서 $\\nabla_\\theta^2 F$는 $(i, j)$ 항목이 $\\frac{\\partial^2 F}{\\partial \\theta_i \\partial \\theta_j}$인 Hessian 행렬을 나타냅니다.\n",
        "\n",
        "이 수식의 좌변은 매개변수 벡터 $\\theta_0$에서 $\\left{p(\\cdot | \\theta)\\right}_{\\theta \\in \\mathcal{T}}$ 분포군의 **Fisher 정보**라고 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF-ac0Bk-HmR"
      },
      "source": [
        "#### 주장 증명\n",
        "\n",
        "다음을 가지고 있습니다.\n",
        "\n",
        "## $$ \\begin{align*} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\left(\\nabla*\\theta^2 \\log p(Y | \\theta)\\right)*{\\theta=\\theta_0} \\right] &amp;\\stackrel{\\text{(1)}}{=} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\left(\\nabla_\\theta^\\top \\frac{ \\nabla_\\theta p(Y | \\theta) }{ p(Y|\\theta) }\\right)*{\\theta=\\theta_0} \\right] \\ &amp;\\stackrel{\\text{(2)}}{=} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\frac{ \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) }\n",
        "\n",
        "## \\left(\\frac{ \\left(\\nabla_\\theta, p(Y|\\theta)\\right)*{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) }\\right) \\left(\\frac{ \\left(\\nabla*\\theta, p(Y|\\theta)\\right)*{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) }\\right)^\\top \\right] \\ &amp;\\stackrel{\\text{(3)}}{=} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\frac{ \\left(\\nabla^2_\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) }\n",
        "\n",
        "\\text{score}(Y, \\theta_0) ,\\text{score}(Y, \\theta_0)^\\top \\right], \\end{align*} $$\n",
        "\n",
        "여기서 우리는 다음을 사용했습니다. (1) 미분에 대한 연쇄 법칙, (2) 미분에 대한 몫 법칙, (3) 연쇄 법칙을 역으로 이용.\n",
        "\n",
        "증명을 완료하려면 다음을 보여주는 것으로 충분합니다.\n",
        "\n",
        "$$ \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\frac{ \\left(\\nabla^2*\\theta p(Y | \\theta)\\right)_{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) } \\right] \\stackrel{\\text{?}}{=} 0. $$\n",
        "\n",
        "이를 위해 적분 기호로 미분을 두 번 전달합니다.\n",
        "\n",
        "$$ \\begin{align*} \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)}\\left[ \\frac{ \\left(\\nabla^2*\\theta p(Y | \\theta)\\right)*{\\theta=\\theta_0} }{ p(Y|\\theta=\\theta_0) } \\right] &amp;= \\int*{\\mathcal{Y}} \\left[ \\frac{ \\left(\\nabla^2_\\theta p(y | \\theta)\\right)*{\\theta=\\theta_0} }{ p(y|\\theta=\\theta_0) } \\right] , p(y | \\theta=\\theta_0), dy \\ &amp;= \\int*{\\mathcal{Y}} \\left(\\nabla^2_\\theta p(y | \\theta)\\right)*{\\theta=\\theta_0} , dy \\ &amp;= \\left[ \\nabla*\\theta^2 \\left( \\int_{\\mathcal{Y}} p(y | \\theta) , dy \\right) \\right]*{\\theta=\\theta_0} \\ &amp;= \\left[ \\nabla*\\theta^2 , 1 \\right]_{\\theta=\\theta_0} \\ &amp;= 0. \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAIJfX7IX_lP"
      },
      "source": [
        "### 로그 분할 함수의 도함수에 대한 보조 정리\n",
        "\n",
        "$a$, $b$ 및 $c$가 스칼라 값 함수인 경우 $c$는 두 배 미분 가능하므로 다음에 의해 정의되는 $\\left{p(\\cdot | \\theta)\\right}_{\\theta \\ \\mathcal{T}}$ 분포군은\n",
        "\n",
        "$$ p(y|\\theta) = a(y) \\exp\\left(b(y), \\theta - c(\\theta)\\right) $$\n",
        "\n",
        "$y$에 대한 적분 하에서 $\\theta$에 대한 미분을 전달할 수 있도록 하는 강하지 않은 규칙성 조건을 충족하며, 그러면 다음과 같이 됩니다.\n",
        "\n",
        "$$ \\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right] = c'(\\theta_0) $$\n",
        "\n",
        "그리고\n",
        "\n",
        "$$ \\text{Var}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right] = c''(\\theta_0). $$\n",
        "\n",
        "(여기서 $'$는 미분을 나타내므로 $c'$와 $c''$는 $c$의 1차 도함수와 2차 도함수입니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYBH-KwpfWhr"
      },
      "source": [
        "#### 증명\n",
        "\n",
        "이 분포군에 대해 $\\text{score}(y, \\theta_0) = b(y) - c'(\\theta_0)$를 갖습니다. 그러면 $\\mathbb{E}_{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\text{score}(y, \\theta_0) \\right] = 0$라는 사실로부터 첫 번째 수식이 따라옵니다. 그리고 다음과 같습니다.\n",
        "\n",
        "$$ \\begin{align*} \\text{Var}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ b(Y) \\right] &amp;= \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\left(b(Y) - c'(\\theta_0)\\right)^2 \\right] \\ &amp;= \\text{the one entry of } \\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\text{score}(y, \\theta_0) \\text{score}(y, \\theta_0)^\\top \\right] \\ &amp;= \\text{the one entry of } -\\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ \\left(\\nabla_\\theta^2 \\log p(\\cdot | \\theta)\\right)*{\\theta=\\theta_0} \\right] \\ &amp;= -\\mathbb{E}*{Y \\sim p(\\cdot | \\theta=\\theta_0)} \\left[ -c''(\\theta_0) \\right] \\ &amp;= c''(\\theta_0). \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYpWUvvKcX-e"
      },
      "source": [
        "## 과분산 지수군\n",
        "\n",
        "(스칼라) **과분산 지수군**은 밀도가 다음 형식을 취하는 분포군입니다.\n",
        "\n",
        "$$ p_{\\text{OEF}(m,  T)}(y, |, \\theta, \\phi) = m(y, \\phi) \\exp\\left(\\frac{\\theta, T(y) - A(\\theta)}{\\phi}\\right), $$\n",
        "\n",
        "여기서 $m$ 및 $T$는 알려진 스칼라 값 함수이고 $\\theta$ 및 $\\phi$는 스칼라 매개변수입니다.\n",
        "\n",
        "*[$A$는 과도하게 결정됨: $\\phi_0$에 대해 $A$ 함수는 모든 $\\theta$에서 $\\int p_{\\text{OEF}(m, T)}(y\\ |\\ \\theta, \\phi=\\phi_0), dy = 1$라는 제약 조건에 의해 완전히 결정됩니다. $\\phi_0$의 다른 값으로 생성된 $A$는 모드 동일해야 하며, 이에 따라 $m$ 및 $T$에 제약 조건이 부여됩니다.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgpoijwPf7TV"
      },
      "source": [
        "### 충분한 통계량의 평균과 분산\n",
        "\n",
        "\"로그 분할 함수의 도함수에 대한 보조 정리\"와 동일한 조건에서 다음과 같습니다.\n",
        "\n",
        "# $$ \\mathbb{E}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[ T(Y) \\right]\n",
        "\n",
        "A'(\\theta) $$\n",
        "\n",
        "그리고\n",
        "\n",
        "# $$ \\text{Var}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[ T(Y) \\right]\n",
        "\n",
        "\\phi A''(\\theta). $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyf51flphGOK"
      },
      "source": [
        "#### 증명\n",
        "\n",
        "\"로그 분할 도함수의 미분에 대한 보조 정리\"에 의해 다음과 같습니다.\n",
        "\n",
        "# $$ \\mathbb{E}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[ \\frac{T(Y)}{\\phi} \\right]\n",
        "\n",
        "\\frac{A'(\\theta)}{\\phi} $$\n",
        "\n",
        "그리고\n",
        "\n",
        "# $$ \\text{Var}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta, \\phi)} \\left[ \\frac{T(Y)}{\\phi} \\right]\n",
        "\n",
        "\\frac{A''(\\theta)}{\\phi}. $$\n",
        "\n",
        "그러면 기대치가 선형($\\mathbb{E}[aX] = a\\mathbb{E}[X]$)이고 분산이 2차 동차($\\text{Var}[aX] = a^2 ,\\text{Var}[X]$)라는 사실로부터 결과가 나옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYOnAZv9d4XH"
      },
      "source": [
        "## 일반화된 선형 모델\n",
        "\n",
        "일반화된 선형 모델에서 반응 변수 $Y$에 대한 예측 분포는 관측된 예측 변수 $x$의 벡터와 연관됩니다. 분포는 과분산 지수군의 구성원이고 매개변수 $\\theta$는 $h(\\eta)$로 대체됩니다. 여기서 $h$는 알려진 함수이고 $\\eta := x^\\top \\beta$는 이른바 **선형 응답**이며, $\\beta$는 학습할 매개변수(회귀 계수)의 벡터입니다. 일반적으로 분산 매개변수 $\\phi$도 학습할 수 있지만 여기서의 설정에서는 $\\phi$를 알려진 대로 취급합니다. 그래서 설정은 다음과 같습니다.\n",
        "\n",
        "$$ Y \\sim p_{\\text{OEF}(m, T)}(\\cdot, |, \\theta = h(\\eta), \\phi) $$\n",
        "\n",
        "여기서 모델 구조는 분포 $p_{\\text{OEF}(m, T)}$와 선형 응답을 매개변수로 변환하는 함수 $h$로 특징지어집니다.\n",
        "\n",
        "전통적으로, 선형 응답 $\\eta$에서 평균 $\\mu := \\mathbb{E} *{Y \\sim p* {\\text{OEF}(m, T)}(\\cdot, |, \\theta = h (\\eta), \\phi)}\\left[ Y\\right]$로의 매핑은 다음으로 표기됩니다.\n",
        "\n",
        "$$ \\mu = g^{-1}(\\eta). $$\n",
        "\n",
        "이 매핑은 일대일이어야 하며 그 반대인 $g$를 이 GLM에 대한 **링크 함수**라고 합니다. 일반적으로 GLM은 연결 함수와 분포군의 이름을 지정하여 설명합니다(예: \"Bernoulli 분포 및 로짓 연결 함수가 있는 GLM\"(로지스틱 회귀 모델이라고도 함)). GLM을 완전히 특성화하려면 $h$ 함수도 지정해야 합니다. $h$가 ID이면 $g$는 **표준 링크 함수**라고 말합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-mrWHH2-wtv"
      },
      "source": [
        "### 주장: 충분한 통계량의 측면에서 $h'$ 표현하기\n",
        "\n",
        "다음을 정의합니다.\n",
        "\n",
        "$$ {\\text{Mean}*T}(\\eta) := \\mathbb{E}*{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(\\eta), \\phi)} \\left[ T(Y) \\right] $$\n",
        "\n",
        "그리고\n",
        "\n",
        "$$ {\\text{Var}*T}(\\eta) := \\text{Var}*{Y \\sim p_{\\text{OEF}(m, T)}(\\cdot | \\theta = h(\\eta), \\phi)} \\left[ T(Y) \\right]. $$\n",
        "\n",
        "그러면 다음을 얻습니다.\n",
        "\n",
        "$$ h'(\\eta) = \\frac{\\phi, {\\text{Mean}_T}'(\\eta)}{{\\text{Var}_T}(\\eta)}. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z36iGKlf_-3F"
      },
      "source": [
        "#### 증명\n",
        "\n",
        "\"충분한 통계량의 평균과 분산\"에 의해 다음을 얻습니다.\n",
        "\n",
        "$$ {\\text{Mean}_T}(\\eta) = A'(h(\\eta)). $$\n",
        "\n",
        "연쇄 법칙으로 미분하면 $$ {\\text{Mean}_T}'(\\eta) = A''(h(\\eta)), h'(\\eta), $$를 얻습니다.\n",
        "\n",
        "그리고 \"충분한 통계량의 평균과 분산\"에 의해 다음과 같습니다.\n",
        "\n",
        "$$ \\cdots = \\frac{1}{\\phi} {\\text{Var}_T}(\\eta)\\ h'(\\eta). $$\n",
        "\n",
        "결론은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8LV_QHPx-wV"
      },
      "source": [
        "## 데이터에 GLM 매개변수 피팅\n",
        "\n",
        "위에서 파생된 속성은 GLM 매개변수 $\\beta$를 데이터세트에 피팅하는 데 매우 적합합니다. Fisher 점수와 같은 준뉴턴 방법은 로그 가능도의 그래디언트와 Fisher 정보에 의존하며, 이제 GLM에 대해 특히 효율적으로 계산할 수 있음을 보여줍니다.\n",
        "\n",
        "예측 변수 벡터 $x_i$ 및 연관된 스칼라 응답 $y_i$를 관찰했다고 가정합니다. 행렬 형식에서 우리는 예측 변수 $\\mathbf{x}$와 응답 $\\mathbf{y}$를 관찰했다고 말할 것입니다. 여기서 $\\mathbf{x}$는 $i$번째 행이 $x_i^\\top$인 행렬이고, $\\mathbf{y}$는 $i$번째 요소가 $y_i$인 벡터입니다. 그러면 매개변수 $\\beta$의 로그 가능도는 다음과 같습니다.\n",
        "\n",
        "$$ \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{N} \\log p_{\\text{OEF}(m, T)}(y_i, |, \\theta = h(x_i^\\top \\beta), \\phi). $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aghNxiO_HFW1"
      },
      "source": [
        "### 단일 데이터 샘플의 경우\n",
        "\n",
        "표기법을 단순화하기 위해 먼저 단일 데이터 포인트인 $N=1$의 경우를 살펴보겠습니다. 그런 다음 가산성을 통해 일반적인 경우로 확장합니다.\n",
        "\n",
        "#### 그래디언트\n",
        "\n",
        "다음을 가지고 있습니다.\n",
        "\n",
        "$$ \\begin{align*} \\ell(\\beta, ;, x, y) &amp;= \\log p_{\\text{OEF}(m, T)}(y, |, \\theta = h(x^\\top \\beta), \\phi) \\ &amp;= \\log m(y, \\phi) + \\frac{\\theta, T(y) - A(\\theta)}{\\phi}, \\quad\\text{where}\\  \\theta = h(x^\\top \\beta). \\end{align*} $$\n",
        "\n",
        "따라서 연쇄 법칙에 의해 다음과 같습니다.\n",
        "\n",
        "$$ \\nabla_\\beta \\ell(\\beta, ; , x, y) = \\frac{T(y) - A'(\\theta)}{\\phi}, h'(x^\\top \\beta), x. $$\n",
        "\n",
        "이와 별도로 \"충분한 통계량의 평균과 분산\"에 의해 $A'(\\theta) = {\\text{Mean}_T}(x^\\top \\beta)$가 됩니다. 따라서 \"주장: 충분한 통계량의 관점에서 $h'$ 표현\"에 의해 다음과 같습니다.\n",
        "\n",
        "$$ \\cdots = \\left(T(y) - {\\text{Mean}_T}(x^\\top \\beta)\\right) \\frac{{\\text{Mean}_T}'(x^\\top \\beta)}{{\\text{Var}_T}(x^\\top \\beta)} ,x. $$\n",
        "\n",
        "#### Hessian\n",
        "\n",
        "두 번째 미분하면 곱의 법칙에 의해 다음을 얻습니다.\n",
        "\n",
        "$$ \\begin{align*} \\nabla_\\beta^2 \\ell(\\beta, ;, x, y) &amp;= \\left[ -A''(h(x^\\top \\beta)), h'(x^\\top \\beta) \\right] h'(x^\\top \\beta), x x^\\top + \\left[ T(y) - A'(h(x^\\top \\beta)) \\right] h''(x^\\top \\beta), xx^\\top ] \\ &amp;= \\left( -{\\text{Mean}_T}'(x^\\top \\beta), h'(x^\\top \\beta) + \\left[T(y) - A'(h(x^\\top \\beta))\\right] \\right), x x^\\top. \\end{align*} $$\n",
        "\n",
        "#### Fisher 정보\n",
        "\n",
        "\"충분한 통계량의 평균과 분산\"에 의해 다음을 얻습니다.\n",
        "\n",
        "$$ \\mathbb{E}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x^\\top \\beta), \\phi)} \\left[ T(y) - A'(h(x^\\top \\beta)) \\right] = 0. $$\n",
        "\n",
        "따라서\n",
        "\n",
        "$$ \\begin{align*} \\mathbb{E}*{Y \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x^\\top \\beta), \\phi)} \\left[ \\nabla_\\beta^2 \\ell(\\beta, ;, x, y) \\right] &amp;= -{\\text{Mean}_T}'(x^\\top \\beta), h'(x^\\top \\beta) x x^\\top \\ &amp;= -\\frac{\\phi, {\\text{Mean}_T}'(x^\\top \\beta)^2}{{\\text{Var}_T}(x^\\top \\beta)}, x x^\\top. \\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrA1A583HOng"
      },
      "source": [
        "### 여러 데이터 샘플의 경우\n",
        "\n",
        "이제 $N=1$ 케이스를 일반 케이스로 확장합니다. $\\boldsymbol{\\eta} := \\mathbf{x} \\beta$는 $i$번째 좌표가 $i$번째 데이터 샘플의 선형 응답인 벡터를 나타낸다고 하겠습니다. $\\mathbf{T}$(해당 ${\\textbf{Mean}_T}$, 해당 ${\\textbf{Var}_T}$)는 스칼라 값 함수 $T$(해당 ${\\text{Mean}_T}$, 해당 ${\\text{Var}_T}$)를 각 좌표에 적용하는 브로드캐스트된(벡터화된) 함수를 나타낸다고 하겠습니다. 그러면 다음과 같습니다.\n",
        "\n",
        "$$ \\begin{align*} \\nabla_\\beta \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{y}) &amp;= \\sum_{i=1}^{N} \\nabla_\\beta \\ell(\\beta, ;, x_i, y_i) \\ &amp;= \\sum_{i=1}^{N} \\left(T(y) - {\\text{Mean}_T}(x_i^\\top \\beta)\\right) \\frac{{\\text{Mean}_T}'(x_i^\\top \\beta)}{{\\text{Var}_T}(x_i^\\top \\beta)} , x_i \\ &amp;= \\mathbf{x}^\\top ,\\text{diag}\\left(\\frac{ {\\textbf{Mean}_T}'(\\mathbf{x} \\beta) }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta) }\\right) \\left(\\mathbf{T}(\\mathbf{y}) - {\\textbf{Mean}_T}(\\mathbf{x} \\beta)\\right) \\ \\end{align*} $$\n",
        "\n",
        "그리고\n",
        "\n",
        "$$ \\begin{align*} \\mathbb{E}*{Y_i \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta), \\phi)} \\left[ \\nabla_\\beta^2 \\ell(\\beta, ;, \\mathbf{x}, \\mathbf{Y}) \\right] &amp;= \\sum_{i=1}^{N} \\mathbb{E}*{Y_i \\sim p*{\\text{OEF}(m, T)}(\\cdot | \\theta = h(x_i^\\top \\beta), \\phi)} \\left[ \\nabla_\\beta^2 \\ell(\\beta, ;, x_i, Y_i) \\right] \\ &amp;= \\sum_{i=1}^{N} -\\frac{\\phi, {\\text{Mean}_T}'(x_i^\\top \\beta)^2}{{\\text{Var}_T}(x_i^\\top \\beta)}, x_i x_i^\\top \\ &amp;= -\\mathbf{x}^\\top ,\\text{diag}\\left( \\frac{ \\phi, {\\textbf{Mean}_T}'(\\mathbf{x} \\beta)^2 }{ {\\textbf{Var}_T}(\\mathbf{x} \\beta) }\\right), \\mathbf{x}, \\end{align*} $$\n",
        "\n",
        "여기서 분수는 요소별 나눗셈을 나타냅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUrOmdt395hZ"
      },
      "source": [
        "## 수식을 수치적으로 검증하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVp59IBW-TK6"
      },
      "source": [
        "이제 로그 가능도 그래디언트에 대한 위의 수식을 `tf.gradients`를 사용하여 수치적으로 검증하고, Fisher 정보 수식을 `tf.hessians`를 사용하여 Monte Carlo 추정으로 검증합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM-HDPdPepE-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coordinatewise relative error between naively computed gradients and formula-based gradients (should be zero):\n",
            "[[2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]\n",
            " [1.96118673e-16 3.13789877e-16 1.96118673e-16]\n",
            " [2.08845965e-16 1.67076772e-16 2.08845965e-16]]\n",
            "\n",
            "Coordinatewise relative error between average of naively computed Hessian and formula-based FIM (should approach zero as num_trials -> infinity):\n",
            "[[0.00072369 0.00072369 0.00072369]\n",
            " [0.00072369 0.00072369 0.00072369]\n",
            " [0.00072369 0.00072369 0.00072369]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def VerifyGradientAndFIM():\n",
        "  model = tfp.glm.BernoulliNormalCDF()\n",
        "  model_matrix = np.array([[1., 5, -2],\n",
        "                           [8, -1, 8]])\n",
        "\n",
        "  def _naive_grad_and_hessian_loss_fn(x, response):\n",
        "    # Computes gradient and Hessian of negative log likelihood using autodiff.\n",
        "    predicted_linear_response = tf.linalg.matvec(model_matrix, x)\n",
        "    log_probs = model.log_prob(response, predicted_linear_response)\n",
        "    grad_loss = tf.gradients(-log_probs, [x])[0]\n",
        "    hessian_loss = tf.hessians(-log_probs, [x])[0]\n",
        "    return [grad_loss, hessian_loss]\n",
        "\n",
        "  def _grad_neg_log_likelihood_and_fim_fn(x, response):\n",
        "    # Computes gradient of negative log likelihood and Fisher information matrix\n",
        "    # using the formulas above.\n",
        "    predicted_linear_response = tf.linalg.matvec(model_matrix, x)\n",
        "    mean, variance, grad_mean = model(predicted_linear_response)\n",
        "\n",
        "    v = (response - mean) * grad_mean / variance\n",
        "    grad_log_likelihood = tf.linalg.matvec(model_matrix, v, adjoint_a=True)\n",
        "    w = grad_mean**2 / variance\n",
        "\n",
        "    fisher_info = tf.linalg.matmul(\n",
        "        model_matrix,\n",
        "        w[..., tf.newaxis] * model_matrix,\n",
        "        adjoint_a=True)\n",
        "    return [-grad_log_likelihood, fisher_info]\n",
        "\n",
        "  @tf.function(autograph=False)\n",
        "  def compute_grad_hessian_estimates():\n",
        "    # Monte Carlo estimate of E[Hessian(-LogLikelihood)], where the expectation is\n",
        "    # as written in \"Claim (Fisher information)\" above.\n",
        "    num_trials = 20\n",
        "    trial_outputs = []\n",
        "    np.random.seed(10)\n",
        "    model_coefficients_ = np.random.random(size=(model_matrix.shape[1],))\n",
        "    model_coefficients = tf.convert_to_tensor(model_coefficients_)\n",
        "    for _ in range(num_trials):\n",
        "      # Sample from the distribution of `model`\n",
        "      response = np.random.binomial(\n",
        "          1,\n",
        "          scipy.stats.norm().cdf(np.matmul(model_matrix, model_coefficients_))\n",
        "      ).astype(np.float64)\n",
        "      trial_outputs.append(\n",
        "          list(_naive_grad_and_hessian_loss_fn(model_coefficients, response)) +\n",
        "          list(\n",
        "              _grad_neg_log_likelihood_and_fim_fn(model_coefficients, response))\n",
        "      )\n",
        "\n",
        "    naive_grads = tf.stack(\n",
        "        list(naive_grad for [naive_grad, _, _, _] in trial_outputs), axis=0)\n",
        "    fancy_grads = tf.stack(\n",
        "        list(fancy_grad for [_, _, fancy_grad, _] in trial_outputs), axis=0)\n",
        "\n",
        "    average_hess = tf.reduce_mean(tf.stack(\n",
        "        list(hess for [_, hess, _, _] in trial_outputs), axis=0), axis=0)\n",
        "    [_, _, _, fisher_info] = trial_outputs[0]\n",
        "    return naive_grads, fancy_grads, average_hess, fisher_info\n",
        "  \n",
        "  naive_grads, fancy_grads, average_hess, fisher_info = [\n",
        "      t.numpy() for t in compute_grad_hessian_estimates()]\n",
        "\n",
        "  print(\"Coordinatewise relative error between naively computed gradients and\"\n",
        "        \" formula-based gradients (should be zero):\\n{}\\n\".format(\n",
        "            (naive_grads - fancy_grads) / naive_grads))\n",
        "\n",
        "  print(\"Coordinatewise relative error between average of naively computed\"\n",
        "        \" Hessian and formula-based FIM (should approach zero as num_trials\"\n",
        "        \" -> infinity):\\n{}\\n\".format(\n",
        "                (average_hess - fisher_info) / average_hess))\n",
        "    \n",
        "VerifyGradientAndFIM()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAiNubQ-WDHN"
      },
      "source": [
        "# 참고 자료\n",
        "\n",
        "<a name=\"1\"></a>[1]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for L1-regularized Logistic Regression. *Journal of Machine Learning Research*, 13, 2012. http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf\n",
        "\n",
        "<a name=\"2\"></a>[2]: skd. Derivation of Soft Thresholding Operator.  2018. https://math.stackexchange.com/q/511106\n",
        "\n",
        "<a name=\"3\"></a>[3]: Wikipedia Contributors. Proximal gradient methods for learning. *Wikipedia, The Free Encyclopedia*, 2018. https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n",
        "\n",
        "<a name=\"4\"></a>[4]: Yao-Liang Yu. The Proximity Operator. https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Generalized_Linear_Models.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
