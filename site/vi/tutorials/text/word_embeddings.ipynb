{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GE91qWZkm8ZQ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "YS3NA-i6nAFC"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SN5USFEIIK3"
   },
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aojnnc7sXrab"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/word_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Cộng đồng TensorFlow Việt Nam đã dịch các tài liệu này từ nguyên bản tiếng Anh.\n",
    "Vì bản dịch này dựa trên sự cố gắng từ các tình nguyện viên, nên không thể đám bảo luôn bám sát\n",
    "[Tài liệu chính thức bằng tiếng Anh](https://www.tensorflow.org/?hl=en).\n",
    "Nếu bạn có đề xuất để cải thiện bản dịch này, vui lòng tạo PR đến repository trên GitHub của [tensorflow/docs](https://github.com/tensorflow/docs)\n",
    "\n",
    "Để đăng ký dịch hoặc duyệt lại nội dung bản dịch, các bạn hãy liên hệ \n",
    "[docs@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6mJg1g3apaz"
   },
   "source": [
    "Chủ đề của hướng dẫn lần này sẽ xoay quanh word embedding. Hướng dẫn sẽ bao gồm code để huấn luyện word embedding hoàn toàn từ đầu trên một tập dữ liệu nhỏ và trực quan hóa các embedding thu được với [Embedding Projector](http://projector.tensorflow.org) (như hình bên dưới).\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding.jpg?raw=1\" alt=\"Screenshot of the embedding projector\" width=\"400\"/>\n",
    " \n",
    "## Biểu diễn văn bản dưới dạng số\n",
    "\n",
    "Các mô hình máy học nhận đầu vào là các vector (dãy các số). Khi làm việc với văn bản, điều đầu tiên chúng ta cần thực hiện là tìm cách chuyển đổi các chuỗi kí tự sang số (hay \"vector hóa\" văn bản) trước khi đưa vào mô hình. Trong phần này, chúng ta sẽ đi qua ba cách để thực hiện mục tiêu trên.\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "Cách làm đầu tiên chúng ta có thể nghĩ tới là encode \"one-hot\" từng từ trong danh sách từ vựng. Xét câu \"The cat sat on the mat\". Danh sách từ vựng (hay các từ phân biệt) trong câu này là (cat, mat, on, sat, the). Để biểu diễn mỗi từ, ta tạo một vector 0 có độ dài bằng với số lượng từ trong danh sách từ vựng và thay 1 vào vị trí tương ứng của từ cần biểu diễn. Cách làm này được minh họa bằng sơ đồ bên dưới.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1\" alt=\"Diagram of one-hot encodings\" width=\"400\" />\n",
    "\n",
    "Để  thu được vector encode của một câu, ta chỉ việc nối các vector one-hot của từng từ trong câu.\n",
    "\n",
    "Lưu ý: Cách làm này không hiệu quả vì các vector thu được từ phương pháp one-hot encoding đều là các vector thưa (nghĩa là hầu hết các phần tử đều là 0). Hãy tưởng tượng nếu chúng ta có 10,000 từ trong danh sách từ vựng. Để thực hiện one-hot encoding một từ, ta cần tạo ra một vector mà ở đó 99.9% các phần tử đều là 0.\n",
    "\n",
    "### Encode mỗi từ bằng một số phân biệt\n",
    "\n",
    "Cách làm thứ hai mà ta có thể thử là sử dụng các số phân biệt để encode từng từ. Xét ví dụ trên, ta có thể chọn số 1 để encode cho từ \"cat\", số 2 cho từ \"mat\" và cứ như thế. Lúc này, chúng ta có thể encode cả câu \"The cat sat on the mat\" bằng một vector dày đặc, chẳng hạn [5, 1, 4, 3, 5, 2]. Cách làm này mang lại hiệu quả hơn vì ta thu được một vector dày đặc (tất cả các phần tử đều khác 0) thay vì vector thưa.\n",
    "\n",
    "Tuy nhiên, cách encode này có hai điểm yếu:\n",
    "\n",
    "* Các số dùng để encode là hoàn toàn ngẫu nhiên (không thể hiện được mối quan hệ giữa các từ với nhau).\n",
    "\n",
    "* Encode bằng số gây khó khăn cho mô hình. Ví dụ trong thuật toán phân loại tuyến tính, với mỗi thuộc tính mô hình cần học một trọng số tương ứng. Tuy nhiên vì hai từ tương đồng nhau chưa chắc cách encode của chúng sẽ tương tự nhau, trọng số mà mô hình học được từ sự tương đồng trong cách biểu diễn các từ sẽ không còn ý nghĩa.\n",
    "\n",
    "### Word embedding\n",
    "\n",
    "Word embedding cung cấp cho ta cách thức biểu diễn từ hiệu quả và dày đặc mà ở đó các từ tương đồng sẽ được encode tương tự nhau. Hơn nữa, chúng ta sẽ không phải trực tiếp xác định cách encode. Mỗi embedding là một vector số thực dày đặc (độ dài của vector sẽ do người dùng xác định). Thay vì trực tiếp xác định các giá trị cho embedding, các giá trị này hoàn toàn có thể huấn luyện được (bằng cách cho mô hình học các trọng số trong quá trình huấn luyện, tương tự như cách học các trọng số đối với lớp dense). Word embedding thường được sử dụng có số chiều là 8 (đối với các tập dữ liệu nhỏ) và có thể lên đến 1024 khi làm việc với các tập dữ liệu lớn. Embedding có số chiều càng lớn thì càng có khả năng biểu diễn được các mối quan hệ \"sâu\" giữa các từ, tuy nhiên cũng đòi hỏi một lượng lớn dữ liệu để học. \n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1\" alt=\"Diagram of an embedding\" width=\"400\"/>\n",
    "\n",
    "Trên đây là sơ đồ của một word embedding. Mỗi từ được biểu diễn bằng một vector số thực 4 chiều. Ta cũng có thể hình dung word embedding như một \"bảng tra\". Sau khi các trọng số đã được học xong, ta có thể encode từng từ bằng cách tra vào bảng để thu được vector dày đặc là biểu diễn của từ tương ứng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZUQErGewZxE"
   },
   "source": [
    "## Chuẩn bị"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SIXEk5ON5P7h"
   },
   "outputs": [],
   "source": [
    "!pip install tf-nightly\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eqBazMiVQkj1"
   },
   "source": [
    "## Sử dụng lớp Embedding\n",
    "Thư viện Keras giúp việc sử dụng word embedding trở nên vô cùng đơn giản. Sau đây, chúng ta sẽ cùng tìm hiểu về [lớp Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) được hỗ trợ trong thư viện này.\n",
    "\n",
    "Lớp Embedding có thể được hiểu như một bảng tra cho phép ánh xạ từ một chỉ mục (đại diện cho một từ nhất định) sang một vector dày đặc (embedding của từ đó). Số chiều (hay độ dài) của embedding là tham số mà bạn có thể tùy biến để phù hợp với bài toán của mình, tương tự như việc tùy biển số lượng neuron khác nhau trong một lớp Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OjxLVrMvWUE"
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dKKV1L2Rk7e"
   },
   "source": [
    "Khi khởi tạo một lớp Embedding, các trọng số hỗ trợ việc embedding sẽ được gán các giá trị ngẫu nhiên (tương tự như quá trình khởi tạo ở các lớp khác). Các trọng số này sẽ dần được điều chỉnh thông qua quá trình truyền ngược ở giai đoạn huấn luyện mô hình. Sau khi hoàn tất huấn luyện, các word embedding có thể encode sự tương đồng giữa các từ một cách tương đối (kết quả của việc huấn luyện mô hình trên một bài toán cụ thể).\n",
    "\n",
    "Nếu truyền một số nguyên vào lớp embedding này, ta sẽ thu được một vector tương ứng thuộc bảng embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YUjPgP7w0PO"
   },
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([1,2,3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O4PC4QzsxTGx"
   },
   "source": [
    "Đối với các bài toán xử lí văn bản hay chuỗi văn bản liên tiếp (sequence), lớp Embedding nhận đầu vào là một tensor số nguyên 2 chiều có kích thước `(số lượng mẫu, độ dài chuỗi)`. Các chuỗi trong dữ liệu đầu vào có thể có độ dài bất kì. Ví dụ, ta có thể đưa vào lớp embedding trên một batch có kích thước `(32, 10)` (gồm 32 chuỗi có độ dài 10) hoặc `(64, 15)` (gồm 64 chuỗi có độ dài 15).\n",
    "\n",
    "Tensor trả về sẽ có nhiều hơn một chiều so với dữ liệu đầu vào. Tập hợp các vector embedding được xếp vào chiều cuối cùng. Chẳng hạn, nếu ta đưa vào một batch có kích thước `(2, 3)` thì sẽ nhận được kết quả với kích thước `(2, 3, N)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwSYepRjyRGy"
   },
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGQp2N92yOyB"
   },
   "source": [
    "<!-- When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. You could use an RNN, Attention, or pooling layer before passing it to a Dense layer. This tutorial uses pooling because it's simplest. The [Text Classification with an RNN](text_classification_rnn.ipynb) tutorial is a good next step. -->\n",
    "\n",
    "Khi nhận đầu vào là một batch các chuỗi, lớp embedding sẽ trả về một tensor số thực 3 chiều có kích thước `(số lượng mẫu, độ dài chuỗi, số chiều embedding)`. Có nhiều cách hỗ trợ chuyển đổi chuỗi có độ dài bất kì trên sang một biểu diễn có kích thước cố định để đưa vào lớp dense: sử dụng RNN, Attention hoặc lớp pooling. Hướng dẫn này sử dụng lớp pooling vì đây được xem phương pháp đơn giản nhất. Ngoài ra, bạn có thể tìm hiểu thêm ở [Hướng dẫn phân loại văn bản với RNN](text_classification_rnn.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGicgV5qT0wh"
   },
   "source": [
    "## Học embedding từ đầu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Bh8B1TUT6mV"
   },
   "source": [
    "Trong hướng dẫn này, chúng ta sẽ tiến hành huấn luyện mô hình phân loại sắc thái dựa trên các bình luận phim ở trang IMDB. Mô hình sẽ học cách embedding hoàn toàn từ đầu. Chúng ta sẽ phải thực hiện tiền xử lí trên bộ dữ liệu được cho.\n",
    "\n",
    "Xem thêm về cách tải dữ liệu văn bản tại [Hướng dẫn tải dữ liệu văn bản](../load_data/text.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yg6tyxPtp1TE"
   },
   "outputs": [],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    'imdb_reviews/subwords8k', \n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST), \n",
    "    with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjnBsFXaLVPL"
   },
   "source": [
    "Để lấy danh sách từ vựng, ta truy cập vào encoder của tập dữ liệu (`tfds.features.text.SubwordTextEncoder`).\n",
    "\n",
    "Kí hiệu \"\\_\" được dùng để biểu diễn khoảng trắng. Danh sách từ vựng sẽ bao gồm cả các từ toàn vẹn (kết thúc bằng \"\\_\") lẫn các từ không toàn vẹn (có thể ghép với nhau tạo thành từ có nghĩa): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYrsTgxhLBfl"
   },
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "encoder.subwords[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwCTfSG63Qth"
   },
   "source": [
    "<!-- Movie reviews can be different lengths. We will use the `padded_batch` method to standardize the lengths of the reviews. -->\n",
    "\n",
    "Bình luận phim có thể có độ dài khác nhau. Do đó, chúng ta sẽ sử dụng `padded_batch` để chuẩn hóa độ dài của các bình luận."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRSnJkx4cs9P"
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwSCxER_2Lef"
   },
   "outputs": [],
   "source": [
    "train_batches = train_data.shuffle(1000).padded_batch(10, padded_shapes=([None],[]))\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10, padded_shapes=([None],[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upiYr1-Dc7CF"
   },
   "source": [
    "Lưu ý: Phiên bản **TensorFlow 2.2** không yêu cầu khai báo tham số padded_shapes. Ở chế độ mặc định, dữ liệu đầu vào sẽ được đệm thêm các phần tử sao cho tất cả dữ liệu ở mọi chiều đều có độ dài bằng nhau và bằng với độ dài lớn nhất theo chiều được xét của batch đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZucJ_jzoc6Sv"
   },
   "outputs": [],
   "source": [
    "train_batches = train_data.shuffle(1000).padded_batch(10)\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dF8ORMt2U9lj"
   },
   "source": [
    "Như mong đợi, các bình luận dưới dạng văn bản đã được encode thành các số nguyên (mỗi số nguyên đại diện cho một từ toàn vẹn hoặc không toàn vẹn trong danh sách từ vựng).\n",
    "\n",
    "Các số 0 được thêm vào ở cuối là kết quả của việc chuẩn hóa độ dài mặc định đã được đề cập ở trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Se-phCknsoan"
   },
   "outputs": [],
   "source": [
    "train_batch, train_labels = next(iter(train_batches))\n",
    "train_batch.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zI9_wLIiWO8Z"
   },
   "source": [
    "### Xây dựng mô hình đơn giản\n",
    "\n",
    "Để định nghĩa kiến trúc mô hình, ta sử dụng [Keras Sequential API](../../guide/keras). Trong hướng dẫn này này, ta sẽ xây dựng một mô hình đơn giản theo kiểu \"Continous bag of words\":\n",
    "\n",
    "* Lớp Embedding nhận danh sách từ vựng đã được encode bằng số và thực hiện tìm kiếm vector embedding đối với mỗi từ. Các vector này được học thông qua quá trình huấn luyện. Tập hợp các vector embedding thu tạo thành một chiều mới trong dữ liệu đầu ra. Kết quả thu được sẽ có kích thước `(kích thước batch, độ dài chuỗi, số chiều embedding)`.\n",
    "\n",
    "* Với mỗi mẫu trong batch, lớp GlobalAveragePooling1D sẽ trả về một vector có kích thước cố thước cố định là kết quả của việc tính trung bình dọc theo độ dài chuỗi. Đây được xem là cách làm đơn giản nhất để  xử lý dữ liệu đầu vào có kích thước không cố định.\n",
    "\n",
    "* Các vector (lúc này đều đã tương đồng nhau về mặt kích thước) tiếp tục được đưa vào một lớp Dense gồm 16 unit ẩn.\n",
    "\n",
    "* Lớp cuối cùng sẽ chứa các kết nối hoàn toàn tới một node đầu ra duy nhất. Hàm kích hoạt được sử dụng là hàm sigmoid. Kết quả trả về sẽ là một số thực nằm trong khoảng từ 0 đến 1. Số thực này đại diện cho xác suất (hay độ tin cậy) một bình luận mang ý nghĩa tích cực.\n",
    "\n",
    "Chú ý: Mô hình trên không sử dụng kỹ thuật mask. Do đó, các phần tử 0 thêm vào trong quá trình chuẩn hóa độ dài cũng được xem như một phần của dữ liệu đầu vào. Như vậy, kết quả đầu ra có thể bị ảnh huởng bởi độ dài của phần đệm. Để khắc phục, xem thêm [Hướng dẫn mask và đệm dữ liệu](../../guide/keras/masking_and_padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHLcFtn5Wsqj"
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjLNgKO7W2fe"
   },
   "source": [
    "### Biên dịch và huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCUgdP69Wzix"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=10,\n",
    "    validation_data=test_batches, validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQjpKVYTXU-1"
   },
   "source": [
    "Với cách làm đã nêu, mô hình đạt độ chính xác khoảng 88% trên tập kiểm thử (để ý rằng mô hình của ta đang bị overfit vì độ chính xác thu được trên tập huấn luyện cao hơn rất nhiều)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0D3OTmOT1z1O"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss=history_dict['loss']\n",
    "val_loss=history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCoA6qwqP836"
   },
   "source": [
    "## Lấy các embedding đã được học\n",
    "\n",
    "Tiếp theo, ta sẽ lấy các word embedding đã được học trong quá trình huấn luyện. Tập hợp các embedding này là một ma trận có kích thước `(số từ trong danh sách từ vựng, số chiều embedding)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8WwbsXCXtpa"
   },
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8MiCA77X8B8"
   },
   "source": [
    "Thực hiện lưu lại các trọng số này. Biết rằng để trực quan hóa các embedding thu được với [Embedding Projector](http://projector.tensorflow.org), ta cần hai tệp dữ liệu: một tệp chứa các vector embedding và một tệp chứa meta data (là các từ trong danh sách từ vựng)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsjempweP9Lq"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(encoder.subwords):\n",
    "  vec = weights[num+1] # skip 0, it's padding.\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQyMZWyxYjMr"
   },
   "source": [
    "Nếu đang chạy hướng dẫn này trên [Colaboratory](https://colab.research.google.com), bạn có thể sử dụng đoạn chương trình sau để tải các tệp này về máy (hoặc sử dụng trình duyệt tệp bằng cách chọn *View -> Table of contents -> File browser*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gFbbMmvYvhp"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "   pass\n",
    "else:\n",
    "  files.download('vecs.tsv')\n",
    "  files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXLfFA54Yz-o"
   },
   "source": [
    "## Trực quan hóa các embedding\n",
    "\n",
    "Để thực hiện trực quan hóa các embedding thu được, trước tiên ta cần tải chúng lên chương trình embedding projector.\n",
    "\n",
    "Mở [Embedding Projector](http://projector.tensorflow.org/) (hoặc chạy TensorBoard ở local).\n",
    "\n",
    "* Chọn \"Load data\".\n",
    "\n",
    "* Tải lên hai tệp mà ta đã tạo ở trên gồm: `vecs.tsv` và `meta.tsv`.\n",
    "\n",
    "<!-- The embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\". \n",
    "\n",
    "Note: your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer.\n",
    "\n",
    "Note: experimentally, you may be able to produce more interpretable embeddings by using a simpler model. Try deleting the `Dense(16)` layer, retraining the model, and visualizing the embeddings again. -->\n",
    "\n",
    "Lúc này các embedding đã huấn luyện sẽ được hiển thị lên màn hình. Bạn có thể sử dụng chức năng tìm kiếm từ để xem các từ khác nằm gần nó nhất. Chẳng hạn, nếu thử với từ \"beautiful\", ta sẽ thấy các từ gần đó chẳng hạn như \"wonderful\".\n",
    "\n",
    "Lưu ý: Kết quả mỗi người thu được có thể khác nhau đôi chút tùy thuộc vào các trọng số được khởi tạo ngẫu nhiên trước khi huấn luyện lớp embedding.\n",
    "\n",
    "Lưu ý: Một số thực nghiệm sử dụng mô hình đơn giản hơn có thể cho kết quả embedding tốt hơn. Hãy thử xóa lớp `Dense (16)`, huấn luyện lại mô hình và trực quan hóa các embedding một lần nữa để kiểm tra.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding.jpg?raw=1\" alt=\"Screenshot of the embedding projector\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iS_uMeMw3Xpj"
   },
   "source": [
    "## Tiếp theo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSgAZpwF5xF_"
   },
   "source": [
    "<!-- This tutorial has shown you how to train and visualize word embeddings from scratch on a small dataset.\n",
    "\n",
    "* To learn about recurrent networks see the [Keras RNN Guide](../../guide/keras/rnn.ipynb).\n",
    "\n",
    "* To learn more about text classification (including the overall workflow, and if you're curious about when to use embeddings vs one-hot encodings) we recommend this practical text classification [guide](https://developers.google.com/machine-learning/guides/text-classification/step-2-5). -->\n",
    "\n",
    "Trong hướng dẫn này, chúng ta đã đi qua các bước để huấn luyện và trực quan hóa các word embedding từ đầu trên một tập dữ liệu nhỏ.\n",
    "\n",
    "* Để tìm hiểu thêm về mạng hồi quy, xem thêm tại [Keras RNN Guide](../../guide/keras/rnn.ipynb).\n",
    "\n",
    "* Để tìm hiểu thêm về bài toán phân loại văn bản (bao gồm cách thức thực hiện hoặc cho những bạn tò mò khi nào nên sử dụng embedding và khi nào nên sử dụng one-hot encoding), hãy xem qua [Hướng dẫn phân loại văn bản](https://developers.google.com/machine-learning/guides/text-classification/step-2-5)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
