{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdeKOEkv1Fe8"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c2jyGuiG1gHr"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW"
      },
      "source": [
        "# TFX Estimator 组件教程\n",
        "\n",
        "***TensorFlow Extended (TFX) 逐组件简介***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LidV2qsXm4XC"
      },
      "source": [
        "注：我们建议在 Colab 笔记本中运行本教程，无需进行设置！只需点击“在 Google Colab 中运行”\n",
        "\n",
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://tensorflow.google.cn/tfx/tutorials/tfx/components\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在  TensorFlow.org 上查看</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tfx/tutorials/tfx/components.ipynb\"> <img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tfx/tutorials/tfx/components.ipynb\"><img width=\"32px\" src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 Github 上查看源代码</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/tfx/tutorials/tfx/components.ipynb\"><img width=\"32px\" src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a></td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBbTLeWmWs8q"
      },
      "source": [
        "> 警告：不建议将 Estimator 用于新代码。Estimator 运行 `v1.Session` 风格的代码，此类代码更加难以正确编写，并且可能会出现意外行为，尤其是与 TF 2 代码结合使用时。Estimator 确实在我们的[兼容性保证](https://tensorflow.org/guide/versions)范围内，但除了安全漏洞之外不会得到任何修复。请参阅[迁移指南](https://tensorflow.org/guide/migrate)以了解详情。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAD1tLoTm_QS"
      },
      "source": [
        "此基于 Colab 的教程将以交互方式演练 TensorFlow Extended (TFX) 的每个内置组件。\n",
        "\n",
        "教程涵盖了端到端机器学习流水线中的每个步骤，从数据提取到将模型推送到应用。\n",
        "\n",
        "完成后，此笔记本的内容可以自动导出为 TFX 流水线源代码，您可以使用 Apache Airflow 和 Apache Beam 对其进行编排。\n",
        "\n",
        "注：此笔记本及其关联的 API 是**实验性的**，并且正在积极开发中。功能、行为和演示都将发生重大变更。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfSQ-kX-MLEr"
      },
      "source": [
        "## 背景\n",
        "\n",
        "此笔记本演示如何在 Jupyter/Colab 环境中使用 TFX。在这里，我们将在交互式笔记本中演练芝加哥出租车示例。\n",
        "\n",
        "在交互式笔记本中工作是熟悉 TFX 流水线结构的有用方式。将您自己的流水线作为轻量级开发环境进行开发时，它也十分有用，但您应该意识到，交互式笔记本的编排方式与它们访问元数据工件的方式存在差异。\n",
        "\n",
        "### 编排\n",
        "\n",
        "在 TFX 的生产部署中，您将使用编排器（如 Apache Airflow、Kubeflow Pipelines 或 Apache Beam）来编排 TFX 组件的预定义流水线计算图。在交互式笔记本中，笔记本自身就是编排器，会在执行笔记本代码单元时运行每个 TFX 组件。\n",
        "\n",
        "### 元数据\n",
        "\n",
        "在 TFX 的生产部署中，您将通过 ML Metadata (MLMD) API 访问元数据。MLMD 会将元数据属性存储在数据库（如 MySQL 或 SQLite）中，并将元数据载荷存储在永久性存储空间（如文件系统）中。在交互式笔记本中，属性和载荷都存储在 Jupyter 笔记本或 Colab 服务器上 `/tmp` 目录下的临时 SQLite 数据库中。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GivNBNYjb3b"
      },
      "source": [
        "## 设置\n",
        "\n",
        "首先，我们安装并导入必要的软件包，设置路径，并下载数据。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDl_6DkqJ-pG"
      },
      "source": [
        "### 升级 Pip\n",
        "\n",
        "为了避免在本地运行时升级系统中的 Pip，请检查以确保我们在 Colab 中运行。当然，可以单独升级本地系统。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFhBChv4J_PD"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZOYTt1RW4TK"
      },
      "source": [
        "### 安装 TFX\n",
        "\n",
        "**注：在 Google Colab 中，由于软件包更新，第一次运行此代码单元时必须重新启动运行时 (Runtime &gt; Restart runtime ...)。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4SQA7Q5nej3"
      },
      "outputs": [],
      "source": [
        "!pip install tfx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szPQ2MDYPZ5j"
      },
      "source": [
        "## 是否已重新启动运行时？\n",
        "\n",
        "如果您使用的是 Google Colab，首次运行上面的代码单元时，必须重新启动运行时 (Runtime &gt; Restart runtime ...)。这样做的原因是 Colab 加载软件包的方式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ePgV0Lj68Q"
      },
      "source": [
        "### 导入软件包\n",
        "\n",
        "我们导入必要的软件包，包括标准 TFX 组件类。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIqpWK9efviJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "tf.get_logger().propagate = False\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCZTHRy0N1D6"
      },
      "source": [
        "我们检查一下库版本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ4K18_DN2D8"
      },
      "outputs": [],
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufJKQ6OvkJlY"
      },
      "source": [
        "### 设置流水线路径"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad5JLpKbf6sN"
      },
      "outputs": [],
      "source": [
        "# This is the root directory for your TFX pip package installation.\n",
        "_tfx_root = tfx.__path__[0]\n",
        "\n",
        "# This is the directory containing the TFX Chicago Taxi Pipeline example.\n",
        "_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n",
        "\n",
        "# This is the path where your model will be pushed for serving.\n",
        "_serving_model_dir = os.path.join(\n",
        "    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n",
        "\n",
        "# Set up logging.\n",
        "absl.logging.set_verbosity(absl.logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cMMAbSkGfX"
      },
      "source": [
        "### 下载示例数据\n",
        "\n",
        "我们下载要在 TFX 流水线中使用的示例数据集。\n",
        "\n",
        "我们使用的数据集是芝加哥市发布的 [Taxi Trips 数据集](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew)。该数据集中的各列为：\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td>pickup_community_area</td>\n",
        "<td>fare</td>\n",
        "<td>trip_start_month</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>trip_start_hour</td>\n",
        "<td>trip_start_day</td>\n",
        "<td>trip_start_timestamp</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>pickup_latitude</td>\n",
        "<td>pickup_longitude</td>\n",
        "<td>dropoff_latitude</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>dropoff_longitude</td>\n",
        "<td>trip_miles</td>\n",
        "<td>pickup_census_tract</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>dropoff_census_tract</td>\n",
        "<td>payment_type</td>\n",
        "<td>company</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>trip_seconds</td>\n",
        "<td>dropoff_community_area</td>\n",
        "<td>tips</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "我们将使用此数据集构建一个预测行程 `tips` 的模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BywX6OUEhAqn"
      },
      "outputs": [],
      "source": [
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n",
        "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
        "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blZC1sIQOWfH"
      },
      "source": [
        "快速浏览一下 CSV 文件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5YPeLPFOXaD"
      },
      "outputs": [],
      "source": [
        "!head {_data_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QioyhunCImwE"
      },
      "source": [
        "*免责声明：本网站提供的应用所使用的数据来自原始源（www.cityofchicago.org，芝加哥市官方网站），但在使用时进行了修改。芝加哥市不对本网站提供的任何数据的内容、准确性、时效性或完整性承担任何责任。本网站提供的数据可能会随时更改。您了解并同意，使用本网站提供的数据须自担风险。*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ONIE_hdkPS4"
      },
      "source": [
        "### 创建 InteractiveContext\n",
        "\n",
        "最后，创建一个 InteractiveContext，我们可以通过它在此笔记本中以交互方式运行 TFX 组件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Rh6K5sUf9dd"
      },
      "outputs": [],
      "source": [
        "# Here, we create an InteractiveContext using default parameters. This will\n",
        "# use a temporary directory with an ephemeral ML Metadata database instance.\n",
        "# To use your own pipeline root or database, the optional properties\n",
        "# `pipeline_root` and `metadata_connection_config` may be passed to\n",
        "# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n",
        "# notebook.\n",
        "context = InteractiveContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdQWxfsVkzdJ"
      },
      "source": [
        "## 以交互方式运行 TFX 组件\n",
        "\n",
        "在接下来的代码单元中，我们将逐个创建 TFX 组件，运行每个组件，并呈现它们的输出工件。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9fwt9gQk3BR"
      },
      "source": [
        "### ExampleGen\n",
        "\n",
        "`ExampleGen` 组件通常位于 TFX 流水线的开始。它具有以下功能：\n",
        "\n",
        "1. 将数据集拆分为训练集和评估集（默认情况下，2/3 训练 + 1/3 评估）\n",
        "2. 将数据转换为 `tf.Example` 格式（请参阅[这里](https://tensorflow.google.cn/tutorials/load_data/tfrecord)了解详细信息）\n",
        "3. 将数据复制到 `_tfx_root` 目录供其他组件访问\n",
        "\n",
        "`ExampleGen` 将数据源的路径作为输入。在我们的示例中，这是包含下载的 CSV 文件的 `_data_root` 路径。\n",
        "\n",
        "注：在此笔记本中，我们可以逐个实例化组件，然后使用 `InteractiveContext.run()` 运行它们。相比之下，在生产设置中，我们会在 `Pipeline` 中预先指定所有组件以传递给编排器（请参阅[构建 TFX 流水线指南](https://tensorflow.google.cn/tfx/guide/build_tfx_pipeline)）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyXjuMt8f-9u"
      },
      "outputs": [],
      "source": [
        "example_gen = tfx.components.CsvExampleGen(input_base=_data_root)\n",
        "context.run(example_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqCoZh7KPUm9"
      },
      "source": [
        "我们检查一下 `ExampleGen` 的输出工件。该组件会生成两种工件，训练样本和评估样本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "880KkTAkPeUg"
      },
      "outputs": [],
      "source": [
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "print(artifact.split_names, artifact.uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6vcbW_wPqvl"
      },
      "source": [
        "我们还可以看一下前三个训练样本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4XIXjiCPwzQ"
      },
      "outputs": [],
      "source": [
        "# Get the URI of the output artifact representing the training examples, which is a directory\n",
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "# Iterate over the first 3 records and decode them.\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gluYjccf-IP"
      },
      "source": [
        "现在，`ExampleGen` 已经完成数据提取，下一步是数据分析。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csM6BFhtk5Aa"
      },
      "source": [
        "### StatisticsGen\n",
        "\n",
        "`StatisticsGen` 组件会计算数据集的统计信息，用于数据分析以及下游组件。它使用 [TensorFlow Data Validation](https://tensorflow.google.cn/tfx/data_validation/get_started) 库。\n",
        "\n",
        "`StatisticsGen` 会将我们刚刚使用 `ExampleGen` 提取的数据集作为输入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAscCCYWgA-9"
      },
      "outputs": [],
      "source": [
        "statistics_gen = tfx.components.StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLI6cb_5WugZ"
      },
      "source": [
        "`StatisticsGen` 完成运行后，我们可以呈现输出的统计信息。请尝试使用不同的图表！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLjXy7K6Tp_G"
      },
      "outputs": [],
      "source": [
        "context.show(statistics_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLKLTO9Nk60p"
      },
      "source": [
        "### SchemaGen\n",
        "\n",
        "`SchemaGen` 组件会根据数据统计信息生成架构（架构定义了数据集中特征的预期边界、类型和属性）。它同样使用 [TensorFlow Data Validation](https://tensorflow.google.cn/tfx/data_validation/get_started) 库。\n",
        "\n",
        "`SchemaGen` 会将我们使用 `StatisticsGen` 生成的统计信息作为输入，默认情况下会查看训练拆分。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygQvZ6hsiQ_J"
      },
      "outputs": [],
      "source": [
        "schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(schema_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6TxTUKXM6b"
      },
      "source": [
        "`SchemaGen` 完成运行后，我们可以将生成的架构呈现为表格。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec9vqDXpXeMb"
      },
      "outputs": [],
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZWWdbA-m7zp"
      },
      "source": [
        "在架构中，数据集中的每个特征都会显示为一行，旁边是其属性。架构还会捕获分类特征所采用的所有值，表示为它的域。\n",
        "\n",
        "要详细了解架构，请参阅 [SchemaGen 文档](https://tensorflow.google.cn/tfx/guide/schemagen)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qcUuO9k9f8"
      },
      "source": [
        "### ExampleValidator\n",
        "\n",
        "`ExampleValidator` 组件会根据由架构定义的期望来检测数据中的异常。它同样使用 [TensorFlow Data Validation](https://tensorflow.google.cn/tfx/data_validation/get_started) 库。\n",
        "\n",
        "`ExampleValidator` 会将来自 `StatisticsGen` 的统计信息和来自 `SchemaGen` 的架构作为输入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRlRUuGgiXks"
      },
      "outputs": [],
      "source": [
        "example_validator = tfx.components.ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "context.run(example_validator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855mrHgJcoer"
      },
      "source": [
        "`ExampleValidator` 完成运行后，我们可以将异常呈现为表格。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDyAAozQcrk3"
      },
      "outputs": [],
      "source": [
        "context.show(example_validator.outputs['anomalies'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znMoJj60ybZx"
      },
      "source": [
        "在异常表中，我们可以看到没有异常。这正是我们所期望的结果，因为这是我们分析的第一个数据集，并且该架构是针对数据集量身定做的。您应该检查此模式，任何意外情况都意味着数据中存在异常。检查后，该架构就可用于保护未来的数据，此处产生的异常可以用来调试模型性能，了解数据如何随时间的变化，以及识别数据错误。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPViEz5RlA36"
      },
      "source": [
        "### Transform\n",
        "\n",
        "`Transform` 组件为训练和应用执行特征工程。它使用 [TensorFlow Transform](https://tensorflow.google.cn/tfx/transform/get_started) 库。\n",
        "\n",
        "`Transform` 会将来自 `ExampleGen` 的数据、来自`SchemaGen` 的架构，以及包含用户定义的 Transform 代码的模块作为输入。\n",
        "\n",
        "我们在下面看一个用户定义的 Transform 代码的示例（有关 TensorFlow Transform API 的介绍，请参阅[教程](https://tensorflow.google.cn/tfx/tutorials/transform/simple)）。首先，我们为特征工程定义一些常量：\n",
        "\n",
        "注：`%%writefile` 单元魔法会将单元的内容作为 `.py` 文件保存到磁盘上。这样您就可以通过 `Transform` 组件将代码作为模块进行加载。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuNSiUKb4YJf"
      },
      "outputs": [],
      "source": [
        "_taxi_constants_module_file = 'taxi_constants.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPjhXuIF4YJh"
      },
      "outputs": [],
      "source": [
        "%%writefile {_taxi_constants_module_file}\n",
        "\n",
        "# Categorical features are assumed to each have a maximum value in the dataset.\n",
        "MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n",
        "\n",
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
        "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
        "    'dropoff_community_area'\n",
        "]\n",
        "\n",
        "DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n",
        "\n",
        "# Number of buckets used by tf.transform for encoding each feature.\n",
        "FEATURE_BUCKET_COUNT = 10\n",
        "\n",
        "BUCKET_FEATURE_KEYS = [\n",
        "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
        "    'dropoff_longitude'\n",
        "]\n",
        "\n",
        "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
        "OOV_SIZE = 10\n",
        "\n",
        "VOCAB_FEATURE_KEYS = [\n",
        "    'payment_type',\n",
        "    'company',\n",
        "]\n",
        "\n",
        "# Keys\n",
        "LABEL_KEY = 'tips'\n",
        "FARE_KEY = 'fare'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duj2Ax5z4YJl"
      },
      "source": [
        "接下来，我们编写 `preprocessing_fn`，它将原始数据作为输入，并返回转换后的特征，我们的模型可以用这些特征进行训练："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AJ9hBs94YJm"
      },
      "outputs": [],
      "source": [
        "_taxi_transform_module_file = 'taxi_transform.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYmxxx9A4YJn"
      },
      "outputs": [],
      "source": [
        "%%writefile {_taxi_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "import taxi_constants\n",
        "\n",
        "_DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n",
        "_BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS\n",
        "_CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS\n",
        "_FARE_KEY = taxi_constants.FARE_KEY\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "  Args:\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\n",
        "  Returns:\n",
        "    Map from string feature key to transformed feature operations.\n",
        "  \"\"\"\n",
        "  outputs = {}\n",
        "  for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
        "    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n",
        "    outputs[key] = tft.scale_to_z_score(\n",
        "        _fill_in_missing(inputs[key]))\n",
        "\n",
        "  for key in _VOCAB_FEATURE_KEYS:\n",
        "    # Build a vocabulary for this feature.\n",
        "    outputs[key] = tft.compute_and_apply_vocabulary(\n",
        "        _fill_in_missing(inputs[key]),\n",
        "        top_k=_VOCAB_SIZE,\n",
        "        num_oov_buckets=_OOV_SIZE)\n",
        "\n",
        "  for key in _BUCKET_FEATURE_KEYS:\n",
        "    outputs[key] = tft.bucketize(\n",
        "        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)\n",
        "\n",
        "  for key in _CATEGORICAL_FEATURE_KEYS:\n",
        "    outputs[key] = _fill_in_missing(inputs[key])\n",
        "\n",
        "  # Was this passenger a big tipper?\n",
        "  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n",
        "  tips = _fill_in_missing(inputs[_LABEL_KEY])\n",
        "  outputs[_LABEL_KEY] = tf.where(\n",
        "      tf.math.is_nan(taxi_fare),\n",
        "      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n",
        "      # Test if the tip was > 20% of the fare.\n",
        "      tf.cast(\n",
        "          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n",
        "\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def _fill_in_missing(x):\n",
        "  \"\"\"Replace missing values in a SparseTensor.\n",
        "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
        "  Args:\n",
        "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
        "      in the second dimension.\n",
        "  Returns:\n",
        "    A rank 1 tensor where missing values of `x` have been filled in.\n",
        "  \"\"\"\n",
        "  if not isinstance(x, tf.sparse.SparseTensor):\n",
        "    return x\n",
        "\n",
        "  default_value = '' if x.dtype == tf.string else 0\n",
        "  return tf.squeeze(\n",
        "      tf.sparse.to_dense(\n",
        "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "          default_value),\n",
        "      axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgbmZr3sgbWW"
      },
      "source": [
        "现在，我们将此特征工程代码传递给 `Transform` 组件，然后运行它来转换您的数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHfhth_GiZI9"
      },
      "outputs": [],
      "source": [
        "transform = tfx.components.Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_taxi_transform_module_file))\n",
        "context.run(transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwAwb4rARRQ2"
      },
      "source": [
        "我们检查一下 `Transform` 的输出工件。该组件会生成两种类型的输出：\n",
        "\n",
        "- `transform_graph` 是可以执行预处理运算的计算图（此计算图将包含在应用和评估模型中）。\n",
        "- `transformed_examples` 表示预处理的训练数据和评估数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SClrAaEGR1O5"
      },
      "outputs": [],
      "source": [
        "transform.outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyFkBd9AR1sy"
      },
      "source": [
        "来看一下 `transform_graph` 工件。它指向一个包含三个子目录的目录。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tRw4DneR3i7"
      },
      "outputs": [],
      "source": [
        "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "os.listdir(train_uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fqV54CIR6Pu"
      },
      "source": [
        "`transformed_metadata` 子目录包含预处理数据的架构。`transform_fn` 子目录包含实际的预处理计算图。`metadata` 子目录包含原始数据的架构。\n",
        "\n",
        "我们还可以看一下前三个转换后的样本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwbW2zPKR_S4"
      },
      "outputs": [],
      "source": [
        "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
        "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "# Iterate over the first 3 records and decode them.\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_b_V6eN4f69"
      },
      "source": [
        "在 `Transform` 组件将数据转换为特征之后，下一步是训练模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBJFtnl6lCg9"
      },
      "source": [
        "### Trainer\n",
        "\n",
        "`Trainer` 组件将训练您在 TensorFlow 中定义（使用 Estimator API 或带有 [`model_to_estimator`](https://tensorflow.google.cn/api_docs/python/tf/keras/estimator/model_to_estimator) 的 Keras API）的模型。\n",
        "\n",
        "`Trainer` 会将来自 `SchemaGen` 的架构、来自 `Transform` 的转换后数据与计算图、训练参数，以及包含用户定义的模型代码的模块作为输入。\n",
        "\n",
        "我们在下面看一个用户定义的模型代码的示例（有关 TensorFlow Estimator API 的介绍，请参阅[教程](https://tensorflow.google.cn/tutorials/estimator/premade)）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1376oq04YJt"
      },
      "outputs": [],
      "source": [
        "_taxi_trainer_module_file = 'taxi_trainer.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf9UuNng4YJu"
      },
      "outputs": [],
      "source": [
        "%%writefile {_taxi_trainer_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "from tfx_bsl.tfxio import dataset_options\n",
        "\n",
        "import taxi_constants\n",
        "\n",
        "_DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n",
        "_BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS\n",
        "_CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS\n",
        "_MAX_CATEGORICAL_FEATURE_VALUES = taxi_constants.MAX_CATEGORICAL_FEATURE_VALUES\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
        "\n",
        "\n",
        "# Tf.Transform considers these features as \"raw\"\n",
        "def _get_raw_feature_spec(schema):\n",
        "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "\n",
        "def _build_estimator(config, hidden_units=None, warm_start_from=None):\n",
        "  \"\"\"Build an estimator for predicting the tipping behavior of taxi riders.\n",
        "  Args:\n",
        "    config: tf.estimator.RunConfig defining the runtime environment for the\n",
        "      estimator (including model_dir).\n",
        "    hidden_units: [int], the layer sizes of the DNN (input layer first)\n",
        "    warm_start_from: Optional directory to warm start from.\n",
        "  Returns:\n",
        "    A dict of the following:\n",
        "      - estimator: The estimator that will be used for training and eval.\n",
        "      - train_spec: Spec for training.\n",
        "      - eval_spec: Spec for eval.\n",
        "      - eval_input_receiver_fn: Input function for eval.\n",
        "  \"\"\"\n",
        "  real_valued_columns = [\n",
        "      tf.feature_column.numeric_column(key, shape=())\n",
        "      for key in _DENSE_FLOAT_FEATURE_KEYS\n",
        "  ]\n",
        "  categorical_columns = [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n",
        "      for key in _VOCAB_FEATURE_KEYS\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n",
        "      for key in _BUCKET_FEATURE_KEYS\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension\n",
        "          key,\n",
        "          num_buckets=num_buckets,\n",
        "          default_value=0) for key, num_buckets in zip(\n",
        "              _CATEGORICAL_FEATURE_KEYS,\n",
        "              _MAX_CATEGORICAL_FEATURE_VALUES)\n",
        "  ]\n",
        "  return tf.estimator.DNNLinearCombinedClassifier(\n",
        "      config=config,\n",
        "      linear_feature_columns=categorical_columns,\n",
        "      dnn_feature_columns=real_valued_columns,\n",
        "      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n",
        "      warm_start_from=warm_start_from)\n",
        "\n",
        "\n",
        "def _example_serving_receiver_fn(tf_transform_graph, schema):\n",
        "  \"\"\"Build the serving in inputs.\n",
        "  Args:\n",
        "    tf_transform_graph: A TFTransformOutput.\n",
        "    schema: the schema of the input data.\n",
        "  Returns:\n",
        "    Tensorflow graph which parses examples, applying tf-transform to them.\n",
        "  \"\"\"\n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "  raw_feature_spec.pop(_LABEL_KEY)\n",
        "\n",
        "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "      raw_feature_spec, default_batch_size=None)\n",
        "  serving_input_receiver = raw_input_fn()\n",
        "\n",
        "  transformed_features = tf_transform_graph.transform_raw_features(\n",
        "      serving_input_receiver.features)\n",
        "\n",
        "  return tf.estimator.export.ServingInputReceiver(\n",
        "      transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "\n",
        "def _eval_input_receiver_fn(tf_transform_graph, schema):\n",
        "  \"\"\"Build everything needed for the tf-model-analysis to run the model.\n",
        "  Args:\n",
        "    tf_transform_graph: A TFTransformOutput.\n",
        "    schema: the schema of the input data.\n",
        "  Returns:\n",
        "    EvalInputReceiver function, which contains:\n",
        "      - Tensorflow graph which parses raw untransformed features, applies the\n",
        "        tf-transform preprocessing operators.\n",
        "      - Set of raw, untransformed features.\n",
        "      - Label against which predictions will be compared.\n",
        "  \"\"\"\n",
        "  # Notice that the inputs are raw features, not transformed features here.\n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "\n",
        "  serialized_tf_example = tf.compat.v1.placeholder(\n",
        "      dtype=tf.string, shape=[None], name='input_example_tensor')\n",
        "\n",
        "  # Add a parse_example operator to the tensorflow graph, which will parse\n",
        "  # raw, untransformed, tf examples.\n",
        "  features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
        "\n",
        "  # Now that we have our raw examples, process them through the tf-transform\n",
        "  # function computed during the preprocessing step.\n",
        "  transformed_features = tf_transform_graph.transform_raw_features(\n",
        "      features)\n",
        "\n",
        "  # The key name MUST be 'examples'.\n",
        "  receiver_tensors = {'examples': serialized_tf_example}\n",
        "\n",
        "  # NOTE: Model is driven by transformed features (since training works on the\n",
        "  # materialized output of TFT, but slicing will happen on raw features.\n",
        "  features.update(transformed_features)\n",
        "\n",
        "  return tfma.export.EvalInputReceiver(\n",
        "      features=features,\n",
        "      receiver_tensors=receiver_tensors,\n",
        "      labels=transformed_features[_LABEL_KEY])\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern, data_accessor, tf_transform_output, batch_size=200):\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      dataset_options.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "      tf_transform_output.transformed_metadata.schema)\n",
        "\n",
        "\n",
        "# TFX will call this function\n",
        "def trainer_fn(trainer_fn_args, schema):\n",
        "  \"\"\"Build the estimator using the high level API.\n",
        "  Args:\n",
        "    trainer_fn_args: Holds args used to train the model as name/value pairs.\n",
        "    schema: Holds the schema of the training examples.\n",
        "  Returns:\n",
        "    A dict of the following:\n",
        "      - estimator: The estimator that will be used for training and eval.\n",
        "      - train_spec: Spec for training.\n",
        "      - eval_spec: Spec for eval.\n",
        "      - eval_input_receiver_fn: Input function for eval.\n",
        "  \"\"\"\n",
        "  # Number of nodes in the first layer of the DNN\n",
        "  first_dnn_layer_size = 100\n",
        "  num_dnn_layers = 4\n",
        "  dnn_decay_factor = 0.7\n",
        "\n",
        "  train_batch_size = 40\n",
        "  eval_batch_size = 40\n",
        "\n",
        "  tf_transform_graph = tft.TFTransformOutput(trainer_fn_args.transform_output)\n",
        "\n",
        "  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda\n",
        "      trainer_fn_args.train_files,\n",
        "      trainer_fn_args.data_accessor,\n",
        "      tf_transform_graph,\n",
        "      batch_size=train_batch_size)\n",
        "\n",
        "  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda\n",
        "      trainer_fn_args.eval_files,\n",
        "      trainer_fn_args.data_accessor,\n",
        "      tf_transform_graph,\n",
        "      batch_size=eval_batch_size)\n",
        "\n",
        "  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda\n",
        "      train_input_fn,\n",
        "      max_steps=trainer_fn_args.train_steps)\n",
        "\n",
        "  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda\n",
        "      tf_transform_graph, schema)\n",
        "\n",
        "  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(\n",
        "      eval_input_fn,\n",
        "      steps=trainer_fn_args.eval_steps,\n",
        "      exporters=[exporter],\n",
        "      name='chicago-taxi-eval')\n",
        "\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      save_checkpoints_steps=999, keep_checkpoint_max=1)\n",
        "\n",
        "  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)\n",
        "\n",
        "  estimator = _build_estimator(\n",
        "      # Construct layers sizes with exponetial decay\n",
        "      hidden_units=[\n",
        "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
        "          for i in range(num_dnn_layers)\n",
        "      ],\n",
        "      config=run_config,\n",
        "      warm_start_from=trainer_fn_args.base_model)\n",
        "\n",
        "  # Create an input receiver for TFMA processing\n",
        "  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda\n",
        "      tf_transform_graph, schema)\n",
        "\n",
        "  return {\n",
        "      'estimator': estimator,\n",
        "      'train_spec': train_spec,\n",
        "      'eval_spec': eval_spec,\n",
        "      'eval_input_receiver_fn': receiver_fn\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY4yTRaX4YJx"
      },
      "source": [
        "现在，我们将此模型代码传递给 `Trainer` 组件，然后运行它来训练模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "429-vvCWibO0"
      },
      "outputs": [],
      "source": [
        "from tfx.components.trainer.executor import Executor\n",
        "from tfx.dsl.components.base import executor_spec\n",
        "\n",
        "trainer = tfx.components.Trainer(\n",
        "    module_file=os.path.abspath(_taxi_trainer_module_file),\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(Executor),\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    train_args=tfx.proto.TrainArgs(num_steps=10000),\n",
        "    eval_args=tfx.proto.EvalArgs(num_steps=5000))\n",
        "context.run(trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cql1G35StJp"
      },
      "source": [
        "#### 使用 TensorBoard 分析训练\n",
        "\n",
        "（可选）我们可以将 TensorBoard 连接到 Trainer 来分析模型的训练曲线。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXe62WE0S0Ek"
      },
      "outputs": [],
      "source": [
        "# Get the URI of the output artifact representing the training logs, which is a directory\n",
        "model_run_dir = trainer.outputs['model_run'].get()[0].uri\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {model_run_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmPftrv0lEQy"
      },
      "source": [
        "### Evaluator\n",
        "\n",
        "`Evaluator` 组件会在评估集上计算模型性能指标。它使用 [TensorFlow Model Analysis](https://tensorflow.google.cn/tfx/model_analysis/get_started) 库。`Evaluator` 还可以选择性地验证新训练的模型是否优于先前的模型。这在生产流水线设置中十分有用，您可以每天自动训练和验证模型。在此笔记本中，我们只训练一个模型，因此 `Evaluator` 会自动将模型标记为“良好”。\n",
        "\n",
        "`Evaluator` 会将来自 `ExampleGen` 的数据、来自 `Trainer` 的训练的模型，以及切片配置作为输入。您可以通过切片配置根据特征值对指标进行切分（例如，模型在上午 8 点和晚上 8 点开始的出租车行程上的表现如何？）。请参阅下面的配置示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVhfzzh9PDEx"
      },
      "outputs": [],
      "source": [
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[\n",
        "        # Using signature 'eval' implies the use of an EvalSavedModel. To use\n",
        "        # a serving model remove the signature to defaults to 'serving_default'\n",
        "        # and add a label_key.\n",
        "        tfma.ModelSpec(signature_name='eval')\n",
        "    ],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(\n",
        "            # The metrics added here are in addition to those saved with the\n",
        "            # model (assuming either a keras model or EvalSavedModel is used).\n",
        "            # Any metrics added into the saved model (for example using\n",
        "            # model.compile(..., metrics=[...]), etc) will be computed\n",
        "            # automatically.\n",
        "            metrics=[\n",
        "                tfma.MetricConfig(class_name='ExampleCount')\n",
        "            ],\n",
        "            # To add validation thresholds for metrics saved with the model,\n",
        "            # add them keyed by metric name to the thresholds map.\n",
        "            thresholds = {\n",
        "                'accuracy': tfma.MetricThreshold(\n",
        "                    value_threshold=tfma.GenericValueThreshold(\n",
        "                        lower_bound={'value': 0.5}),\n",
        "                    # Change threshold will be ignored if there is no\n",
        "                    # baseline model resolved from MLMD (first run).\n",
        "                    change_threshold=tfma.GenericChangeThreshold(\n",
        "                       direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                       absolute={'value': -1e-10}))\n",
        "            }\n",
        "        )\n",
        "    ],\n",
        "    slicing_specs=[\n",
        "        # An empty slice spec means the overall slice, i.e. the whole dataset.\n",
        "        tfma.SlicingSpec(),\n",
        "        # Data can be sliced along a feature column. In this case, data is\n",
        "        # sliced along feature column trip_start_hour.\n",
        "        tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mBdKH1F8JuT"
      },
      "source": [
        "接下来，我们将此配置提供给 `Evaluator` 并运行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zjcx8g6mihSt"
      },
      "outputs": [],
      "source": [
        "# Use TFMA to compute a evaluation statistics over features of a model and\n",
        "# validate them against a baseline.\n",
        "\n",
        "# The model resolver is only required if performing model validation in addition\n",
        "# to evaluation. In this case we validate against the latest blessed model. If\n",
        "# no model has been blessed before (as in this case) the evaluator will make our\n",
        "# candidate the first blessed model.\n",
        "model_resolver = tfx.dsl.Resolver(\n",
        "      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
        "      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
        "      model_blessing=tfx.dsl.Channel(\n",
        "          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
        "              'latest_blessed_model_resolver')\n",
        "context.run(model_resolver)\n",
        "\n",
        "evaluator = tfx.components.Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    eval_config=eval_config)\n",
        "context.run(evaluator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeCVkBusS_8g"
      },
      "source": [
        "现在，我们检查一下 `Evaluator` 的输出工件。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4GghePOTJxL"
      },
      "outputs": [],
      "source": [
        "evaluator.outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5TMskWe9LL0"
      },
      "source": [
        "我们可以使用 `evaluation` 输出来显示整个评估集上全局指标的默认可视化效果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U729j5X5QQUQ"
      },
      "outputs": [],
      "source": [
        "context.show(evaluator.outputs['evaluation'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-tI4p6m-OAn"
      },
      "source": [
        "要查看切分后的评估指标的可视化效果，我们可以直接调用 TensorFlow Model Analysis 库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyis6iy0HLdi"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "# Get the TFMA output result path and load the result.\n",
        "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
        "tfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n",
        "\n",
        "# Show data sliced along feature column trip_start_hour.\n",
        "tfma.view.render_slicing_metrics(\n",
        "    tfma_result, slicing_column='trip_start_hour')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uvYrUf2-r_6"
      },
      "source": [
        "该可视化效果会显示相同的指标，但根据 `trip_start_hour` 的每个特征值而非整个评估集计算得出。\n",
        "\n",
        "TensorFlow Model Analysis 支持许多其他可视化效果，例如 Fairness Indicators 和绘制模型性能的时间序列。要了解详情，请参阅[教程](https://tensorflow.google.cn/tfx/tutorials/model_analysis/tfma_basic)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEotnkxEswUb"
      },
      "source": [
        "由于我们在配置中添加了阈值，验证输出同样可用。`blessing` 工件的出现表示模型通过了验证。由于这是执行的第一次验证，候选模型将自动获得祝福。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZmiRtg6TKtR"
      },
      "outputs": [],
      "source": [
        "blessing_uri = evaluator.outputs['blessing'].get()[0].uri\n",
        "!ls -l {blessing_uri}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM1tFkOVSBa0"
      },
      "source": [
        "现在，还可以通过加载验证结果记录来验证是否成功："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxa5G08bSJ8a"
      },
      "outputs": [],
      "source": [
        "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
        "print(tfma.load_validation_result(PATH_TO_RESULT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8DYekCZlHfj"
      },
      "source": [
        "### Pusher\n",
        "\n",
        "`Pusher` 组件通常位于 TFX 流水线的末端。它会检查模型是否已通过验证，如果通过，则将模型导出到 `_serving_model_dir`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r45nQ69eikc9"
      },
      "outputs": [],
      "source": [
        "pusher = tfx.components.Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    model_blessing=evaluator.outputs['blessing'],\n",
        "    push_destination=tfx.proto.PushDestination(\n",
        "        filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctUErBYoTO9I"
      },
      "source": [
        "我们来检查一下 `Pusher` 的输出工件。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRkWo-MzTSss"
      },
      "outputs": [],
      "source": [
        "pusher.outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peH2PPS3VgkL"
      },
      "source": [
        "特别是，Pusher 会将模型导出为 SavedModel 格式，如下所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zyIqWl9TSdG"
      },
      "outputs": [],
      "source": [
        "push_uri = pusher.outputs['pushed_model'].get()[0].uri\n",
        "model = tf.saved_model.load(push_uri)\n",
        "\n",
        "for item in model.signatures.items():\n",
        "  pp.pprint(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-YPNUuHANtj"
      },
      "source": [
        "我们对内置 TFX 组件的介绍到此结束！"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wdeKOEkv1Fe8"
      ],
      "name": "components.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
