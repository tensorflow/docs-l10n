{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X80i_girFR2o"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bB8gHCR3FVC0"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu4gy3TAw4As"
      },
      "source": [
        "# 推荐电影：TFX 中的推荐模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z17OmgavQfp4"
      },
      "source": [
        "注：我们建议在 Colab 笔记本中运行本教程，无需进行设置！只需点击“在 Google Colab 中运行”。\n",
        "\n",
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://tensorflow.google.cn/tfx/tutorials/tfx/recommenders\"> <img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org 上查看</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tfx/tutorials/tfx/recommenders.ipynb\"> <img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tfx/tutorials/tfx/recommenders.ipynb\"> <img width=\"32px\" src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 GitHub 上查看源代码</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/tfx/tutorials/tfx/recommenders.ipynb\"> <img width=\"32px\" src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a></td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeYA79m1DEX"
      },
      "source": [
        "## 移植到 TFX 的 TFRS 教程\n",
        "\n",
        "下面是基本 TensorFlow Recommenders (TFRS) 教程到 TFX 的移植版，旨在演示如何在 TFX 流水线中使用 TFRS。它是[基本教程](https://tensorflow.google.cn/recommenders/examples/basic_retrieval)的镜像版。\n",
        "\n",
        "对于上下文，现实世界的推荐系统通常由两个阶段组成：\n",
        "\n",
        "1. 检索阶段负责从所有可能的候选项中选择数百个候选初始集。此模型的主要目标是有效剔除用户不感兴趣的所有候选项。由于检索模型可能要处理数百万个候选项，在计算上必须高效。\n",
        "2. 排名阶段获取检索模型的输出并对其进行微调，以选择最佳的少量推荐。它的任务是将用户可能感兴趣的条目集缩小到可能的候选项名单。\n",
        "\n",
        "在本教程中，我们将重点关注第一阶段“检索”。检索模型通常由两个子模型组成：\n",
        "\n",
        "1. 使用查询特征计算查询表示（通常是固定维度嵌入向量）的查询模型。\n",
        "2. 使用候选特征计算候选项表示（大小相等的向量）的候选项模型\n",
        "\n",
        "然后，将这两个模型的输出相乘以给出查询-候选项亲和度分数，分数越高表示候选项与查询之间的匹配度越高。\n",
        "\n",
        "在本教程中，我们将使用 Movielens 数据集构建和训练这样的一个双塔模型。\n",
        "\n",
        "我们要完成以下步骤：\n",
        "\n",
        "1. 注入并检查 MovieLens 数据集。\n",
        "2. 实现检索模型。\n",
        "3. 训练和导出模型。\n",
        "4. 进行预测"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7QZ3kkMQo48"
      },
      "source": [
        "## 数据集\n",
        "\n",
        "Movielens 数据集是来自明尼苏达大学 [GroupLens](https://grouplens.org/datasets/movielens/) 研究小组的经典数据集。它包含一组用户对电影的评分，是推荐系统研究的主力。\n",
        "\n",
        "可以通过两种方式处理数据：\n",
        "\n",
        "1. 它可以被解释为表达用户观看（和评分）的电影，以及他们没有观看的电影。这是一种隐式反馈形式，用户的观看行为会告诉我们他们喜欢看哪些内容以及不喜欢看哪些内容。\n",
        "2. 此外，它也可以被视为表达了用户对他们看过的电影的喜爱程度。这是一种显式反馈形式：假设用户观看了一部电影，我们可以通过查看他们给出的评分来大致判断他们的喜欢程度。\n",
        "\n",
        "在本教程中，我们将重点关注检索系统：一个从目录中预测用户可能观看的一组电影的模型。通常，隐式数据在这里更有用，因此我们将把 Movielens 视为一个隐式系统。这意味着用户观看的每一部电影都是一个正样本，而他们没有看过的每一部电影都是一个隐式负样本。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sawo1x8kQk9b"
      },
      "source": [
        "## 导入\n",
        "\n",
        "我们首先进行导入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtR3txiwrT9w"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq tfx\n",
        "!pip install -Uq tensorflow-recommenders\n",
        "!pip install -Uq tensorflow-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJrgGNTHhzlq"
      },
      "source": [
        "### 卸载 Shapely\n",
        "\n",
        "TODO(b/263441833) 这是避免 ImportError 的临时解决方案。最终，应该通过支持最新版本的 Bigquery 来处理，而不是卸载其他额外的依赖项。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w90AGSpJhz8X"
      },
      "outputs": [],
      "source": [
        "!pip uninstall shapely -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCnUpuaJGKa0"
      },
      "source": [
        "### 是否已重新启动运行时？\n",
        "\n",
        "如果您使用的是 Google Colab，首次运行上面的代码单元时，必须重新启动运行时 (Runtime &gt; Restart runtime ...)。这样做的原因是 Colab 加载软件包的方式。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZGYDaF-m5wZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import absl\n",
        "import json\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Any, Dict, List, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs\n",
        "import apache_beam as beam\n",
        "\n",
        "from absl import logging\n",
        "\n",
        "from tfx.components.example_gen.base_example_gen_executor import BaseExampleGenExecutor\n",
        "from tfx.components.example_gen.component import FileBasedExampleGen\n",
        "from tfx.components.example_gen import utils\n",
        "from tfx.dsl.components.base import executor_spec\n",
        "\n",
        "from tfx.types import artifact\n",
        "from tfx.types import artifact_utils\n",
        "from tfx.types import channel\n",
        "from tfx.types import standard_artifacts\n",
        "from tfx.types.standard_artifacts import Examples\n",
        "\n",
        "from tfx.dsl.component.experimental.annotations import InputArtifact\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "\n",
        "# Set up logging.\n",
        "tf.get_logger().propagate = False\n",
        "absl.logging.set_verbosity(absl.logging.INFO)\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"TFX version: {tfx.__version__}\")\n",
        "print(f\"TensorFlow Recommenders version: {tfrs.__version__}\")\n",
        "\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znn3FAHAsomB"
      },
      "source": [
        "## 创建 TFDS ExampleGen\n",
        "\n",
        "我们创建一个[自定义 ExampleGen 组件](https://tensorflow.google.cn/tfx/guide/examplegen#custom_examplegen)，用于加载 TensorFlow Datasets (TFDS) 数据集。它使用 FileBasedExampleGen 中的自定义执行器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcVgf7rLsv70"
      },
      "outputs": [],
      "source": [
        "@beam.ptransform_fn\n",
        "@beam.typehints.with_input_types(beam.Pipeline)\n",
        "@beam.typehints.with_output_types(tf.train.Example)\n",
        "def _TFDatasetToExample(  # pylint: disable=invalid-name\n",
        "    pipeline: beam.Pipeline,\n",
        "    exec_properties: Dict[str, Any],\n",
        "    split_pattern: str\n",
        "    ) -> beam.pvalue.PCollection:\n",
        "    \"\"\"Read a TensorFlow Dataset and create tf.Examples\"\"\"\n",
        "    custom_config = json.loads(exec_properties['custom_config'])\n",
        "    dataset_name = custom_config['dataset']\n",
        "    split_name = custom_config['split']\n",
        "\n",
        "    builder = tfds.builder(dataset_name)\n",
        "    builder.download_and_prepare()\n",
        "\n",
        "    return (pipeline\n",
        "            | 'MakeExamples' >> tfds.beam.ReadFromTFDS(builder, split=split_name)\n",
        "            | 'AsNumpy' >> beam.Map(tfds.as_numpy)\n",
        "            | 'ToDict' >> beam.Map(dict)\n",
        "            | 'ToTFExample' >> beam.Map(utils.dict_to_example)\n",
        "            )\n",
        "\n",
        "class TFDSExecutor(BaseExampleGenExecutor):\n",
        "  def GetInputSourceToExamplePTransform(self) -> beam.PTransform:\n",
        "    \"\"\"Returns PTransform for TF Dataset to TF examples.\"\"\"\n",
        "    return _TFDatasetToExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srH6ChVCtR55"
      },
      "source": [
        "## 初始化 TFX 流水线上下文"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM-46D40tW_V"
      },
      "outputs": [],
      "source": [
        "context = InteractiveContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAqjR4a1RR4"
      },
      "source": [
        "## 准备数据集\n",
        "\n",
        "我们将在 `FileBasedExampleGen` 中使用自定义执行器从 TFDS 加载数据集。由于我们有两个数据集，将创建两个 `ExampleGen` 组件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaQhqcLGP0jL"
      },
      "outputs": [],
      "source": [
        "# Ratings data.\n",
        "ratings_example_gen = FileBasedExampleGen(\n",
        "    input_base='dummy',\n",
        "    custom_config={'dataset':'movielens/100k-ratings', 'split':'train'},\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor))\n",
        "context.run(ratings_example_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlUFANrRvKDW"
      },
      "outputs": [],
      "source": [
        "# Features of all the available movies.\n",
        "movies_example_gen = FileBasedExampleGen(\n",
        "    input_base='dummy',\n",
        "    custom_config={'dataset':'movielens/100k-movies', 'split':'train'},\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor))\n",
        "context.run(movies_example_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRHorm8W1yf3"
      },
      "source": [
        "## 创建 `inspect_examples` 效用函数\n",
        "\n",
        "我们创建一个方便的效用函数来检查 TF.Examples 的数据集。评分数据集返回一个包含电影 ID、用户 ID、指定评分、时间戳、电影信息和用户信息的字典："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1-KQV2ynMdh"
      },
      "outputs": [],
      "source": [
        "def inspect_examples(component,\n",
        "                     channel_name='examples',\n",
        "                     split_name='train',\n",
        "                     num_examples=1):\n",
        "  # Get the URI of the output artifact, which is a directory\n",
        "  full_split_name = 'Split-{}'.format(split_name)\n",
        "  print('channel_name: {}, split_name: {} (\\\"{}\\\"), num_examples: {}\\n'.format(\n",
        "      channel_name, split_name, full_split_name, num_examples))\n",
        "  train_uri = os.path.join(\n",
        "      component.outputs[channel_name].get()[0].uri, full_split_name)\n",
        "\n",
        "  # Get the list of files in this directory (all compressed TFRecord files)\n",
        "  tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                        for name in os.listdir(train_uri)]\n",
        "\n",
        "  # Create a `TFRecordDataset` to read these files\n",
        "  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "  # Iterate over the records and print them\n",
        "  for tfrecord in dataset.take(num_examples):\n",
        "    serialized_example = tfrecord.numpy()\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(serialized_example)\n",
        "    pp.pprint(example)\n",
        "\n",
        "inspect_examples(ratings_example_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGLGCjSt_q96"
      },
      "source": [
        "电影数据集包含电影 ID、电影标题和有关其所属类型的数据。请注意，类型是用整数标签编码的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHLsIHhw_x1d"
      },
      "outputs": [],
      "source": [
        "inspect_examples(movies_example_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu4XSa_G1nyN"
      },
      "source": [
        "## ExampleGen 进行拆分\n",
        "\n",
        "当我们提取电影镜头数据集时，我们的 `ExampleGen` 组件将数据拆分为 `train` 和 `eval` 两部分。它们实际上被命名为 `Split-train` 和 `Split-eval`。默认情况下，66% 为训练，34% 为评估。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk7Y7hWGKH9e"
      },
      "source": [
        "## 为电影和评分生成统计数据\n",
        "\n",
        "对于 TFX 流水线，我们需要为数据集生成统计数据。我们通过使用 [StatisticsGen 组件](https://tensorflow.google.cn/tfx/guide/statsgen)来实现此目的。当我们为数据集生成模式时，下面的 [SchemaGen 组件](https://tensorflow.google.cn/tfx/guide/schemagen)将使用这些统计数据。无论如何，这是一种不错的做法，因为持续检查和分析数据至关重要。由于我们有两个数据集，将创建两个 StatisticsGen 组件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7-ZI8IsKT2P"
      },
      "outputs": [],
      "source": [
        "movies_stats_gen = tfx.components.StatisticsGen(\n",
        "    examples=movies_example_gen.outputs['examples'])\n",
        "context.run(movies_stats_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlKLrrgnKzIe"
      },
      "outputs": [],
      "source": [
        "context.show(movies_stats_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmTThijxKmhA"
      },
      "outputs": [],
      "source": [
        "ratings_stats_gen = tfx.components.StatisticsGen(\n",
        "    examples=ratings_example_gen.outputs['examples'])\n",
        "context.run(ratings_stats_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoRcgChqK62O"
      },
      "outputs": [],
      "source": [
        "context.show(ratings_stats_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Lj0OBwLElb"
      },
      "source": [
        "## 为电影和评分创建模式\n",
        "\n",
        "对于 TFX 流水线，我们需要从数据集中生成数据模式。我们通过使用 [SchemaGen 组件](https://tensorflow.google.cn/tfx/guide/schemagen)来实现此目的。下面的 [Transform 组件](https://tensorflow.google.cn/tfx/guide/transform)将使用此数据模式以一种高度可扩展到大型数据集的方式执行特征工程，并避免训练/提供偏差。由于我们有两个数据集，将创建两个 SchemaGen 组件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL85CAcILJiw"
      },
      "outputs": [],
      "source": [
        "movies_schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=movies_stats_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(movies_schema_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eMtN1U1Lha1"
      },
      "outputs": [],
      "source": [
        "context.show(movies_schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-DkVUOeLmvX"
      },
      "outputs": [],
      "source": [
        "ratings_schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=ratings_stats_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(ratings_schema_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxD9oAhZLt_Z"
      },
      "outputs": [],
      "source": [
        "context.show(ratings_schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN648jrYLxmH"
      },
      "source": [
        "## 使用 Transform 执行特征工程\n",
        "\n",
        "对于 TFX 流水线的结构化和可重复设计，我们需要一种可扩展的特征工程方法。这样我们便能处理通常是许多推荐系统一部分的大型数据集，并且还避免了训练/应用偏差。我们将使用 [Transform 组件](https://tensorflow.google.cn/tfx/guide/transform)来实现此目的。\n",
        "\n",
        "Transform 组件使用模块文件为我们想要执行的特征工程提供用户代码，因此我们的第一步是创建该模块文件。由于我们有两个数据集，将创建其中两个模块文件和两个 Transform 组件。\n",
        "\n",
        "我们的推荐系统需要的内容之一是 `user_id` 和 `movie_title` 字段的词汇表。在 [basic_retrieval 教程](https://tensorflow.google.cn/recommenders/examples/basic_retrieval)中，它们是使用内嵌 Numpy 创建的，但在这里我们将使用 Transform。\n",
        "\n",
        "注：下面的 `%%writefile {_movies_transform_module_file}` 单元魔法会创建该单元的内容并将其写入运行此笔记本的笔记本服务器（例如，Colab VM）上的文件中。在笔记本之外执行此操作时，只需创建一个 Python 文件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oqzx5mSI8zm"
      },
      "outputs": [],
      "source": [
        "_movies_transform_module_file = 'movies_transform_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKROCiPo_5LJ"
      },
      "outputs": [],
      "source": [
        "%%writefile {_movies_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  # We only want the movie title\n",
        "  return {'movie_title':inputs['movie_title']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQcQBN9SIzIa"
      },
      "outputs": [],
      "source": [
        "movies_transform = tfx.components.Transform(\n",
        "    examples=movies_example_gen.outputs['examples'],\n",
        "    schema=movies_schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_movies_transform_module_file))\n",
        "context.run(movies_transform, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5oai0TlNWlv"
      },
      "outputs": [],
      "source": [
        "context.show(movies_transform.outputs['post_transform_schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWahQqCiBXqA"
      },
      "outputs": [],
      "source": [
        "inspect_examples(movies_transform, channel_name='transformed_examples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4PmR-a8O-mD"
      },
      "outputs": [],
      "source": [
        "_ratings_transform_module_file = 'ratings_transform_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWXuBqivPDuK"
      },
      "outputs": [],
      "source": [
        "%%writefile {_ratings_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import pdb\n",
        "\n",
        "NUM_OOV_BUCKETS = 1\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  # We only want the user ID and the movie title, but we also need vocabularies\n",
        "  # for both of them.  The vocabularies aren't features, they're only used by\n",
        "  # the lookup.\n",
        "  outputs = {}\n",
        "  outputs['user_id'] = tft.sparse_tensor_to_dense_with_shape(inputs['user_id'], [None, 1], '-1')\n",
        "  outputs['movie_title'] = tft.sparse_tensor_to_dense_with_shape(inputs['movie_title'], [None, 1], '-1')\n",
        "\n",
        "  tft.compute_and_apply_vocabulary(\n",
        "      inputs['user_id'],\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS,\n",
        "      vocab_filename='user_id_vocab')\n",
        "\n",
        "  tft.compute_and_apply_vocabulary(\n",
        "      inputs['movie_title'],\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS,\n",
        "      vocab_filename='movie_title_vocab')\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4NgpBOkPXsj"
      },
      "outputs": [],
      "source": [
        "ratings_transform = tfx.components.Transform(\n",
        "    examples=ratings_example_gen.outputs['examples'],\n",
        "    schema=ratings_schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_ratings_transform_module_file))\n",
        "context.run(ratings_transform, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9Vqby34Dvzd"
      },
      "outputs": [],
      "source": [
        "context.show(ratings_transform.outputs['post_transform_schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_ec39jiaMG-"
      },
      "outputs": [],
      "source": [
        "inspect_examples(ratings_transform, channel_name='transformed_examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCi-seR86qqa"
      },
      "source": [
        "## 在 TFX 中实现模型\n",
        "\n",
        "在 [basic_retrieval](https://tensorflow.google.cn/recommenders/examples/basic_retrieval) 教程中，模型是在 Python 运行时中以内嵌方式创建的。在 TFX 流水线中，模型、指标和损失在[名为 Trainer 的流水线组件](https://tensorflow.google.cn/tfx/guide/trainer)的模块文件中定义和训练。这可让模型、指标和损失成为可自动化和受监视的可重复过程的一部分。\n",
        "\n",
        "### TensorFlow Recommenders 模型架构\n",
        "\n",
        "我们将构建一个双塔检索模型。双塔的概念意味着我们将拥有一个使用用户特征计算用户表示的查询塔，以及另一个使用电影特征计算电影表示的条目塔。我们可以单独构建每个塔（在下面的 `_build_user_model()` 和 `_build_movie_model()` 方法中），然后将它们组合到最终模型中（如在 `MobieLensModel` 类中）。`MovieLensModel` 是 `tfrs.Model` 基类（可简化模型构建）的子类：我们需要做的就是在 `__init__` 方法中设置组件，并实现 `compute_loss` 方法，获取原始特征并返回损失值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_mmYhjAJP4g"
      },
      "outputs": [],
      "source": [
        "# We're now going to create the module file for Trainer, which will include the\n",
        "# code above with some modifications for TFX.\n",
        "\n",
        "_trainer_module_file = 'trainer_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHQZJEhXP93N"
      },
      "outputs": [],
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import Dict, List, Text\n",
        "\n",
        "import pdb\n",
        "\n",
        "import os\n",
        "import absl\n",
        "import datetime\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "from absl import logging\n",
        "from tfx.types import artifact_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.coders import example_coder\n",
        "from tfx_bsl.public import tfxio\n",
        "\n",
        "absl.logging.set_verbosity(absl.logging.INFO)\n",
        "\n",
        "EMBEDDING_DIMENSION = 32\n",
        "INPUT_FN_BATCH_SIZE = 1\n",
        "\n",
        "\n",
        "def extract_str_feature(dataset, feature_name):\n",
        "  np_dataset = []\n",
        "  for example in dataset:\n",
        "    np_example = example_coder.ExampleToNumpyDict(example.numpy())\n",
        "    np_dataset.append(np_example[feature_name][0].decode())\n",
        "  return tf.data.Dataset.from_tensor_slices(np_dataset)\n",
        "\n",
        "\n",
        "class MovielensModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, user_model, movie_model, tf_transform_output, movies_uri):\n",
        "    super().__init__()\n",
        "    self.movie_model: tf.keras.Model = movie_model\n",
        "    self.user_model: tf.keras.Model = user_model\n",
        "\n",
        "    movies_artifact = movies_uri.get()[0]\n",
        "    input_dir = artifact_utils.get_split_uri([movies_artifact], 'train')\n",
        "    movie_files = glob.glob(os.path.join(input_dir, '*'))\n",
        "    movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")\n",
        "    movies_dataset = extract_str_feature(movies, 'movie_title')\n",
        "\n",
        "    loss_metrics = tfrs.metrics.FactorizedTopK(\n",
        "        candidates=movies_dataset.batch(128).map(movie_model)\n",
        "        )\n",
        "\n",
        "    self.task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "        metrics=loss_metrics\n",
        "        )\n",
        "\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    try:\n",
        "      user_embeddings = tf.squeeze(self.user_model(features['user_id']), axis=1)\n",
        "      # And pick out the movie features and pass them into the movie model,\n",
        "      # getting embeddings back.\n",
        "      positive_movie_embeddings = self.movie_model(features['movie_title'])\n",
        "\n",
        "      # The task computes the loss and the metrics.\n",
        "      _task = self.task(user_embeddings, positive_movie_embeddings)\n",
        "    except BaseException as err:\n",
        "      logging.error('######## ERROR IN compute_loss:\\n{}\\n###############'.format(err))\n",
        "\n",
        "    return _task\n",
        "\n",
        "\n",
        "# This function will apply the same transform operation to training data\n",
        "# and serving requests.\n",
        "def _apply_preprocessing(raw_features, tft_layer):\n",
        "  try:\n",
        "    transformed_features = tft_layer(raw_features)\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN _apply_preprocessing:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return transformed_features\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size),\n",
        "      tf_transform_output.transformed_metadata.schema)\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN _input_fn:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "  \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
        "  try:\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "      \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "      try:\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        result = model(transformed_features)\n",
        "      except BaseException as err:\n",
        "        logging.error('######## ERROR IN serve_tf_examples_fn:\\n{}\\n###############'.format(err))\n",
        "      return result\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN _get_serve_tf_examples_fn:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _build_user_model(\n",
        "    tf_transform_output: tft.TFTransformOutput, # Specific to ratings\n",
        "    embedding_dimension: int = 32) -> tf.keras.Model:\n",
        "  \"\"\"Creates a Keras model for the query tower.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: [tft.TFTransformOutput], the results of Transform\n",
        "    embedding_dimension: [int], the dimensionality of the embedding space\n",
        "\n",
        "  Returns:\n",
        "    A keras Model.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    unique_user_ids = tf_transform_output.vocabulary_by_name('user_id_vocab')\n",
        "    users_vocab_str = [b.decode() for b in unique_user_ids]\n",
        "\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.StringLookup(\n",
        "             vocabulary=users_vocab_str, mask_token=None),\n",
        "         # We add an additional embedding to account for unknown tokens.\n",
        "         tf.keras.layers.Embedding(len(users_vocab_str) + 1, embedding_dimension)\n",
        "         ])\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN _build_user_model:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def _build_movie_model(\n",
        "    tf_transform_output: tft.TFTransformOutput, # Specific to movies\n",
        "    embedding_dimension: int = 32) -> tf.keras.Model:\n",
        "  \"\"\"Creates a Keras model for the candidate tower.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: [tft.TFTransformOutput], the results of Transform\n",
        "    embedding_dimension: [int], the dimensionality of the embedding space\n",
        "\n",
        "  Returns:\n",
        "    A keras Model.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    unique_movie_titles = tf_transform_output.vocabulary_by_name('movie_title_vocab')\n",
        "    titles_vocab_str = [b.decode() for b in unique_movie_titles]\n",
        "\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.StringLookup(\n",
        "             vocabulary=titles_vocab_str, mask_token=None),\n",
        "         # We add an additional embedding to account for unknown tokens.\n",
        "         tf.keras.layers.Embedding(len(titles_vocab_str) + 1, embedding_dimension)\n",
        "        ])\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN _build_movie_model:\\n{}\\n###############'.format(err))\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n",
        "                              tf_transform_output, INPUT_FN_BATCH_SIZE)\n",
        "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n",
        "                            tf_transform_output, INPUT_FN_BATCH_SIZE)\n",
        "\n",
        "    model = MovielensModel(\n",
        "        _build_user_model(tf_transform_output, EMBEDDING_DIMENSION),\n",
        "        _build_movie_model(tf_transform_output, EMBEDDING_DIMENSION),\n",
        "        tf_transform_output,\n",
        "        fn_args.custom_config['movies']\n",
        "        )\n",
        "\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=fn_args.model_run_dir, update_freq='batch')\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN run_fn before fit:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  try:\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=fn_args.custom_config['epochs'],\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        callbacks=[tensorboard_callback])\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN run_fn during fit:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  try:\n",
        "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "\n",
        "    movies_artifact = fn_args.custom_config['movies'].get()[0]\n",
        "    input_dir = artifact_utils.get_split_uri([movies_artifact], 'eval')\n",
        "    movie_files = glob.glob(os.path.join(input_dir, '*'))\n",
        "    movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")\n",
        "\n",
        "    movies_dataset = extract_str_feature(movies, 'movie_title')\n",
        "\n",
        "    index.index_from_dataset(\n",
        "      tf.data.Dataset.zip((\n",
        "          movies_dataset.batch(100),\n",
        "          movies_dataset.batch(100).map(model.movie_model))\n",
        "      )\n",
        "    )\n",
        "\n",
        "    # Run once so that we can get the right signatures into SavedModel\n",
        "    _, titles = index(tf.constant([\"42\"]))\n",
        "    print(f\"Recommendations for user 42: {titles[0, :3]}\")\n",
        "\n",
        "    signatures = {\n",
        "        'serving_default':\n",
        "            _get_serve_tf_examples_fn(index,\n",
        "                                      tf_transform_output).get_concrete_function(\n",
        "                                          tf.TensorSpec(\n",
        "                                              shape=[None],\n",
        "                                              dtype=tf.string,\n",
        "                                              name='examples')),\n",
        "    }\n",
        "    index.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n",
        "\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN run_fn during export:\\n{}\\n###############'.format(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDN_LJGlnRGo"
      },
      "source": [
        "## 训练模型\n",
        "\n",
        "定义模型后，我们可以运行 [Trainer 组件](https://tensorflow.google.cn/tfx/guide/trainer)来进行模型训练。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsWC8UpVrngY"
      },
      "outputs": [],
      "source": [
        "trainer = tfx.components.Trainer(\n",
        "    module_file=os.path.abspath(_trainer_module_file),\n",
        "    examples=ratings_transform.outputs['transformed_examples'],\n",
        "    transform_graph=ratings_transform.outputs['transform_graph'],\n",
        "    schema=ratings_transform.outputs['post_transform_schema'],\n",
        "    train_args=tfx.proto.TrainArgs(num_steps=500),\n",
        "    eval_args=tfx.proto.EvalArgs(num_steps=10),\n",
        "    custom_config={\n",
        "        'epochs':5,\n",
        "        'movies':movies_transform.outputs['transformed_examples'],\n",
        "        'movie_schema':movies_transform.outputs['post_transform_schema'],\n",
        "        'ratings':ratings_transform.outputs['transformed_examples'],\n",
        "        'ratings_schema':ratings_transform.outputs['post_transform_schema']\n",
        "        })\n",
        "\n",
        "context.run(trainer, enable_cache=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WXkKKZpRkaj"
      },
      "source": [
        "## 导出模型\n",
        "\n",
        "训练模型后，我们可以使用 [Pusher 组件](https://tensorflow.google.cn/tfx/guide/pusher)导出模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hXlzwMTRkaj"
      },
      "outputs": [],
      "source": [
        "_serving_model_dir = os.path.join(tempfile.mkdtemp(), 'serving_model/tfrs_retrieval')\n",
        "\n",
        "pusher = tfx.components.Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    push_destination=tfx.proto.PushDestination(\n",
        "        filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher, enable_cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB2v43NJU3Xf"
      },
      "source": [
        "## 进行预测\n",
        "\n",
        "现在我们获得了一个模型，我们将其加载回来并进行预测。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUwd9QoGRkaj"
      },
      "outputs": [],
      "source": [
        "loaded = tf.saved_model.load(pusher.outputs['pushed_model'].get()[0].uri)\n",
        "scores, titles = loaded([\"42\"])\n",
        "\n",
        "print(f\"Recommendations: {titles[0][:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ipAubFuRkak"
      },
      "source": [
        "## 后续步骤\n",
        "\n",
        "在本教程中，您学习了如何使用 TensorFlow Recommenders 和 TFX 实现检索模型。要扩展此处介绍的内容，请查阅[使用 TFX 进行 TFRS 排名](https://tensorflow.google.cn/recommenders/examples/ranking_tfx)教程。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "X80i_girFR2o"
      ],
      "name": "recommenders.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
