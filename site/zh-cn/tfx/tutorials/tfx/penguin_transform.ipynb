{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjUA6S30k52h"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SpNWyqewk8fE"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x1ypzczQCwy"
      },
      "source": [
        "# 使用 TFX Pipeline 和 TensorFlow Transform 进行特征工程\n",
        "\n",
        "***转换输入数据并使用 TFX 流水线训练模型。***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU9YYythm0dx"
      },
      "source": [
        "注：我们建议在 Colab 笔记本中运行本教程，无需进行设置！只需点击“在 Google Colab 中运行”\n",
        "\n",
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://tensorflow.google.cn/tfx/tutorials/tfx/penguin_tft\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org上查看</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tfx/tutorials/tfx/penguin_transform.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tfx/tutorials/tfx/penguin_transform.ipynb\"><img width=\"32px\" src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 Github 上查看源代码</a></td>\n",
        "<td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/tfx/tutorials/tfx/penguin_transform.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a></td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VuwrlnvQJ5k"
      },
      "source": [
        "在这个基于笔记本的教程中，我们将创建并运行 TFX 流水线来采集原始输入数据并对其进行适当的预处理以进行 ML 训练。此笔记本基于我们在[使用 TFX Pipeline 和 TensorFlow Data Validation 进行数据验证教程](https://tensorflow.google.cn/tfx/tutorials/tfx/penguin_tfdv)中构建的 TFX 流水线。如果您尚未阅读该教程，应在继续使用此笔记本之前阅读。\n",
        "\n",
        "您可以通过特征工程化提高数据的预测质量和/或降低维度。使用 TFX 的好处之一是您只需编写一次转换代码，生成的转换将在训练和应用之间保持一致，以避免训练/应用偏差。\n",
        "\n",
        "我们将向流水线中添加一个 `Transform` 组件。Transform 组件是使用 [tf.transform](https://tensorflow.google.cn/tfx/transform/get_started) 库实现的。\n",
        "\n",
        "要了解有关 TFX 中各种概念的更多信息，请参阅[了解 TFX 流水线](https://tensorflow.google.cn/tfx/guide/understanding_tfx_pipelines)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmgi8ZvQkScg"
      },
      "source": [
        "## 安装\n",
        "\n",
        "我们首先需要安装 TFX Python 软件包并下载将用于模型的数据集。\n",
        "\n",
        "### 升级 Pip\n",
        "\n",
        "为了避免在本地运行时升级系统中的 Pip，请检查以确保在 Colab 中运行。当然，可以对本地系统单独升级。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as4OTe2ukSqm"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZOYTt1RW4TK"
      },
      "source": [
        "### 安装 TFX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyQtljP-qPHY"
      },
      "outputs": [],
      "source": [
        "!pip install -U tfx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQnYqtqOlA5l"
      },
      "source": [
        "### 卸载 Shapely\n",
        "\n",
        "TODO(b/263441833) 这是避免 ImportError 的临时解决方案。最终，应该通过支持最新版本的 Bigquery 来处理，而不是卸载其他额外的依赖项。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e8hUMPrlFXJ"
      },
      "outputs": [],
      "source": [
        "!pip uninstall shapely -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwT0nov5QO1M"
      },
      "source": [
        "### 是否已重新启动运行时？\n",
        "\n",
        "如果您使用的是 Google Colab，则在首次运行上面的代码单元时必须重新启动运行时，方法是单击上面的“重新启动运行时”按钮或使用“运行时 &gt; 重新启动运行时...”菜单。这样做的原因是 Colab 加载软件包的方式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDnPgN8UJtzN"
      },
      "source": [
        "检查 TensorFlow 和 TFX 版本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jh7vKSRqPHb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDtLdSkvqPHe"
      },
      "source": [
        "### 设置变量\n",
        "\n",
        "有一些变量用于定义流水线。您可以根据需要自定义这些变量。默认情况下，流水线的所有输出都将在当前目录下生成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcUseqJaE2XN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PIPELINE_NAME = \"penguin-transform\"\n",
        "\n",
        "# Output directory to store artifacts generated from the pipeline.\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
        "# Path to a SQLite DB file to use as an MLMD storage.\n",
        "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
        "# Output directory where created models from the pipeline will be exported.\n",
        "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
        "\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.INFO)  # Set default logging level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsO0l5F3dzOr"
      },
      "source": [
        "### 准备示例数据\n",
        "\n",
        "我们将下载要在我们的 TFX 流水线中使用的示例数据集。我们要使用的数据集为 [Palmer Penguins 数据集](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)。\n",
        "\n",
        "然而，与之前使用已经预处理的数据集的教程不同，我们将使用**原始** Palmer Penguins 数据集。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11J7XiCq6AFP"
      },
      "source": [
        "因为 TFX ExampleGen 组件从目录中读取输入，所以我们需要创建一个目录并将数据集复制到其中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fxMs6u86acP"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import tempfile\n",
        "\n",
        "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
        "_data_path = 'https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv'\n",
        "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
        "urllib.request.urlretrieve(_data_path, _data_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASpoNmxKSQjI"
      },
      "source": [
        "快速浏览一下原始数据的样子。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eSz28UDSnlG"
      },
      "outputs": [],
      "source": [
        "!head {_data_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTtQNq1DdVvG"
      },
      "source": [
        "有一些缺失值的条目表示为 `NA`。我们将在本教程中删除这些条目。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQhpoaqff9ca"
      },
      "outputs": [],
      "source": [
        "!sed -i '/\\bNA\\b/d' {_data_filepath}\n",
        "!head {_data_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8EOfCy1dzO2"
      },
      "source": [
        "您应该能够看到描述企鹅的七个特征。我们将使用与之前教程相同的一组特征 -“culmen_length_mm”、“culmen_depth_mm”、“flipper_length_mm”、“body_mass_g”，并将预测企鹅的“物种”。\n",
        "\n",
        "**唯一的区别是输入数据没有经过预处理。**请注意，我们不会在本教程中使用其他特征，例如“island”或“sex”。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtbrkjjc-IKA"
      },
      "source": [
        "### 准备架构文件\n",
        "\n",
        "如[使用 TFX Pipeline 和 TensorFlow Data Validation 进行数据验证教程](https://tensorflow.google.cn/tfx/tutorials/tfx/penguin_tfdv)中所述，我们需要数据集的架构文件。因为数据集与上一个教程不同，所以我们需要再次生成它。在本教程中，我们将跳过这些步骤，只使用准备好的架构文件。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDoB97m8B9nG"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "SCHEMA_PATH = 'schema'\n",
        "\n",
        "_schema_uri = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/schema/raw/schema.pbtxt'\n",
        "_schema_filename = 'schema.pbtxt'\n",
        "_schema_filepath = os.path.join(SCHEMA_PATH, _schema_filename)\n",
        "\n",
        "os.makedirs(SCHEMA_PATH, exist_ok=True)\n",
        "urllib.request.urlretrieve(_schema_uri, _schema_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKJ_HDJQB94b"
      },
      "source": [
        "此架构文件是使用与上一教程中相同的流水线创建的，无需任何手动更改。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH6gizcpSwWV"
      },
      "source": [
        "## 创建流水线\n",
        "\n",
        "TFX 流水线是使用 Python API 定义的。我们将向已在 [Data Validation 教程](https://tensorflow.google.cn/tfx/tutorials/tfx/penguin_tfdv)中创建的流水线中添加 `Transform` 组件。\n",
        "\n",
        "Transform 组件需要来自 `ExampleGen` 组件的输入数据和来自 `SchemaGen` 组件的架构，并生成“转换图”。输出将用于 `Trainer` 组件。另外，Transform 还可以选择性地生成“转换后的数据”，也就是转换后的具体化数据。但是，我们将在本教程中的训练期间转换数据，而不会具体化中间转换数据。\n",
        "\n",
        "需要注意的一点是，我们需要定义一个 Python 函数 (`preprocessing_fn`) 来描述应如何转换输入数据。这类似于 Trainer 组件，同样需要用户代码来定义模型。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjDv93eS5xV"
      },
      "source": [
        "### 编写预处理和训练代码\n",
        "\n",
        "我们需要定义两个 Python 函数。一个用于 Transform，一个用于 Trainer。\n",
        "\n",
        "#### preprocessing_fn\n",
        "\n",
        "Transform 组件将在给定的模块文件中找到一个名为 `preprocessing_fn` 的函数，就像我们针对 `Trainer` 组件所做的那样。您还可以使用 Transform 组件的 <a data-md-type=\"raw_html\" href=\"https://github.com/tensorflow/tfx/blob/142de6e887f26f4101ded7925f60d7d4fe9d42ed/tfx/components/transform/component.py#L113\">`preprocessing_fn` 参数</a>指定具体函数。\n",
        "\n",
        "在本例中，我们将进行两种转换。对于像 `culmen_length_mm` 和 `body_mass_g` 这样的连续数字特征，我们将使用 [tft.scale_to_z_score](https://tensorflow.google.cn/tfx/transform/api_docs/python/tft/scale_to_z_score) 函数对这些值进行规范化。对于标签特征，我们需要将字符串标签转换为数字索引值。我们将使用 [`tf.lookup.StaticHashTable`](https://tensorflow.google.cn/api_docs/python/tf/lookup/StaticHashTable) 进行转换。\n",
        "\n",
        "为了轻松找出转换后的字段，我们将 `_xf` 后缀附加到转换后的特征名称。\n",
        "\n",
        "#### run_fn\n",
        "\n",
        "模型本身与之前的教程几乎相同，但这次我们将使用来自 Transform 组件的转换图来转换输入数据。\n",
        "\n",
        "与上一教程相比，一个更重要的区别是，我们现在导出了用于提供服务的模型，其不仅包括模型的计算图，还包括在 Transform 组件中生成的用于预处理的转换图。我们需要定义一个单独的函数来为传入的请求提供服务。您可以看到，训练数据和为请求提供服务使用的是同一函数 `_apply_preprocessing`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aES7Hv5QTDK3"
      },
      "outputs": [],
      "source": [
        "_module_file = 'penguin_utils.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnc67uQNTDfW"
      },
      "outputs": [],
      "source": [
        "%%writefile {_module_file}\n",
        "\n",
        "\n",
        "from typing import List, Text\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.public import tfxio\n",
        "\n",
        "# Specify features that we will use.\n",
        "_FEATURE_KEYS = [\n",
        "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
        "]\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "_EVAL_BATCH_SIZE = 10\n",
        "\n",
        "\n",
        "# NEW: TFX Transform will call this function.\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "\n",
        "  Args:\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\n",
        "\n",
        "  Returns:\n",
        "    Map from string feature key to transformed feature.\n",
        "  \"\"\"\n",
        "  outputs = {}\n",
        "\n",
        "  # Uses features defined in _FEATURE_KEYS only.\n",
        "  for key in _FEATURE_KEYS:\n",
        "    # tft.scale_to_z_score computes the mean and variance of the given feature\n",
        "    # and scales the output based on the result.\n",
        "    outputs[key] = tft.scale_to_z_score(inputs[key])\n",
        "\n",
        "  # For the label column we provide the mapping from string to index.\n",
        "  # We could instead use `tft.compute_and_apply_vocabulary()` in order to\n",
        "  # compute the vocabulary dynamically and perform a lookup.\n",
        "  # Since in this example there are only 3 possible values, we use a hard-coded\n",
        "  # table for simplicity.\n",
        "  table_keys = ['Adelie', 'Chinstrap', 'Gentoo']\n",
        "  initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "      keys=table_keys,\n",
        "      values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
        "      key_dtype=tf.string,\n",
        "      value_dtype=tf.int64)\n",
        "  table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
        "  outputs[_LABEL_KEY] = table.lookup(inputs[_LABEL_KEY])\n",
        "\n",
        "  return outputs\n",
        "\n",
        "\n",
        "# NEW: This function will apply the same transform operation to training data\n",
        "#      and serving requests.\n",
        "def _apply_preprocessing(raw_features, tft_layer):\n",
        "  transformed_features = tft_layer(raw_features)\n",
        "  if _LABEL_KEY in raw_features:\n",
        "    transformed_label = transformed_features.pop(_LABEL_KEY)\n",
        "    return transformed_features, transformed_label\n",
        "  else:\n",
        "    return transformed_features, None\n",
        "\n",
        "\n",
        "# NEW: This function will create a handler function which gets a serialized\n",
        "#      tf.example, preprocess and run an inference with it.\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "  # We must save the tft_layer to the model to ensure its assets are kept and\n",
        "  # tracked.\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
        "  ])\n",
        "  def serve_tf_examples_fn(serialized_tf_examples):\n",
        "    # Expected input is a string which is serialized tf.Example format.\n",
        "    feature_spec = tf_transform_output.raw_feature_spec()\n",
        "    # Because input schema includes unnecessary fields like 'species' and\n",
        "    # 'island', we filter feature_spec to include required keys only.\n",
        "    required_feature_spec = {\n",
        "        k: v for k, v in feature_spec.items() if k in _FEATURE_KEYS\n",
        "    }\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples,\n",
        "                                          required_feature_spec)\n",
        "\n",
        "    # Preprocess parsed input with transform operation defined in\n",
        "    # preprocessing_fn().\n",
        "    transformed_features, _ = _apply_preprocessing(parsed_features,\n",
        "                                                   model.tft_layer)\n",
        "    # Run inference with ML model.\n",
        "    return model(transformed_features)\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  dataset = data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(batch_size=batch_size),\n",
        "      schema=tf_transform_output.raw_metadata.schema)\n",
        "\n",
        "  transform_layer = tf_transform_output.transform_features_layer()\n",
        "  def apply_transform(raw_features):\n",
        "    return _apply_preprocessing(raw_features, transform_layer)\n",
        "\n",
        "  return dataset.map(apply_transform).repeat()\n",
        "\n",
        "\n",
        "def _build_keras_model() -> tf.keras.Model:\n",
        "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model.\n",
        "  \"\"\"\n",
        "  # The model below is built with Functional API, please refer to\n",
        "  # https://tensorflow.google.cn/guide/keras/overview for all API options.\n",
        "  inputs = [\n",
        "      keras.layers.Input(shape=(1,), name=key)\n",
        "      for key in _FEATURE_KEYS\n",
        "  ]\n",
        "  d = keras.layers.concatenate(inputs)\n",
        "  for _ in range(2):\n",
        "    d = keras.layers.Dense(8, activation='relu')(d)\n",
        "  outputs = keras.layers.Dense(3)(d)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  model.summary(print_fn=logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "  train_dataset = _input_fn(\n",
        "      fn_args.train_files,\n",
        "      fn_args.data_accessor,\n",
        "      tf_transform_output,\n",
        "      batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(\n",
        "      fn_args.eval_files,\n",
        "      fn_args.data_accessor,\n",
        "      tf_transform_output,\n",
        "      batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  model = _build_keras_model()\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps)\n",
        "\n",
        "  # NEW: Save a computation graph including transform layer.\n",
        "  signatures = {\n",
        "      'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output),\n",
        "  }\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blaw0rs-emEf"
      },
      "source": [
        "现在，您已完成构建 TFX 流水线的所有准备步骤。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3OkNz3gTLwM"
      },
      "source": [
        "### 编写流水线定义\n",
        "\n",
        "我们定义一个函数来创建 TFX 流水线。`Pipeline` 对象表示 TFX 流水线，可使用 TFX 支持的流水线编排系统之一来运行。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M49yYVNBTPd4"
      },
      "outputs": [],
      "source": [
        "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
        "                     schema_path: str, module_file: str, serving_model_dir: str,\n",
        "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
        "  \"\"\"Implements the penguin pipeline with TFX.\"\"\"\n",
        "  # Brings data into the pipeline or otherwise joins/converts training data.\n",
        "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
        "\n",
        "  # Computes statistics over data for visualization and example validation.\n",
        "  statistics_gen = tfx.components.StatisticsGen(\n",
        "      examples=example_gen.outputs['examples'])\n",
        "\n",
        "  # Import the schema.\n",
        "  schema_importer = tfx.dsl.Importer(\n",
        "      source_uri=schema_path,\n",
        "      artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n",
        "          'schema_importer')\n",
        "\n",
        "  # Performs anomaly detection based on statistics and data schema.\n",
        "  example_validator = tfx.components.ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=schema_importer.outputs['result'])\n",
        "\n",
        "  # NEW: Transforms input data using preprocessing_fn in the 'module_file'.\n",
        "  transform = tfx.components.Transform(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=schema_importer.outputs['result'],\n",
        "      materialize=False,\n",
        "      module_file=module_file)\n",
        "\n",
        "  # Uses user-provided Python function that trains a model.\n",
        "  trainer = tfx.components.Trainer(\n",
        "      module_file=module_file,\n",
        "      examples=example_gen.outputs['examples'],\n",
        "\n",
        "      # NEW: Pass transform_graph to the trainer.\n",
        "      transform_graph=transform.outputs['transform_graph'],\n",
        "\n",
        "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
        "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
        "\n",
        "  # Pushes the model to a filesystem destination.\n",
        "  pusher = tfx.components.Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      push_destination=tfx.proto.PushDestination(\n",
        "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)))\n",
        "\n",
        "  components = [\n",
        "      example_gen,\n",
        "      statistics_gen,\n",
        "      schema_importer,\n",
        "      example_validator,\n",
        "\n",
        "      transform,  # NEW: Transform component was added to the pipeline.\n",
        "\n",
        "      trainer,\n",
        "      pusher,\n",
        "  ]\n",
        "\n",
        "  return tfx.dsl.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      metadata_connection_config=tfx.orchestration.metadata\n",
        "      .sqlite_metadata_connection_config(metadata_path),\n",
        "      components=components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJbq07THU2GV"
      },
      "source": [
        "## 运行流水线\n",
        "\n",
        "我们将像上一教程一样使用 `LocalDagRunner`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAtfOZTYWJu-"
      },
      "outputs": [],
      "source": [
        "tfx.orchestration.LocalDagRunner().run(\n",
        "  _create_pipeline(\n",
        "      pipeline_name=PIPELINE_NAME,\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      data_root=DATA_ROOT,\n",
        "      schema_path=SCHEMA_PATH,\n",
        "      module_file=_module_file,\n",
        "      serving_model_dir=SERVING_MODEL_DIR,\n",
        "      metadata_path=METADATA_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppERq0Mj6xvW"
      },
      "source": [
        "如果流水线成功完成，您应该看到“INFO:absl:Component Pusher is finished.”。\n",
        "\n",
        "如果您在之前的步骤中未更改变量，Pusher 组件则会将经过训练的模型推送到 `SERVING_MODEL_DIR`，即 `serving_model/penguin-transform` 目录。您可以从 Colab 左侧面板中的文件浏览器查看结果，或者使用以下命令："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTHROkqX6yHx"
      },
      "outputs": [],
      "source": [
        "# List files in created model directory.\n",
        "!find {SERVING_MODEL_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTqM-WiZkPbt"
      },
      "source": [
        "您还可以使用 [`saved_model_cli` 工具](https://tensorflow.google.cn/guide/saved_model#show_command)检查生成的模型的签名。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBfUzD_OkOq_"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir {SERVING_MODEL_DIR}/$(ls -1 {SERVING_MODEL_DIR} | sort -nr | head -1) --tag_set serve --signature_def serving_default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkAxFs_QszoZ"
      },
      "source": [
        "因为我们使用自己的 `serve_tf_examples_fn` 函数定义了 `serving_default`，所以签名显示它采用单个字符串。此字符串是 tf.Examples 的序列化字符串，将使用我们之前定义的 [tf.io.parse_example()](https://tensorflow.google.cn/api_docs/python/tf/io/parse_example) 函数进行解析（在[此处](https://tensorflow.google.cn/tutorials/load_data/tfrecord)了解有关 tf.Examples 的更多信息）。\n",
        "\n",
        "我们可以加载导出的模型，并通过几个示例尝试一些推断。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1Yw5yYdvqKf"
      },
      "outputs": [],
      "source": [
        "# Find a model with the latest timestamp.\n",
        "model_dirs = (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir())\n",
        "model_path = max(model_dirs, key=lambda i: int(i.name)).path\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "inference_fn = loaded_model.signatures['serving_default']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrOHIvnIv0-4"
      },
      "outputs": [],
      "source": [
        "# Prepare an example and run inference.\n",
        "features = {\n",
        "  'culmen_length_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[49.9])),\n",
        "  'culmen_depth_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[16.1])),\n",
        "  'flipper_length_mm': tf.train.Feature(int64_list=tf.train.Int64List(value=[213])),\n",
        "  'body_mass_g': tf.train.Feature(int64_list=tf.train.Int64List(value=[5400])),\n",
        "}\n",
        "example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "examples = example_proto.SerializeToString()\n",
        "\n",
        "result = inference_fn(examples=tf.constant([examples]))\n",
        "print(result['output_0'].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cri3mTgZ0SQ2"
      },
      "source": [
        "对应于“Gentoo”物种的第三个元素预计是三个元素中最大的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08R8qvweThRf"
      },
      "source": [
        "## 后续步骤\n",
        "\n",
        "如果您想了解有关 Transform 组件的更多信息，请参阅 [Transform 组件指南](https://tensorflow.google.cn/tfx/guide/transform)。您可以在 https://tensorflow.google.cn/tfx/tutorials 上找到更多资源。\n",
        "\n",
        "要了解有关 TFX 中各种概念的更多信息，请参阅[了解 TFX Pipelines](https://tensorflow.google.cn/tfx/guide/understanding_tfx_pipelines)。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DjUA6S30k52h"
      ],
      "name": "penguin_transform.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
