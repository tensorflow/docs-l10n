{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAttKaKmT435"
      },
      "source": [
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://tensorflow.google.cn/tfx/tutorials/transform/census\">\n",
        "<img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\" />View on TensorFlow.org</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tfx/tutorials/transform/census.ipynb\">\n",
        "<img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">Run in Google Colab</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tfx/tutorials/transform/census.ipynb\">\n",
        "<img width=32px src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">View source on GitHub</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/tfx/tutorials/transform/census.ipynb\">\n",
        "<img width=32px src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">Download notebook</a></td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tghWegsjhpkt"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rSGJWC5biBiG"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPt5BHTwy_0F"
      },
      "source": [
        "# 使用 TensorFlow Transform 预处理数据\n",
        "\n",
        "***TensorFlow Extended (TFX) 的特征工程组件***\n",
        "\n",
        "此示例 Colab 笔记本提供了一个更高级的示例，说明了如何使用 <a target=\"_blank\" href=\"https://tensorflow.google.cn/tfx/transform/\">TensorFlow Transform</a> (`tf.Transform`) 预处理数据，此示例使用完全相同的代码训练模型和在生产环境中应用推断。\n",
        "\n",
        "TensorFlow Transform 是一个用于预处理 TensorFlow 输入数据的库，包括创建需要在训练数据集上进行完整传递的特征。利用 TensorFlow Transform，您可以：\n",
        "\n",
        "- 使用平均值和标准差归一化输入值\n",
        "- 通过在所有输入值上生成词汇将字符串转换为整数\n",
        "- 根据观测到的数据分布，通过分配给桶将浮点数转换为整数\n",
        "\n",
        "TensorFlow 内置了对在单个样本或一批样本上进行操作的支持。`tf.Transform` 扩展了这些功能，支持在整个训练数据集上进行完整传递。\n",
        "\n",
        "`tf.Transform` 的输出将导出为可用于训练和应用的 TensorFlow 计算图。将同一个计算图用于训练和应用可以避免偏差，因为会在两个阶段应用相同的转换。\n",
        "\n",
        "要点：要了解 `tf.Transform` 以及它如何与 Apache Beam 配合使用，您需要对 Apache Beam 有所了解。最好是从<a target=\"_blank\" href=\"https://beam.apache.org/documentation/programming-guide/\">Beam 编程指南</a>开始着手。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQUubddMvnP"
      },
      "source": [
        "##我们在此示例中执行的操作\n",
        "\n",
        "在此示例中，我们将处理<a target=\"_blank\" href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult\">包含人口普查数据的广泛使用的数据集</a>，并训练模型进行分类。在这个过程中，我们将使用 `tf.Transform` 转换数据。\n",
        "\n",
        "要点：作为模型创建者和开发者，思考如何使用这些数据以及模型预测的潜在好处和危害。此类模型可能会加剧社会偏见和差距。某个特征与您要解决的问题相关，还是会引入偏差？有关更多信息，请阅读 <a target=\"_blank\" href=\"https://developers.google.com/machine-learning/fairness-overview/\">ML 公平性</a>。\n",
        "\n",
        "注：<a target=\"_blank\" href=\"https://tensorflow.google.cn/tfx/model_analysis\">TensorFlow Model Analysis</a> 是了解模型对数据各个部分的预测能力的强大工具，包括了解模型如何加剧社会偏见和差距。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7z7z62TmTNT"
      },
      "source": [
        "### 升级 Pip\n",
        "\n",
        "为了避免在本地运行时升级系统中的 Pip，请检查以确保我们在 Colab 中运行。当然，可以单独升级本地系统。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pjqv52BOmTau"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeonII4omTr1"
      },
      "source": [
        "### 安装 TensorFlow Transform\n",
        "\n",
        "**注：在 Google Colab 中，由于软件包更新，第一次运行此代码单元时必须重新启动运行时 (Runtime &gt; Restart runtime ...)。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ak6XDO5mT3m"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RptgLn2RYuK3"
      },
      "source": [
        "## Python 检查、导入和全局\n",
        "\n",
        "首先，我们要确保使用的是 Python 3，然后继续安装并导入所需的内容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFcdSuXTidhH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Confirm that we're using Python 3\n",
        "assert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4QXVIM7iglN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TF: {}'.format(tf.__version__))\n",
        "\n",
        "import apache_beam as beam\n",
        "print('Beam: {}'.format(beam.__version__))\n",
        "\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "print('Transform: {}'.format(tft.__version__))\n",
        "\n",
        "from tfx_bsl.public import tfxio\n",
        "from tfx_bsl.coders.example_coder import RecordBatchToExamples\n",
        "\n",
        "!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data\n",
        "!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.test\n",
        "\n",
        "train = './adult.data'\n",
        "test = './adult.test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxOxaaOYRfl7"
      },
      "source": [
        "### 为列命名\n",
        "\n",
        "我们将创建一些方便的列表来引用数据集中的列。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bsr1nLHqyg_"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'native-country',\n",
        "]\n",
        "NUMERIC_FEATURE_KEYS = [\n",
        "    'age',\n",
        "    'capital-gain',\n",
        "    'capital-loss',\n",
        "    'hours-per-week',\n",
        "]\n",
        "OPTIONAL_NUMERIC_FEATURE_KEYS = [\n",
        "    'education-num',\n",
        "]\n",
        "ORDERED_CSV_COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n",
        "]\n",
        "LABEL_KEY = 'label'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTn4at8rurk"
      },
      "source": [
        "###定义特征和架构<br>我们根据输入中列的类型来定义一个架构。这将有助于正确导入它们，也将惠及其他操作。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oS2RfyCrzMr"
      },
      "outputs": [],
      "source": [
        "RAW_DATA_FEATURE_SPEC = dict(\n",
        "    [(name, tf.io.FixedLenFeature([], tf.string))\n",
        "     for name in CATEGORICAL_FEATURE_KEYS] +\n",
        "    [(name, tf.io.FixedLenFeature([], tf.float32))\n",
        "     for name in NUMERIC_FEATURE_KEYS] +\n",
        "    [(name, tf.io.VarLenFeature(tf.float32))\n",
        "     for name in OPTIONAL_NUMERIC_FEATURE_KEYS] +\n",
        "    [(LABEL_KEY, tf.io.FixedLenFeature([], tf.string))]\n",
        ")\n",
        "\n",
        "SCHEMA = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
        "    tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)).schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdXy9lo4t45d"
      },
      "source": [
        "###设置超参数和基本整理<br>常量和超参数用于训练。桶大小包括数据集描述中列出的所有类别以及一个表示未知的“?”的额外类别。\n",
        "\n",
        "注：在未来版本中，实例数将由 `tf.Transform` 计算，在这种情况下，可以从元数据中读取实例数。同理，将不需要 BUCKET_SIZES，因为此信息将存储在每个列的元数据中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WHyOkC9uL71"
      },
      "outputs": [],
      "source": [
        "testing = os.getenv(\"WEB_TEST_BROWSER\", False)\n",
        "NUM_OOV_BUCKETS = 1\n",
        "if testing:\n",
        "  TRAIN_NUM_EPOCHS = 1\n",
        "  NUM_TRAIN_INSTANCES = 1\n",
        "  TRAIN_BATCH_SIZE = 1\n",
        "  NUM_TEST_INSTANCES = 1\n",
        "else:\n",
        "  TRAIN_NUM_EPOCHS = 16\n",
        "  NUM_TRAIN_INSTANCES = 32561\n",
        "  TRAIN_BATCH_SIZE = 128\n",
        "  NUM_TEST_INSTANCES = 16281\n",
        "\n",
        "# Names of temp files\n",
        "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
        "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
        "EXPORTED_MODEL_DIR = 'exported_model_dir'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a1ns5KswDb2"
      },
      "source": [
        "##使用 `tf.Transform` 进行预处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKd3mCLNVYmg"
      },
      "source": [
        "###创建一个 `tf.Transform` preprocessing_fn<br>*预处理函数*是 tf.Transform 最重要的概念。预处理函数是真正发生数据集转换的地方。它接受并返回一个张量字典，其中张量是指 [`Tensor`](https://tensorflow.google.cn/versions/r1.15/api_docs/python/tf/Tensor) 或 [`SparseTensor`](https://tensorflow.google.cn/versions/r1.15/api_docs/python/tf/SparseTensor)。有两组主要的 API 调用，它们通常构成预处理函数的核心：\n",
        "\n",
        "1. **TensorFlow 运算**：接受并返回张量的任何函数，通常是指 TensorFlow 运算。这些函数会将 TensorFlow 运算添加到计算图中，计算图能够以每次一个特征向量的方式转换原始数据。在训练和应用期间，将为每个样本运行这种转换。\n",
        "2. **TensorFlow Transform 分析器**：tf.Transform 提供的任何分析器。这些分析器还会接受并返回张量，但与 TensorFlow 运算不同的是，它们在训练期间仅运行一次，并且通常在整个训练数据集上进行完整传递。它们会创建随后添加到计算图中的[张量常量](https://tensorflow.google.cn/versions/r1.15/api_docs/python/tf/constant)。例如，`tft.min` 可以计算某个张量在训练数据集上的最小值。tf.Transform 提供了一组固定的分析器，但是在未来版本中会对这些分析器进行扩展。\n",
        "\n",
        "警告：将预处理函数用于应用推断时，分析器在训练过程中创建的常量不会更改。如果您的数据包含趋势或季节性分量，请相应地制定计划。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDrzuYH0WFc2"
      },
      "outputs": [],
      "source": [
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "  # Since we are modifying some features and leaving others unchanged, we\n",
        "  # start by setting `outputs` to a copy of `inputs.\n",
        "  outputs = inputs.copy()\n",
        "\n",
        "  # Scale numeric columns to have range [0, 1].\n",
        "  for key in NUMERIC_FEATURE_KEYS:\n",
        "    outputs[key] = tft.scale_to_0_1(inputs[key])\n",
        "\n",
        "  for key in OPTIONAL_NUMERIC_FEATURE_KEYS:\n",
        "    # This is a SparseTensor because it is optional. Here we fill in a default\n",
        "    # value when it is missing.\n",
        "    sparse = tf.sparse.SparseTensor(inputs[key].indices, inputs[key].values,\n",
        "                                    [inputs[key].dense_shape[0], 1])\n",
        "    dense = tf.sparse.to_dense(sp_input=sparse, default_value=0.)\n",
        "    # Reshaping from a batch of vectors of size 1 to a batch to scalars.\n",
        "    dense = tf.squeeze(dense, axis=1)\n",
        "    outputs[key] = tft.scale_to_0_1(dense)\n",
        "\n",
        "  # For all categorical columns except the label column, we generate a\n",
        "  # vocabulary but do not modify the feature.  This vocabulary is instead\n",
        "  # used in the trainer, by means of a feature column, to convert the feature\n",
        "  # from a string to an integer id.\n",
        "  for key in CATEGORICAL_FEATURE_KEYS:\n",
        "    outputs[key] = tft.compute_and_apply_vocabulary(\n",
        "        tf.strings.strip(inputs[key]),\n",
        "        num_oov_buckets=NUM_OOV_BUCKETS,\n",
        "        vocab_filename=key)\n",
        "\n",
        "  # For the label column we provide the mapping from string to index.\n",
        "  table_keys = ['>50K', '<=50K']\n",
        "  with tf.init_scope():\n",
        "    initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=table_keys,\n",
        "        values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
        "        key_dtype=tf.string,\n",
        "        value_dtype=tf.int64)\n",
        "    table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
        "  # Remove trailing periods for test data when the data is read with tf.data.\n",
        "  label_str = tf.strings.regex_replace(inputs[LABEL_KEY], r'\\.', '')\n",
        "  label_str = tf.strings.strip(label_str)\n",
        "  data_labels = table.lookup(label_str)\n",
        "  transformed_label = tf.one_hot(\n",
        "      indices=data_labels, depth=len(table_keys), on_value=1.0, off_value=0.0)\n",
        "  outputs[LABEL_KEY] = tf.reshape(transformed_label, [-1, len(table_keys)])\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgAGOAdFWRn2"
      },
      "source": [
        "###转换数据<br>现在，我们准备开始在 Apache Beam 流水线中转换数据。\n",
        "\n",
        "1. 使用 CSV 阅读器读入数据\n",
        "2. 使用预处理流水线转换数据，此流水线可对数值数据进行缩放，并将分类数据从字符串转换为 int64 值索引，方法是为每个类别创建一个词汇\n",
        "3. 将结果作为 `Example` proto 的 `TFRecord` 写出来，我们稍后会使用它来训练模型\n",
        "\n",
        "<aside class=\"key-term\"><b>关键词</b>：<a target=\"_blank\" href=\"https://beam.apache.org/\">Apache Beam</a> 使用<a target=\"_blank\" href=\"https://beam.apache.org/documentation/programming-guide/#applying-transforms\">特殊的语法来定义和调用 Transform</a>。例如，在下面的代码行中：</aside>\n",
        "\n",
        "<code><blockquote>result = pass_this | 'name this step' &gt;&gt; to_this_call</blockquote></code>\n",
        "\n",
        "方法 <code>to_this_call</code> 正在被调用并传递给名为 <code>pass_this</code> 的对象，<a target=\"_blank\" href=\"https://stackoverflow.com/questions/50519662/what-does-the-redirection-mean-in-apache-beam-python\">在堆栈跟踪中，此运算被称为 <code>name this step</code></a>。调用 <code>to_this_call</code> 的结果将在 <code>result</code> 中返回。您经常会看到流水线的各个阶段像这样链接在一起：\n",
        "\n",
        "<code><blockquote>result = apache_beam.Pipeline() | 'first step' &gt;&gt; do_this_first() | 'second step' &gt;&gt; do_this_last()</blockquote></code>\n",
        "\n",
        "由于这是从一个新的流水线开始的，因此您可以按以下方式继续：\n",
        "\n",
        "<code><blockquote>next_result = result | 'doing more stuff' &gt;&gt; another_function()</blockquote></code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQAPzdCRwi5d"
      },
      "outputs": [],
      "source": [
        "def transform_data(train_data_file, test_data_file, working_dir):\n",
        "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
        "\n",
        "  Read in the data using the CSV reader, and transform it using a\n",
        "  preprocessing pipeline that scales numeric data and converts categorical data\n",
        "  from strings to int64 values indices, by creating a vocabulary for each\n",
        "  category.\n",
        "\n",
        "  Args:\n",
        "    train_data_file: File containing training data\n",
        "    test_data_file: File containing test data\n",
        "    working_dir: Directory to write transformed data and metadata to\n",
        "  \"\"\"\n",
        "\n",
        "  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
        "  # of the block.\n",
        "  with beam.Pipeline() as pipeline:\n",
        "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "      # Create a TFXIO to read the census data with the schema. To do this we\n",
        "      # need to list all columns in order since the schema doesn't specify the\n",
        "      # order of columns in the csv.\n",
        "      # We first read CSV files and use BeamRecordCsvTFXIO whose .BeamSource()\n",
        "      # accepts a PCollection[bytes] because we need to patch the records first\n",
        "      # (see \"FixCommasTrainData\" below). Otherwise, tfxio.CsvTFXIO can be used\n",
        "      # to both read the CSV files and parse them to TFT inputs:\n",
        "      # csv_tfxio = tfxio.CsvTFXIO(...)\n",
        "      # raw_data = (pipeline | 'ToRecordBatches' >> csv_tfxio.BeamSource())\n",
        "      csv_tfxio = tfxio.BeamRecordCsvTFXIO(\n",
        "          physical_format='text',\n",
        "          column_names=ORDERED_CSV_COLUMNS,\n",
        "          schema=SCHEMA)\n",
        "\n",
        "      # Read in raw data and convert using CSV TFXIO.  Note that we apply\n",
        "      # some Beam transformations here, which will not be encoded in the TF\n",
        "      # graph since we don't do the from within tf.Transform's methods\n",
        "      # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
        "      # to get data into a format that the CSV TFXIO can read, in particular\n",
        "      # removing spaces after commas.\n",
        "      raw_data = (\n",
        "          pipeline\n",
        "          | 'ReadTrainData' >> beam.io.ReadFromText(\n",
        "              train_data_file, coder=beam.coders.BytesCoder())\n",
        "          | 'FixCommasTrainData' >> beam.Map(\n",
        "              lambda line: line.replace(b', ', b','))\n",
        "          | 'DecodeTrainData' >> csv_tfxio.BeamSource())\n",
        "\n",
        "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
        "      # the schema to read the CSV data, but we also need it to interpret\n",
        "      # raw_data.\n",
        "      raw_dataset = (raw_data, csv_tfxio.TensorAdapterConfig())\n",
        "\n",
        "      # The TFXIO output format is chosen for improved performance.\n",
        "      transformed_dataset, transform_fn = (\n",
        "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(\n",
        "              preprocessing_fn, output_record_batches=True))\n",
        "\n",
        "      # Transformed metadata is not necessary for encoding.\n",
        "      transformed_data, _ = transformed_dataset\n",
        "\n",
        "      # Extract transformed RecordBatches, encode and write them to the given\n",
        "      # directory.\n",
        "      _ = (\n",
        "          transformed_data\n",
        "          | 'EncodeTrainData' >>\n",
        "          beam.FlatMapTuple(lambda batch, _: RecordBatchToExamples(batch))\n",
        "          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n",
        "\n",
        "      # Now apply transform function to test data.  In this case we remove the\n",
        "      # trailing period at the end of each line, and also ignore the header line\n",
        "      # that is present in the test data file.\n",
        "      raw_test_data = (\n",
        "          pipeline\n",
        "          | 'ReadTestData' >> beam.io.ReadFromText(\n",
        "              test_data_file, skip_header_lines=1,\n",
        "              coder=beam.coders.BytesCoder())\n",
        "          | 'FixCommasTestData' >> beam.Map(\n",
        "              lambda line: line.replace(b', ', b','))\n",
        "          | 'RemoveTrailingPeriodsTestData' >> beam.Map(lambda line: line[:-1])\n",
        "          | 'DecodeTestData' >> csv_tfxio.BeamSource())\n",
        "\n",
        "      raw_test_dataset = (raw_test_data, csv_tfxio.TensorAdapterConfig())\n",
        "\n",
        "      # The TFXIO output format is chosen for improved performance.\n",
        "      transformed_test_dataset = (\n",
        "          (raw_test_dataset, transform_fn)\n",
        "          | tft_beam.TransformDataset(output_record_batches=True))\n",
        "\n",
        "      # Transformed metadata is not necessary for encoding.\n",
        "      transformed_test_data, _ = transformed_test_dataset\n",
        "\n",
        "      # Extract transformed RecordBatches, encode and write them to the given\n",
        "      # directory.\n",
        "      _ = (\n",
        "          transformed_test_data\n",
        "          | 'EncodeTestData' >>\n",
        "          beam.FlatMapTuple(lambda batch, _: RecordBatchToExamples(batch))\n",
        "          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n",
        "\n",
        "      # Will write a SavedModel and metadata to working_dir, which can then\n",
        "      # be read by the tft.TFTransformOutput class.\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnaMyRMJ03bR"
      },
      "source": [
        "##使用预处理数据通过 tf.keras 训练模型\n",
        "\n",
        "为了展示 `tf.Transform` 如何使我们能够将相同的代码用于训练和应用，进而避免偏差，我们将训练一个模型。要训​​练模型并为生产环境准备经过训练的模型，我们需要创建输入函数。训练输入函数与应用输入函数之间的主要区别在于，训练数据包含标签，而生产数据则不包含标签。两者的参数和返回值也有所不同。\n",
        "\n",
        "注：本部分使用 tf.keras 进行训练。如果您要寻找使用 tf.estimator 进行训练的示例，请参阅下一部分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8xCZKNc2wAS"
      },
      "source": [
        "###创建训练输入函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775Y7BTpHBmb"
      },
      "outputs": [],
      "source": [
        "def _make_training_input_fn(tf_transform_output, transformed_examples,\n",
        "                            batch_size):\n",
        "  \"\"\"An input function reading from transformed data, converting to model input.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    transformed_examples: Base filename of examples.\n",
        "    batch_size: Batch size.\n",
        "\n",
        "  Returns:\n",
        "    The input data for training or eval, in the form of k.\n",
        "  \"\"\"\n",
        "  def input_fn():\n",
        "    return tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=transformed_examples,\n",
        "        batch_size=batch_size,\n",
        "        features=tf_transform_output.transformed_feature_spec(),\n",
        "        reader=tf.data.TFRecordDataset,\n",
        "        label_key=LABEL_KEY,\n",
        "        shuffle=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYeuthrs27vl"
      },
      "source": [
        "###创建应用输入函数\n",
        "\n",
        "我们创建一个可以在生产环境中使用的输入函数，并针对应用准备经过训练的模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4TH_ZoSHSny"
      },
      "outputs": [],
      "source": [
        "def _make_serving_input_fn(tf_transform_output, raw_examples, batch_size):\n",
        "  \"\"\"An input function reading from raw data, converting to model input.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    raw_examples: Base filename of examples.\n",
        "    batch_size: Batch size.\n",
        "\n",
        "  Returns:\n",
        "    The input data for training or eval, in the form of k.\n",
        "  \"\"\"\n",
        "\n",
        "  def get_ordered_raw_data_dtypes():\n",
        "    result = []\n",
        "    for col in ORDERED_CSV_COLUMNS:\n",
        "      if col not in RAW_DATA_FEATURE_SPEC:\n",
        "        result.append(0.0)\n",
        "        continue\n",
        "      spec = RAW_DATA_FEATURE_SPEC[col]\n",
        "      if isinstance(spec, tf.io.FixedLenFeature):\n",
        "        result.append(spec.dtype)\n",
        "      else:\n",
        "        result.append(0.0)\n",
        "    return result\n",
        "\n",
        "  def input_fn():\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        file_pattern=raw_examples,\n",
        "        batch_size=batch_size,\n",
        "        column_names=ORDERED_CSV_COLUMNS,\n",
        "        column_defaults=get_ordered_raw_data_dtypes(),\n",
        "        prefetch_buffer_size=0,\n",
        "        ignore_errors=True)\n",
        "\n",
        "    tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    def transform_dataset(data):\n",
        "      raw_features = {}\n",
        "      for key, val in data.items():\n",
        "        if key not in RAW_DATA_FEATURE_SPEC:\n",
        "          continue\n",
        "        if isinstance(RAW_DATA_FEATURE_SPEC[key], tf.io.VarLenFeature):\n",
        "          raw_features[key] = tf.RaggedTensor.from_tensor(\n",
        "              tf.expand_dims(val, -1)).to_sparse()\n",
        "          continue\n",
        "        raw_features[key] = val\n",
        "      transformed_features = tft_layer(raw_features)\n",
        "      data_labels = transformed_features.pop(LABEL_KEY)\n",
        "      return (transformed_features, data_labels)\n",
        "\n",
        "    return dataset.map(\n",
        "        transform_dataset,\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
        "            tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyNTX7CO8AAz"
      },
      "source": [
        "##训练、评估并导出模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzE9HwLDIPqL"
      },
      "outputs": [],
      "source": [
        "def export_serving_model(tf_transform_output, model, output_dir):\n",
        "  \"\"\"Exports a keras model for serving.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    model: A keras model to export for serving.\n",
        "    output_dir: A directory where the model will be exported to.\n",
        "  \"\"\"\n",
        "  # The layer has to be saved to the model for keras tracking purpases.\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function\n",
        "  def serve_tf_examples_fn(serialized_tf_examples):\n",
        "    \"\"\"Serving tf.function model wrapper.\"\"\"\n",
        "    feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
        "    feature_spec.pop(LABEL_KEY)\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "    transformed_features = model.tft_layer(parsed_features)\n",
        "    outputs = model(transformed_features)\n",
        "    classes_names = tf.constant([['0', '1']])\n",
        "    classes = tf.tile(classes_names, [tf.shape(outputs)[0], 1])\n",
        "    return {'classes': classes, 'scores': outputs}\n",
        "\n",
        "  concrete_serving_fn = serve_tf_examples_fn.get_concrete_function(\n",
        "      tf.TensorSpec(shape=[None], dtype=tf.string, name='inputs'))\n",
        "  signatures = {'serving_default': concrete_serving_fn}\n",
        "\n",
        "  # This is required in order to make this model servable with model_server.\n",
        "  versioned_output_dir = os.path.join(output_dir, '1')\n",
        "  model.save(versioned_output_dir, save_format='tf', signatures=signatures)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i_lhWH8IZrk"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(working_dir,\n",
        "                       num_train_instances=NUM_TRAIN_INSTANCES,\n",
        "                       num_test_instances=NUM_TEST_INSTANCES):\n",
        "  \"\"\"Train the model on training data and evaluate on test data.\n",
        "\n",
        "  Args:\n",
        "    working_dir: The location of the Transform output.\n",
        "    num_train_instances: Number of instances in train set\n",
        "    num_test_instances: Number of instances in test set\n",
        "\n",
        "  Returns:\n",
        "    The results from the estimator's 'evaluate' method\n",
        "  \"\"\"\n",
        "  train_data_path_pattern = os.path.join(working_dir,\n",
        "                                 TRANSFORMED_TRAIN_DATA_FILEBASE + '*')\n",
        "  eval_data_path_pattern = os.path.join(working_dir,\n",
        "                            TRANSFORMED_TEST_DATA_FILEBASE + '*')\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  train_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output, train_data_path_pattern, batch_size=TRAIN_BATCH_SIZE)\n",
        "  train_dataset = train_input_fn()\n",
        "\n",
        "  # Evaluate model on test dataset.\n",
        "  eval_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output, eval_data_path_pattern, batch_size=TRAIN_BATCH_SIZE)\n",
        "  validation_dataset = eval_input_fn()\n",
        "\n",
        "  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
        "  feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  inputs = {}\n",
        "  for key, spec in feature_spec.items():\n",
        "    if isinstance(spec, tf.io.VarLenFeature):\n",
        "      inputs[key] = tf.keras.layers.Input(\n",
        "          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n",
        "    elif isinstance(spec, tf.io.FixedLenFeature):\n",
        "      inputs[key] = tf.keras.layers.Input(\n",
        "          shape=spec.shape, name=key, dtype=spec.dtype)\n",
        "    else:\n",
        "      raise ValueError('Spec type is not supported: ', key, spec)\n",
        "\n",
        "  encoded_inputs = {}\n",
        "  for key in inputs:\n",
        "    feature = tf.expand_dims(inputs[key], -1)\n",
        "    if key in CATEGORICAL_FEATURE_KEYS:\n",
        "      num_buckets = tf_transform_output.num_buckets_for_transformed_feature(key)\n",
        "      encoding_layer = (\n",
        "          tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n",
        "              max_tokens=num_buckets, output_mode='binary', sparse=False))\n",
        "      encoded_inputs[key] = encoding_layer(feature)\n",
        "    else:\n",
        "      encoded_inputs[key] = feature\n",
        "\n",
        "  stacked_inputs = tf.concat(tf.nest.flatten(encoded_inputs), axis=1)\n",
        "  output = tf.keras.layers.Dense(100, activation='relu')(stacked_inputs)\n",
        "  output = tf.keras.layers.Dense(70, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dense(50, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dense(20, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dense(2, activation='sigmoid')(output)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  pprint.pprint(model.summary())\n",
        "\n",
        "  model.fit(train_dataset, validation_data=validation_dataset,\n",
        "            epochs=TRAIN_NUM_EPOCHS,\n",
        "            steps_per_epoch=math.ceil(num_train_instances / TRAIN_BATCH_SIZE),\n",
        "            validation_steps=math.ceil(num_test_instances / TRAIN_BATCH_SIZE))\n",
        "\n",
        "  # Export the model.\n",
        "  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
        "  export_serving_model(tf_transform_output, model, exported_model_dir)\n",
        "\n",
        "  metrics_values = model.evaluate(validation_dataset, steps=num_test_instances)\n",
        "  metrics_labels = model.metrics_names\n",
        "  return {l: v for l, v in zip(metrics_labels, metrics_values)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrFJ9Zax8QAh"
      },
      "source": [
        "##总结<br>我们已经创建了所需的一切来预处理人口普查数据，训练模型，并针对应用准备模型。到目前为止，我们已经做好了一切准备。是时候开始运行了！\n",
        "\n",
        "注：滚动此单元的输出可查看整个流程。结果位于底部。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe7JdSin86Ez"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "temp = os.path.join(tempfile.gettempdir(), 'keras')\n",
        "\n",
        "transform_data(train, test, temp)\n",
        "results = train_and_evaluate(temp)\n",
        "pprint.pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APEUSA9boKgT"
      },
      "source": [
        "## （可选）使用预处理数据通过 tf.estimator 训练模型\n",
        "\n",
        "如果您更愿意使用 Estimator 模型而不是 Keras 模型，本部分中的代码将展示如何进行操作。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcBWjr3ioZbl"
      },
      "source": [
        "###创建训练输入函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFO0MeWQ228a"
      },
      "outputs": [],
      "source": [
        "def _make_training_input_fn(tf_transform_output, transformed_examples,\n",
        "                            batch_size):\n",
        "  \"\"\"Creates an input function reading from transformed data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    transformed_examples: Base filename of examples.\n",
        "    batch_size: Batch size.\n",
        "\n",
        "  Returns:\n",
        "    The input function for training or eval.\n",
        "  \"\"\"\n",
        "  def input_fn():\n",
        "    \"\"\"Input function for training and eval.\"\"\"\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=transformed_examples,\n",
        "        batch_size=batch_size,\n",
        "        features=tf_transform_output.transformed_feature_spec(),\n",
        "        reader=tf.data.TFRecordDataset,\n",
        "        shuffle=True)\n",
        "\n",
        "    transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n",
        "        dataset).get_next()\n",
        "\n",
        "    # Extract features and label from the transformed tensors.\n",
        "    transformed_labels = tf.where(\n",
        "        tf.equal(transformed_features.pop(LABEL_KEY), 1))\n",
        "\n",
        "    return transformed_features, transformed_labels[:,1]\n",
        "\n",
        "  return input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22XOsZ-noez-"
      },
      "source": [
        "###创建应用输入函数\n",
        "\n",
        "我们创建一个可以在生产环境中使用的输入函数，并针对应用准备经过训练的模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "NN5FVg343Jea"
      },
      "outputs": [],
      "source": [
        "def _make_serving_input_fn(tf_transform_output):\n",
        "  \"\"\"Creates an input function reading from raw data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "\n",
        "  Returns:\n",
        "    The serving input function.\n",
        "  \"\"\"\n",
        "  raw_feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
        "  # Remove label since it is not available during serving.\n",
        "  raw_feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  def serving_input_fn():\n",
        "    \"\"\"Input function for serving.\"\"\"\n",
        "    # Get raw features by generating the basic serving input_fn and calling it.\n",
        "    # Here we generate an input_fn that expects a parsed Example proto to be fed\n",
        "    # to the model at serving time.  See also\n",
        "    # tf.estimator.export.build_raw_serving_input_receiver_fn.\n",
        "    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "        raw_feature_spec, default_batch_size=None)\n",
        "    serving_input_receiver = raw_input_fn()\n",
        "\n",
        "    # Apply the transform function that was used to generate the materialized\n",
        "    # data.\n",
        "    raw_features = serving_input_receiver.features\n",
        "    transformed_features = tf_transform_output.transform_raw_features(\n",
        "        raw_features)\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(\n",
        "        transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "  return serving_input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc9Edp8A7dsI"
      },
      "source": [
        "###将输入数据封装到 FeatureColumns 中<br>模型希望我们的数据处于 TensorFlow FeatureColumns 中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qOFOvBk7oJX"
      },
      "outputs": [],
      "source": [
        "def get_feature_columns(tf_transform_output):\n",
        "  \"\"\"Returns the FeatureColumns for the model.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: A `TFTransformOutput` object.\n",
        "\n",
        "  Returns:\n",
        "    A list of FeatureColumns.\n",
        "  \"\"\"\n",
        "  # Wrap scalars as real valued columns.\n",
        "  real_valued_columns = [tf.feature_column.numeric_column(key, shape=())\n",
        "                         for key in NUMERIC_FEATURE_KEYS]\n",
        "\n",
        "  # Wrap categorical columns.\n",
        "  one_hot_columns = [\n",
        "      tf.feature_column.indicator_column(\n",
        "          tf.feature_column.categorical_column_with_identity(\n",
        "              key=key,\n",
        "              num_buckets=(NUM_OOV_BUCKETS +\n",
        "                  tf_transform_output.vocabulary_size_by_name(\n",
        "                      vocab_filename=key))))\n",
        "      for key in CATEGORICAL_FEATURE_KEYS]\n",
        "\n",
        "  return real_valued_columns + one_hot_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6FyMzMcpOgT"
      },
      "source": [
        "##训练、评估并导出模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iGQ0jeq8IWr"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,\n",
        "                       num_test_instances=NUM_TEST_INSTANCES):\n",
        "  \"\"\"Train the model on training data and evaluate on test data.\n",
        "\n",
        "  Args:\n",
        "    working_dir: Directory to read transformed data and metadata from and to\n",
        "        write exported model to.\n",
        "    num_train_instances: Number of instances in train set\n",
        "    num_test_instances: Number of instances in test set\n",
        "\n",
        "  Returns:\n",
        "    The results from the estimator's 'evaluate' method\n",
        "  \"\"\"\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  run_config = tf.estimator.RunConfig()\n",
        "\n",
        "  estimator = tf.estimator.LinearClassifier(\n",
        "      feature_columns=get_feature_columns(tf_transform_output),\n",
        "      config=run_config,\n",
        "      loss_reduction=tf.losses.Reduction.SUM)\n",
        "\n",
        "  # Fit the model using the default optimizer.\n",
        "  train_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
        "      batch_size=TRAIN_BATCH_SIZE)\n",
        "  estimator.train(\n",
        "      input_fn=train_input_fn,\n",
        "      max_steps=TRAIN_NUM_EPOCHS * num_train_instances / TRAIN_BATCH_SIZE)\n",
        "\n",
        "  # Evaluate model on test dataset.\n",
        "  eval_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n",
        "      batch_size=1)\n",
        "\n",
        "  # Export the model.\n",
        "  serving_input_fn = _make_serving_input_fn(tf_transform_output)\n",
        "  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
        "  estimator.export_saved_model(exported_model_dir, serving_input_fn)\n",
        "\n",
        "  return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k8LrDPZpZsK"
      },
      "source": [
        "##总结<br>我们已经创建了所需的一切来预处理人口普查数据、训练模型并针对应用准备模型。到目前为止，我们已经做好了一切准备。是时候开始运行了！\n",
        "\n",
        "注：滚动此单元的输出可查看整个流程。结果位于底部。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_1_2dB6pdc2"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "temp = os.path.join(tempfile.gettempdir(), 'estimator')\n",
        "\n",
        "transform_data(train, test, temp)\n",
        "results = train_and_evaluate(temp)\n",
        "pprint.pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICqetCnSjwp1"
      },
      "source": [
        "##我们做了什么<br>在此示例中，我们使用 `tf.Transform` 预处理了人口普查数据的数据集，并使用清理和转换后的数据训练了一个模型。此外，我们还创建了一个输入函数，当我们在生产环境中部署经过训练的模型以执行推断时，可以使用该输入函数。通过将相同的代码用于训练和推断，我们可以避免数据偏差方面的任何问题。在此过程中，我们学习了如何创建 Apache Beam Transform 来执行清理数据所需的转换。我们还看到了如何使用转换后的数据通过 `tf.keras` 或 `tf.estimator` 训练模型。这只是 TensorFlow Transform 功能的一小部分！我们鼓励您深入研究 `tf.Transform`，并发现它可以为您做些什么。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "census.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
