{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fluF3_oOgkWF"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AJs7HHFmg1M9"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# 简单的音频识别：识别关键词"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbqmZy0gbyE"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/simple_audio\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">在 TensorFlow.org 上查看</a>\n",
        "</td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">在 Google Colab 中运行</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">在 Github 上查看源代码</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">下载笔记本</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfDNFlb66XF"
      },
      "source": [
        "本教程演示了如何预处理 WAV 格式的音频文件，并构建和训练一个基本的<a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" class=\"external\">自动语音识别</a> (ASR) 模型来识别十个不同的单词。您将使用 [Speech Commands 数据集](https://www.tensorflow.org/datasets/catalog/speech_commands)（<a href=\"https://arxiv.org/abs/1804.03209\" class=\"external\">Warden，2018 年</a>）的一部分，其中包含命令的短（一秒或更短）音频片段，例如“down”、“go”、“left”、“no”、“right”、“stop”、“up”和“yes”。\n",
        "\n",
        "现实世界的语音和音频识别<a href=\"https://ai.googleblog.com/search/label/Speech%20Recognition\" class=\"external\">系统</a>很复杂。但是，就像[使用 MNIST 数据集进行图像分类](../quickstart/beginner.ipynb)一样，本教程应该能够使您对所涉及的技术有一个基本的了解。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9C3uLL8Izc"
      },
      "source": [
        "## 设置\n",
        "\n",
        "导入必要的模块和依赖项。请注意，您将在本教程中使用 <a href=\"https://seaborn.pydata.org/\" class=\"external\">seaborn</a> 进行可视化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## 导入迷你 Speech Commands 数据集\n",
        "\n",
        "为了节省数据加载时间，您将使用较小版本的 Speech Commands 数据集。[原始数据集](https://www.tensorflow.org/datasets/catalog/speech_commands)包含超过 105,000 个音频文件，采用 <a href=\"https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf\" class=\"external\">WAV（波形）音频文件格式</a>，内容是不同的人们说出 35 个不同的单词。此数据由 Google 收集并根据 CC BY 许可发布。\n",
        "\n",
        "使用 `tf.keras.utils.get_file` 下载并提取包含较 Speech Commands 数据集的 `mini_speech_commands.zip`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-rayb7-3Y0I"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = 'data/mini_speech_commands'\n",
        "\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgvFq3uYiS5G"
      },
      "source": [
        "数据集的音频片段存储在与每个语音命令对应的八个文件夹中：`no`、`yes`、`down`、`go`、`left`、`up`、`right` 和 `stop`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70IBxSKxA1N9"
      },
      "outputs": [],
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[commands != 'README.md']\n",
        "print('Commands:', commands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMvdU9SY8WXN"
      },
      "source": [
        "将音频片段提取到名为 `filenames` 的列表中，然后对其进行乱序："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlX685l1wD9k"
      },
      "outputs": [],
      "source": [
        "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
        "filenames = tf.random.shuffle(filenames)\n",
        "num_samples = len(filenames)\n",
        "print('Number of total examples:', num_samples)\n",
        "print('Number of examples per label:',\n",
        "      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))\n",
        "print('Example file tensor:', filenames[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vK3ymy23MCP"
      },
      "source": [
        "分别使用 80:10:10 的比例将 `filenames` 拆分为训练集、验证集和测试集："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv_wts-l3KgD"
      },
      "outputs": [],
      "source": [
        "train_files = filenames[:6400]\n",
        "val_files = filenames[6400: 6400 + 800]\n",
        "test_files = filenames[-800:]\n",
        "\n",
        "print('Training set size', len(train_files))\n",
        "print('Validation set size', len(val_files))\n",
        "print('Test set size', len(test_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Cj9FyvfweD"
      },
      "source": [
        "## 读取音频文件及其标签"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1zjcWteOcBy"
      },
      "source": [
        "在本部分，您将预处理数据集，为波形和相应的标签创建解码张量。注意：\n",
        "\n",
        "- 每个 WAV 文件都包含时间序列数据，每秒具有一定数量的样本。\n",
        "- 每个样本代表该特定时间的音频信号的<a href=\"https://en.wikipedia.org/wiki/Amplitude\" class=\"external\">幅度</a>。\n",
        "- 在 <a href=\"https://en.wikipedia.org/wiki/Audio_bit_depth\" class=\"external\">16 位</a>系统（如 mini Speech Commands 数据集中的 WAV 文件）中，幅度值范围为 -32,768 到 32,767。\n",
        "- 该数据集的<a href=\"https://en.wikipedia.org/wiki/Sampling_(signal_processing)#Audio_sampling\" class=\"external\">采样率为 </a>16kHz。\n",
        "\n",
        "`tf.audio.decode_wav` 返回的张量的形状是 `[samples, channels]`，其中 `channels` 为 `1` 表示单声道，`2` 表示立体声。迷你 Speech Commands 数据集仅包含单声道录音。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d16bb8416f90"
      },
      "outputs": [],
      "source": [
        "test_file = tf.io.read_file(DATASET_PATH+'/down/0a9f9af7_nohash_0.wav')\n",
        "test_audio, _ = tf.audio.decode_wav(contents=test_file)\n",
        "test_audio.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6bb8defd2ef"
      },
      "source": [
        "接下来，我们来定义一个函数，将数据集的原始 WAV 音频文件预处理为音频张量："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PjJ2iXYwftD"
      },
      "outputs": [],
      "source": [
        "def decode_audio(audio_binary):\n",
        "  # Decode WAV-encoded audio files to `float32` tensors, normalized\n",
        "  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n",
        "  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
        "  # Since all the data is single channel (mono), drop the `channels`\n",
        "  # axis from the array.\n",
        "  return tf.squeeze(audio, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPQseZElOjVN"
      },
      "source": [
        "定义一个使用每个文件的父目录创建标签的函数：\n",
        "\n",
        "- 将文件路径拆分为 `tf.RaggedTensor`（具有不规则维度的张量 - 带有可能具有不同长度的切片）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VTtX1nr3YT-"
      },
      "outputs": [],
      "source": [
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(\n",
        "      input=file_path,\n",
        "      sep=os.path.sep)\n",
        "  # Note: You'll use indexing here instead of tuple unpacking to enable this\n",
        "  # to work in a TensorFlow graph.\n",
        "  return parts[-2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Y9w_5MOsr-"
      },
      "source": [
        "定义另一个辅助函数 `get_waveform_and_label`，将它们整合在一起：\n",
        "\n",
        "- 输入是 WAV 音频文件名。\n",
        "- 输出是一个元组，其中包含准备好进行监督学习的音频和标签张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdgUD5T93NyT"
      },
      "outputs": [],
      "source": [
        "def get_waveform_and_label(file_path):\n",
        "  label = get_label(file_path)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  waveform = decode_audio(audio_binary)\n",
        "  return waveform, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvN8W_dDjYjc"
      },
      "source": [
        "构建训练集以提取音频标签对：\n",
        "\n",
        "- 使用前面定义的 `get_waveform_and_label`，创建具有 `Dataset.from_tensor_slices` 和 `Dataset.map` 的 `tf.data.Dataset`。\n",
        "\n",
        "稍后您将使用类似的过程构建验证集和测试集。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SQl8yXl3kNP"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "\n",
        "waveform_ds = files_ds.map(\n",
        "    map_func=get_waveform_and_label,\n",
        "    num_parallel_calls=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxGEwvuh2L7"
      },
      "source": [
        "我们来绘制一些音频波形："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yuX6Nqzf6wT"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows * cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
        "\n",
        "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  ax.plot(audio.numpy())\n",
        "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  ax.set_title(label)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXPphxm0B4m"
      },
      "source": [
        "## 将波形转换为频谱图\n",
        "\n",
        "数据集中的波形在时域中表示。接下来，您将通过计算<a href=\"https://en.wikipedia.org/wiki/Short-time_Fourier_transform\" class=\"external\">短时傅里叶变换 (STFT)</a> 将波形从时域信号转换为时频域信号，以将波形转换为<a href=\"https://en.wikipedia.org/wiki/Spectrogram\" clas=\"external\">频谱图</a>，显示频率随时间的变化，并且可以表示为二维图像。您将把频谱图图像输入您的神经网络以训练模型。\n",
        "\n",
        "傅里叶变换 (`tf.signal.fft`) 会将信号转换为其分量频率，但会丢失所有时间信息。相比之下，STFT (`tf.signal.stft`) 会将信号拆分为时间窗口，并在每个窗口上运行傅里叶变换，保留一些时间信息，并返回可以运行标准卷积的二维张量。\n",
        "\n",
        "创建用于将波形转换为频谱图的效用函数：\n",
        "\n",
        "- 这些波形需要具有相同的长度，以便将它们转换为频谱图时，结果具有相似的维度。这可以通过简单地对短于一秒的音频片段进行零填充（使用 `tf.zeros`）来完成。\n",
        "- 调用 `tf.signal.stft` 时，请选择 `frame_length` 和 `frame_step` 参数，使生成的频谱图“图像”几乎为方形。有关 STFT 参数选择的更多信息，请参阅有关音频信号处理和 STFT 的 <a href=\"https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe\" class=\"external\">Coursera 视频</a>。\n",
        "- STFT 会产生表示幅度和相位的复数数组。但是，在本教程中，您将只使用幅度，您可以通过在 tf.signal.stft 的输出上应用 `tf.abs` 来获得该 `tf.signal.stft`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4CK75DHz_OR"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "  # Zero-padding for an audio waveform with less than 16,000 samples.\n",
        "  input_len = 16000\n",
        "  waveform = waveform[:input_len]\n",
        "  zero_padding = tf.zeros(\n",
        "      [16000] - tf.shape(waveform),\n",
        "      dtype=tf.float32)\n",
        "  # Cast the waveform tensors' dtype to float32.\n",
        "  waveform = tf.cast(waveform, dtype=tf.float32)\n",
        "  # Concatenate the waveform with `zero_padding`, which ensures all audio\n",
        "  # clips are of the same length.\n",
        "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "  # Convert the waveform to a spectrogram via a STFT.\n",
        "  spectrogram = tf.signal.stft(\n",
        "      equal_length, frame_length=255, frame_step=128)\n",
        "  # Obtain the magnitude of the STFT.\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  # Add a `channels` dimension, so that the spectrogram can be used\n",
        "  # as image-like input data with convolution layers (which expect\n",
        "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdPiPYJphs2"
      },
      "source": [
        "接下来，开始探索数据。打印一个样本的张量波形形状和相应的频谱图，并播放原始音频："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mu6Y7Yz3C-V"
      },
      "outputs": [],
      "source": [
        "for waveform, label in waveform_ds.take(1):\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "print('Label:', label)\n",
        "print('Waveform shape:', waveform.shape)\n",
        "print('Spectrogram shape:', spectrogram.shape)\n",
        "print('Audio playback')\n",
        "display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnSuqyxJ1isF"
      },
      "source": [
        "现在，定义一个显示频谱图的函数："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e62jzb36-Jog"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "  # Convert the frequencies to log scale and transpose, so that the time is\n",
        "  # represented on the x-axis (columns).\n",
        "  # Add an epsilon to avoid taking a log of zero.\n",
        "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baa5c91e8603"
      },
      "source": [
        "绘制样本随时间变化的波形和相应的频谱图（随时间变化的频率）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2_CikgY1tjv"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYXjW07jCHA"
      },
      "source": [
        "接下来，定义一个函数，将波形数据集转换为频谱图，并将其对应的标签作为整数 ID："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43IS2IouEV40"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram_and_label_id(audio, label):\n",
        "  spectrogram = get_spectrogram(audio)\n",
        "  label_id = tf.math.argmax(label == commands)\n",
        "  return spectrogram, label_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf5d5b033a45"
      },
      "source": [
        "使用 `Dataset.map` 在数据集的元素之间映射 `get_spectrogram_and_label_id`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEVb_oK0oBLQ"
      },
      "outputs": [],
      "source": [
        "spectrogram_ds = waveform_ds.map(\n",
        "  map_func=get_spectrogram_and_label_id,\n",
        "  num_parallel_calls=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gQpAAgMnyDi"
      },
      "source": [
        "检查数据集不同样本的频谱图："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUbHfTuon4iF"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
        "\n",
        "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  plot_spectrogram(spectrogram.numpy(), ax)\n",
        "  ax.set_title(commands[label_id.numpy()])\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KdY8IF8rkt"
      },
      "source": [
        "## 构建并训练模型\n",
        "\n",
        "对验证集和测试集重复训练集预处理："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10UI32QH_45b"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(files):\n",
        "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = files_ds.map(\n",
        "      map_func=get_waveform_and_label,\n",
        "      num_parallel_calls=AUTOTUNE)\n",
        "  output_ds = output_ds.map(\n",
        "      map_func=get_spectrogram_and_label_id,\n",
        "      num_parallel_calls=AUTOTUNE)\n",
        "  return output_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNv4xwYkB2P6"
      },
      "outputs": [],
      "source": [
        "train_ds = spectrogram_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "test_ds = preprocess_dataset(test_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assnWo6SB3lR"
      },
      "source": [
        "批处理用于模型训练的训练集和验证集："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgY9WYzn61EX"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1uIh6F_TN9"
      },
      "source": [
        "添加 `Dataset.cache` 和 `Dataset.prefetch` 运算以减少训练模型时的读取延迟："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdZ6M-F5_QzY"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwHkKCQQb5oW"
      },
      "source": [
        "对于模型，您将使用简单的卷积神经网络 (CNN)，因为您已将音频文件转换为频谱图图像。\n",
        "\n",
        "您的 `tf.keras.Sequential` 模型将使用以下 Keras 预处理层：\n",
        "\n",
        "- `tf.keras.layers.Resizing`：对输入进行下采样以使模型训练得更快。\n",
        "- `tf.keras.layers.Normalization`：根据图像的均值和标准差对图像中的每个像素进行归一化。\n",
        "\n",
        "对于 `Normalization` 层，首先需要在训练数据上调用其 `adapt` 方法，以计算聚合统计数据（即均值和标准差）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALYz7PFCHblP"
      },
      "outputs": [],
      "source": [
        "for spectrogram, _ in spectrogram_ds.take(1):\n",
        "  input_shape = spectrogram.shape\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(commands)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(32, 32),\n",
        "    # Normalize.\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de52e5afa2f3"
      },
      "source": [
        "使用 Adam 优化器和交叉熵损失配置 Keras 模型："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFjj7-EmsTD-"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42b9e3a4705"
      },
      "source": [
        "出于演示目的，将模型训练超过 10 个周期："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttioPJVMcGtq"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpCDeQ4mUfS"
      },
      "source": [
        "我们来绘制训练和验证损失曲线，以检查您的模型在训练期间的改进情况："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzhipg3Gu2AY"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZTt3kO3mfm4"
      },
      "source": [
        "## 评估模型性能\n",
        "\n",
        "在测试集上运行模型并检查模型的性能："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biU2MwzyAo8o"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktUanr9mRZky"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
        "y_true = test_labels\n",
        "\n",
        "test_acc = sum(y_pred == y_true) / len(y_true)\n",
        "print(f'Test set accuracy: {test_acc:.0%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en9Znt1NOabH"
      },
      "source": [
        "### 显示混淆矩阵\n",
        "\n",
        "使用<a href=\"https://developers.google.com/machine-learning/glossary#confusion-matrix\" class=\"external\">混淆矩阵</a>检查模型对测试集中每个命令的分类效果：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvoSAOiXU3lL"
      },
      "outputs": [],
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx,\n",
        "            xticklabels=commands,\n",
        "            yticklabels=commands,\n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGi_mzPcLvl"
      },
      "source": [
        "## 对音频文件运行推断\n",
        "\n",
        "最后，使用某人说“no”的输入音频文件验证模型的预测输出。您的模型表现如何？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxauKMdhofU"
      },
      "outputs": [],
      "source": [
        "sample_file = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
        "\n",
        "sample_ds = preprocess_dataset([str(sample_file)])\n",
        "\n",
        "for spectrogram, label in sample_ds.batch(1):\n",
        "  prediction = model(spectrogram)\n",
        "  plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
        "  plt.title(f'Predictions for \"{commands[label[0]]}\"')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWICqdqQNaQ"
      },
      "source": [
        "如输出所示，您的模型应该已将音频命令识别为“no”。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3jF933m9z1J"
      },
      "source": [
        "## 后续步骤\n",
        "\n",
        "本教程演示了如何使用带有 TensorFlow 和 Python 的卷积神经网络执行简单的音频分类/自动语音识别。要了解更多信息，请考虑以下资源：\n",
        "\n",
        "- [使用 YAMNet 进行声音分类](https://www.tensorflow.org/hub/tutorials/yamnet)教程展示了如何使用迁移学习进行音频分类。\n",
        "- 来自 <a href=\"https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview\" class=\"external\">Kaggle 的 TensorFlow 语音识别挑战</a>的笔记本。\n",
        "- <a href=\"https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0\" class=\"external\">TensorFlow.js - 使用迁移学习的音频识别代码实验室</a>教授如何构建您自己的交互式 Web 应用以进行音频分类。\n",
        "- arXiv 上的<a href=\"https://arxiv.org/abs/1709.04396\" class=\"external\">音乐信息检索深度学习教程</a>（Choi 等人，2017 年）。\n",
        "- TensorFlow 还为[音频数据准备和增强](https://www.tensorflow.org/io/tutorials/audio)提供额外支持，以帮助您完成基于音频的项目。\n",
        "- 请考虑使用 <a href=\"https://librosa.org/\" class=\"external\">librosa</a> 库，这是一个用于音乐和音频分析的 Python 包。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "simple_audio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
