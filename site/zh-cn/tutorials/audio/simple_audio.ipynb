{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fluF3_oOgkWF"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AJs7HHFmg1M9"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# 简单的音频识别：识别关键词"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbqmZy0gbyE"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://tensorflow.google.cn/tutorials/audio/simple_audio\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org 上查看</a> </td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/audio/simple_audio.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/audio/simple_audio.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 Github 上查看源代码</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/tutorials/audio/simple_audio.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfDNFlb66XF"
      },
      "source": [
        "本教程演示了如何预处理 WAV 格式的音频文件，并构建和训练一个基本的<a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" class=\"external\">自动语音识别</a> (ASR) 模型来识别十个不同的单词。您将使用 [Speech Commands 数据集](https://tensorflow.google.cn/datasets/catalog/speech_commands)（<a href=\"https://arxiv.org/abs/1804.03209\" class=\"external\">Warden，2018 年</a>）的一部分，其中包含命令的短（一秒或更短）音频片段，例如“down”、“go”、“left”、“no”、“right”、“stop”、“up”和“yes”。\n",
        "\n",
        "现实世界的语音和音频识别<a href=\"https://ai.googleblog.com/search/label/Speech%20Recognition\" class=\"external\">系统</a>很复杂。但是，就像[使用 MNIST 数据集进行图像分类](../quickstart/beginner.ipynb)一样，本教程应该能够使您对所涉及的技术有一个基本的了解。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9C3uLL8Izc"
      },
      "source": [
        "## 设置\n",
        "\n",
        "导入必要的模块和依赖项。您将使用 `tf.keras.utils.audio_dataset_from_directory`（在 TensorFlow 2.10 中引入），它有助于从 `.wav` 文件的目录生成音频分类数据集。在本教程中，您还需要 [seaborn](https://seaborn.pydata.org) 进行呈现。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhNW45sjDEDe"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q tensorflow tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## 导入迷你 Speech Commands 数据集\n",
        "\n",
        "为了节省数据加载时间，您将使用较小版本的 Speech Commands 数据集。[原始数据集](https://tensorflow.google.cn/datasets/catalog/speech_commands)包含超过 105,000 个音频文件，采用 <a href=\"https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf\" class=\"external\">WAV（波形）音频文件格式</a>，内容是不同的人们说出 35 个不同的单词。此数据由 Google 收集并根据 CC BY 许可发布。\n",
        "\n",
        "使用 `tf.keras.utils.get_file` 下载并提取包含较 Speech Commands 数据集的 `mini_speech_commands.zip`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-rayb7-3Y0I"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = 'data/mini_speech_commands'\n",
        "\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgvFq3uYiS5G"
      },
      "source": [
        "数据集的音频片段存储在与每个语音命令对应的八个文件夹中：`no`、`yes`、`down`、`go`、`left`、`up`、`right` 和 `stop`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70IBxSKxA1N9"
      },
      "outputs": [],
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
        "print('Commands:', commands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ7GJjDvHqtt"
      },
      "source": [
        "以这种方式划分目录，您可以使用 `keras.utils.audio_dataset_from_directory` 轻松加载数据。\n",
        "\n",
        "音频剪辑在 16kHz 时为 1 秒或更短。`output_sequence_length=16000` 会将短音频剪辑填充到恰好 1 秒（并且会修剪较长的音频剪辑），以便可以轻松地对它们进行批处理。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFM4c3aMC8Qv"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=16000,\n",
        "    subset='both')\n",
        "\n",
        "label_names = np.array(train_ds.class_names)\n",
        "print()\n",
        "print(\"label names:\", label_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cestp83qFnU5"
      },
      "source": [
        "数据集现在包含批量的音频剪辑和整数标签。音频剪辑的形状为 `(batch, samples, channels)`。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yU6SQGIFb3H"
      },
      "outputs": [],
      "source": [
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppG9Dgq2Ex8R"
      },
      "source": [
        "此数据集仅包含单声道音频，因此使用 `tf.squeeze` 函数删除额外的轴："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl-tnniUIBlM"
      },
      "outputs": [],
      "source": [
        "def squeeze(audio, labels):\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  return audio, labels\n",
        "\n",
        "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtsCSWZN5ILv"
      },
      "source": [
        "`utils.audio_dataset_from_directory` 函数最多只返回两个拆分。将测试集与验证集分开是一个好主意。理想情况下，您会将其保存在单独的目录中，但在这种情况下，您可以使用 `Dataset.shard` 将验证集拆分成两半。请注意，遍历**任何**分片将加载**所有**数据，并且只保留它的片段。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5UEGsqM5Gss"
      },
      "outputs": [],
      "source": [
        "test_ds = val_ds.shard(num_shards=2, index=0)\n",
        "val_ds = val_ds.shard(num_shards=2, index=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIeoJcwJH5h9"
      },
      "outputs": [],
      "source": [
        "for example_audio, example_labels in train_ds.take(1):  \n",
        "  print(example_audio.shape)\n",
        "  print(example_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxGEwvuh2L7"
      },
      "source": [
        "我们来绘制一些音频波形："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYtGq2zYNHuT"
      },
      "outputs": [],
      "source": [
        "label_names[[1,1,3,0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yuX6Nqzf6wT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows * cols\n",
        "for i in range(n):\n",
        "  plt.subplot(rows, cols, i+1)\n",
        "  audio_signal = example_audio[i]\n",
        "  plt.plot(audio_signal)\n",
        "  plt.title(label_names[example_labels[i]])\n",
        "  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  plt.ylim([-1.1, 1.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXPphxm0B4m"
      },
      "source": [
        "## 将波形转换为频谱图\n",
        "\n",
        "数据集中的波形在时域中表示。接下来，您将通过计算<a href=\"https://en.wikipedia.org/wiki/Short-time_Fourier_transform\" class=\"external\">短时傅里叶变换 (STFT)</a> 将波形从时域信号转换为时频域信号，以将波形转换为<a href=\"https://en.wikipedia.org/wiki/Spectrogram\" clas=\"external\">频谱图</a>，显示频率随时间的变化，并且可以表示为二维图像。您将把频谱图图像输入您的神经网络以训练模型。\n",
        "\n",
        "傅里叶变换 (`tf.signal.fft`) 会将信号转换为其分量频率，但会丢失所有时间信息。相比之下，STFT (`tf.signal.stft`) 会将信号拆分为时间窗口，并在每个窗口上运行傅里叶变换，保留一些时间信息，并返回可以运行标准卷积的二维张量。\n",
        "\n",
        "创建用于将波形转换为频谱图的效用函数：\n",
        "\n",
        "- 这些波形需要具有相同的长度，以便将它们转换为频谱图时，结果具有相似的维度。这可以通过简单地对短于一秒的音频片段进行零填充（使用 `tf.zeros`）来完成。\n",
        "- 调用 `tf.signal.stft` 时，请选择 `frame_length` 和 `frame_step` 参数，使生成的频谱图“图像”几乎为方形。有关 STFT 参数选择的更多信息，请参阅有关音频信号处理和 STFT 的 <a href=\"https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe\" class=\"external\">Coursera 视频</a>。\n",
        "- STFT 会产生表示幅度和相位的复数数组。但是，在本教程中，您将只使用幅度，您可以通过在 tf.signal.stft 的输出上应用 `tf.abs` 来获得该 `tf.signal.stft`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4CK75DHz_OR"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "  # Convert the waveform to a spectrogram via a STFT.\n",
        "  spectrogram = tf.signal.stft(\n",
        "      waveform, frame_length=255, frame_step=128)\n",
        "  # Obtain the magnitude of the STFT.\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  # Add a `channels` dimension, so that the spectrogram can be used\n",
        "  # as image-like input data with convolution layers (which expect\n",
        "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdPiPYJphs2"
      },
      "source": [
        "接下来，开始探索数据。打印一个样本的张量波形形状和相应的频谱图，并播放原始音频："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mu6Y7Yz3C-V"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "  label = label_names[example_labels[i]]\n",
        "  waveform = example_audio[i]\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "  print('Label:', label)\n",
        "  print('Waveform shape:', waveform.shape)\n",
        "  print('Spectrogram shape:', spectrogram.shape)\n",
        "  print('Audio playback')\n",
        "  display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnSuqyxJ1isF"
      },
      "source": [
        "现在，定义一个显示频谱图的函数："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e62jzb36-Jog"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "  # Convert the frequencies to log scale and transpose, so that the time is\n",
        "  # represented on the x-axis (columns).\n",
        "  # Add an epsilon to avoid taking a log of zero.\n",
        "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baa5c91e8603"
      },
      "source": [
        "绘制样本随时间变化的波形和相应的频谱图（随时间变化的频率）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2_CikgY1tjv"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.suptitle(label.title())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYXjW07jCHA"
      },
      "source": [
        "接下来，从音频数据集创建频谱图数据集："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAD0LpkgqtQo"
      },
      "outputs": [],
      "source": [
        "def make_spec_ds(ds):\n",
        "  return ds.map(\n",
        "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
        "      num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEVb_oK0oBLQ"
      },
      "outputs": [],
      "source": [
        "train_spectrogram_ds = make_spec_ds(train_ds)\n",
        "val_spectrogram_ds = make_spec_ds(val_ds)\n",
        "test_spectrogram_ds = make_spec_ds(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gQpAAgMnyDi"
      },
      "source": [
        "检查数据集不同样本的频谱图："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaM2q5aGis-d"
      },
      "outputs": [],
      "source": [
        "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUbHfTuon4iF"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
        "\n",
        "for i in range(n):\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
        "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KdY8IF8rkt"
      },
      "source": [
        "## 构建并训练模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1uIh6F_TN9"
      },
      "source": [
        "添加 `Dataset.cache` 和 `Dataset.prefetch` 运算以减少训练模型时的读取延迟："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdZ6M-F5_QzY"
      },
      "outputs": [],
      "source": [
        "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwHkKCQQb5oW"
      },
      "source": [
        "对于模型，您将使用简单的卷积神经网络 (CNN)，因为您已将音频文件转换为频谱图图像。\n",
        "\n",
        "您的 `tf.keras.Sequential` 模型将使用以下 Keras 预处理层：\n",
        "\n",
        "- `tf.keras.layers.Resizing`：对输入进行下采样以使模型训练得更快。\n",
        "- `tf.keras.layers.Normalization`：根据图像的均值和标准差对图像中的每个像素进行归一化。\n",
        "\n",
        "对于 `Normalization` 层，首先需要在训练数据上调用其 `adapt` 方法，以计算聚合统计数据（即均值和标准差）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALYz7PFCHblP"
      },
      "outputs": [],
      "source": [
        "input_shape = example_spectrograms.shape[1:]\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(label_names)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(32, 32),\n",
        "    # Normalize.\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de52e5afa2f3"
      },
      "source": [
        "使用 Adam 优化器和交叉熵损失配置 Keras 模型："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFjj7-EmsTD-"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42b9e3a4705"
      },
      "source": [
        "出于演示目的，将模型训练超过 10 个周期："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttioPJVMcGtq"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_spectrogram_ds,\n",
        "    validation_data=val_spectrogram_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpCDeQ4mUfS"
      },
      "source": [
        "我们来绘制训练和验证损失曲线，以检查您的模型在训练期间的改进情况："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzhipg3Gu2AY"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [CrossEntropy]')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.ylim([0, 100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy [%]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZTt3kO3mfm4"
      },
      "source": [
        "## 评估模型性能\n",
        "\n",
        "在测试集上运行模型并检查模型的性能："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FapuRT_SsWGQ"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_spectrogram_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en9Znt1NOabH"
      },
      "source": [
        "### 显示混淆矩阵\n",
        "\n",
        "使用<a href=\"https://developers.google.com/machine-learning/glossary#confusion-matrix\" class=\"external\">混淆矩阵</a>检查模型对测试集中每个命令的分类效果：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6vmWWQuuT1"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_spectrogram_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6F0il82u7lW"
      },
      "outputs": [],
      "source": [
        "y_pred = tf.argmax(y_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHSNoBYLvX81"
      },
      "outputs": [],
      "source": [
        "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvoSAOiXU3lL"
      },
      "outputs": [],
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx,\n",
        "            xticklabels=label_names,\n",
        "            yticklabels=label_names,\n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGi_mzPcLvl"
      },
      "source": [
        "## 对音频文件运行推断\n",
        "\n",
        "最后，使用某人说“no”的输入音频文件验证模型的预测输出。您的模型表现如何？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxauKMdhofU"
      },
      "outputs": [],
      "source": [
        "x = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
        "x = tf.io.read_file(str(x))\n",
        "x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
        "x = tf.squeeze(x, axis=-1)\n",
        "waveform = x\n",
        "x = get_spectrogram(x)\n",
        "x = x[tf.newaxis,...]\n",
        "\n",
        "prediction = model(x)\n",
        "x_labels = ['no', 'yes', 'down', 'go', 'left', 'up', 'right', 'stop']\n",
        "plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
        "plt.title('No')\n",
        "plt.show()\n",
        "\n",
        "display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWICqdqQNaQ"
      },
      "source": [
        "如输出所示，您的模型应该已将音频命令识别为“no”。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1icqlM3ISW0"
      },
      "source": [
        "## 通过预处理导出模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7HX-MjgIbji"
      },
      "source": [
        "如果您必须在将数据传递给模型进行推断之前应用这些预处理步骤，该模型不是很易于使用。因此，构建一个端到端的版本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lIeXdWjIbDE"
      },
      "outputs": [],
      "source": [
        "class ExportModel(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "    # Accept either a string-filename or a batch of waveforms.\n",
        "    # YOu could add additional signatures for a single wave, or a ragged-batch. \n",
        "    self.__call__.get_concrete_function(\n",
        "        x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
        "    self.__call__.get_concrete_function(\n",
        "       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x):\n",
        "    # If they pass a string, load the file and decode it. \n",
        "    if x.dtype == tf.string:\n",
        "      x = tf.io.read_file(x)\n",
        "      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
        "      x = tf.squeeze(x, axis=-1)\n",
        "      x = x[tf.newaxis, :]\n",
        "    \n",
        "    x = get_spectrogram(x)  \n",
        "    result = self.model(x, training=False)\n",
        "    \n",
        "    class_ids = tf.argmax(result, axis=-1)\n",
        "    class_names = tf.gather(label_names, class_ids)\n",
        "    return {'predictions':result,\n",
        "            'class_ids': class_ids,\n",
        "            'class_names': class_names}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtZBmUiB9HGY"
      },
      "source": [
        "测试运行“导出”模型："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1_8TYaCIRue"
      },
      "outputs": [],
      "source": [
        "export = ExportModel(model)\n",
        "export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J6Iuz829Cxo"
      },
      "source": [
        "保存并重新加载模型，重新加载的模型给出了相同的输出："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTAg4vsn3oEb"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(export, \"saved\")\n",
        "imported = tf.saved_model.load(\"saved\")\n",
        "imported(waveform[tf.newaxis, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3jF933m9z1J"
      },
      "source": [
        "## 后续步骤\n",
        "\n",
        "本教程演示了如何使用带有 TensorFlow 和 Python 的卷积神经网络执行简单的音频分类/自动语音识别。要了解更多信息，请考虑以下资源：\n",
        "\n",
        "- [使用 YAMNet 进行声音分类](https://tensorflow.google.cn/hub/tutorials/yamnet)教程展示了如何使用迁移学习进行音频分类。\n",
        "- 来自 <a href=\"https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview\" class=\"external\">Kaggle 的 TensorFlow 语音识别挑战</a>的笔记本。\n",
        "- <a href=\"https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0\" class=\"external\">TensorFlow.js - 使用迁移学习的音频识别代码实验室</a>教授如何构建您自己的交互式 Web 应用以进行音频分类。\n",
        "- arXiv 上的<a href=\"https://arxiv.org/abs/1709.04396\" class=\"external\">音乐信息检索深度学习教程</a>（Choi 等人，2017 年）。\n",
        "- TensorFlow 还为[音频数据准备和增强](https://tensorflow.google.cn/io/tutorials/audio)提供额外支持，以帮助您完成基于音频的项目。\n",
        "- 考虑使用 [librosa](https://librosa.org/) 库进行音乐和音频分析。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "CPU",
    "colab": {
      "collapsed_sections": [],
      "name": "simple_audio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
