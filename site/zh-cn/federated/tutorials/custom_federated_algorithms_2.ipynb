{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqrD7Yzlmlsk"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "2k8X1C1nmpKv"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32xflLc4NTx-"
      },
      "source": [
        "# 自定义联合算法，第 2 部分：实现联合平均"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtATV6DlqPs0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://tensorflow.google.cn/federated/tutorials/custom_federated_algorithms_2\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org 上查看</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/federated/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/federated/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 GitHub 上查看源代码</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/federated/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_igJ2sfaNWS8"
      },
      "source": [
        "本系列教程包括两个部分，此为第二部分。该系列演示了如何使用 [Federated Core (FC)](../federated_core.md) 在 TFF 中实现自定义类型的联合算法，它是[联合学习 (FL)](../federated_learning.md) 层（`tff.learning`）的基础。\n",
        "\n",
        "我们建议您先阅读[本系列的第一部分](custom_federated_algorithms_1.ipynb)，其中介绍了此处使用的一些关键概念和编程抽象。\n",
        "\n",
        "本系列的第二部分使用第一部分中介绍的机制来实现简单版本的联合训练和评估算法。\n",
        "\n",
        "我们建议您查看[图像分类](federated_learning_for_image_classification.ipynb)和[文本生成](federated_learning_for_text_generation.ipynb)教程，以获得对 TFF 的 Federated Learning API 更高级和更循序渐进的介绍，因为它们将帮助您在上下文中理解我们在此描述的概念。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuJuLEh2TfZG"
      },
      "source": [
        "## 准备工作\n",
        "\n",
        "在开始之前，请尝试运行以下“Hello World”示例，以确保您的环境已正确配置。如果无法正常运行，请参阅[安装](../install.md)指南查看说明。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rB1ovcX1mBxQ"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-skNC6aovM46"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "# Must use the Python context because it\n",
        "# supports tff.sequence_* intrinsics.\n",
        "executor_factory = tff.framework.local_executor_factory(\n",
        "    support_sequence_ops=True)\n",
        "execution_context = tff.framework.ExecutionContext(\n",
        "    executor_fn=executor_factory)\n",
        "tff.framework.set_default_context(execution_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zzXwGnZamIMM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'Hello, World!'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@tff.federated_computation\n",
        "def hello_world():\n",
        "  return 'Hello, World!'\n",
        "\n",
        "hello_world()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu5Gd8D6W33s"
      },
      "source": [
        "## 实现联合平均\n",
        "\n",
        "与[图像分类联合学习](federated_learning_for_image_classification.ipynb)一样，我们将使用 MNIST 示例，但由于这是一个低级教程，我们将绕过 Keras API 和 `tff.simulation`，编写原始模型代码，并从头开始构造联合数据集。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6qCjef350c_"
      },
      "source": [
        "### 准备联合数据集\n",
        "\n",
        "为了进行演示，我们将模拟一个场景，其中有来自 10 个用户的数据，每个用户都会提供如何识别不同数字的知识。这是能够得到的最非[独立同分布](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)的情况。\n",
        "\n",
        "首先，加载标准 MNIST 数据："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uThZM4Ds-KDQ"
      },
      "outputs": [],
      "source": [
        "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PkJc5rHA2no_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(dtype('uint8'), (60000, 28, 28)), (dtype('uint8'), (60000,))]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[(x.dtype, x.shape) for x in mnist_train]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFET4BKJFbkP"
      },
      "source": [
        "数据以 Numpy 数组的形式出现，一个带有图像，另一个带有数字标签，其中第一个维度都遍历各个样本。我们来编写一个辅助函数，并使用与将联合序列馈送到 TFF 计算的方式相兼容的方式（即作为列表的列表，外部列表包括用户（数字），内部列表包括每个客户端序列中的数据批次）对其进行格式化。按照惯例，我们将每个批次构造为一对名为 `x` 和 `y` 的张量，每个张量都具有与首个批次相同的维度。同时，我们还将每个图像展平为一个具有 784 个元素的向量，并将其中的像素重新缩放到 `0..1` 范围内，这样我们就不必在模型逻辑上进行数据转换了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XTaTLiq5GNqy"
      },
      "outputs": [],
      "source": [
        "NUM_EXAMPLES_PER_USER = 1000\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "\n",
        "def get_data_for_digit(source, digit):\n",
        "  output_sequence = []\n",
        "  all_samples = [i for i, d in enumerate(source[1]) if d == digit]\n",
        "  for i in range(0, min(len(all_samples), NUM_EXAMPLES_PER_USER), BATCH_SIZE):\n",
        "    batch_samples = all_samples[i:i + BATCH_SIZE]\n",
        "    output_sequence.append({\n",
        "        'x':\n",
        "            np.array([source[0][i].flatten() / 255.0 for i in batch_samples],\n",
        "                     dtype=np.float32),\n",
        "        'y':\n",
        "            np.array([source[1][i] for i in batch_samples], dtype=np.int32)\n",
        "    })\n",
        "  return output_sequence\n",
        "\n",
        "\n",
        "federated_train_data = [get_data_for_digit(mnist_train, d) for d in range(10)]\n",
        "\n",
        "federated_test_data = [get_data_for_digit(mnist_test, d) for d in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpNdBimWaMHD"
      },
      "source": [
        "作为快速的健全性检查，我们来看一下第五个客户端（对应数字 `5`）所贡献的最后一个数据批次中的 `Y` 张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bTNuL1W4bcuc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], dtype=int32)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "federated_train_data[5][-1]['y']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgvcwv7Obhat"
      },
      "source": [
        "保险起见，我们再检查一下该批次最后一个元素对应的图像。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cI4aat1za525"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.imshow(federated_train_data[5][-1]['x'][-1].reshape(28, 28), cmap='gray')\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-ox58PA56f8"
      },
      "source": [
        "### 关于 TensorFlow 与 TFF 的结合\n",
        "\n",
        "在本教程中，出于紧凑考虑，我们使用 `tff.tf_computation` 对引入 TensorFlow 逻辑的函数进行了直接装饰。但对于更复杂的逻辑，我们不建议使用这种模式。调试 TensorFlow 本身就是一种挑战，如果在 TensorFlow 完全序列化并重新导入后再对其进行调试，必然会丢失部分元数据并限制交互性，这会使调试面临更大挑战。\n",
        "\n",
        "因此，**我们强烈建议将复杂的 TF 逻辑编写为独立的 Python 函数**（即不使用 `tff.tf_computation` 装饰）。这样，在序列化 TFF 计算之前（例如，通过将 Python 函数用作参数调用 `tff.tf_computation`），可以使用 TF 最佳做法和工具（如 Eager 模式）对 TensorFlow 逻辑进行开发和测试。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSd6UatXbzw-"
      },
      "source": [
        "### 定义损失函数\n",
        "\n",
        "现在有了数据，我们来定义一个可以用于训练的损失函数。首先，将输入类型定义为 TFF 命名元组。由于数据批次的大小可能会有所不同，因此我们将批次维度设置为 `None`，表示该维度的大小未知。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "653xv5NXd4fy"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<x=float32[?,784],y=int32[?]>'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BATCH_SPEC = collections.OrderedDict(\n",
        "    x=tf.TensorSpec(shape=[None, 784], dtype=tf.float32),\n",
        "    y=tf.TensorSpec(shape=[None], dtype=tf.int32))\n",
        "BATCH_TYPE = tff.to_type(BATCH_SPEC)\n",
        "\n",
        "str(BATCH_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb6qPUvyh5A1"
      },
      "source": [
        "您可能想知道为什么我们不能只定义普通的 Python 类型。回想一下[第 1 部分](custom_federated_algorithms_1.ipynb)中讨论的内容，我们解释了虽然可以使用 Python 来表达 TFF 计算的逻辑，但实际上 TFF 计算*不是* Python。上面定义的符号 `BATCH_TYPE` 表示抽象的 TFF 类型规范。区分这种*抽象的* TFF 类型与具体的 Python *表示* 类型（可用来表示 Python 函数主体中 TFF 类型的容器，如 `dict` 或 `collections.namedtuple`）很重要。与 Python 不同，针对类似元组的容器，TFF 具有单个抽象类型构造函数 `tff.StructType`，其元素可以单独命名或不命名。这种类型还用于对计算的形式化参数进行建模，因为 TFF 计算形式上只能声明一个参数和一个结果（稍后您将看到相关示例）。\n",
        "\n",
        "现在，我们来定义模型参数的 TFF 类型，仍然将其定义为*权重*和*偏差*的 TFF 命名元组。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Og7VViafh-30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<weights=float32[784,10],bias=float32[10]>\n"
          ]
        }
      ],
      "source": [
        "MODEL_SPEC = collections.OrderedDict(\n",
        "    weights=tf.TensorSpec(shape=[784, 10], dtype=tf.float32),\n",
        "    bias=tf.TensorSpec(shape=[10], dtype=tf.float32))\n",
        "MODEL_TYPE = tff.to_type(MODEL_SPEC)\n",
        "print(MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHhdaWSpfQxo"
      },
      "source": [
        "有了这些定义，现在我们可以在单个批次上定义给定模型的损失。请注意 `@tf.function` 装饰器在 `@tff.tf_computation` 装饰器内部的用法。通过这种用法，即使在由 `tff.tf_computation` 装饰器创建的 `tf.Graph` 上下文中，我们也可以使用类似 Python 的语义来编写 TF。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4EObiz_Ke0uK"
      },
      "outputs": [],
      "source": [
        "# NOTE: `forward_pass` is defined separately from `batch_loss` so that it can \n",
        "# be later called from within another tf.function. Necessary because a\n",
        "# @tf.function  decorated method cannot invoke a @tff.tf_computation.\n",
        "\n",
        "@tf.function\n",
        "def forward_pass(model, batch):\n",
        "  predicted_y = tf.nn.softmax(\n",
        "      tf.matmul(batch['x'], model['weights']) + model['bias'])\n",
        "  return -tf.reduce_mean(\n",
        "      tf.reduce_sum(\n",
        "          tf.one_hot(batch['y'], 10) * tf.math.log(predicted_y), axis=[1]))\n",
        "\n",
        "@tff.tf_computation(MODEL_TYPE, BATCH_TYPE)\n",
        "def batch_loss(model, batch):\n",
        "  return forward_pass(model, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K0UZHGnr8SB"
      },
      "source": [
        "和预期一样，在给定模型和单个数据批次的情况下，计算 `batch_loss` 返回 `float32` 损失。请注意 `MODEL_TYPE` 和 `BATCH_TYPE` 合并为形式参数的二维元组的方式；您可以将 `batch_loss` 的类型识别为 `(<MODEL_TYPE,BATCH_TYPE> -> float32)`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4WXEAY8Nr89V"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(<model=<weights=float32[784,10],bias=float32[10]>,batch=<x=float32[?,784],y=int32[?]>> -> float32)'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(batch_loss.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAnt_UcdnvGa"
      },
      "source": [
        "作为健全性检查，我们来构造一个用零填充的初始模型，并计算上文中可视化的那批数据的损失。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "U8Ne8igan3os"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.3025851"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_model = collections.OrderedDict(\n",
        "    weights=np.zeros([784, 10], dtype=np.float32),\n",
        "    bias=np.zeros([10], dtype=np.float32))\n",
        "\n",
        "sample_batch = federated_train_data[5][-1]\n",
        "\n",
        "batch_loss(initial_model, sample_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckigEAyDAWFV"
      },
      "source": [
        "请注意，我们使用定义为 `dict` 的初始模型为 TFF 计算馈送数据，即便定义它的 Python 函数的主体将模型参数用作 `model['weight']` 和 `model['bias']` 。`batch_loss` 调用的参数并不是简单地传递给该函数的主体。\n",
        "\n",
        "当我们调用 `batch_loss` 时会发生什么情况？`batch_loss` 的 Python 主体已在上面的单元格中（在对其进行定义的位置）进行了跟踪和序列化。TFF 在计算定义时充当 `batch_loss` 的调用者，并在 `batch_loss` 被调用时充当调用的目标。在这两个角色中，TFF 均充当 TFF 的抽象类型系统和 Python 表示类型之间的桥梁。在调用时，TFF 将接受大多数标准 Python 容器类型（`dict`、`list`、`tuple`、`collections.namedtuple` 等），以将其作为抽象 TFF 元组的具体表示。虽然我们在上文中提到，TFF 计算在形式上仅接受单个参数，但如果参数的类型是元组，则可以将熟悉的 Python 调用语法与位置和/或关键字参数一起使用，它会按预期工作。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB510nILYbId"
      },
      "source": [
        "### 单个批次上的梯度下降\n",
        "\n",
        "现在，我们来定义一个使用下面的损失函数来执行单步梯度下降的计算。请注意我们在定义此函数时，如何将 `batch_loss` 用作子组件。您可以在另一个计算的主体内部调用使用 `tff.tf_computation` 构造的计算，但正如我们在上文中提到的，您通常没有必要进行此操作。这是因为，序列化会丢失部分调试信息，因此对于不使用 `tff.tf_computation` 装饰器来编写和测试所有 TensorFlow 的更复杂的计算来说，这种方式更加可取。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O4uaVxw3AyYS"
      },
      "outputs": [],
      "source": [
        "@tff.tf_computation(MODEL_TYPE, BATCH_TYPE, tf.float32)\n",
        "def batch_train(initial_model, batch, learning_rate):\n",
        "  # Define a group of model variables and set them to `initial_model`. Must\n",
        "  # be defined outside the @tf.function.\n",
        "  model_vars = collections.OrderedDict([\n",
        "      (name, tf.Variable(name=name, initial_value=value))\n",
        "      for name, value in initial_model.items()\n",
        "  ])\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "  @tf.function\n",
        "  def _train_on_batch(model_vars, batch):\n",
        "    # Perform one step of gradient descent using loss from `batch_loss`.\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = forward_pass(model_vars, batch)\n",
        "    grads = tape.gradient(loss, model_vars)\n",
        "    optimizer.apply_gradients(\n",
        "        zip(tf.nest.flatten(grads), tf.nest.flatten(model_vars)))\n",
        "    return model_vars\n",
        "\n",
        "  return _train_on_batch(model_vars, batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y84gQsaohC38"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(<initial_model=<weights=float32[784,10],bias=float32[10]>,batch=<x=float32[?,784],y=int32[?]>,learning_rate=float32> -> <weights=float32[784,10],bias=float32[10]>)'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(batch_train.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID8xg9FCUL2A"
      },
      "source": [
        "当您在另一个此类函数的主体中调用使用 `tff.tf_computation` 装饰的 Python 函数时，内部 TFF 计算的逻辑会嵌入（本质上为内嵌）到外部计算的逻辑中。如上所述，如果要编写这两个计算，最好将内部函数（在本例中为 `batch_loss`）设置为常规 Python 或 `tf.function` 函数，而非 `tff.tf_computation` 函数。但这里我们演示了，在 `tff.tf_computation` 内部调用与其相同的函数基本上可以按预期工作。例如，如果您没有定义 `batch_loss` 的 Python 代码，而只有它的序列化 TFF 表示，则可能必须进行此操作。\n",
        "\n",
        "现在，将这个函数在初始模型上应用几次，以查看损失是否会减少。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8edcJTlXUULm"
      },
      "outputs": [],
      "source": [
        "model = initial_model\n",
        "losses = []\n",
        "for _ in range(5):\n",
        "  model = batch_train(model, sample_batch, 0.1)\n",
        "  losses.append(batch_loss(model, sample_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3n1onojT1zHv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.19690023, 0.13176313, 0.10113225, 0.08273812, 0.070301384]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQk4Ha8PU-3P"
      },
      "source": [
        "### 本地数据序列上的梯度下降\n",
        "\n",
        "现在，由于 `batch_train` 似乎可以正常工作，我们来编写一个类似的训练函数 `local_train`，它会使用一个用户所有批次的整个序列，而不仅仅是一个批次。现在，新的计算将需要使用 `tff.SequenceType(BATCH_TYPE)` 而不是 `BATCH_TYPE`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EfPD5a6QVNXM"
      },
      "outputs": [],
      "source": [
        "LOCAL_DATA_TYPE = tff.SequenceType(BATCH_TYPE)\n",
        "\n",
        "@tff.federated_computation(MODEL_TYPE, tf.float32, LOCAL_DATA_TYPE)\n",
        "def local_train(initial_model, learning_rate, all_batches):\n",
        "\n",
        "  @tff.tf_computation(LOCAL_DATA_TYPE, tf.float32)\n",
        "  def _insert_learning_rate_to_sequence(dataset, learning_rate):\n",
        "    return dataset.map(lambda x: (x, learning_rate))\n",
        "\n",
        "  batches_with_learning_rate = _insert_learning_rate_to_sequence(all_batches, learning_rate)\n",
        "\n",
        "  # Mapping function to apply to each batch.\n",
        "  @tff.federated_computation(MODEL_TYPE, batches_with_learning_rate.type_signature.element)\n",
        "  def batch_fn(model, batch_with_lr):\n",
        "    batch, lr = batch_with_lr\n",
        "    return batch_train(model, batch, lr)\n",
        "\n",
        "  return tff.sequence_reduce(batches_with_learning_rate, initial_model, batch_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sAhkS5yKUgjC"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(<initial_model=<weights=float32[784,10],bias=float32[10]>,learning_rate=float32,all_batches=<x=float32[?,784],y=int32[?]>*> -> <weights=float32[784,10],bias=float32[10]>)'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(local_train.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYT-SiopYBtH"
      },
      "source": [
        "这段简短的代码中包含了很多细节，我们将逐一进行介绍。\n",
        "\n",
        "首先，虽然我们完全可以用 TensorFlow 实现此逻辑，像之前那样利用 `tf.data.Dataset.reduce` 来处理序列，但这次我们选择用胶水语言将此逻辑表达为 `tff.federated_computation`。我们已使用联合算子 `tff.sequence_reduce` 来执行归约。\n",
        "\n",
        "算子 `tff.sequence_reduce` 的用法类似于 `tf.data.Dataset.reduce`。您可以认为它在本质上与 `tf.data.Dataset.reduce` 相同，但是前者用于联合计算内部（您也许还记得，它不能包含 TensorFlow 代码）。它是一个模板算子，其形式参数三维元组由 `T` 型元素的*序列*、某种类型 `U` 的归约初始状态（我们将其抽象地称为*零*），以及类型 `(<U,T> -> U)` 的*归约算子*（通过处理单个元素改变归约状态）组成。得到的结果是按顺序处理所有元素后归约的最终状态。在我们的示例中，归约状态是在数据前缀上训练的模型，且元素是数据批次。\n",
        "\n",
        "其次，请注意，我们再次将一个计算（`batch_train`）用作了另一个计算（`local_train`）中的组件，而非直接使用。我们不能将其用作归约算子，因为它需要一个额外参数，即学习率。为了解决这个问题，我们定义一个嵌入式联合计算 `batch_fn`，该计算绑定到其主体中 `local_train` 的参数 `learning_rate`。因此，以这种方式定义的子计算可以捕获其父级的形式参数，只要子计算未在其父级的主体之外调用。您可以将此模式视为 Python 中 `functools.partial` 的等效项。\n",
        "\n",
        "当然，以这种方式捕获 `learning_rate` 的实际含义是，在所有批次中都使用相同的学习率值。\n",
        "\n",
        "现在，我们在整个数据序列上尝试新定义的本地训练函数，该数据序列由贡献了样本批次的同一用户（数字 `5`）提供。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EnWFLoZGcSby"
      },
      "outputs": [],
      "source": [
        "locally_trained_model = local_train(initial_model, 0.1, federated_train_data[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0UXUqGk9zoF"
      },
      "source": [
        "有效果吗？为了回答这个问题，我们需要实现评估。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8WDKu6WYy__"
      },
      "source": [
        "### 本地评估\n",
        "\n",
        "下面是一种通过将所有数据批次的损失加总起来实现本地评估的方法（也可以算出平均值；我们将把它作为练习留给读者）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0RiODuc6z7Ln"
      },
      "outputs": [],
      "source": [
        "@tff.federated_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
        "def local_eval(model, all_batches):\n",
        "\n",
        "  @tff.tf_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
        "  def _insert_model_to_sequence(model, dataset):\n",
        "    return dataset.map(lambda x: (model, x))\n",
        "\n",
        "  model_plus_data = _insert_model_to_sequence(model, all_batches)\n",
        "\n",
        "  @tff.tf_computation(tf.float32, batch_loss.type_signature.result)\n",
        "  def tff_add(accumulator, arg):\n",
        "    return accumulator + arg\n",
        "\n",
        "  return tff.sequence_reduce(\n",
        "      tff.sequence_map(\n",
        "          batch_loss,\n",
        "          model_plus_data), 0., tff_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pH2XPEAKa4Dg"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(<model=<weights=float32[784,10],bias=float32[10]>,all_batches=<x=float32[?,784],y=int32[?]>*> -> float32)'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(local_eval.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efX81HuE-BcO"
      },
      "source": [
        "同样，此代码演示了一些新的元素，我们将逐一进行介绍。\n",
        "\n",
        "首先，我们使用了两个新的联合算子来处理序列：一个是 `tff.sequence_map`，它接受*映射函数* `T->U` 和 `T` 的*序列*，然后发出通过逐点应用映射函数获得的 `U` 的序列；另一个是 `tff.sequence_sum`，它只是把所有元素加总起来。在这里，我们将每个数据批次映射到损失值，然后将生成的损失值加总以计算总损失。\n",
        "\n",
        "请注意，我们可以再次使用 `tff.sequence_reduce`，但这不是最佳选择，根据定义，归约过程是顺序的，而映射和求和可以并行计算。如果有选择的话，最好坚持使用不限制实现选择的算子，这样，当将来编译 TFF 计算以部署到特定环境时，就可以充分利用所有潜在机会，实现更快、扩展性更强、更节省资源的执行。\n",
        "\n",
        "其次，请注意，正如在 `local_train` 中一样，我们需要的组件函数（`batch_loss`）接受的参数比联合算子（`tff.sequence_map`）所期望的参数要多，因此我们再次定义了部分参数（内嵌），这次是通过直接将 `lambda` 封装为 `tff.federated_computation`。如果要使用 `tff.tf_computation` 将 TensorFlow 逻辑嵌入 TFF，建议将封装容器与函数一起作为参数内嵌使用。\n",
        "\n",
        "现在，看看我们的训练是否有效。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vPw6JSVf5q_x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_model loss = 23.025854\n",
            "locally_trained_model loss = 0.43484688\n"
          ]
        }
      ],
      "source": [
        "print('initial_model loss =', local_eval(initial_model,\n",
        "                                         federated_train_data[5]))\n",
        "print('locally_trained_model loss =',\n",
        "      local_eval(locally_trained_model, federated_train_data[5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tvu70cnBsUf"
      },
      "source": [
        "确实，损失减少了。但如果我们根据其他用户的数据对其进行评估，会发生什么呢？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gjF0NYAj5wls"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_model loss = 23.025854\n",
            "locally_trained_model loss = 74.50075\n"
          ]
        }
      ],
      "source": [
        "print('initial_model loss =', local_eval(initial_model,\n",
        "                                         federated_train_data[0]))\n",
        "print('locally_trained_model loss =',\n",
        "      local_eval(locally_trained_model, federated_train_data[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WPumnRTBzUs"
      },
      "source": [
        "情况果然变得更糟了。该模型经过训练可以识别 `5`，但从未看到 `0`。这就出现了一个问题，即从全局角度来看，本地训练会对模型质量产生什么影响？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJnL2mQRZKTO"
      },
      "source": [
        "### 联合评估\n",
        "\n",
        "至此，我们终于回到了联合类型和联合计算，即我们最开始讨论的主题。下面是一对源自服务器的模型的 TFF 类型定义，以及保留在客户端上的数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LjGGhpoEBh_6"
      },
      "outputs": [],
      "source": [
        "SERVER_MODEL_TYPE = tff.type_at_server(MODEL_TYPE)\n",
        "CLIENT_DATA_TYPE = tff.type_at_clients(LOCAL_DATA_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gTXV2-jZtE3"
      },
      "source": [
        "根据目前为止介绍的所有定义，在 TFF 中对联合评估的表达均为一行式，我们将模型分发给客户端，让每个客户端在其本地数据部分上调用本地评估，然后对损失进行平均。下面是一种编写方法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2zChEPzEBx4T"
      },
      "outputs": [],
      "source": [
        "@tff.federated_computation(SERVER_MODEL_TYPE, CLIENT_DATA_TYPE)\n",
        "def federated_eval(model, data):\n",
        "  return tff.federated_mean(\n",
        "      tff.federated_map(local_eval, [tff.federated_broadcast(model),  data]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWcNONNWaE0N"
      },
      "source": [
        "我们已经在更简单的场景中看到了 `tff.federated_mean` 和 `tff.federated_map` 的示例，直观来看，他们可以按照预期工作，但这部分代码并不像看上去那么简单，下面我们来仔细研究一下。\n",
        "\n",
        "首先，我们来分解一下*让每个客户端在其本地数据部分上调用本地评估*这个部分。您可能还记得前几部分的内容，`local_eval` 具有形式为 `(<MODEL_TYPE, LOCAL_DATA_TYPE> -> float32)` 的类型签名。\n",
        "\n",
        "联合算子 `tff.federated_map` 是一个模版，它接受二维元组作为参数，该二维元组由某种类型 `T->U` 的*映射函数*和类型 `{T}@CLIENTS` 的联合值（即，具有与映射函数的参数相同类型的成员组成）组成，并返回 `{U}@CLIENTS` 类型的结果。\n",
        "\n",
        "由于我们将 `local_eval` 作为映射函数馈送给每个客户端，因此第二个参数应为联合类型 `{<MODEL_TYPE, LOCAL_DATA_TYPE>}@CLIENTS`（即，根据前几部分的命名，它应该是一个联合元组）。每个客户端应将 `local_eval` 的完整参数集作为成员组成。相反，我们向它馈送的是 2 个元素的 Python `list`。这是什么情况？\n",
        "\n",
        "实际上，这是 TFF 中*隐式类型转换*的示例，它类似于您可能在其他地方遇到的隐式类型转换（例如，当您向接受 `float` 的函数馈送 `int`时）。目前很少使用隐式转换，但我们计划使它在 TFF 中更加普遍，以尽量减少样板文件。\n",
        "\n",
        "在这种情况下，应用的隐式转换在形式为 `{<X,Y>}@Z` 的联合元组和联合值为 `<{X}@Z,{Y}@Z>` 的元组之间等效。虽然二者是不同的类型签名，从程序员的角度来看，`Z` 中的每个设备都包含数据 `X` 和 `Y` 的两个单元。这里发生的情况与 Python 中的 `zip` 没什么区别，实际上，我们提供了一种算子 `tff.federated_zip`，使您可以显式地执行此类转换。当 `tff.federated_map` 遇到作为第二个参数的元组时，它将为您直接调用 `tff.federated_zip`。\n",
        "\n",
        "根据上述信息，您现在应该能够将表达式 `tff.federated_broadcast(model)` 识别为表示 TFF 类型 `{MODEL_TYPE}@CLIENTS` 的值，并将 `data` 识别为 TFF 类型 `{LOCAL_DATA_TYPE}@CLIENTS`（或简写为 `CLIENT_DATA_TYPE`）的值，两者通过隐式 `tff.federated_zip` 一起筛选，以形成 `tff.federated_map` 的第二个参数。\n",
        "\n",
        "如您所料，算子 `tff.federated_broadcast` 只是将数据从服务器传输到客户端。\n",
        "\n",
        "现在，我们来看看本地训练如何影响系统的平均损失。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tbmtJItcn94j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_model loss = 23.025852\n",
            "locally_trained_model loss = 54.432625\n"
          ]
        }
      ],
      "source": [
        "print('initial_model loss =', federated_eval(initial_model,\n",
        "                                             federated_train_data))\n",
        "print('locally_trained_model loss =',\n",
        "      federated_eval(locally_trained_model, federated_train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQi2rGX_fK7i"
      },
      "source": [
        "确实，和预期一样，损失增加了。为了改进所有用户的模型，我们需要用每个用户自己的数据进行训练。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkw9f59qfS7o"
      },
      "source": [
        "### 联合训练\n",
        "\n",
        "实现联合训练的最简单方法是进行本地训练，然后对模型进行平均。这会用到我们讨论过的相同构建块和模式，如下所示。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mBOC4uoG6dd-"
      },
      "outputs": [],
      "source": [
        "SERVER_FLOAT_TYPE = tff.type_at_server(tf.float32)\n",
        "\n",
        "\n",
        "@tff.federated_computation(SERVER_MODEL_TYPE, SERVER_FLOAT_TYPE,\n",
        "                           CLIENT_DATA_TYPE)\n",
        "def federated_train(model, learning_rate, data):\n",
        "  return tff.federated_mean(\n",
        "      tff.federated_map(local_train, [\n",
        "          tff.federated_broadcast(model),\n",
        "          tff.federated_broadcast(learning_rate), data\n",
        "      ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2vACMsQjzO1"
      },
      "source": [
        "请注意，在 `tff.learning` 所提供的联合平均的全功能实现中，由于多种原因（例如，裁剪更新范数的能力、用于压缩等），我们更喜欢对模型增量进行平均，而不是对模型进行平均。\n",
        "\n",
        "让我们通过进行几轮训练并比较前后的平均损失，来看看训练是否有效。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NLx-3rLs9jGY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 0, loss=21.60552215576172\n",
            "round 1, loss=20.365678787231445\n",
            "round 2, loss=19.27480125427246\n",
            "round 3, loss=18.311111450195312\n",
            "round 4, loss=17.45725440979004\n"
          ]
        }
      ],
      "source": [
        "model = initial_model\n",
        "learning_rate = 0.1\n",
        "for round_num in range(5):\n",
        "  model = federated_train(model, learning_rate, federated_train_data)\n",
        "  learning_rate = learning_rate * 0.9\n",
        "  loss = federated_eval(model, federated_train_data)\n",
        "  print('round {}, loss={}'.format(round_num, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0VjSLQzlUIp"
      },
      "source": [
        "现在，为了完整起见，我们也在测试数据上运行一下，以确认我们的模型能够很好地泛化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZaZT45yFMOaM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_model test loss = 22.795593\n",
            "trained_model test loss = 17.278767\n"
          ]
        }
      ],
      "source": [
        "print('initial_model test loss =',\n",
        "      federated_eval(initial_model, federated_test_data))\n",
        "print('trained_model test loss =', federated_eval(model, federated_test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxlHHwLGlgFB"
      },
      "source": [
        "我们的教程到此结束。\n",
        "\n",
        "当然，我们的简化示例并不能反映您在更实际的场景中需要做的许多事情（例如，除了损失之外，我们没有计算指标）。我们鼓励您将 <code>tff.learning</code> 中联合平均的<a>实现</a>作为更完整的示例进行学习，并以此作为演示我们所建议的一些编程做法的方式。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "custom_federated_algorithms_2.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
