{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQxl99l0bZac"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YHz2D-oIqBWa"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXslvcRocA-0"
      },
      "source": [
        "# 隐私频繁项"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XBJJIqwcXKd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://tensorflow.google.cn/federated/tutorials/private_heavy_hitters\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org 上查看</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/federated/tutorials/private_heavy_hitters.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/federated/tutorials/private_heavy_hitters.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 GitHub 上查看源代码</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/federated/tutorials/private_heavy_hitters.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a>   </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJqFp24bb2JN"
      },
      "source": [
        "**注**：本 Colab 已经过验证，可与[最新发布版本](https://github.com/tensorflow/federated#compatibility)的 `tensorflow_federated` pip 软件包一起使用。本 Colab 可能不会更新为适用于 `main`。\n",
        "\n",
        "本教程展示了如何使用 `tff.analytics.heavy_hitters.iblt.build_iblt_computation` API 来构建联合分析计算，以发现群体中最常见的字符串（隐私频繁项）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnUwFbCAKB2r"
      },
      "source": [
        "## 环境设置\n",
        "\n",
        "请运行以下代码来确保您的环境已正确设置。如果未看到问候语，请参阅[安装](../install.md)指南查看说明。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrGitA_KnRO0"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "# tensorflow_federated_nightly also bring in tf_nightly, which\n",
        "# can causes a duplicate tensorboard install, leading to errors.\n",
        "!pip install --quiet tensorflow-text-nightly\n",
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BKyHkMxKHfV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'Hello, World!'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "np.random.seed(0)\n",
        "tff.backends.test.set_test_python_execution_context()\n",
        "\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhLs5GNQ-wWu"
      },
      "source": [
        "## 背景：联合分析中的隐私频繁项"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgGacXm1mVE3"
      },
      "source": [
        "考虑以下设置：每个客户端都有一个字符串列表，每个字符串都来自一个开集，这意味着它可以是任意的。目标是在联合设置中隐秘发现最常见的字符串（**频繁项**）及其数量。本 Colab 演示了使用以下隐私属性解决此问题的解决方案：\n",
        "\n",
        "- 安全聚合：计算聚合字符串计数，使服务器不可能学习任何客户端的单个值。请参阅 `tff.federated_secure_sum`，以了解更多信息。\n",
        "- 差分隐私 (DP)：一种广泛使用的方法，用于限制和量化分析中敏感数据的隐私泄露。可以将用户级中央 DP 应用于频繁项结果。\n",
        "\n",
        "安全聚合 API `tff.federated_secure_sum` 支持整数向量的线性和。如果字符串来自大小为 `n` 的闭集，那么将每个客户端的字符串编码为大小为 `n` 的向量相当轻松：让向量的索引 `i` 处的值是闭集中第 `i`<sup></sup> 个字符串的计数。随后，您可以安全地对所有客户端的向量求和，以获得整个群体中的字符串计数。但是，如果字符串来自一个开集，那么如何适当地对它们进行编码以实现安全求和并非显而易见。在这项工作中，可以将字符串编码为[可逆布隆查找表 (IBLT)](https://arxiv.org/abs/1101.2245)，这是一种概率数据结构，能够以高效的方式对大型（或开放）域中的项目进行编码。IBLT sketch 可以线性求和，因此它们与安全求和兼容。\n",
        "\n",
        "可以使用 `tff.analytics.heavy_hitters.iblt.build_iblt_computation` 创建将每个客户端的本地字符串编码为 IBLT 结构的 TFF 计算。这些结构通过加密安全多方计算协议安全地汇总到服务器可以解码的聚合 IBLT 结构中。随后，服务器可以返回排名靠前的频繁项。以下部分展示了如何使用此 API 创建 TFF 计算并使用莎士比亚数据集运行模拟。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFY_3z-x-3r6"
      },
      "source": [
        "## 加载并预处理联合莎士比亚数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O1CHhdDJcij"
      },
      "source": [
        "莎士比亚数据集包含莎士比亚戏剧的角色台词。本示例中选择了一部分角色（即客户端）。预处理器将每个角色的台词转换为字符串列表，并删除任何只有标点符号或符号的字符串。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b65q5mp4r1n7"
      },
      "outputs": [],
      "source": [
        "# Load the simulation data.\n",
        "source, _ = tff.simulation.datasets.shakespeare.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReoTRs8ntJw7"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function to tokenize a line into words.\n",
        "def tokenize(ds):\n",
        "  \"\"\"Tokenizes a line into words with alphanum characters.\"\"\"\n",
        "  def extract_strings(example):\n",
        "    return tf.expand_dims(example['snippets'], 0)\n",
        "\n",
        "  def tokenize_line(line):\n",
        "    return tf.data.Dataset.from_tensor_slices(tokenizer.tokenize(line)[0])\n",
        "\n",
        "  def mask_all_symbolic_words(word):\n",
        "    return tf.math.logical_not(\n",
        "        tf_text.wordshape(word, tf_text.WordShape.IS_PUNCT_OR_SYMBOL))\n",
        "\n",
        "  tokenizer = tf_text.WhitespaceTokenizer()\n",
        "  ds = ds.map(extract_strings)\n",
        "  ds = ds.flat_map(tokenize_line)\n",
        "  ds = ds.map(tf_text.case_fold_utf8)\n",
        "  ds = ds.filter(mask_all_symbolic_words)\n",
        "  return ds\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return tokenize(source.create_tf_dataset_for_client(\n",
        "      source.client_ids[n])).batch(batch_size)\n",
        "\n",
        "# Pick a subset of client devices to participate in the computation.\n",
        "dataset = [client_data(n) for n in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGwJsssK9_e"
      },
      "source": [
        "## 模拟"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtCRYhI0nKcm"
      },
      "source": [
        "要运行模拟以发现莎士比亚数据集中最常见的单词（频繁项），首先需要使用 `tff.analytics.heavy_hitters.iblt.build_iblt_computation` API 和以下参数创建一个 TFF 计算：\n",
        "\n",
        "- `capacity`：IBLT sketch 的容量。此数字应该大致等于一轮计算中可能出现的唯一字符串的总数。默认值为 `1000`。如果此数字太小，解码可能由于散列值的冲突而失败。如果此数字太大，它会消耗更多不必要的内存。\n",
        "- `string_max_bytes`：IBLT 中字符串的最大长度。默认值为 `10`。必须为正数。长度超过 `string_max_bytes` 的字符串将被截断。\n",
        "- `max_words_per_user`：每个客户端允许贡献的最大字符串数。如果不是 `None`，则必须是正整数。默认值为 `None`，这意味着所有客户端都贡献了其所有字符串。\n",
        "- `max_heavy_hitters`：要返回的最大项目数。如果解码结果的项目数超过此数量，将按估计计数递减排序并返回前 max_heavy_hitters 个项目。默认值为 `None`，这意味着返回结果中的所有频繁项。\n",
        "- `secure_sum_bitwidth`：用于安全求和的位宽。默认值为 `None`，该值会禁用安全求和。如果不是 `None`，则必须处于 `[1,62]` 范围内。请参阅 `tff.federated_secure_sum`。\n",
        "- `multi_contribution`：是否允许每个客户端为每个唯一单词贡献多个计数或仅贡献一个计数。默认值为 `True`。当需要差分隐私时，此参数可以改善效果。\n",
        "- `batch_size`：每批数据集中的元素数量。默认值为 `1`，表示输入数据集由 `tf.data.Dataset.batch(1)` 处理。必须是正整数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iyRWmV529qY"
      },
      "outputs": [],
      "source": [
        "max_words_per_user = 8\n",
        "iblt_computation = tff.analytics.heavy_hitters.iblt.build_iblt_computation(\n",
        "    capacity=100,\n",
        "    string_max_bytes=20,\n",
        "    max_words_per_user=max_words_per_user,\n",
        "    max_heavy_hitters=10,\n",
        "    secure_sum_bitwidth=32,\n",
        "    multi_contribution=False,\n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe8ZUIwH4C1y"
      },
      "source": [
        "现在已准备好使用 TFF 计算 `iblt_computation` 和预处理输入数据集运行模拟。`iblt_computation` 的输出有以下四个属性：\n",
        "\n",
        "- 客户端：参与计算的客户端的标量值。\n",
        "- heavy_hitters：聚合的频繁项列表。\n",
        "- heavy_hitters_counts：聚合的频繁项的计数列表。\n",
        "- num_not_decoded：未成功解码的字符串的标量值。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r8Y6GL-zhPv"
      },
      "outputs": [],
      "source": [
        "def run_simulation(one_round_computation: tff.Computation, dataset):\n",
        "  output = one_round_computation(dataset)\n",
        "  heavy_hitters = output.heavy_hitters\n",
        "  heavy_hitters_counts = output.heavy_hitters_counts\n",
        "  heavy_hitters = [word.decode('utf-8', 'ignore') for word in heavy_hitters]\n",
        "\n",
        "  results = {}\n",
        "  for index in range(len(heavy_hitters)):\n",
        "    results[heavy_hitters[index]] = heavy_hitters_counts[index]\n",
        "  return output.clients, dict(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w99wVdhW0OIR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of clients participated: 10\n",
            "Discovered heavy hitters and counts:\n",
            "{'to': 8, 'the': 8, 'and': 7, 'you': 4, 'i': 4, 'a': 3, 'he': 3, 'your': 3, 'is': 3, 'of': 2}\n"
          ]
        }
      ],
      "source": [
        "clients, result = run_simulation(iblt_computation, dataset)\n",
        "print(f'Number of clients participated: {clients}')\n",
        "print('Discovered heavy hitters and counts:')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4SdslRULCox"
      },
      "source": [
        "## 使用差分隐私的隐私频繁项"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F4O2U7nGL1A"
      },
      "source": [
        "为了获得具有中心 DP 的隐私频繁项，将 DP 机制应用于开集直方图。其思想是向聚合直方图中的字符串计数添加噪声，然后只保留计数高于某个阈值的字符串。噪声和阈值取决于 (epsilon, delta)-DP 预算，有关详细算法和证明，请参阅[此文档](https://github.com/google/differential-privacy/blob/main/common_docs/Delta_For_Thresholding.pdf)。作为后处理步骤，噪声计数四舍五入为整数，这不会削弱 DP 保证。请注意，当需要 DP 时，您会发现更少的频繁项。这是因为阈值步骤会过滤掉计数较低的字符串。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryZZgH8nJi9v"
      },
      "outputs": [],
      "source": [
        "iblt_computation = tff.analytics.heavy_hitters.iblt.build_iblt_computation(\n",
        "    capacity=100,\n",
        "    string_max_bytes=20,\n",
        "    max_words_per_user=max_words_per_user,\n",
        "    secure_sum_bitwidth=32,\n",
        "    multi_contribution=False,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "clients, result = run_simulation(iblt_computation, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxhBSUFs3Ku6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovered heavy hitters and counts with central DP:\n",
            "{'the': 8, 'you': 4, 'to': 7, 'tear': 3, 'and': 7, 'i': 3}\n"
          ]
        }
      ],
      "source": [
        "# DP parameters\n",
        "eps = 20\n",
        "delta = 0.01\n",
        "\n",
        "# Calculating scale for Laplace noise\n",
        "scale = max_words_per_user / eps\n",
        "\n",
        "# Calculating the threshold\n",
        "tau = 1 + (max_words_per_user / eps) * np.log(max_words_per_user / (2 * delta))\n",
        "\n",
        "result_with_dp = {}\n",
        "for word in result:\n",
        "  noised_count = result[word] + np.random.laplace(scale=scale)\n",
        "  if noised_count >= tau:\n",
        "    result_with_dp[word] = int(noised_count)\n",
        "print(f'Discovered heavy hitters and counts with central DP:')\n",
        "print(result_with_dp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "private_heavy_hitters.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
