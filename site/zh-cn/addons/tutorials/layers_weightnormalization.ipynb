{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# TensorFlow Addons 层：WeightNormalization\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://tensorflow.google.cn/addons/tutorials/layers_weightnormalization\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org 上查看 </a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/addons/tutorials/layers_weightnormalization.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行 </a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/addons/tutorials/layers_weightnormalization.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">View source on GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/addons/tutorials/layers_weightnormalization.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">Download notebook</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## 概述\n",
        "\n",
        "此笔记本将演示如何使用权重归一化层以及如何提升收敛。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR01t9v_fxbT"
      },
      "source": [
        "# WeightNormalization\n",
        "\n",
        "用于加速深度神经网络训练的一个简单的重新参数化：\n",
        "\n",
        "Tim Salimans、Diederik P. Kingma (2016)\n",
        "\n",
        "> 通过以这种方式重新参数化权重，我们改善了优化问题的条件，并加快了随机梯度下降的收敛速度。我们的重新参数化受到批次归一化的启发，但没有在小批次中的样本之间引入任何依赖项。这意味着我们的方法也可以成功应用于递归模型（例如 LSTM）和对噪声敏感的应用（例如深度强化学习或生成模型），而批次归一化则不太适合这类模型和应用。尽管我们的方法要简单得多，但它仍可很大程度上为完整批次归一化提供加速。另外，我们方法的计算开销较低，从而允许在相同的时间内执行更多的优化步骤。\n",
        "\n",
        "> https://arxiv.org/abs/1602.07868\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/seanpmorgan/tf-weightnorm/master/static/wrapped-graph.png\" width=\"80%\"><br><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## 设置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyWHXw9mQ6mp"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OywLbs7EXiE_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQMhhq1qXiFF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULWHqMAnTVZD"
      },
      "outputs": [],
      "source": [
        "# Hyper Parameters\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "num_classes=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhM0ieDpSnKh"
      },
      "source": [
        "## 构建模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XZXnBYgRPSk"
      },
      "outputs": [],
      "source": [
        "# Standard ConvNet\n",
        "reg_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(6, 5, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(16, 5, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(120, activation='relu'),\n",
        "    tf.keras.layers.Dense(84, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZd6V90eR4Gm"
      },
      "outputs": [],
      "source": [
        "# WeightNorm ConvNet\n",
        "wn_model = tf.keras.Sequential([\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Conv2D(6, 5, activation='relu')),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Conv2D(16, 5, activation='relu')),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Dense(120, activation='relu')),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Dense(84, activation='relu')),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_classes, activation='softmax')),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA5dti8AS2Y7"
      },
      "source": [
        "## 加载数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8Isjc7W8MEn"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH1CG9E7S34C"
      },
      "source": [
        "## 训练模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvNKxfaI7vSm"
      },
      "outputs": [],
      "source": [
        "reg_model.compile(optimizer='adam', \n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "reg_history = reg_model.fit(x_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esmMh-5g7wmp"
      },
      "outputs": [],
      "source": [
        "wn_model.compile(optimizer='adam', \n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "wn_history = wn_model.fit(x_train, y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          validation_data=(x_test, y_test),\n",
        "                          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yujf2YRbwX55"
      },
      "outputs": [],
      "source": [
        "reg_accuracy = reg_history.history['accuracy']\n",
        "wn_accuracy = wn_history.history['accuracy']\n",
        "\n",
        "plt.plot(np.linspace(0, epochs,  epochs), reg_accuracy,\n",
        "             color='red', label='Regular ConvNet')\n",
        "\n",
        "plt.plot(np.linspace(0, epochs, epochs), wn_accuracy,\n",
        "         color='blue', label='WeightNorm ConvNet')\n",
        "\n",
        "plt.title('WeightNorm Accuracy Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "layers_weightnormalization.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
