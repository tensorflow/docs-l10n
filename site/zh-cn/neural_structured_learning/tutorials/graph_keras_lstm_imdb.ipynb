{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24gYiJcWNlpA"
      },
      "source": [
        "##### Copyright 2019 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ioaprt5q5US7"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# 使用计算图正则化实现利用合成计算图的情感分类\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://tensorflow.google.cn/neural_structured_learning/tutorials/graph_keras_lstm_imdb\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org上查看</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行 </a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 GitHub 中查看源代码</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3otbdCMmJiJ"
      },
      "source": [
        "## 文本特征向量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "此笔记本利用评论文本将电影评论分类为*正面*或*负面*评价。这是一个*二元*分类示例，也是一个重要且应用广泛的机器学习问题。\n",
        "\n",
        "在此笔记本中，我们将通过根据给定的输入构建计算图来演示如何使用计算图正则化。当输入不包含显式计算图时，使用神经结构学习 (NSL) 框架构建计算图正则化模型的一般方法如下：\n",
        "\n",
        "1. 为输入中的每个文本样本创建嵌入向量。该操作可使用 [word2vec](https://arxiv.org/pdf/1310.4546.pdf)、[Swivel](https://arxiv.org/abs/1602.02215)、[BERT](https://arxiv.org/abs/1810.04805) 等预训练模型来完成。\n",
        "2. 通过使用诸如“L2”距离、“余弦”距离等相似度指标，基于这些嵌入向量构建计算图。计算图中的节点对应于样本，计算图中的边对应于样本对之间的相似度。\n",
        "3. 基于上述合成计算图和样本特征生成训练数据。除原始节点特征外，所得的训练数据还将包含近邻特征。\n",
        "4. 使用 Keras 序列式、函数式或子类 API 作为基础模型创建神经网络。\n",
        "5. 使用 NSL 框架提供的 GraphRegularization 包装器类包装基础模型，以创建新的计算图 Keras 模型。这个新模型将包含计算图正则化损失作为其训练目标中的一个正规化项。\n",
        "6. 训练和评估计算图 Keras 模型。\n",
        "\n",
        "**注**：我们预计读者阅读本教程所需时间为 1 小时左右。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOFbB34KY1R"
      },
      "source": [
        "## 要求\n",
        "\n",
        "1. 安装 Neural Structured Learning 软件包。\n",
        "2. 安装 tensorflow-hub。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVnjPmOaQlnH"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet neural-structured-learning\n",
        "!pip install --quiet tensorflow-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6FJ64qMNLez"
      },
      "source": [
        "## 依赖项和导入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ew7HTbPpCJH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import neural_structured_learning as nsl\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Resets notebook state\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\n",
        "    \"GPU is\",\n",
        "    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGwwFd99n42P"
      },
      "source": [
        "## IMDB 数据集\n",
        "\n",
        "[IMDB 数据集](https://tensorflow.google.cn/api_docs/python/tf/keras/datasets/imdb)包含 [Internet Movie Database](https://www.imdb.com/) 中的 50,000 条电影评论文本 。我们将这些评论分为两组，其中 25,000 条用于训练，另外 25,000 条用于测试。训练组和测试组是*均衡的*，也就是说其中包含相等数量的正面评价和负面评价。\n",
        "\n",
        "在本教程中，我们将使用 IMDB 数据集的预处理版本。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "source": [
        "### 下载预处理的 IMDB 数据集\n",
        "\n",
        "TensorFlow 随附 IMDB 数据集。该数据集经过预处理，已将评论（单词序列）转换为整数序列，其中每个整数均代表字典中的特定单词。\n",
        "\n",
        "以下代码可下载 IMDB 数据集（如已下载，则使用缓存副本）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXXx5Oc3pOmN"
      },
      "outputs": [],
      "source": [
        "imdb = tf.keras.datasets.imdb\n",
        "(pp_train_data, pp_train_labels), (pp_test_data, pp_test_labels) = (\n",
        "    imdb.load_data(num_words=10000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odr-KlzO-lkL"
      },
      "source": [
        "参数 `num_words=10000` 会将训练数据中的前 10,000 个最频繁出现的单词保留下来。稀有单词将被丢弃以保持词汇量的可管理性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l50X3GfjpU4r"
      },
      "source": [
        "### 探索数据\n",
        "\n",
        "我们花一点时间来了解数据的格式。数据集经过预处理：每个样本都是一个整数数组，每个整数代表电影评论中的单词。每个标签是一个整数值（0 或 1），其中 0 表示负面评价，而 1 表示正面评价。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8qCnve_-lkO"
      },
      "outputs": [],
      "source": [
        "print('Training entries: {}, labels: {}'.format(\n",
        "    len(pp_train_data), len(pp_train_labels)))\n",
        "training_samples_count = len(pp_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnKvHWW4-lkW"
      },
      "source": [
        "评论文本已转换为整数，其中每个整数均代表字典中的特定单词。第一条评论如下所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtTS4kpEpjbi"
      },
      "outputs": [],
      "source": [
        "print(pp_train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIE4l_72x7DP"
      },
      "source": [
        "电影评论的长度可能各不相同。以下代码显示了第一条评论和第二条评论中的单词数。由于神经网络的输入必须具有相同的长度，因此我们稍后需要解决长度问题。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-6Ii9Pfx6Nr"
      },
      "outputs": [],
      "source": [
        "len(pp_train_data[0]), len(pp_train_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wJg2FiYpuoX"
      },
      "source": [
        "### 将整数重新转换为单词\n",
        "\n",
        "了解如何将整数重新转换为相应的文本可能非常实用。在这里，我们将创建一个辅助函数来查询包含整数到字符串映射的字典对象："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr5s_1alpzop"
      },
      "outputs": [],
      "source": [
        "def build_reverse_word_index():\n",
        "  # A dictionary mapping words to an integer index\n",
        "  word_index = imdb.get_word_index()\n",
        "\n",
        "  # The first indices are reserved\n",
        "  word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "  word_index['<PAD>'] = 0\n",
        "  word_index['<START>'] = 1\n",
        "  word_index['<UNK>'] = 2  # unknown\n",
        "  word_index['<UNUSED>'] = 3\n",
        "  return dict((value, key) for (key, value) in word_index.items())\n",
        "\n",
        "reverse_word_index = build_reverse_word_index()\n",
        "\n",
        "def decode_review(text):\n",
        "  return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3CNRvEZVppl"
      },
      "source": [
        "现在，我们可以使用 `decode_review` 函数来显示第一条评论的文本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_OqxmH6-lkn"
      },
      "outputs": [],
      "source": [
        "decode_review(pp_train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVmqL-zcWm8v"
      },
      "source": [
        "## 计算图构造\n",
        "\n",
        "计算图的构造涉及为文本样本创建嵌入向量，然后使用相似度函数比较嵌入向量。\n",
        "\n",
        "在继续之前，我们先创建一个目录来存储在本教程中创建的工件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZicFxFOeL2J"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /tmp/imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUyHEa-3TB2X"
      },
      "source": [
        "### 创建样本嵌入向量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCe9vOy7-Br9"
      },
      "source": [
        "我们将使用预训练的 Swivel 嵌入向量为输入中的每个样本创建 `tf.train.Example` 格式的嵌入向量。我们将以 `TFRecord` 格式存储生成的嵌入向量以及代表每个样本 ID 的附加特征。这有助于我们在未来能够将样本嵌入向量与计算图中的相应节点进行匹配。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq2Ohd9CuZv_"
      },
      "outputs": [],
      "source": [
        "pretrained_embedding = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\n",
        "\n",
        "hub_layer = hub.KerasLayer(\n",
        "    pretrained_embedding, input_shape=[], dtype=tf.string, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXJ3RaboTSKQ"
      },
      "outputs": [],
      "source": [
        "def _int64_feature(value):\n",
        "  \"\"\"Returns int64 tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value.tolist()))\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns bytes tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(\n",
        "      bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
        "\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns float tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=value.tolist()))\n",
        "\n",
        "\n",
        "def create_embedding_example(word_vector, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's embedding and its ID.\"\"\"\n",
        "\n",
        "  text = decode_review(word_vector)\n",
        "\n",
        "  # Shape = [batch_size,].\n",
        "  sentence_embedding = hub_layer(tf.reshape(text, shape=[-1,]))\n",
        "\n",
        "  # Flatten the sentence embedding back to 1-D.\n",
        "  sentence_embedding = tf.reshape(sentence_embedding, shape=[-1])\n",
        "\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'embedding': _float_feature(sentence_embedding.numpy())\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "\n",
        "def create_embeddings(word_vectors, output_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(output_path) as writer:\n",
        "    for word_vector in word_vectors:\n",
        "      example = create_embedding_example(word_vector, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "\n",
        "# Persist TF.Example features containing embeddings for training data in\n",
        "# TFRecord format.\n",
        "create_embeddings(pp_train_data, '/tmp/imdb/embeddings.tfr', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8s06RuI_vKs"
      },
      "source": [
        "### 构建计算图\n",
        "\n",
        "现在有了样本嵌入向量，我们将使用它们来构建相似度计算图：此计算图中的节点将与样本对应，此计算图中的边将与节点对之间的相似度对应。\n",
        "\n",
        "神经结构学习提供了一个计算图构建库，用于基于样本嵌入向量构建计算图。它使用[**余弦相似度**](https://en.wikipedia.org/wiki/Cosine_similarity)作为相似度指标来比较嵌入向量并在它们之间构建边。它还支持指定相似度阈值，用于从最终计算图中丢弃不相似的边。在本示例中，使用 0.99 作为相似度阈值，使用 12345 作为随机种子，我们最终得到一个具有 429,415 条双向边的计算图。在这里，我们借助计算图构建器对[局部敏感哈希](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) (LSH) 算法的支持来加快计算图构建。有关使用计算图构建器的 LSH 支持的详细信息，请参阅 [`build_graph_from_config`](https://tensorflow.google.cn/neural_structured_learning/api_docs/python/nsl/tools/build_graph_from_config) API 文档。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY6lqhNkBh2Q"
      },
      "outputs": [],
      "source": [
        "graph_builder_config = nsl.configs.GraphBuilderConfig(\n",
        "    similarity_threshold=0.99, lsh_splits=32, lsh_rounds=15, random_seed=12345)\n",
        "nsl.tools.build_graph_from_config(['/tmp/imdb/embeddings.tfr'],\n",
        "                                  '/tmp/imdb/graph_99.tsv',\n",
        "                                  graph_builder_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dk9xfQcK553"
      },
      "source": [
        "在输出 TSV 文件中，每条双向边均由两条有向边表示，因此该文件共含 429,415 * 2 = 858,830 行："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDPwTpZcJ3zF"
      },
      "outputs": [],
      "source": [
        "!wc -l /tmp/imdb/graph_99.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06QrEVCIlTvV"
      },
      "source": [
        "**注**：计算图质量以及与之相关的嵌入向量质量对于计算图正则化非常重要。虽然我们在此笔记本中使用了 Swivel 嵌入向量，但如果使用 BERT 等嵌入向量，可能会更准确地捕获评论语义。我们鼓励用户根据自身需求选用合适的嵌入向量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USkfut69gNW"
      },
      "source": [
        "## 样本特征\n",
        "\n",
        "我们使用 `tf.train.Example` 格式为问题创建样本特征，并将其保留为 `TFRecord` 格式。每个样本将包含以下三个特征：\n",
        "\n",
        "1. **id**：样本的节点 ID。\n",
        "2. **words**：包含单词 ID 的 int64 列表。\n",
        "3. **label**：用于标识评论的目标类的单例 int64。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PcUF4_B9grB"
      },
      "outputs": [],
      "source": [
        "def create_example(word_vector, label, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's word vector, label, and ID.\"\"\"\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'words': _int64_feature(np.asarray(word_vector)),\n",
        "      'label': _int64_feature(np.asarray([label])),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "def create_records(word_vectors, labels, record_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(record_path) as writer:\n",
        "    for word_vector, label in zip(word_vectors, labels):\n",
        "      example = create_example(word_vector, label, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "# Persist TF.Example features (word vectors and labels) for training and test\n",
        "# data in TFRecord format.\n",
        "next_record_id = create_records(pp_train_data, pp_train_labels,\n",
        "                                '/tmp/imdb/train_data.tfr', 0)\n",
        "create_records(pp_test_data, pp_test_labels, '/tmp/imdb/test_data.tfr',\n",
        "               next_record_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhFO9sZ8Aa_g"
      },
      "source": [
        "## 使用计算图近邻增强训练数据\n",
        "\n",
        "拥有样本特征与合成计算图后，我们可以生成用于神经结构学习的增强训练数据。NSL 框架提供了一个将计算图和样本特征相结合的库，二者结合可生成用于计算图正则化的最终训练数据。所得的训练数据将包括原始样本特征及其相应近邻的特征。\n",
        "\n",
        "在本教程中，我们考虑无向边并为每个样本最多使用 3 个近邻，以使用计算图近邻来增强训练数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSCHj4rIBj_A"
      },
      "outputs": [],
      "source": [
        "nsl.tools.pack_nbrs(\n",
        "    '/tmp/imdb/train_data.tfr',\n",
        "    '',\n",
        "    '/tmp/imdb/graph_99.tsv',\n",
        "    '/tmp/imdb/nsl_train_data.tfr',\n",
        "    add_undirected_edges=True,\n",
        "    max_nbrs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzBWdWkBqlMy"
      },
      "source": [
        "## 基础模型\n",
        "\n",
        "现在，我们已准备好构建无计算图正则化的基础模型。为了构建此模型，我们可以使用在构建计算图时使用的嵌入向量，也可以与分类任务一起学习新的嵌入向量。在此笔记本中，我们将使用后者。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSbRFguBUNl"
      },
      "source": [
        "### 全局变量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsA8HuvvwGri"
      },
      "outputs": [],
      "source": [
        "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
        "NBR_WEIGHT_SUFFIX = '_weight'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8gMVBw6t6CI"
      },
      "source": [
        "### 超参数\n",
        "\n",
        "我们将使用 `HParams` 的实例来包含用于训练和评估的各种超参数和常量。以下为各项内容的简要介绍：\n",
        "\n",
        "- **num_classes**：有 2 个 类 - *正面*和*负面*。\n",
        "\n",
        "- **max_seq_length**：在本示例中，此参数为每条电影评论中考虑的最大单词数。\n",
        "\n",
        "- **vocab_size**：此参数为本示例考虑的词汇量。\n",
        "\n",
        "- **distance_type**：此参数为用于正则化样本与其近邻的距离指标。\n",
        "\n",
        "- **graph_regularization_multiplier**：此参数控制计算图正则化项在总体损失函数中的相对权重。\n",
        "\n",
        "- **num_neighbors**：用于计算图正则化的近邻数。此值必须小于或等于调用 `nsl.tools.pack_nbrs` 时上文使用的 `max_nbrs` 参数。\n",
        "\n",
        "- **num_fc_units**：神经网络的全连接层中的单元数。\n",
        "\n",
        "- **train_epochs**：训练周期数。\n",
        "\n",
        "- **batch_size**：用于训练和评估的批次大小。\n",
        "\n",
        "- **eval_steps**：认定评估完成之前需要处理的批次数。如果设置为 `None`，则将评估测试集中的所有实例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlTmug7auQ2r"
      },
      "outputs": [],
      "source": [
        "class HParams(object):\n",
        "  \"\"\"Hyperparameters used for training.\"\"\"\n",
        "  def __init__(self):\n",
        "    ### dataset parameters\n",
        "    self.num_classes = 2\n",
        "    self.max_seq_length = 256\n",
        "    self.vocab_size = 10000\n",
        "    ### neural graph learning parameters\n",
        "    self.distance_type = nsl.configs.DistanceType.L2\n",
        "    self.graph_regularization_multiplier = 0.1\n",
        "    self.num_neighbors = 2\n",
        "    ### model architecture\n",
        "    self.num_embedding_dims = 16\n",
        "    self.num_lstm_dims = 64\n",
        "    self.num_fc_units = 64\n",
        "    ### training parameters\n",
        "    self.train_epochs = 10\n",
        "    self.batch_size = 128\n",
        "    ### eval parameters\n",
        "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
        "\n",
        "HPARAMS = HParams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFP_XKVRp4_S"
      },
      "source": [
        "### 准备数据\n",
        "\n",
        "评论（整数数组）必须先转换为张量，然后才能馈入神经网络。可以通过以下两种方式完成此转换：\n",
        "\n",
        "- 将数组转换为指示单词是否出现的 `0` 和 `1` 向量，类似于独热编码。例如，序列 `[3, 5]` 将成为 `10000`-维向量，除了索引 `3` 和 `5` 为 1 之外，其余均为 0。然后，使其成为我们网络中的第一层（`Dense` 层），可以处理浮点向量数据。但是，此方法需要占用大量内存，需要 `num_words * num_reviews` 大小的矩阵。\n",
        "\n",
        "- 另外，我们可以填充数组以使其均具有相同的长度，然后创建形状为 `max_length * num_reviews` 的整数张量。我们可以使用能够处理此形状的嵌入向量层作为网络中的第一层。\n",
        "\n",
        "在本教程中，我们将使用第二种方法。\n",
        "\n",
        "由于电影评论长度必须相同，因此我们将使用如下定义的 `pad_sequence` 函数来标准化长度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5lkZVynuHWs"
      },
      "outputs": [],
      "source": [
        "def make_dataset(file_path, training=False):\n",
        "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
        "\n",
        "  Args:\n",
        "    file_path: Name of the file in the `.tfrecord` format containing\n",
        "      `tf.train.Example` objects.\n",
        "    training: Boolean indicating if we are in training mode.\n",
        "\n",
        "  Returns:\n",
        "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
        "    objects.\n",
        "  \"\"\"\n",
        "\n",
        "  def pad_sequence(sequence, max_seq_length):\n",
        "    \"\"\"Pads the input sequence (a `tf.SparseTensor`) to `max_seq_length`.\"\"\"\n",
        "    pad_size = tf.maximum([0], max_seq_length - tf.shape(sequence)[0])\n",
        "    padded = tf.concat(\n",
        "        [sequence.values,\n",
        "         tf.fill((pad_size), tf.cast(0, sequence.dtype))],\n",
        "        axis=0)\n",
        "    # The input sequence may be larger than max_seq_length. Truncate down if\n",
        "    # necessary.\n",
        "    return tf.slice(padded, [0], [max_seq_length])\n",
        "\n",
        "  def parse_example(example_proto):\n",
        "    \"\"\"Extracts relevant fields from the `example_proto`.\n",
        "\n",
        "    Args:\n",
        "      example_proto: An instance of `tf.train.Example`.\n",
        "\n",
        "    Returns:\n",
        "      A pair whose first value is a dictionary containing relevant features\n",
        "      and whose second value contains the ground truth labels.\n",
        "    \"\"\"\n",
        "    # The 'words' feature is a variable length word ID vector.\n",
        "    feature_spec = {\n",
        "        'words': tf.io.VarLenFeature(tf.int64),\n",
        "        'label': tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
        "    }\n",
        "    # We also extract corresponding neighbor features in a similar manner to\n",
        "    # the features above during training.\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i,\n",
        "                                         NBR_WEIGHT_SUFFIX)\n",
        "        feature_spec[nbr_feature_key] = tf.io.VarLenFeature(tf.int64)\n",
        "\n",
        "        # We assign a default value of 0.0 for the neighbor weight so that\n",
        "        # graph regularization is done on samples based on their exact number\n",
        "        # of neighbors. In other words, non-existent neighbors are discounted.\n",
        "        feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
        "            [1], tf.float32, default_value=tf.constant([0.0]))\n",
        "\n",
        "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "\n",
        "    # Since the 'words' feature is a variable length word vector, we pad it to a\n",
        "    # constant maximum length based on HPARAMS.max_seq_length\n",
        "    features['words'] = pad_sequence(features['words'], HPARAMS.max_seq_length)\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        features[nbr_feature_key] = pad_sequence(features[nbr_feature_key],\n",
        "                                                 HPARAMS.max_seq_length)\n",
        "\n",
        "    labels = features.pop('label')\n",
        "    return features, labels\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset([file_path])\n",
        "  if training:\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.map(parse_example)\n",
        "  dataset = dataset.batch(HPARAMS.batch_size)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = make_dataset('/tmp/imdb/nsl_train_data.tfr', True)\n",
        "test_dataset = make_dataset('/tmp/imdb/test_data.tfr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "### 构建模型\n",
        "\n",
        "神经网络是通过堆叠层创建的，这需要确定两个主要架构决策：\n",
        "\n",
        "- 在模型中使用多少个层？\n",
        "- 为每个层使用多少个*隐藏单元*？\n",
        "\n",
        "在本示例中，输入数据由单词索引数组组成。要预测的标签为 0 或 1。\n",
        "\n",
        "在本教程中，我们将使用双向 LSTM 作为基础模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpKOoWgu-llD"
      },
      "outputs": [],
      "source": [
        "# This function exists as an alternative to the bi-LSTM model used in this\n",
        "# notebook.\n",
        "def make_feed_forward_model():\n",
        "  \"\"\"Builds a simple 2 layer feed forward neural network.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "  embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size, 16)(inputs)\n",
        "  pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
        "  dense_layer = tf.keras.layers.Dense(16, activation='relu')(pooling_layer)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_layer)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "def make_bilstm_model():\n",
        "  \"\"\"Builds a bi-directional LSTM model.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "  embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size,\n",
        "                                              HPARAMS.num_embedding_dims)(\n",
        "                                                  inputs)\n",
        "  lstm_layer = tf.keras.layers.Bidirectional(\n",
        "      tf.keras.layers.LSTM(HPARAMS.num_lstm_dims))(\n",
        "          embedding_layer)\n",
        "  dense_layer = tf.keras.layers.Dense(\n",
        "      HPARAMS.num_fc_units, activation='relu')(\n",
        "          lstm_layer)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_layer)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "# Feel free to use an architecture of your choice.\n",
        "model = make_bilstm_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "按顺序有效堆叠层以构建分类器：\n",
        "\n",
        "1. 第一层为接受整数编码词汇的 `Input` 层。\n",
        "2. 第二层为 `Embedding` 层，该层接受整数编码词汇并查找嵌入向量中的每个单词索引。在模型训练时会学习这些向量。向量会向输出数组添加维度。得到的维度为：<code>(batch, sequence, embedding)</code>。\n",
        "3. 接下来，双向 LSTM 层会为每个样本返回固定长度的输出向量。\n",
        "4. 此固定长度的输出向量穿过一个包含 64 个隐藏单元的全连接 (`Dense`) 层。\n",
        "5. 最后一层与单个输出节点密集连接。利用 `sigmoid` 激活函数，得出此值是 0 到 1 之间的浮点数，表示概率或置信度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XMwnDOp-llH"
      },
      "source": [
        "### 隐藏单元\n",
        "\n",
        "上述模型在输入和输出之间有两个中间（或称“隐藏”）层（不包括 `Embedding` 层）。输出（单元、节点或神经元）的数量是层的表示空间的维度。换言之，即网络学习内部表示时允许的自由度。\n",
        "\n",
        "模型的隐藏单元越多（更高维度的表示空间）和/或层越多，则网络可以学习的表示越复杂。但是，这会导致网络的计算开销增加，并且可能导致学习不需要的模式——提高在训练数据（而不是测试数据）上的性能的模式。这就叫*过拟合*。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### 损失函数和优化器\n",
        "\n",
        "模型训练需要一个损失函数和一个优化器。由于这是二元分类问题，并且模型输出概率（具有 Sigmoid 激活的单一单元层），我们将使用 `binary_crossentropy` 损失函数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCWYwkug-llQ"
      },
      "source": [
        "### 创建验证集\n",
        "\n",
        "训练时，我们希望检验该模型在未见过的数据上的准确率。为此，需要将原始训练数据中的一部分分离出来，创建一个*验证集*。（为何现在不使用测试集？因为我们的目标是仅使用训练数据开发和调整模型，然后只使用一次测试数据来评估准确率）。\n",
        "\n",
        "在本教程中，我们将大约 10% 的初始训练样本（25000 的 10%）作为用于训练的带标签数据，其余作为验证数据。由于初始训练/测试数据集以 50/50 的比例拆分（每个数据集 25000 个样本），因此我们现在的有效训练/验证/测试数据集拆分比例为 5/45/50。\n",
        "\n",
        "请注意，“train_dataset”已进行批处理并且已打乱顺序。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYTf7zkZQ-Dl"
      },
      "outputs": [],
      "source": [
        "validation_fraction = 0.9\n",
        "validation_size = int(validation_fraction *\n",
        "                      int(training_samples_count / HPARAMS.batch_size))\n",
        "print(validation_size)\n",
        "validation_dataset = train_dataset.take(validation_size)\n",
        "train_dataset = train_dataset.skip(validation_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "### 训练模型。\n",
        "\n",
        "以 mini-batch 训练模型。训练时，基于验证集监测模型的损失和准确率："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLWzgfF1xpDu"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "### 评估模型\n",
        "\n",
        "现在，我们来看看模型的表现。模型将返回两个值：损失（表示错误的数字，值越低越好）和准确率。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q7CoDfoCJ5h"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KggXVeL-llZ"
      },
      "source": [
        "### Create a graph of accuracy/loss over time\n",
        "\n",
        "`model.fit()` 会返回包含一个字典的 `History` 对象。该字典包含训练过程中产生的所有信息："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcvSXvhp-llb"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRKsqL40-lle"
      },
      "source": [
        "其中有四个条目：每个条目代表训练和验证过程中的一项监测指标。我们可以使用这些指标来绘制用于比较的训练和验证图表，以及训练和验证准确率图表："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGoYf2Js-lle"
      },
      "outputs": [],
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hXx-xOv-llh"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEmZ5zq-llk"
      },
      "source": [
        "请注意，训练损失会逐周期*下降*，而训练准确率则逐周期*上升*。使用梯度下降优化时，这是预期结果——它应该在每次迭代中最大限度减少所需的数量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SymtYWWiMUum"
      },
      "source": [
        "## 计算图正则化\n",
        "\n",
        "现在，我们已准备好尝试使用上面构建的基础模型来执行计算图正则化。我们将使用神经结构学习框架提供的 `GraphRegularization` 包装器类来包装基础 (bi-LSTM) 模型以包含计算图正则化。训练和评估计算图正则化模型的其余步骤与基础模型相似。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCIkVe_QFX38"
      },
      "source": [
        "### 创建计算图正则化模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIGN8KQH0jR"
      },
      "source": [
        "为了评估计算图正则化的增量收益，我们将创建一个新的基础模型实例。这是因为 `model` 已完成了几次训练迭代，重用这个经过训练的模型来创建计算图正则化模型对于 `model` 的比较而言，结果将有失偏颇。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOEElnbtPzSr"
      },
      "outputs": [],
      "source": [
        "# Build a new base LSTM model.\n",
        "base_reg_model = make_bilstm_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGaDeyjEOMLC"
      },
      "outputs": [],
      "source": [
        "# Wrap the base model with graph regularization.\n",
        "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
        "    max_neighbors=HPARAMS.num_neighbors,\n",
        "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
        "    distance_type=HPARAMS.distance_type,\n",
        "    sum_over_axis=-1)\n",
        "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
        "                                                graph_reg_config)\n",
        "graph_reg_model.compile(\n",
        "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSZSqJOKFdgX"
      },
      "source": [
        "### 训练模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aONZhwc9FWoo"
      },
      "outputs": [],
      "source": [
        "graph_reg_history = graph_reg_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD1oHiGHFjPB"
      },
      "source": [
        "### 评估模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdFMEfe2e5JY"
      },
      "outputs": [],
      "source": [
        "graph_reg_results = graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(graph_reg_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BshURAbF49R"
      },
      "source": [
        "### 创建准确率/损失随时间变化的图表"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHxshrYLah9v"
      },
      "outputs": [],
      "source": [
        "graph_reg_history_dict = graph_reg_history.history\n",
        "graph_reg_history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBrp0Y0jHu5k"
      },
      "source": [
        "字典中共有五个条目：训练损失、训练准确率、训练计算图损失、验证损失和验证准确率。我们可以共同绘制这些条目以便比较。请注意，计算图损失仅在训练期间计算。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhjhH4n_aprb"
      },
      "outputs": [],
      "source": [
        "acc = graph_reg_history_dict['accuracy']\n",
        "val_acc = graph_reg_history_dict['val_accuracy']\n",
        "loss = graph_reg_history_dict['loss']\n",
        "graph_loss = graph_reg_history_dict['scaled_graph_loss']\n",
        "val_loss = graph_reg_history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.clf()   # clear figure\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-gD\" is for solid green line with diamond markers.\n",
        "plt.plot(epochs, graph_loss, '-gD', label='Training graph loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE0vcDiqa1Id"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su1TOgT3mgrk"
      },
      "source": [
        "## 半监督学习的能力\n",
        "\n",
        "当训练数据量很少时，半监督学习（更具体地说，即本教程背景中的计算图正则化）将非常实用。可通过利用训练样本之间的相似度来弥补缺乏训练数据的不足，这在传统的监督学习中是无法实现的。\n",
        "\n",
        "我们将***监督比率***定义为训练样本与样本总数（包括训练样本、验证样本和测试样本）之间的比率。在此笔记本中，我们使用了 0.05 的监督比率（即带标签数据的 5％）来训练基础模型和计算图正则化模型。我们在下面的单元中展示了监督比率对模型准确率的影响。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWWa384R5vSm"
      },
      "outputs": [],
      "source": [
        "# Accuracy values for both the Bi-LSTM model and the feed forward NN model have\n",
        "# been precomputed for the following supervision ratios.\n",
        "\n",
        "supervision_ratios = [0.3, 0.15, 0.05, 0.03, 0.02, 0.01, 0.005]\n",
        "\n",
        "model_tags = ['Bi-LSTM model', 'Feed Forward NN model']\n",
        "base_model_accs = [[84, 84, 83, 80, 65, 52, 50], [87, 86, 76, 74, 67, 52, 51]]\n",
        "graph_reg_model_accs = [[84, 84, 83, 83, 65, 63, 50],\n",
        "                        [87, 86, 80, 75, 67, 52, 50]]\n",
        "\n",
        "plt.clf()  # clear figure\n",
        "\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "fig.set_size_inches((12, 5))\n",
        "\n",
        "for ax, model_tag, base_model_acc, graph_reg_model_acc in zip(\n",
        "    axes, model_tags, base_model_accs, graph_reg_model_accs):\n",
        "\n",
        "  # \"-r^\" is for solid red line with triangle markers.\n",
        "  ax.plot(base_model_acc, '-r^', label='Base model')\n",
        "  # \"-gD\" is for solid green line with diamond markers.\n",
        "  ax.plot(graph_reg_model_acc, '-gD', label='Graph-regularized model')\n",
        "  ax.set_title(model_tag)\n",
        "  ax.set_xlabel('Supervision ratio')\n",
        "  ax.set_ylabel('Accuracy(%)')\n",
        "  ax.set_ylim((25, 100))\n",
        "  ax.set_xticks(range(len(supervision_ratios)))\n",
        "  ax.set_xticklabels(supervision_ratios)\n",
        "  ax.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tukoIryKugX_"
      },
      "source": [
        "可以观察到，随着监督比率的降低，模型的准确率也会降低。这一规律对于基础模型和计算图正则化模型均是如此，无论使用哪种模型架构。但请注意，在两种架构中，计算图正则化模型的性能均优于基础模型。特别是对于 Bi-LSTM 模型，当监督比率为 0.01 时，计算图正则化模型的准确率将比基础模型高 **20% 左右**。这主要归功于计算图正则化模型的半监督学习，除训练样本本身之外，半监督学习还使用了训练样本之间的结构相似度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4zCEyPhIp-"
      },
      "source": [
        "## 结论\n",
        "\n",
        "我们演示了如何使用计算图正则化来实现利用神经结构学习 (NSL) 框架的情感分类（即使在输入不包含显式计算图时）。我们选取了 IMDB 电影评论的情感分类任务，为此，我们基于评论嵌入向量合成了相似度计算图。我们鼓励用户通过更改超参数、监督量以及使用不同的模型架构来进行进一步实验。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "24gYiJcWNlpA"
      ],
      "name": "graph_keras_lstm_imdb.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
