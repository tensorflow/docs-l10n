{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACbjNjyO4f_8"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCM50vaM4jiK"
      },
      "outputs": [],
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qOVy-_vmuUP"
      },
      "source": [
        "# 使用近似最近邻和文本嵌入向量构建语义搜索\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://tensorflow.google.cn/hub/tutorials/semantic_approximate_nearest_neighbors\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org 上查看</a>   </td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/hub/tutorials/semantic_approximate_nearest_neighbors.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行 </a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/hub/tutorials/semantic_approximate_nearest_neighbors.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 GitHub 上查看源代码</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/hub/tutorials/semantic_approximate_nearest_neighbors.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a>   </td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/universal-sentence-encoder/2\"><img src=\"https://tensorflow.google.cn/images/hub_logo_32px.png\">查看 TF Hub 模型</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hks9F5qq6m2"
      },
      "source": [
        "本教程演示了如何在给定输入数据的情况下，从 [TensorFlow Hub](https://tfhub.dev) (TF-Hub) 模块生成嵌入向量，并使用提取的嵌入向量构建近似最近邻 (ANN) 索引。之后，可以将该索引用于实时相似度匹配和检索。\n",
        "\n",
        "在处理包含大量数据的语料库时，通过扫描整个存储库实时查找与给定查询最相似的条目来执行精确匹配的效率不高。因此，我们使用一种近似相似度匹配算法。利用这种算法，我们在查找精确的最近邻匹配时会牺牲一点准确率，但是可以显著提高速度。\n",
        "\n",
        "在本教程中，我们将展示一个示例，在新闻标题语料库上进行实时文本搜索，以查找与查询最相似的标题。与关键字搜索不同，此过程会捕获在文本嵌入向量中编码的语义相似度。\n",
        "\n",
        "本教程的操作步骤如下：\n",
        "\n",
        "1. 下载样本数据\n",
        "2. 使用 TF-Hub 模块为数据生成嵌入向量\n",
        "3. 为嵌入向量构建 ANN 索引\n",
        "4. 使用索引进行相似度匹配\n",
        "\n",
        "我们将 Apache Beam 与 TensorFlow Transform (TF-Transform) 结合使用，从 TF-Hub 模块生成嵌入向量。我们还使用 Spotify 的 ANNOY 库来构建近似最近邻索引。您可以在此 Github 仓库中找到 ANN 框架的基准测试。\n",
        "\n",
        "本教程使用 TensorFlow 1.0，并且仅适用于 TF-Hub 中的 TF1 [Hub 模块](https://tensorflow.google.cn/hub/tf1_hub_module)。请参阅本教程更新后的 [TF2 版本](https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_semantic_approximate_nearest_neighbors.ipynb)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0jr0QK9qO5P"
      },
      "source": [
        "## 设置"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whMRj9qeqed4"
      },
      "source": [
        "安装所需的库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmXkLPoaqS--"
      },
      "outputs": [],
      "source": [
        "!pip install -q apache_beam\n",
        "!pip install -q 'scikit_learn~=0.23.0'  # For gaussian_random_matrix.\n",
        "!pip install -q annoy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-vBZiCCqld0"
      },
      "source": [
        "导入所需的库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NTYbdWcseuK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import apache_beam as beam\n",
        "import annoy\n",
        "from sklearn.random_projection import gaussian_random_matrix\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GF0GnLqGdPQ"
      },
      "outputs": [],
      "source": [
        "# TFT needs to be installed afterwards\n",
        "!pip install -q tensorflow_transform==0.24\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx0SZa6-7b-f"
      },
      "outputs": [],
      "source": [
        "print('TF version: {}'.format(tf.__version__))\n",
        "print('TF-Hub version: {}'.format(hub.__version__))\n",
        "print('TF-Transform version: {}'.format(tft.__version__))\n",
        "print('Apache Beam version: {}'.format(beam.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Imq876rLWx"
      },
      "source": [
        "## 1. 下载样本数据\n",
        "\n",
        "[A Million News Headlines](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SYBGZL#) 数据集包含著名的澳大利亚广播公司 (ABC) 在 15 年内发布的新闻标题。此新闻数据集汇总了从 2003 年初至 2017 年底在全球范围内发生的重大事件的历史记录，其中对澳大利亚的关注更为细致。\n",
        "\n",
        "**格式**：以制表符分隔的两列数据：1) 发布日期和 2) 标题文本。我们只对标题文本感兴趣。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpF57n8e5C9D"
      },
      "outputs": [],
      "source": [
        "!wget 'https://dataverse.harvard.edu/api/access/datafile/3450625?format=tab&gbrecs=true' -O raw.tsv\n",
        "!wc -l raw.tsv\n",
        "!head raw.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reeoc9z0zTxJ"
      },
      "source": [
        "为了简单起见，我们仅保留标题文本并移除发布日期。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INPWa4upv_yJ"
      },
      "outputs": [],
      "source": [
        "!rm -r corpus\n",
        "!mkdir corpus\n",
        "\n",
        "with open('corpus/text.txt', 'w') as out_file:\n",
        "  with open('raw.tsv', 'r') as in_file:\n",
        "    for line in in_file:\n",
        "      headline = line.split('\\t')[1].strip().strip('\"')\n",
        "      out_file.write(headline+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-oedX40z6o2"
      },
      "outputs": [],
      "source": [
        "!tail corpus/text.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls0Zh7kYz3PM"
      },
      "source": [
        "## 用于加载 TF-Hub 模块的辅助函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSt_jmyKz3Xp"
      },
      "outputs": [],
      "source": [
        "def load_module(module_url):\n",
        "  embed_module = hub.Module(module_url)\n",
        "  placeholder = tf.placeholder(dtype=tf.string)\n",
        "  embed = embed_module(placeholder)\n",
        "  session = tf.Session()\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  print('TF-Hub module is loaded.')\n",
        "\n",
        "  def _embeddings_fn(sentences):\n",
        "    computed_embeddings = session.run(\n",
        "        embed, feed_dict={placeholder: sentences})\n",
        "    return computed_embeddings\n",
        "\n",
        "  return _embeddings_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AngMtH50jNb"
      },
      "source": [
        "## 2. 为数据生成嵌入向量\n",
        "\n",
        "在本教程中，我们使用 [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/2) 为标题数据生成嵌入向量。之后，可以轻松地使用句子嵌入向量计算句子级别的含义相似度。我们使用 Apache Beam 和 TF-Transform 来运行嵌入向量生成过程。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_DvXnDB1pEX"
      },
      "source": [
        "### 嵌入向量提取方法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL7OEY1E0A35"
      },
      "outputs": [],
      "source": [
        "encoder = None\n",
        "\n",
        "def embed_text(text, module_url, random_projection_matrix):\n",
        "  # Beam will run this function in different processes that need to\n",
        "  # import hub and load embed_fn (if not previously loaded)\n",
        "  global encoder\n",
        "  if not encoder:\n",
        "    encoder = hub.Module(module_url)\n",
        "  embedding = encoder(text)\n",
        "  if random_projection_matrix is not None:\n",
        "    # Perform random projection for the embedding\n",
        "    embedding = tf.matmul(\n",
        "        embedding, tf.cast(random_projection_matrix, embedding.dtype))\n",
        "  return embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_don5gXy9D59"
      },
      "source": [
        "### 创建 TFT preprocess_fn 方法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwYlrzzK9ECE"
      },
      "outputs": [],
      "source": [
        "def make_preprocess_fn(module_url, random_projection_matrix=None):\n",
        "  '''Makes a tft preprocess_fn'''\n",
        "\n",
        "  def _preprocess_fn(input_features):\n",
        "    '''tft preprocess_fn'''\n",
        "    text = input_features['text']\n",
        "    # Generate the embedding for the input text\n",
        "    embedding = embed_text(text, module_url, random_projection_matrix)\n",
        "    \n",
        "    output_features = {\n",
        "        'text': text, \n",
        "        'embedding': embedding\n",
        "        }\n",
        "        \n",
        "    return output_features\n",
        "  \n",
        "  return _preprocess_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ492LN7A-NZ"
      },
      "source": [
        "### 创建数据集元数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2D4332VA-2V"
      },
      "outputs": [],
      "source": [
        "def create_metadata():\n",
        "  '''Creates metadata for the raw data'''\n",
        "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "  from tensorflow_transform.tf_metadata import schema_utils\n",
        "  feature_spec = {'text': tf.FixedLenFeature([], dtype=tf.string)}\n",
        "  schema = schema_utils.schema_from_feature_spec(feature_spec)\n",
        "  metadata = dataset_metadata.DatasetMetadata(schema)\n",
        "  return metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zlSLPzRBm6H"
      },
      "source": [
        "### Beam 流水线"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCGUIB172m2G"
      },
      "outputs": [],
      "source": [
        "def run_hub2emb(args):\n",
        "  '''Runs the embedding generation pipeline'''\n",
        "\n",
        "  options = beam.options.pipeline_options.PipelineOptions(**args)\n",
        "  args = namedtuple(\"options\", args.keys())(*args.values())\n",
        "\n",
        "  raw_metadata = create_metadata()\n",
        "  converter = tft.coders.CsvCoder(\n",
        "      column_names=['text'], schema=raw_metadata.schema)\n",
        "\n",
        "  with beam.Pipeline(args.runner, options=options) as pipeline:\n",
        "    with tft_beam.Context(args.temporary_dir):\n",
        "      # Read the sentences from the input file\n",
        "      sentences = ( \n",
        "          pipeline\n",
        "          | 'Read sentences from files' >> beam.io.ReadFromText(\n",
        "              file_pattern=args.data_dir)\n",
        "          | 'Convert to dictionary' >> beam.Map(converter.decode)\n",
        "      )\n",
        "\n",
        "      sentences_dataset = (sentences, raw_metadata)\n",
        "      preprocess_fn = make_preprocess_fn(args.module_url, args.random_projection_matrix)\n",
        "      # Generate the embeddings for the sentence using the TF-Hub module\n",
        "      embeddings_dataset, _ = (\n",
        "          sentences_dataset\n",
        "          | 'Extract embeddings' >> tft_beam.AnalyzeAndTransformDataset(preprocess_fn)\n",
        "      )\n",
        "\n",
        "      embeddings, transformed_metadata = embeddings_dataset\n",
        "      # Write the embeddings to TFRecords files\n",
        "      embeddings | 'Write embeddings to TFRecords' >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "          file_path_prefix='{}/emb'.format(args.output_dir),\n",
        "          file_name_suffix='.tfrecords',\n",
        "          coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHbq4t2gCDAG"
      },
      "source": [
        "### 生成随机投影权重矩阵\n",
        "\n",
        "[随机投影](https://en.wikipedia.org/wiki/Random_projection)是一种简单而强大的技术，用于减少位于欧几里得空间中的一组点的维数。有关理论背景，请参阅[约翰逊-林登斯特劳斯引理](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma)。\n",
        "\n",
        "利用随机投影降低嵌入向量的维数，这样，构建和查询 ANN 索引需要的时间将减少。\n",
        "\n",
        "在本教程中，我们使用 [Scikit-learn](https://scikit-learn.org/stable/modules/random_projection.html#gaussian-random-projection) 库中的[高斯随机投影](https://en.wikipedia.org/wiki/Random_projection#Gaussian_random_projection)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1aYPeOUCDIP"
      },
      "outputs": [],
      "source": [
        "def generate_random_projection_weights(original_dim, projected_dim):\n",
        "  random_projection_matrix = None\n",
        "  if projected_dim and original_dim > projected_dim:\n",
        "    random_projection_matrix = gaussian_random_matrix(\n",
        "        n_components=projected_dim, n_features=original_dim).T\n",
        "    print(\"A Gaussian random weight matrix was creates with shape of {}\".format(random_projection_matrix.shape))\n",
        "    print('Storing random projection matrix to disk...')\n",
        "    with open('random_projection_matrix', 'wb') as handle:\n",
        "      pickle.dump(random_projection_matrix, \n",
        "                  handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "  return random_projection_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHxZX2Z3Nk64"
      },
      "source": [
        "### 设置参数\n",
        "\n",
        "如果要使用原始嵌入向量空间构建索引而不进行随机投影，请将 `projected_dim` 参数设置为 `None`。请注意，这会减慢高维嵌入向量的索引编制步骤。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "feMVXFL0NlIM"
      },
      "outputs": [],
      "source": [
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/2' #@param {type:\"string\"}\n",
        "projected_dim = 64  #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On-MbzD922kb"
      },
      "source": [
        "### 运行流水线"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3I1Wv4i21yY"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "output_dir = pathlib.Path(tempfile.mkdtemp())\n",
        "temporary_dir = pathlib.Path(tempfile.mkdtemp())\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  original_dim = load_module(module_url)(['']).shape[1]\n",
        "  random_projection_matrix = None\n",
        "\n",
        "  if projected_dim:\n",
        "    random_projection_matrix = generate_random_projection_weights(\n",
        "        original_dim, projected_dim)\n",
        "\n",
        "args = {\n",
        "    'job_name': 'hub2emb-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S')),\n",
        "    'runner': 'DirectRunner',\n",
        "    'batch_size': 1024,\n",
        "    'data_dir': 'corpus/*.txt',\n",
        "    'output_dir': output_dir,\n",
        "    'temporary_dir': temporary_dir,\n",
        "    'module_url': module_url,\n",
        "    'random_projection_matrix': random_projection_matrix,\n",
        "}\n",
        "\n",
        "print(\"Pipeline args are set.\")\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS9obmeP4ZOA"
      },
      "outputs": [],
      "source": [
        "!rm -r {output_dir}\n",
        "!rm -r {temporary_dir}\n",
        "\n",
        "print(\"Running pipeline...\")\n",
        "%time run_hub2emb(args)\n",
        "print(\"Pipeline is done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAwOo7gQWvVd"
      },
      "outputs": [],
      "source": [
        "!ls {output_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVnee4e6U90u"
      },
      "source": [
        "读取生成的部分嵌入向量…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K7pGXlXOj1N"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "embed_file = os.path.join(output_dir, 'emb-00000-of-00001.tfrecords')\n",
        "sample = 5\n",
        "record_iterator =  tf.io.tf_record_iterator(path=embed_file)\n",
        "for string_record in itertools.islice(record_iterator, sample):\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(string_record)\n",
        "  text = example.features.feature['text'].bytes_list.value\n",
        "  embedding = np.array(example.features.feature['embedding'].float_list.value)\n",
        "  print(\"Embedding dimensions: {}\".format(embedding.shape[0]))\n",
        "  print(\"{}: {}\".format(text, embedding[:10]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agGoaMSgY8wN"
      },
      "source": [
        "## 3. 为嵌入向量构建 ANN 索引\n",
        "\n",
        "[ANNOY](https://github.com/spotify/annoy) (Approximate Nearest Neighbors Oh Yeah) 是一个带有 Python 绑定的 C++ 库，用于搜索空间中接近给定查询点的点。它还会创建基于文件的大型只读数据结构，并将其映射到内存中。它由 [Spotify](https://www.spotify.com) 构建并用于推荐音乐。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcPDspU3WjgH"
      },
      "outputs": [],
      "source": [
        "def build_index(embedding_files_pattern, index_filename, vector_length, \n",
        "    metric='angular', num_trees=100):\n",
        "  '''Builds an ANNOY index'''\n",
        "\n",
        "  annoy_index = annoy.AnnoyIndex(vector_length, metric=metric)\n",
        "  # Mapping between the item and its identifier in the index\n",
        "  mapping = {}\n",
        "\n",
        "  embed_files = tf.gfile.Glob(embedding_files_pattern)\n",
        "  print('Found {} embedding file(s).'.format(len(embed_files)))\n",
        "\n",
        "  item_counter = 0\n",
        "  for f, embed_file in enumerate(embed_files):\n",
        "    print('Loading embeddings in file {} of {}...'.format(\n",
        "      f+1, len(embed_files)))\n",
        "    record_iterator = tf.io.tf_record_iterator(\n",
        "      path=embed_file)\n",
        "\n",
        "    for string_record in record_iterator:\n",
        "      example = tf.train.Example()\n",
        "      example.ParseFromString(string_record)\n",
        "      text = example.features.feature['text'].bytes_list.value[0].decode(\"utf-8\")\n",
        "      mapping[item_counter] = text\n",
        "      embedding = np.array(\n",
        "        example.features.feature['embedding'].float_list.value)\n",
        "      annoy_index.add_item(item_counter, embedding)\n",
        "      item_counter += 1\n",
        "      if item_counter % 100000 == 0:\n",
        "        print('{} items loaded to the index'.format(item_counter))\n",
        "\n",
        "  print('A total of {} items added to the index'.format(item_counter))\n",
        "\n",
        "  print('Building the index with {} trees...'.format(num_trees))\n",
        "  annoy_index.build(n_trees=num_trees)\n",
        "  print('Index is successfully built.')\n",
        "  \n",
        "  print('Saving index to disk...')\n",
        "  annoy_index.save(index_filename)\n",
        "  print('Index is saved to disk.')\n",
        "  print(\"Index file size: {} GB\".format(\n",
        "    round(os.path.getsize(index_filename) / float(1024 ** 3), 2)))\n",
        "  annoy_index.unload()\n",
        "\n",
        "  print('Saving mapping to disk...')\n",
        "  with open(index_filename + '.mapping', 'wb') as handle:\n",
        "    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  print('Mapping is saved to disk.')\n",
        "  print(\"Mapping file size: {} MB\".format(\n",
        "    round(os.path.getsize(index_filename + '.mapping') / float(1024 ** 2), 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgyOQhUq6FNE"
      },
      "outputs": [],
      "source": [
        "embedding_files = \"{}/emb-*.tfrecords\".format(output_dir)\n",
        "embedding_dimension = projected_dim\n",
        "index_filename = \"index\"\n",
        "\n",
        "!rm {index_filename}\n",
        "!rm {index_filename}.mapping\n",
        "\n",
        "%time build_index(embedding_files, index_filename, embedding_dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic31Tm5cgAd5"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maGxDl8ufP-p"
      },
      "source": [
        "## 4. 使用索引进行相似度匹配\n",
        "\n",
        "现在，我们可以使用 ANN 索引查找与输入查询语义接近的新闻标题。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dIs8W78fYPp"
      },
      "source": [
        "### 加载索引和映射文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlTTrbQHayvb"
      },
      "outputs": [],
      "source": [
        "index = annoy.AnnoyIndex(embedding_dimension)\n",
        "index.load(index_filename, prefault=True)\n",
        "print('Annoy index is loaded.')\n",
        "with open(index_filename + '.mapping', 'rb') as handle:\n",
        "  mapping = pickle.load(handle)\n",
        "print('Mapping file is loaded.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6liFMSUh08J"
      },
      "source": [
        "### 相似度匹配方法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUxjTag8hc16"
      },
      "outputs": [],
      "source": [
        "def find_similar_items(embedding, num_matches=5):\n",
        "  '''Finds similar items to a given embedding in the ANN index'''\n",
        "  ids = index.get_nns_by_vector(\n",
        "  embedding, num_matches, search_k=-1, include_distances=False)\n",
        "  items = [mapping[i] for i in ids]\n",
        "  return items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjerNpmZja0A"
      },
      "source": [
        "### 从给定查询中提取嵌入向量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0IIXzfBjZ19"
      },
      "outputs": [],
      "source": [
        "# Load the TF-Hub module\n",
        "print(\"Loading the TF-Hub module...\")\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  embed_fn = load_module(module_url)\n",
        "print(\"TF-Hub module is loaded.\")\n",
        "\n",
        "random_projection_matrix = None\n",
        "if os.path.exists('random_projection_matrix'):\n",
        "  print(\"Loading random projection matrix...\")\n",
        "  with open('random_projection_matrix', 'rb') as handle:\n",
        "    random_projection_matrix = pickle.load(handle)\n",
        "  print('random projection matrix is loaded.')\n",
        "\n",
        "def extract_embeddings(query):\n",
        "  '''Generates the embedding for the query'''\n",
        "  query_embedding =  embed_fn([query])[0]\n",
        "  if random_projection_matrix is not None:\n",
        "    query_embedding = query_embedding.dot(random_projection_matrix)\n",
        "  return query_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCoCNROujEIO"
      },
      "outputs": [],
      "source": [
        "extract_embeddings(\"Hello Machine Learning!\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE_Q60nCk_ZB"
      },
      "source": [
        "### 输入查询以查找最相似的条目"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wC0uLjvfk5nB"
      },
      "outputs": [],
      "source": [
        "#@title { run: \"auto\" }\n",
        "query = \"confronting global challenges\" #@param {type:\"string\"}\n",
        "print(\"Generating embedding for the query...\")\n",
        "%time query_embedding = extract_embeddings(query)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Finding relevant items in the index...\")\n",
        "%time items = find_similar_items(query_embedding, 10)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Results:\")\n",
        "print(\"=========\")\n",
        "for item in items:\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwtMtyOeDKwt"
      },
      "source": [
        "## 了解更多信息\n",
        "\n",
        "您可以在 [tensorflow.org](https://tensorflow.google.cn/) 上详细了解 TensorFlow，并在 [tensorflow.org/hub](https://tensorflow.google.cn/hub/) 上查看 TF-Hub API 文档。此外，还可以在 [tfhub.dev](https://tfhub.dev/) 上找到可用的 TensorFlow Hub 模块，包括更多的文本嵌入向量模块和图像特征向量模块。\n",
        "\n",
        "另外，请查看[机器学习速成课程](https://developers.google.com/machine-learning/crash-course/)，这是 Google 对机器学习的快节奏实用介绍。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ls0Zh7kYz3PM",
        "_don5gXy9D59",
        "SQ492LN7A-NZ"
      ],
      "name": "semantic_approximate_nearest_neighbors.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
