{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJhWonqQN7u0"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MegtYH2UN8tT"
      },
      "outputs": [],
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlHqSdgSEwPE"
      },
      "source": [
        "# Universal Sentence Encoder-Lite 演示\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://tensorflow.google.cn/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">View 在 TensorFlow.org 上查看</a>   </td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行 </a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 GitHub 中查看源代码</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\"><img src=\"https://tensorflow.google.cn/images/hub_logo_32px.png\">查看 TF Hub 模型</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0HuiScHQ3OK"
      },
      "source": [
        "此 Colab 演示如何将 Universal Sentence Encoder-Lite 用于句子相似度任务。本模块与 [Universal Sentence Encoder ](https://tensorflow.google.cn/hub/modules/google/universal-sentence-encoder/2) 非常相似，唯一的区别是您需要对输入的句子运行 [SentencePiece](https://github.com/google/sentencepiece) 处理。\n",
        "\n",
        "Universal Sentence Encoder 使获取句子级别的嵌入向量变得与以往查找单个单词的嵌入向量一样容易。之后，您可以轻松地使用句子嵌入向量计算句子级别的语义相似度，以及使用较少监督的训练数据在下游分类任务中实现更好的性能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqCB2pyK-WSU"
      },
      "source": [
        "# 开始"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWeEjoO5M0Cx"
      },
      "source": [
        "## 设置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5_potQBMzcU"
      },
      "outputs": [],
      "source": [
        "# Install seaborn for pretty visualizations\n",
        "!pip3 install --quiet seaborn\n",
        "# Install SentencePiece package\n",
        "# SentencePiece package is needed for Universal Sentence Encoder Lite. We'll\n",
        "# use it for all the text processing and sentence feature ID lookup.\n",
        "!pip3 install --quiet sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMTa6V4a-cmf"
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import sentencepiece as spm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPXYQDBiFJHd"
      },
      "source": [
        "## 从 TF-Hub 加载模块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEWUT-lmAkxM"
      },
      "outputs": [],
      "source": [
        "module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5277Z-9qARYF"
      },
      "outputs": [],
      "source": [
        "input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
        "encodings = module(\n",
        "    inputs=dict(\n",
        "        values=input_placeholder.values,\n",
        "        indices=input_placeholder.indices,\n",
        "        dense_shape=input_placeholder.dense_shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yydbhuba_nek"
      },
      "source": [
        "## 从 TF-Hub 模块加载 SentencePiece 模型\n",
        "\n",
        "SentencePiece 模型方便地存储在模块的资源中。您必须加载它才能初始化处理器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CyUjKzE_tcJ"
      },
      "outputs": [],
      "source": [
        "with tf.Session() as sess:\n",
        "  spm_path = sess.run(module(signature=\"spm_path\"))\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "with tf.io.gfile.GFile(spm_path, mode=\"rb\") as f:\n",
        "  sp.LoadFromSerializedProto(f.read())\n",
        "print(\"SentencePiece model loaded at {}.\".format(spm_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y5kkN-l-5QV"
      },
      "outputs": [],
      "source": [
        "def process_to_IDs_in_sparse_format(sp, sentences):\n",
        "  # An utility method that processes sentences with the sentence piece processor\n",
        "  # 'sp' and returns the results in tf.SparseTensor-similar format:\n",
        "  # (values, indices, dense_shape)\n",
        "  ids = [sp.EncodeAsIds(x) for x in sentences]\n",
        "  max_len = max(len(x) for x in ids)\n",
        "  dense_shape=(len(ids), max_len)\n",
        "  values=[item for sublist in ids for item in sublist]\n",
        "  indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
        "  return (values, indices, dense_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpHEWrPAdxR"
      },
      "source": [
        "### 使用一些样本测试模块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSkjuGYoCBfU"
      },
      "outputs": [],
      "source": [
        "# Compute a representation for each message, showing various lengths supported.\n",
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the embedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)\n",
        "\n",
        "# Reduce logging output.\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  message_embeddings = session.run(\n",
        "      encodings,\n",
        "      feed_dict={input_placeholder.values: values,\n",
        "                input_placeholder.indices: indices,\n",
        "                input_placeholder.dense_shape: dense_shape})\n",
        "\n",
        "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "    print(\"Message: {}\".format(messages[i]))\n",
        "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "    message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46jrIgHyFDz9"
      },
      "source": [
        "# 语义文本相似度 (STS) 任务示例\n",
        "\n",
        "Universal Sentence Encoder 生成的嵌入向量会被近似归一化。两个句子的语义相似度可以作为编码的内积轻松进行计算。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIQudHgWBGSk"
      },
      "outputs": [],
      "source": [
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "\n",
        "def run_and_plot(session, input_placeholder, messages):\n",
        "  values, indices, dense_shape = process_to_IDs_in_sparse_format(sp,messages)\n",
        "\n",
        "  message_embeddings = session.run(\n",
        "      encodings,\n",
        "      feed_dict={input_placeholder.values: values,\n",
        "                input_placeholder.indices: indices,\n",
        "                input_placeholder.dense_shape: dense_shape})\n",
        "  \n",
        "  plot_similarity(messages, message_embeddings, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlDqttNcE0Bx"
      },
      "source": [
        "## 可视化相似度\n",
        "\n",
        "下面，我们在热图中显示相似度。最终的图形是一个 9x9 矩阵，其中每个条目 `[i, j]` 都根据句子 `i` 和 `j` 的编码的内积进行着色。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GSCW5QIBKVe"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    # Smartphones\n",
        "    \"I like my phone\",\n",
        "    \"My phone is not good.\",\n",
        "    \"Your cellphone looks great.\",\n",
        "\n",
        "    # Weather\n",
        "    \"Will it snow tomorrow?\",\n",
        "    \"Recently a lot of hurricanes have hit the US\",\n",
        "    \"Global warming is real\",\n",
        "\n",
        "    # Food and health\n",
        "    \"An apple a day, keeps the doctors away\",\n",
        "    \"Eating strawberries is healthy\",\n",
        "    \"Is paleo better than keto?\",\n",
        "\n",
        "    # Asking about age\n",
        "    \"How old are you?\",\n",
        "    \"what is your age?\",\n",
        "]\n",
        "\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  run_and_plot(session, input_placeholder, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkZ4sRBYBnL8"
      },
      "source": [
        "## 评估：STS（语义文本相似度）基准\n",
        "\n",
        "[**STS 基准**](https://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark)会根据从句子嵌入向量计算得出的相似度得分与人为判断的一致程度，提供内部评估。该基准要求系统为不同的句子对选择返回相似度得分。然后使用[皮尔逊相关](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)来评估机器相似度得分相对于人为判断的质量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNMVfSelBsHW"
      },
      "source": [
        "### 下载数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zAWVzBMBptq"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import scipy\n",
        "import math\n",
        "\n",
        "\n",
        "def load_sts_dataset(filename):\n",
        "  # Loads a subset of the STS dataset into a DataFrame. In particular both\n",
        "  # sentences and their human rated similarity score.\n",
        "  sent_pairs = []\n",
        "  with tf.gfile.GFile(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      ts = line.strip().split(\"\\t\")\n",
        "      # (sent_1, sent_2, similarity_score)\n",
        "      sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
        "  return pandas.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
        "\n",
        "\n",
        "def download_and_load_sts_data():\n",
        "  sts_dataset = tf.keras.utils.get_file(\n",
        "      fname=\"Stsbenchmark.tar.gz\",\n",
        "      origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
        "      extract=True)\n",
        "\n",
        "  sts_dev = load_sts_dataset(\n",
        "      os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"))\n",
        "  sts_test = load_sts_dataset(\n",
        "      os.path.join(\n",
        "          os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"))\n",
        "\n",
        "  return sts_dev, sts_test\n",
        "\n",
        "\n",
        "sts_dev, sts_test = download_and_load_sts_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8lEawD6B4Fr"
      },
      "source": [
        "### 构建评估计算图"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etiZUkP-B6bR"
      },
      "outputs": [],
      "source": [
        "sts_input1 = tf.sparse_placeholder(tf.int64, shape=(None, None))\n",
        "sts_input2 = tf.sparse_placeholder(tf.int64, shape=(None, None))\n",
        "\n",
        "# For evaluation we use exactly normalized rather than\n",
        "# approximately normalized.\n",
        "sts_encode1 = tf.nn.l2_normalize(\n",
        "    module(\n",
        "        inputs=dict(values=sts_input1.values,\n",
        "                    indices=sts_input1.indices,\n",
        "                    dense_shape=sts_input1.dense_shape)),\n",
        "    axis=1)\n",
        "sts_encode2 = tf.nn.l2_normalize(\n",
        "    module(\n",
        "        inputs=dict(values=sts_input2.values,\n",
        "                    indices=sts_input2.indices,\n",
        "                    dense_shape=sts_input2.dense_shape)),\n",
        "    axis=1)\n",
        "\n",
        "sim_scores = -tf.acos(tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Q34ssLB-rw"
      },
      "source": [
        "### 评估句子嵌入向量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-vRFEFPJPyeF"
      },
      "outputs": [],
      "source": [
        "#@title Choose dataset for benchmark\n",
        "dataset = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n",
        "\n",
        "values1, indices1, dense_shape1 = process_to_IDs_in_sparse_format(sp, dataset['sent_1'].tolist())\n",
        "values2, indices2, dense_shape2 = process_to_IDs_in_sparse_format(sp, dataset['sent_2'].tolist())\n",
        "similarity_scores = dataset['sim'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QJ2DI85CBDh"
      },
      "outputs": [],
      "source": [
        "def run_sts_benchmark(session):\n",
        "  \"\"\"Returns the similarity scores\"\"\"\n",
        "  scores = session.run(\n",
        "      sim_scores,\n",
        "      feed_dict={\n",
        "          sts_input1.values: values1,\n",
        "          sts_input1.indices:  indices1,\n",
        "          sts_input1.dense_shape:  dense_shape1,\n",
        "          sts_input2.values:  values2,\n",
        "          sts_input2.indices:  indices2,\n",
        "          sts_input2.dense_shape:  dense_shape2,\n",
        "      })\n",
        "  return scores\n",
        "\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  scores = run_sts_benchmark(session)\n",
        "\n",
        "pearson_correlation = scipy.stats.pearsonr(scores, similarity_scores)\n",
        "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
        "    pearson_correlation[0], pearson_correlation[1]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IJhWonqQN7u0"
      ],
      "name": "semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
