{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rmpybwysXGV"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m8y3rGtQsYP2"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrXv0rU9sIma"
      },
      "source": [
        "# 基本训练循环"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S0BwJ_8sLu7"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://tensorflow.google.cn/guide/basic_training_loops\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org 上查看</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/guide/basic_training_loops.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/guide/basic_training_loops.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 GitHub 上查看源代码</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/guide/basic_training_loops.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2o3TTG4TFpt"
      },
      "source": [
        "在前面的教程里，您已经了解了[tensors](./tensor.ipynb), [variables](./variable.ipynb), [gradient tape](autodiff.ipynb), 和[modules](./intro_to_modules.ipynb)。在这篇教程，您将把它们放在一起训练模型。\n",
        "\n",
        "此外，TensorFlow 还包括 [tf.Keras API](https://tensorflow.google.cn/guide/keras/overview)，这是一种高级神经网络 API，可提供有用的抽象来减少样板。但是，在本指南中，您将使用基本类。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LXMVuV0VhDr"
      },
      "source": [
        "## 创建"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiolgWMPgpwI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKD__8kFCKNt"
      },
      "source": [
        "## 解决机器学习问题\n",
        "\n",
        "解决一个机器学习问题通常包含以下步骤：\n",
        "\n",
        "- 获得训练数据。\n",
        "- 定义模型。\n",
        "- 定义损失函数。\n",
        "- 遍历训练数据，从目标值计算损失。\n",
        "- 计算该损失的梯度，并使用*optimizer*调整变量以适合数据。\n",
        "- 计算结果。\n",
        "\n",
        "为了便于说明，在本指南中，您将开发一个简单的线性模型, $f(x) = x * W + b$, 其中包含两个变量: $W$ (权重) 和 $b$ (偏差)。\n",
        "\n",
        "这是最基本的机器学习问题：给定 $x$ 和 $y$，尝试通过[简单的线性回归](https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression)来找到直线的斜率和偏移量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qutT_fkl_CBc"
      },
      "source": [
        "## 数据\n",
        "\n",
        "监督学习使用*输入*（通常表示为 *x*）和*输出*（表示为 *y*，通常称为*标签*）。目标是从成对的输入和输出中学习，以便您可以根据输入预测输出的值。\n",
        "\n",
        "TensorFlow中几乎每个输入数据都是由张量表示，并且通常是向量。监督学习中，输出(即想到预测值)同样是个张量。\n",
        "\n",
        "下面是通过将高斯（正态）噪声添加到直线上的点而合成的一些数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzivK2ATByOz"
      },
      "outputs": [],
      "source": [
        "# The actual line\n",
        "TRUE_W = 3.0\n",
        "TRUE_B = 2.0\n",
        "\n",
        "NUM_EXAMPLES = 201\n",
        "\n",
        "# A vector of random x values\n",
        "x = tf.linspace(-2,2, NUM_EXAMPLES)\n",
        "x = tf.cast(x, tf.float32)\n",
        "\n",
        "def f(x):\n",
        "  return x * TRUE_W + TRUE_B\n",
        "\n",
        "# Generate some noise\n",
        "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "\n",
        "# Calculate y\n",
        "y = f(x) + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlFd_HVBFGIF"
      },
      "outputs": [],
      "source": [
        "# Plot all the data\n",
        "plt.plot(x, y, '.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH95XUzhL99d"
      },
      "source": [
        "张量通常以 *batches* 的形式聚集在一起，或者是成组的输入和输出堆叠在一起。批处理能够对训练过程带来一些好处，并且可以与加速器和矢量化计算很好地配合使用。给定此数据集的大小，您可以将整个数据集视为一个批次。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFzH64Jn9PIm"
      },
      "source": [
        "## 定义模型\n",
        "\n",
        "使用 `tf.Variable` 代表模型中的所有权重。`tf.Variable` 能够存储值，并根据需要以张量形式提供它。详情请见 [variable guide](./variable.ipynb)。\n",
        "\n",
        "使用 `tf.Module` 封装变量和计算。您可以使用任何Python对象，但是通过这种方式可以轻松保存它。\n",
        "\n",
        "这里，您可以定义 *w* 和 *b* 为变量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WRu7Pze7wk8"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # Initialize the weights to `5.0` and the bias to `0.0`\n",
        "    # In practice, these should be randomly initialized\n",
        "    self.w = tf.Variable(5.0)\n",
        "    self.b = tf.Variable(0.0)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.w * x + self.b\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "# List the variables tf.modules's built-in variable aggregation.\n",
        "print(\"Variables:\", model.variables)\n",
        "\n",
        "# Verify the model works\n",
        "assert model(3.0).numpy() == 15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdpN_3ssG9D5"
      },
      "source": [
        "初始变量在此处以固定方式设置，但 Keras 提供了您可以与或不与 Keras 其他部分一起使用的许多[初始值设定项](https://tensorflow.google.cn/api_docs/python/tf/keras/initializers)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa6j_yXa-j79"
      },
      "source": [
        "### 定义损失函数\n",
        "\n",
        "损失函数衡量给定输入的模型输出与目标输出的匹配程度。目的是在训练过程中尽量减少这种差异。定义标准的L2损失，也称为“均方误差”:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0ysUFGY924U"
      },
      "outputs": [],
      "source": [
        "# This computes a single loss value for an entire batch\n",
        "def loss(target_y, predicted_y):\n",
        "  return tf.reduce_mean(tf.square(target_y - predicted_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-50nq-wPBsAW"
      },
      "source": [
        "在训练模型之前，您可以可视化损失值。使用红色绘制模型的预测值，使用蓝色绘制训练数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eb83LtrB4nt"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, '.', label=\"Data\")\n",
        "plt.plot(x, f(x), label=\"Ground truth\")\n",
        "plt.plot(x, model(x), label=\"Predictions\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Current loss: %1.6f\" % loss(y, model(x)).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSDP-yeq_4jE"
      },
      "source": [
        "### 定义训练循环\n",
        "\n",
        "训练循环按顺序重复执行以下任务：\n",
        "\n",
        "- 发送一批输入值，通过模型生成输出值\n",
        "- 通过比较输出值与输出（标签），来计算损失值\n",
        "- 使用梯度带(GradientTape)找到梯度值\n",
        "- 使用这些梯度优化变量\n",
        "\n",
        "这个例子中，您可以使用 [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)训练数据。\n",
        "\n",
        "`tf.keras.optimizers`中有许多梯度下降的变量。但是本着搭建的第一原则，您将在这里 借助`tf.GradientTape`的自动微分和`tf.assign_sub`的递减值（结合了`tf.assign`和`tf.sub`）自己实现基本数学："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBIACgdnA55X"
      },
      "outputs": [],
      "source": [
        "# Given a callable model, inputs, outputs, and a learning rate...\n",
        "def train(model, x, y, learning_rate):\n",
        "\n",
        "  with tf.GradientTape() as t:\n",
        "    # Trainable variables are automatically tracked by GradientTape\n",
        "    current_loss = loss(y, model(x))\n",
        "\n",
        "  # Use GradientTape to calculate the gradients with respect to W and b\n",
        "  dw, db = t.gradient(current_loss, [model.w, model.b])\n",
        "\n",
        "  # Subtract the gradient scaled by the learning rate\n",
        "  model.w.assign_sub(learning_rate * dw)\n",
        "  model.b.assign_sub(learning_rate * db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwWPaJryD2aN"
      },
      "source": [
        "要查看训练，您可以通过训练循环发送同一批次的 *x* 和 *y*，并观察 `W` 和 `b` 如何变化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdfkR223D9dW"
      },
      "outputs": [],
      "source": [
        "model = MyModel()\n",
        "\n",
        "# Collect the history of W-values and b-values to plot later\n",
        "weights = []\n",
        "biases = []\n",
        "epochs = range(10)\n",
        "\n",
        "# Define a training loop\n",
        "def report(model, loss):\n",
        "  return f\"W = {model.w.numpy():1.2f}, b = {model.b.numpy():1.2f}, loss={loss:2.5f}\"\n",
        "\n",
        "\n",
        "def training_loop(model, x, y):\n",
        "\n",
        "  for epoch in epochs:\n",
        "    # Update the model with the single giant batch\n",
        "    train(model, x, y, learning_rate=0.1)\n",
        "\n",
        "    # Track this before I update\n",
        "    weights.append(model.w.numpy())\n",
        "    biases.append(model.b.numpy())\n",
        "    current_loss = loss(y, model(x))\n",
        "\n",
        "    print(f\"Epoch {epoch:2d}:\")\n",
        "    print(\"    \", report(model, current_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dKKLU4KkQEq"
      },
      "source": [
        "进行训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRuNUghs1lHY"
      },
      "outputs": [],
      "source": [
        "current_loss = loss(y, model(x))\n",
        "\n",
        "print(f\"Starting:\")\n",
        "print(\"    \", report(model, current_loss))\n",
        "\n",
        "training_loop(model, x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPJgimg8kSA4"
      },
      "source": [
        "下面是权重随时间的演变："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND1fQw8sbTNr"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, weights, label='Weights', color=colors[0])\n",
        "plt.plot(epochs, [TRUE_W] * len(epochs), '--',\n",
        "         label = \"True weight\", color=colors[0])\n",
        "\n",
        "plt.plot(epochs, biases, label='bias', color=colors[1])\n",
        "plt.plot(epochs, [TRUE_B] * len(epochs), \"--\",\n",
        "         label=\"True bias\", color=colors[1])\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhlwj1ojkcUP"
      },
      "source": [
        "呈现训练的模型的性能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpTEjWWex568"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, '.', label=\"Data\")\n",
        "plt.plot(x, f(x), label=\"Ground truth\")\n",
        "plt.plot(x, model(x), label=\"Predictions\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Current loss: %1.6f\" % loss(model(x), y).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DODMMmfLIiOC"
      },
      "source": [
        "## 使用Keras完成相同的解决方案\n",
        "\n",
        "将上面的代码与Keras中的等效代码进行对比很有用。\n",
        "\n",
        "如果您将`tf.keras.Model`子类化，则定义模型与其看起来完全相同。请记住，Keras模型最终从模块继承。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z86hCI0x1YX3"
      },
      "outputs": [],
      "source": [
        "class MyModelKeras(tf.keras.Model):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # Initialize the weights to `5.0` and the bias to `0.0`\n",
        "    # In practice, these should be randomly initialized\n",
        "    self.w = tf.Variable(5.0)\n",
        "    self.b = tf.Variable(0.0)\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.w * x + self.b\n",
        "\n",
        "keras_model = MyModelKeras()\n",
        "\n",
        "# Reuse the training loop with a Keras model\n",
        "training_loop(keras_model, x, y)\n",
        "\n",
        "# You can also save a checkpoint using Keras's built-in support\n",
        "keras_model.save_weights(\"my_checkpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kw5P4jt2Az8"
      },
      "source": [
        "您可以使用Keras的内置功能作为捷径，而不必在每次创建模型时都编写新的训练循环。当您不想编写或调试Python训练循环时，这很有用。\n",
        "\n",
        "如果您使用Keras，您将会需要使用 `model.compile()` 去设置参数, 使用`model.fit()` 进行训练。借助Keras实现L2损失和梯度下降需要的代码量更少，就像一个捷径。Keras损失和优化器也可以在这些便利功能之外使用，而前面的示例也可以使用它们。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nbLLfPE2pEl"
      },
      "outputs": [],
      "source": [
        "keras_model = MyModelKeras()\n",
        "\n",
        "# compile sets the training parameters\n",
        "keras_model.compile(\n",
        "    # By default, fit() uses tf.function().  You can\n",
        "    # turn that off for debugging, but it is on now.\n",
        "    run_eagerly=False,\n",
        "\n",
        "    # Using a built-in optimizer, configuring as an object\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "\n",
        "    # Keras comes with built-in MSE error\n",
        "    # However, you could use the loss function\n",
        "    # defined above\n",
        "    loss=tf.keras.losses.mean_squared_error,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrlHODiZccu2"
      },
      "source": [
        "Keras`fit`期望批处理数据或完整的数据集作为NumPy数组。 NumPy数组分为多个批次，默认批次大小为32。\n",
        "\n",
        "这一案例中，为了匹配手写训练循环，您应该以大小为1000的单批次传递x。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfAYqtu136PO"
      },
      "outputs": [],
      "source": [
        "print(x.shape[0])\n",
        "keras_model.fit(x, y, epochs=10, batch_size=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zKZIO9P5s1G"
      },
      "source": [
        "请注意，Keras会在训练后而不是之前打印出损失，因此第一次损失会显得较低。否则，这表明本质上相同的训练效果。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPnIVuaSJwWz"
      },
      "source": [
        "## 下一步\n",
        "\n",
        "在这篇引导中，您已经看到怎样使用tensors, variables, modules, 和gradient tape去构建并训练模型，也进一步了解到这些是如何与Keras对应的。\n",
        "\n",
        "但是，这是一个极其简单的问题。有关更实用的介绍，请参阅[自定义训练演示](../tutorials/customization/custom_training_walkthrough.ipynb)。\n",
        "\n",
        "有关使用内置 Keras 训练循环的更多信息，请参阅[本指南](https://tensorflow.google.cn/guide/keras/train_and_evaluate)。有关训练循环和 Keras 的更多信息，请参阅[本指南](https://tensorflow.google.cn/guide/keras/writing_a_training_loop_from_scratch)。有关编写自定义分布式训练循环的信息，请参阅[本指南](distributed_training.ipynb#using_tfdistributestrategy_with_basic_training_loops_loops)。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5rmpybwysXGV",
        "iKD__8kFCKNt"
      ],
      "name": "basic_training_loops.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
