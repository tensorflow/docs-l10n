{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhGuhbZ6M5tl"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AwOEIRJC6Une"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# 使用 Core API 进行二元分类的逻辑回归"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBIlTPscrIT9"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://tensorflow.google.cn/guide/core/logistic_regression_core\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org 上查看</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/guide/core/logistic_regression_core.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/guide/core/logistic_regression_core.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">在 GitHub 上查看源代码</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/guide/core/logistic_regression_core.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauaqJ7WhIhO"
      },
      "source": [
        "本指南演示了如何使用 [TensorFlow Core 低级 API](https://tensorflow.google.cn/guide/core) 来执行[二元分类](https://developers.google.com/machine-learning/glossary#binary_classification){:.external}和[逻辑回归](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external}。它使用 [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external} 进行肿瘤分类。\n",
        "\n",
        "[逻辑回归](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external}是最流行的二元分类算法之一。给定一组具有特征的样本，逻辑回归的目标是输出 0 到 1 之间的值，这可以解释为每个样本属于特定类的概率。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nchsZfwEVtVs"
      },
      "source": [
        "## 安装\n",
        "\n",
        "本教程使用 [pandas](https://pandas.pydata.org){:.external} 将 CSV 文件读入 [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html){:.external}，使用 [seaborn](https://seaborn.pydata.org){:.external} 在数据集中绘制成对关系，使用 [Scikit-learn](https://scikit-learn.org/){:.external} 计算混淆矩阵以及使用 [matplotlib](https://matplotlib.org/){:.external} 创建可视化效果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lZoUK6AVTos"
      },
      "outputs": [],
      "source": [
        "!pip install -q seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.metrics as sk_metrics\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Preset matplotlib figure sizes.\n",
        "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
        "\n",
        "print(tf.__version__)\n",
        "# To make the results reproducible, set the random seed value.\n",
        "tf.random.set_seed(22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9ne3FZ-On"
      },
      "source": [
        "## 加载数据\n",
        "\n",
        "接下来，从 [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/) {:.external}加载 [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external}。此数据集包含各种特征，例如肿瘤的半径、纹理和凹度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "outputs": [],
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
        "\n",
        "features = ['radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness',\n",
        "            'concavity', 'concave_poinits', 'symmetry', 'fractal_dimension']\n",
        "column_names = ['id', 'diagnosis']\n",
        "\n",
        "for attr in ['mean', 'ste', 'largest']:\n",
        "  for feature in features:\n",
        "    column_names.append(feature + \"_\" + attr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3VR1aTP92nV"
      },
      "source": [
        "使用 [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html){:.external} 将数据集读入 [DataFrame](){:.external}："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvR2Bzb691lJ"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(url, names=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB9eq6Zq-IZ4"
      },
      "outputs": [],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Z1V6Dg-La_"
      },
      "source": [
        "显示前五行："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWxktwbv-KPp"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-Wn2jzVC1W"
      },
      "source": [
        "使用 [`pandas.DataFrame.sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html){:.external}、[`pandas.DataFrame.drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html){:.external} 和 [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html){:.external} 将数据集拆分为训练集和测试集。确保从目标标签中分离特征。测试集用于评估模型对未见数据的泛化性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2O60B-IVG9Q"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset.sample(frac=0.75, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i06vHFv_QB24"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19JaochhaQ3m"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset.drop(train_dataset.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmHRcbAfaSag"
      },
      "outputs": [],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6JxBhBc_wwO"
      },
      "outputs": [],
      "source": [
        "# The `id` column can be dropped since each row is unique\n",
        "x_train, y_train = train_dataset.iloc[:, 2:], train_dataset.iloc[:, 1]\n",
        "x_test, y_test = test_dataset.iloc[:, 2:], test_dataset.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWuJTKEDM-f"
      },
      "source": [
        "## 预处理数据\n",
        "\n",
        "此数据集包含每个样本收集的 10 个肿瘤测量值中每一个的平均值、标准误差和最大值。`\"diagnosis\"` 目标列是分类变量，`'M'` 表示恶性肿瘤，`'B'` 表示良性肿瘤诊断。此列需要转换成数字二进制格式以进行模型训练。\n",
        "\n",
        "[`pandas.Series.map`](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html){:.external} 函数用于将二进制值映射到类别。\n",
        "\n",
        "预处理完成后，还应使用 `tf.convert_to_tensor` 函数将数据集转换为张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEJHhN65a2VV"
      },
      "outputs": [],
      "source": [
        "y_train, y_test = y_train.map({'B': 0, 'M': 1}), y_test.map({'B': 0, 'M': 1})\n",
        "x_train, y_train = tf.convert_to_tensor(x_train, dtype=tf.float32), tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "x_test, y_test = tf.convert_to_tensor(x_test, dtype=tf.float32), tf.convert_to_tensor(y_test, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ubs136WLNp"
      },
      "source": [
        "使用 [`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html){:.external} 查看训练集中几对基于均值的特征的联合分布，并观察它们与目标的关系："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRKO_x8gWKv-"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(train_dataset.iloc[:, 1:6], hue = 'diagnosis', diag_kind='kde');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YOG5iKYKW_3"
      },
      "source": [
        "此变量关系组图表明，半径、周长和面积等某些特征高度相关联。这是意料之中的，因为肿瘤半径直接参与周长和面积的计算。此外，请注意，对于许多特征，恶性诊断似乎更偏右。\n",
        "\n",
        "另外，确保查看一下总体统计信息。请注意每个特征覆盖大不相同的值范围。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi2FzC3T21jR"
      },
      "outputs": [],
      "source": [
        "train_dataset.describe().transpose()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8pDCIFjMla8"
      },
      "source": [
        "鉴于范围不一致，标准化数据以使每个特征具有零均值和单位方差是有利的。这个过程称为[归一化](https://developers.google.com/machine-learning/glossary#normalization){:.external}。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrzKNFNjLQDl"
      },
      "outputs": [],
      "source": [
        "class Normalize(tf.Module):\n",
        "  def __init__(self, x):\n",
        "    # Initialize the mean and standard deviation for normalization\n",
        "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
        "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0))\n",
        "\n",
        "  def norm(self, x):\n",
        "    # Normalize the input\n",
        "    return (x - self.mean)/self.std\n",
        "\n",
        "  def unnorm(self, x):\n",
        "    # Unnormalize the input\n",
        "    return (x * self.std) + self.mean\n",
        "\n",
        "norm_x = Normalize(x_train)\n",
        "x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3CrycBXA2s"
      },
      "source": [
        "## 逻辑回归\n",
        "\n",
        "在构建逻辑回归模型之前，务必了解此方法与传统线性回归的差异。\n",
        "\n",
        "### 逻辑回归基础\n",
        "\n",
        "线性回归返回其输入的线性组合；此输出无界限。[逻辑回归](https://developers.google.com/machine-learning/glossary#logistic_regression){:.external}的输出处于 `(0, 1)` 范围内。对于每个样本，它表示该样本属于*阳性*类的概率。\n",
        "\n",
        "逻辑回归将传统线性回归的连续输出 `(-∞, ∞)` 映射到概率 `(0, 1)`。这种转换也是对称的，因此翻转线性输出的符号会得到原始概率的倒数。\n",
        "\n",
        "让 $Y$ 表示属于第 `1` 类的概率（肿瘤是恶性的）。可以通过将线性回归输出解释为第 `1` 类与第 `0` 类的[对数优势](https://developers.google.com/machine-learning/glossary#log-odds){:.external}比来实现所需的映射：\n",
        "\n",
        "```\n",
        "$$\\ln(\\frac{Y}{1-Y}) = wX + b$$\n",
        "```\n",
        "\n",
        "通过设置 $wX + b = z$，此方程可以求解 $Y$：\n",
        "\n",
        "```\n",
        "$$Y = \\frac{e^{z}}{1 + e^{z}} = \\frac{1}{1 + e^{-z}}$$\n",
        "```\n",
        "\n",
        "表达式 $\\frac{1}{1 + e^{-z}}$ 被称为 [sigmoid 函数](https://developers.google.com/machine-learning/glossary#sigmoid_function){:.external} $\\sigma(z)$。因此，逻辑回归方程可以写为 $Y = \\sigma(wX + b)$。\n",
        "\n",
        "本教程中的数据集处理高维特征矩阵。因此，上述方程必须改写为矩阵向量形式：\n",
        "\n",
        "```\n",
        "$${\\mathrm{Y}} = \\sigma({\\mathrm{X}}w + b)$$\n",
        "```\n",
        "\n",
        "其中：\n",
        "\n",
        "- $\\underset{m\\times 1}{\\mathrm{Y}}$：目标向量\n",
        "- $\\underset{m\\times n}{\\mathrm{X}}$：特征矩阵\n",
        "- $\\underset{n\\times 1}w$：权重向量\n",
        "- $b$：偏置项\n",
        "- $\\sigma$：应用于输出向量的每个元素的 sigmoid 函数\n",
        "\n",
        "首先，呈现 sigmoid 函数，它将线性输出 `(-∞, ∞)` 转换为介于 `0` 和 `1` 之间。sigmoid 函数在 `tf.math.sigmoid` 中可用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThHaV_RmucZl"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-10, 10, 500)\n",
        "x = tf.cast(x, tf.float32)\n",
        "f = lambda x : (1/20)*x + 0.6\n",
        "plt.plot(x, tf.math.sigmoid(x))\n",
        "plt.ylim((-0.1,1.1))\n",
        "plt.title(\"Sigmoid function\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMXEhrZuKECV"
      },
      "source": [
        "### 对数损失函数\n",
        "\n",
        "[对数损失](https://developers.google.com/machine-learning/glossary#Log_Loss){:.external}（即二元交叉熵损失）是使用逻辑回归解决二元分类问题的理想损失函数。对于每个样本，对数损失量化了预测概率与样本真实值之间的相似度。它由以下方程确定：\n",
        "\n",
        "```\n",
        "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}y_i\\cdot\\log(\\hat{y}_i) + (1- y_i)\\cdot\\log(1 - \\hat{y}_i)$$\n",
        "```\n",
        "\n",
        "其中：\n",
        "\n",
        "- $\\hat{y}$：预测概率向量\n",
        "- $y$：真实目标向量\n",
        "\n",
        "可以使用 `tf.nn.sigmoid_cross_entropy_with_logits` 函数计算对数损失。此函数自动将 sigmoid 激活应用于回归输出："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVBInnSqS36W"
      },
      "outputs": [],
      "source": [
        "def log_loss(y_pred, y):\n",
        "  # Compute the log loss function\n",
        "  ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
        "  return tf.reduce_mean(ce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_mutLj0KNUb"
      },
      "source": [
        "### 梯度下降更新规则\n",
        "\n",
        "TensorFlow Core API 支持使用 `tf.GradientTape` 进行自动微分。如果您对逻辑回归[梯度更新](https://developers.google.com/machine-learning/glossary#gradient_descent){:.external}背后的数学感到好奇，下面是推导过程：\n",
        "\n",
        "在上面的对数损失方程中，回顾一下，每个 $\\hat{y}_i$ 可以根据输入重写为 $\\sigma({\\mathrm{X_i}}w + b)$。\n",
        "\n",
        "The goal is to find a $w^*$ and $b^*$ that minimize the log loss:\n",
        "\n",
        "```\n",
        "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}y_i\\cdot\\log(\\sigma({\\mathrm{X_i}}w + b)) + (1- y_i)\\cdot\\log(1 - \\sigma({\\mathrm{X_i}}w + b))$$\n",
        "```\n",
        "\n",
        "By taking the gradient $L$ with respect to $w$, you get the following:\n",
        "\n",
        "```\n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m}(\\sigma({\\mathrm{X}}w + b) - y)X$$\n",
        "```\n",
        "\n",
        "By taking the gradient $L$ with respect to $b$, you get the following:\n",
        "\n",
        "```\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}\\sigma({\\mathrm{X_i}}w + b) - y_i$$\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTCndUecKZho"
      },
      "source": [
        "现在，构建逻辑回归模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0sXM7qLlKfZ"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.built = False\n",
        "    \n",
        "  def __call__(self, x, train=True):\n",
        "    # Initialize the model parameters on the first call\n",
        "    if not self.built:\n",
        "      # Randomly generate the weights and the bias term\n",
        "      rand_w = tf.random.uniform(shape=[x.shape[-1], 1], seed=22)\n",
        "      rand_b = tf.random.uniform(shape=[], seed=22)\n",
        "      self.w = tf.Variable(rand_w)\n",
        "      self.b = tf.Variable(rand_b)\n",
        "      self.built = True\n",
        "    # Compute the model output\n",
        "    z = tf.add(tf.matmul(x, self.w), self.b)\n",
        "    z = tf.squeeze(z, axis=1)\n",
        "    if train:\n",
        "      return z\n",
        "    return tf.sigmoid(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObQu9fDnXGL"
      },
      "source": [
        "要进行验证，确保未训练的模型为训练数据的一个小子集输出 `(0, 1)` 范围内的值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bIovC0Z4QHJ"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ2ievISyf0p"
      },
      "outputs": [],
      "source": [
        "y_pred = log_reg(x_train_norm[:5], train=False)\n",
        "y_pred.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PribnwDHUksC"
      },
      "source": [
        "接下来，编写一个准确率函数来计算训练期间正确分类的比例。为了从预测概率中检索分类，设置一个阈值，所有高于该阈值的概率都属于 1 类。这是一个可配置的超参数，默认设置为 0.5。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssnVcKg7oMe6"
      },
      "outputs": [],
      "source": [
        "def predict_class(y_pred, thresh=0.5):\n",
        "  # Return a tensor with  `1` if `y_pred` > `0.5`, and `0` otherwise\n",
        "  return tf.cast(y_pred > thresh, tf.float32)\n",
        "\n",
        "def accuracy(y_pred, y):\n",
        "  # Return the proportion of matches between `y_pred` and `y`\n",
        "  y_pred = tf.math.sigmoid(y_pred)\n",
        "  y_pred_class = predict_class(y_pred)\n",
        "  check_equal = tf.cast(y_pred_class == y,tf.float32)\n",
        "  acc_val = tf.reduce_mean(check_equal)\n",
        "  return acc_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_0KHQ25_2dF"
      },
      "source": [
        "### 训练模型\n",
        "\n",
        "使用 mini-batch 进行训练既可以提高内存效率，又能加快收敛速度​​。`tf.data.Dataset` API 具有用于批处理和重排的有用函数。借助该 API，您可以从简单、可重用的部分构建复杂的输入流水线。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJD7-4U0etqa"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0]).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test))\n",
        "test_dataset = test_dataset.shuffle(buffer_size=x_test.shape[0]).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLiWZZPBSDip"
      },
      "source": [
        "现在，为逻辑回归模型编写一个训练循环。该循环利用对数损失函数及其相对于输入的梯度来迭代更新模型的参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNC3D1DGsGgK"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "epochs = 200\n",
        "learning_rate = 0.01\n",
        "train_losses, test_losses = [], []\n",
        "train_accs, test_accs = [], []\n",
        "\n",
        "# Set up the training loop and begin training\n",
        "for epoch in range(epochs):\n",
        "  batch_losses_train, batch_accs_train = [], []\n",
        "  batch_losses_test, batch_accs_test = [], []\n",
        "\n",
        "  # Iterate over the training data\n",
        "  for x_batch, y_batch in train_dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred_batch = log_reg(x_batch)\n",
        "      batch_loss = log_loss(y_pred_batch, y_batch)\n",
        "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
        "    # Update the parameters with respect to the gradient calculations\n",
        "    grads = tape.gradient(batch_loss, log_reg.variables)\n",
        "    for g,v in zip(grads, log_reg.variables):\n",
        "      v.assign_sub(learning_rate * g)\n",
        "    # Keep track of batch-level training performance\n",
        "    batch_losses_train.append(batch_loss)\n",
        "    batch_accs_train.append(batch_acc)\n",
        "\n",
        "  # Iterate over the testing data\n",
        "  for x_batch, y_batch in test_dataset:\n",
        "    y_pred_batch = log_reg(x_batch)\n",
        "    batch_loss = log_loss(y_pred_batch, y_batch)\n",
        "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
        "    # Keep track of batch-level testing performance\n",
        "    batch_losses_test.append(batch_loss)\n",
        "    batch_accs_test.append(batch_acc)\n",
        "\n",
        "  # Keep track of epoch-level model performance\n",
        "  train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n",
        "  test_loss, test_acc = tf.reduce_mean(batch_losses_test), tf.reduce_mean(batch_accs_test)\n",
        "  train_losses.append(train_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accs.append(test_acc)\n",
        "  if epoch % 20 == 0:\n",
        "    print(f\"Epoch: {epoch}, Training log loss: {train_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoLiAg7fYft7"
      },
      "source": [
        "### 性能评估\n",
        "\n",
        "观察模型的损失和准确率随时间的变化。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv3oCQPvWhr0"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(epochs), train_losses, label = \"Training loss\")\n",
        "plt.plot(range(epochs), test_losses, label = \"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Log loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Log loss vs training iterations\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2HDVGLPODIE"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(epochs), train_accs, label = \"Training accuracy\")\n",
        "plt.plot(range(epochs), test_accs, label = \"Testing accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy vs training iterations\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jonKhUzuPyfa"
      },
      "outputs": [],
      "source": [
        "print(f\"Final training log loss: {train_losses[-1]:.3f}\")\n",
        "print(f\"Final testing log Loss: {test_losses[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3DF4qyrPyke"
      },
      "outputs": [],
      "source": [
        "print(f\"Final training accuracy: {train_accs[-1]:.3f}\")\n",
        "print(f\"Final testing accuracy: {test_accs[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrj1TbOJasjA"
      },
      "source": [
        "该模型在对训练数据集中的肿瘤进行分类方面表现出高准确率和低损失，并且可以很好地泛化到未知数据。更进一步，您可以探索错误率，从而在总体准确率分数之外提供更深入的见解。二元分类问题最常见的两种错误率是假阳性率 (FPR) 和假阴性率 (FNR)。\n",
        "\n",
        "对于此问题，FPR 是恶性肿瘤预测在实际良性肿瘤中的比例。相反，FNR 是良性肿瘤预测在实际恶性肿瘤中的比例。\n",
        "\n",
        "使用可评估分类的准确率的 [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix){:.external} 计算混淆矩阵，并使用 matplotlib 显示该矩阵："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJO7YkA8ZDMU"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(y, y_classes, typ):\n",
        "  # Compute the confusion matrix and normalize it\n",
        "  plt.figure(figsize=(10,10))\n",
        "  confusion = sk_metrics.confusion_matrix(y.numpy(), y_classes.numpy())\n",
        "  confusion_normalized = confusion / confusion.sum(axis=1, keepdims=True)\n",
        "  axis_labels = range(2)\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.4f', square=True)\n",
        "  plt.title(f\"Confusion matrix: {typ}\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "y_pred_train, y_pred_test = log_reg(x_train_norm, train=False), log_reg(x_test_norm, train=False)\n",
        "train_classes, test_classes = predict_class(y_pred_train), predict_class(y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ5DFcleiDFm"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(y_train, train_classes, 'Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtfcsAp_iCNR"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(y_test, test_classes, 'Testing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlivxaDmTnGq"
      },
      "source": [
        "观察错误率测量值并在此示例的上下文中解释它们的重要性。在许多医学检测研究（例如癌症检测）中，使用高假阳性率来确保低假阴性率是完全可以接受的，实际上也是值得鼓励的，因为错过恶性肿瘤诊断（假阴性）的风险要比将良性肿瘤错误归类为恶性肿瘤（假阳性）严重得多。\n",
        "\n",
        "为了控制 FPR 和 FNR，请尝试在对概率预测进行分类之前更改阈值超参数。较低的阈值会增加模型做出恶性肿瘤分类的总体几率。这不可避免地会增加假阳性的数量和 FPR，但它也有助于减少假阴性的数量和 FNR。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADEN2rb4Nhj"
      },
      "source": [
        "## 保存模型\n",
        "\n",
        "首先，构建一个接受原始数据并执行以下运算的导出模块：\n",
        "\n",
        "- 归一化\n",
        "- 概率预测\n",
        "- 类预测\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KPRHCzg4ZxH"
      },
      "outputs": [],
      "source": [
        "class ExportModule(tf.Module):\n",
        "  def __init__(self, model, norm_x, class_pred):\n",
        "    # Initialize pre- and post-processing functions\n",
        "    self.model = model\n",
        "    self.norm_x = norm_x\n",
        "    self.class_pred = class_pred\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], dtype=tf.float32)])\n",
        "  def __call__(self, x):\n",
        "    # Run the `ExportModule` for new data points\n",
        "    x = self.norm_x.norm(x)\n",
        "    y = self.model(x, train=False)\n",
        "    y = self.class_pred(y)\n",
        "    return y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzRclo5-yjO"
      },
      "outputs": [],
      "source": [
        "log_reg_export = ExportModule(model=log_reg,\n",
        "                              norm_x=norm_x,\n",
        "                              class_pred=predict_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtofGIBN_qFd"
      },
      "source": [
        "如果要将模型保存为当前状态，可使用 `tf.saved_model.save` 函数完成。要加载保存的模型并进行预测，可使用 `tf.saved_model.load` 函数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4Qum1Ts_pmF"
      },
      "outputs": [],
      "source": [
        "models = tempfile.mkdtemp()\n",
        "save_path = os.path.join(models, 'log_reg_export')\n",
        "tf.saved_model.save(log_reg_export, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KPILr1i_M_c"
      },
      "outputs": [],
      "source": [
        "log_reg_loaded = tf.saved_model.load(save_path)\n",
        "test_preds = log_reg_loaded(x_test)\n",
        "test_preds[:10].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgGQuV-yqYZH"
      },
      "source": [
        "## 结论\n",
        "\n",
        "此笔记本介绍了处理逻辑回归问题的几种技术。下面是一些可能有所帮助的提示：\n",
        "\n",
        "- [TensorFlow Core API](https://tensorflow.google.cn/guide/core) 可用于构建具有高度可配置性的机器学习工作流\n",
        "- 分析错误率是一种良好的方法，可以让您更深入地了解分类模型在其总体准确率分数之外的性能。\n",
        "- 过拟合是逻辑回归模型的常见问题，但本教程不存在此问题。有关这方面的更多帮助，请访问[过拟合与欠拟合](../../tutorials/keras/overfit_and_underfit.ipynb)教程。\n",
        "\n",
        "有关使用 TensorFlow Core API 的更多示例，请查阅[教程](https://tensorflow.google.cn/guide/core)。如果您想详细了解如何加载和准备数据，请参阅有关[图像数据加载](../../tutorials/load_data/images.ipynb)或 [CSV 数据加载](../../tutorials/load_data/csv.ipynb)的教程。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "logistic_regression_core.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
