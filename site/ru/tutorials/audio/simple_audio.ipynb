{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fluF3_oOgkWF"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AJs7HHFmg1M9"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Простое распознавание звука: распознавание ключевых слов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbqmZy0gbyE"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/simple_audio\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    Смотреть на TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/community/site/ru/tutorials/audio/simple_audio.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Запустить в Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/community/site/ru/tutorials/audio/simple_audio.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    Смотреть исходные файлы на GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ru/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Скачать ноутбук</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a2322303a3f"
      },
      "source": [
        "Note: Вся информация в этом разделе переведена с помощью русскоговорящего Tensorflow сообщества на общественных началах. Поскольку этот перевод не является официальным, мы не гарантируем что он на 100% аккуратен и соответствует [официальной документации на английском языке](https://www.tensorflow.org/?hl=en). Если у вас есть предложение как исправить этот перевод, мы будем очень рады увидеть pull request в [tensorflow/docs](https://github.com/tensorflow/docs) репозиторий GitHub. Если вы хотите помочь сделать документацию по Tensorflow лучше (сделать сам перевод или проверить перевод подготовленный кем-то другим), напишите нам на [docs-ru@tensorflow.org list](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-ru)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfDNFlb66XF"
      },
      "source": [
        "Из этого руководства Вы узнаете, как построить базовую сеть распознавания речи, которая распознает десять разных слов. Важно знать, что настоящие системы распознавания речи и звука намного сложнее, но, как и MNIST для изображений, они должны дать вам базовое понимание используемых методов. После того, как вы закончите это руководство, у вас будет модель, которая пытается классифицировать односекундный аудиоклип как «вниз», «вперед», «влево», «нет», «вправо», «стоп», «вверх» и \"да\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9C3uLL8Izc"
      },
      "source": [
        "## Установка\n",
        "\n",
        "Для начала установим TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "# Установите случайное начальное число(seed) для воспроизводимости эксперимента\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## Импорт датасета голосовых команд\n",
        "\n",
        "Скрипт начинается с загрузки части [набора данных речевых команд](https://www.tensorflow.org/datasets/catalog/speech_commands). Исходный набор данных состоит из более чем 105 000 аудиофайлов WAVE, в которых люди говорят тридцать разных слов. Эти данные были собраны Google и выпущены под лицензией CC BY, и вы можете помочь улучшить их, [предоставив пять минут своего собственного голоса](https://aiyprojects.withgoogle.com/open_speech_recording).\n",
        "\n",
        "Вы будете использовать часть набора данных, чтобы сэкономить время на загрузке данных. Разархивируйте файл `mini_speech_commands.zip` и загрузите его с помощью `tf.data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-rayb7-3Y0I"
      },
      "outputs": [],
      "source": [
        "data_dir = pathlib.Path('data/mini_speech_commands')\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgvFq3uYiS5G"
      },
      "source": [
        "Проверьте базовую статистику по датасету."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70IBxSKxA1N9"
      },
      "outputs": [],
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[commands != 'README.md']\n",
        "print('Commands:', commands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMvdU9SY8WXN"
      },
      "source": [
        "Извлеките аудиофайлы в список и перемешайте список."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlX685l1wD9k"
      },
      "outputs": [],
      "source": [
        "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
        "filenames = tf.random.shuffle(filenames)\n",
        "num_samples = len(filenames)\n",
        "print('Number of total examples:', num_samples)\n",
        "print('Number of examples per label:',\n",
        "      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))\n",
        "print('Example file tensor:', filenames[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vK3ymy23MCP"
      },
      "source": [
        "Разделите файлы на датасеты для обучения, валидации и тестирования, используя соотношение 80:10:10 соответственно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv_wts-l3KgD"
      },
      "outputs": [],
      "source": [
        "train_files = filenames[:6400]\n",
        "val_files = filenames[6400: 6400 + 800]\n",
        "test_files = filenames[-800:]\n",
        "\n",
        "print('Training set size', len(train_files))\n",
        "print('Validation set size', len(val_files))\n",
        "print('Test set size', len(test_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Cj9FyvfweD"
      },
      "source": [
        "## Чтение аудиофайлов и их меток"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1zjcWteOcBy"
      },
      "source": [
        "Изначально аудиофайл будет считываться как двоичный файл, который вы преобразуете в числовой тензор.\n",
        "\n",
        "Чтобы загрузить аудиофайл, вы будете использовать метод [`tf.audio.decode_wav`](https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav). Этот метод принимает аудио в формате WAV и возвращает тензор и частоту дискретизации.\n",
        "\n",
        "Файл WAV содержат данные таймсерий с заданным количеством выборок в секунду.\n",
        "Каждая выборка представляет собой амплитуду аудиосигнала в определенное время. В 16-битной системе, как и в файлах датасета `mini_speech_commands`, значения варьируются от -32768 до 32767.\n",
        "Частота дискретизации для этого набора данных составляет 16 кГц.\n",
        "Обратите внимание, что `tf.audio.decode_wav` нормализует значения до диапазона [-1.0, 1.0]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PjJ2iXYwftD"
      },
      "outputs": [],
      "source": [
        "def decode_audio(audio_binary):\n",
        "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
        "  return tf.squeeze(audio, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPQseZElOjVN"
      },
      "source": [
        "Меткой для файла wav является его родительский каталог."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VTtX1nr3YT-"
      },
      "outputs": [],
      "source": [
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # Примечание. Здесь вместо распаковки кортежей вы будете использовать индексирование, \n",
        "  # чтобы это работало в TensorFlow графе.\n",
        "  return parts[-2] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Y9w_5MOsr-"
      },
      "source": [
        "Давайте определим метод, который будет принимать имя файла wav и возвращать кортеж из аудио и меток."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdgUD5T93NyT"
      },
      "outputs": [],
      "source": [
        "def get_waveform_and_label(file_path):\n",
        "  label = get_label(file_path)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  waveform = decode_audio(audio_binary)\n",
        "  return waveform, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvN8W_dDjYjc"
      },
      "source": [
        "Теперь примените `get_waveform_and_label` для создания обучающего набора с извлечением пар (аудио, метка) и проверки результатов. Позже вы создадите наборы для проверки и тестирования, используя аналогичную процедуру."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SQl8yXl3kNP"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxGEwvuh2L7"
      },
      "source": [
        "Давайте проверим несколько звуковых сигналов с соответствующими метками."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yuX6Nqzf6wT"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
        "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  ax.plot(audio.numpy())\n",
        "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  ax.set_title(label)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXPphxm0B4m"
      },
      "source": [
        "## Спектрограмма\n",
        "\n",
        "Вы преобразуете сигнал в спектрограмму, которая показывает изменение частоты во времени и может быть представлена в виде двухмерного изображения. Это можно сделать, применив Оконное преобразование Фурье(STFT) для конвертирования звука в частотно-временное представление.\n",
        "\n",
        "Преобразование Фурье ([`tf.signal.fft`](https://www.tensorflow.org/api_docs/python/tf/signal/fft)) преобразует сигнал в его составляющие частоты, но теряет всю информацию о времени. STFT ([`tf.signal.stft`](https://www.tensorflow.org/api_docs/python/tf/signal/stft)) разбивает сигнал на временные окна и выполняет преобразование Фурье в каждом окне, сохраняя некоторую информацию о времени и возвращая двумерный тензор, на котором вы можете запускать стандартные свертки.\n",
        "\n",
        "STFT создает массив комплексных чисел, представляющих величину сигнала и фазу. Однако, в данном руководстве, вам понадобится только величина сигнала, которую можно получить, применив `tf.abs` к выходным данным `tf.signal.stft`.\n",
        "\n",
        "Выберите параметры `frame_length`(размер окна) и `frame_step`(шаг окна) так, чтобы сгенерированное \"изображение\" спектрограммы было почти квадратным. Для получения дополнительной информации о выборе параметров STFT вы можете обратиться к [этому видео](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe).\n",
        "\n",
        "Вам также нужно, чтобы аудиограммы имели одинаковую длину, чтобы при преобразовании их в спектрограммы результаты имели аналогичные размеры. Это можно сделать путем простого заполнения нулями аудиоклипов, которые короче одной секунды. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4CK75DHz_OR"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "  # Дополняем нулями файлы в которых менее 16000 сэмплов\n",
        "  zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
        "\n",
        "  # Объединяем аудио с дополнениями, чтобы все аудиоклипы были одинаковой длины\n",
        "  waveform = tf.cast(waveform, tf.float32)\n",
        "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "  spectrogram = tf.signal.stft(\n",
        "      equal_length, frame_length=255, frame_step=128)\n",
        "      \n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "\n",
        "  return spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdPiPYJphs2"
      },
      "source": [
        "Изучите данные. Сравните форму волны, спектрограмму и фактическое аудио одного примера из набора данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mu6Y7Yz3C-V"
      },
      "outputs": [],
      "source": [
        "for waveform, label in waveform_ds.take(1):\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "print('Label:', label)\n",
        "print('Waveform shape:', waveform.shape)\n",
        "print('Spectrogram shape:', spectrogram.shape)\n",
        "print('Audio playback')\n",
        "display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e62jzb36-Jog"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  # Преобразовываем в частоты для логарифмической шкалы и транспонируем их так, \n",
        "  # чтобы время было представлено на оси x(столбцы).\n",
        "  log_spec = np.log(spectrogram.T)\n",
        "  height = log_spec.shape[0]\n",
        "  X = np.arange(16000, step=height + 1)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYXjW07jCHA"
      },
      "source": [
        "Теперь преобразуйте набор аудио-данных так, чтобы получить изображения спектрограммы и соответствующие им метки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43IS2IouEV40"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram_and_label_id(audio, label):\n",
        "  spectrogram = get_spectrogram(audio)\n",
        "  spectrogram = tf.expand_dims(spectrogram, -1)\n",
        "  label_id = tf.argmax(label == commands)\n",
        "  return spectrogram, label_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEVb_oK0oBLQ"
      },
      "outputs": [],
      "source": [
        "spectrogram_ds = waveform_ds.map(\n",
        "    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gQpAAgMnyDi"
      },
      "source": [
        "Изучите «изображения» спектрограммы для различных примеров из набора данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUbHfTuon4iF"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
        "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n",
        "  ax.set_title(commands[label_id.numpy()])\n",
        "  ax.axis('off')\n",
        "  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KdY8IF8rkt"
      },
      "source": [
        "## Создание и обучение модели\n",
        "\n",
        "Перед построением и обучением нашей модели вам нужно применить ту же обработку данных, что и на обучающем наборе, к проверочному и тестовому датасетам."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10UI32QH_45b"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(files):\n",
        "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
        "  output_ds = output_ds.map(\n",
        "      get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)\n",
        "  return output_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNv4xwYkB2P6"
      },
      "outputs": [],
      "source": [
        "train_ds = spectrogram_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "test_ds = preprocess_dataset(test_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assnWo6SB3lR"
      },
      "source": [
        "Разделите на пакеты наборы для обучения и проверки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgY9WYzn61EX"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1uIh6F_TN9"
      },
      "source": [
        "Добавьте методы датасета [`cache ()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) и [`prefetch ()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) для уменьшения задержки чтения при обучении модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdZ6M-F5_QzY"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwHkKCQQb5oW"
      },
      "source": [
        "В качестве модели вы будете использовать простую сверточную нейронную сеть(CNN), поскольку вы преобразовали аудиофайлы в изображения спектрограмм.\n",
        "Модель также имеет следующие дополнительные слои предварительной обработки:\n",
        "- Слой [`Resizing`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Resizing) для уменьшения размера входных данных, чтобы модель могла обучаться быстрее.\n",
        "- Слой [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) для нормализации каждого пикселя в изображении на основе его среднего значения и стандартного отклонения.\n",
        "\n",
        "Для слоя `Normalization` сначала необходимо вызвать его метод `adapt`, чтобы вычислить совокупную статистику(т.е. среднее и стандартное отклонение)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALYz7PFCHblP"
      },
      "outputs": [],
      "source": [
        "for spectrogram, _ in spectrogram_ds.take(1):\n",
        "  input_shape = spectrogram.shape\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(commands)\n",
        "\n",
        "norm_layer = preprocessing.Normalization()\n",
        "norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    preprocessing.Resizing(32, 32), \n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFjj7-EmsTD-"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttioPJVMcGtq"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_ds, \n",
        "    validation_data=val_ds,  \n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpCDeQ4mUfS"
      },
      "source": [
        "Давайте посмотрим кривые потерь при обучении и валидации, чтобы понять, насколько наша модель улучшилась за время обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzhipg3Gu2AY"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZTt3kO3mfm4"
      },
      "source": [
        "## Оценка эффективности на тестовом датасете\n",
        "\n",
        "Запустим модель на тестовом наборе и проверим эффективность."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biU2MwzyAo8o"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktUanr9mRZky"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
        "y_true = test_labels\n",
        "\n",
        "test_acc = sum(y_pred == y_true) / len(y_true)\n",
        "print(f'Test set accuracy: {test_acc:.0%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en9Znt1NOabH"
      },
      "source": [
        "### Просмотр матрицы ошибок\n",
        "\n",
        "Матрица ошибок полезна, чтобы увидеть, насколько хорошо модель справилась с каждой из команд в тестовом датасете."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvoSAOiXU3lL"
      },
      "outputs": [],
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) \n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx, xticklabels=commands, yticklabels=commands, \n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGi_mzPcLvl"
      },
      "source": [
        "## Выполнение вывода для аудиофайла\n",
        "\n",
        "Наконец, проверьте результат предсказания модели, используя входной аудиофайл, в котором кто-то говорит «нет». Насколько хорошо работает наша модель?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxauKMdhofU"
      },
      "outputs": [],
      "source": [
        "sample_file = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
        "\n",
        "sample_ds = preprocess_dataset([str(sample_file)])\n",
        "\n",
        "for spectrogram, label in sample_ds.batch(1):\n",
        "  prediction = model(spectrogram)\n",
        "  plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
        "  plt.title(f'Predictions for \"{commands[label[0]]}\"')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWICqdqQNaQ"
      },
      "source": [
        "Вы можете видеть, что наша модель очень четко распознала звуковую команду как «нет»."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3jF933m9z1J"
      },
      "source": [
        "## Следующие шаги\n",
        "\n",
        "В этом руководстве показано, как можно выполнить простую классификацию аудио с помощью сверточной нейронной сети с использованием TensorFlow и Python.\n",
        "\n",
        "* Чтобы узнать, как использовать переносное обучение для классификации аудио, ознакомьтесь с учебным пособием [Классификация звука с помощью YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet).\n",
        "\n",
        "* Чтобы создать свое собственное интерактивное веб-приложение для классификации аудио, подумайте о том, чтобы взять [TensorFlow.js - Распознавание аудио с использованием кода обучения с переносом](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0).\n",
        "\n",
        "* TensorFlow также имеет дополнительную поддержку для [подготовки и увеличения аудиоданных](https://www.tensorflow.org/io/tutorials/audio), чтобы помочь с вашими собственными аудио-проектами."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "simple_audio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
