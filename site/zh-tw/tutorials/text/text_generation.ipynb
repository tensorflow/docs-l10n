{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "GCCk8_dHpuNf",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# 循環神經網絡（RNN）文本生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hcD2nPQvPOFM"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://tensorflow.org/tutorials/text/text_generation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />在 tensorflow.org 上查看</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/en/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />在 Google Colab 運行</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/en/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />在 GitHub 上查看源代碼</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/en/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />下載此 notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "本教程演示如何使用基於字符的 RNN 生成文本。我們將使用 Andrej Karpathy 在[《循環神經網絡不合理的有效性》](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)一文中提供的莎士比亞作品數據集。給定此數據中的一個字符序列 （“Shakespear”），訓練一個模型以預測該序列的下一個字符（“e”）。通過重複調用該模型，可以生成更長的文本序列。\n",
        "\n",
        "請注意：啟用 GPU 加速可以更快地執行此筆記本。在 Colab 中依次選擇：*運行時 > 更改運行時類型 > 硬件加速器 > GPU*。如果在本地運行，請確保 TensorFlow 的版本為 1.11 或更高。\n",
        "\n",
        "本教程包含使用 [tf.keras](https://www.tensorflow.org/programmers_guide/keras) 和 [eager execution](https://www.tensorflow.org/programmers_guide/eager) 實現的可運行代碼。以下是當本教程中的模型訓練 30 個週期 （epoch），並以字符串 “Q” 開頭時的示例輸出：\n",
        "\n",
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>\n",
        "\n",
        "雖然有些句子符合語法規則，但是大多數句子沒有意義。這個模型尚未學習到單詞的含義，但請考慮以下幾點：\n",
        "\n",
        "* 此模型是基於字符的。訓練開始時，模型不知道如何拼寫一個英文單詞，甚至不知道單詞是文本的一個單位。\n",
        "\n",
        "* 輸出文本的結構類似於劇本 -- 文本塊通常以講話者的名字開始；而且與數據集類似，講話者的名字採用全大寫字母。\n",
        "\n",
        "* 如下文所示，此模型由小批次 （batch） 文本訓練而成（每批 100 個字符）。即便如此，此模型仍然能生成更長的文本序列，並且結構連貫。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## 設置"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### 導入 TensorFlow 和其他庫"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yG_n40gFzf9s",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version 僅存在於 Colab\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### 下載莎士比亞數據集\n",
        "\n",
        "修改下面一行代碼，在你自己的數據上運行此代碼。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pD_55cOxLkAb",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### 讀取數據\n",
        "\n",
        "首先，看一看文本："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aavnuByVymwK",
        "colab": {}
      },
      "source": [
        "# 讀取並為 py2 compat 解碼\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 文本長度是指文本中的字符個數\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Duhg9NrUymwO",
        "colab": {}
      },
      "source": [
        "# 看一看文本中的前 250 個字符\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlCgQBRVymwR",
        "colab": {}
      },
      "source": [
        "# 文本中的非重複字符\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## 處理文本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### 向量化文本\n",
        "\n",
        "在訓練之前，我們需要將字符串映射到數字表示值。創建兩個查找表格：一個將字符映射到數字，另一個將數字映射到字符。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y0FUEx8jjmLk",
        "colab": {}
      },
      "source": [
        "# 創建從非重複字符到索引的映射\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "現在，每個字符都有一個整數表示值。請注意，我們將字符映射至索引 0 至 `len(unique)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FYyNlCNXymwY",
        "colab": {}
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l1VKcQHcymwb",
        "colab": {}
      },
      "source": [
        "# 顯示文本首 13 個字符的整數映射\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### 預測任務"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "給定一個字符或者一個字符序列，下一個最可能出現的字符是什麼？這就是我們訓練模型要執行的任務。輸入進模型的是一個字符序列，我們訓練這個模型來預測輸出 -- 每個時間步（time step）預測下一個字符是什麼。\n",
        "\n",
        "由於 RNN 是根據前面看到的元素維持內部狀態，那麼，給定此時計算出的所有字符，下一個字符是什麼？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### 創建訓練樣本和目標\n",
        "\n",
        "接下來，將文本劃分為樣本序列。每個輸入序列包含文本中的 `seq_length` 個字符。\n",
        "\n",
        "對於每個輸入序列，其對應的目標包含相同長度的文本，但是向右順移一個字符。\n",
        "\n",
        "將文本拆分為長度為 `seq_length+1` 的文本塊。例如，假設 `seq_length` 為 4 而且文本為 “Hello”， 那麼輸入序列將為 “Hell”，目標序列將為 “ello”。\n",
        "\n",
        "為此，首先使用 `tf.data.Dataset.from_tensor_slices` 函數把文本向量轉換為字符索引流。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mQ1jrAoVjmLu",
        "colab": {}
      },
      "source": [
        "# 設定每個輸入句子長度的最大值\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# 創建訓練樣本 / 目標\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "`batch` 方法使我們能輕鬆把單個字符轉換為所需長度的序列。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4hkDU3i7ozi",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "對於每個序列，使用 `map` 方法先複製再順移，以創建輸入文本和目標文本。 `map` 方法可以將一個簡單的函數應用到每一個批次 （batch）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NGu-FkO_kYU",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hiCopyGZymwi"
      },
      "source": [
        "打印第一批樣本的輸入與目標值："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GNbw-iR0ymwj",
        "colab": {}
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_33OHL3b84i0"
      },
      "source": [
        "這些向量的每個索引均作為一個時間步來處理。作為時間步 0 的輸入，模型接收到 “F” 的索引，並嘗試預測 “i” 的索引為下一個字符。在下一個時間步，模型執行相同的操作，但是 `RNN` 不僅考慮當前的輸入字符，還會考慮上一步的信息。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0eBu9WZG84i0",
        "colab": {}
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### 創建訓練批次\n",
        "\n",
        "前面我們使用 `tf.data` 將文本拆分為可管理的序列。但是在把這些數據輸送至模型之前，我們需要將數據重新排列 （shuffle） 並打包為批次。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p2pGotuNzf-S",
        "colab": {}
      },
      "source": [
        "# 批大小\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# 設定緩衝區大小，以重新排列數據集\n",
        "# （TF 數據被設計為可以處理可能是無限的序列，\n",
        "# 所以它不會試圖在內存中重新排列整個序列。相反，\n",
        "# 它維持一個緩衝區，在緩衝區重新排列元素。 ）\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## 創建模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "使用 `tf.keras.Sequential` 定義模型。在這個簡單的例子中，我們使用了三個層來定義模型：\n",
        "\n",
        "* `tf.keras.layers.Embedding`：輸入層。一個可訓練的對照表，它會將每個字符的數字映射到一個 `embedding_dim` 維度的向量。 \n",
        "* `tf.keras.layers.GRU`：一種 RNN 類型，其大小由 `units=rnn_units` 指定（這裡你也可以使用一個 LSTM 層）。\n",
        "* `tf.keras.layers.Dense`：輸出層，帶有 `vocab_size` 個輸出。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zHT8cLh7EAsg",
        "colab": {}
      },
      "source": [
        "# 詞集的長度\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 嵌入的維度\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN 的單元數量\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MtCrdfzEI2N0",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wwsrpOik5zhv",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "對於每個字符，模型會查找嵌入，把嵌入當作輸入運行 GRU 一個時間步，並用密集層生成邏輯回歸 （logits），預測下一個字符的對數可能性。\n",
        "![數據在模型中傳輸的示意圖](https://github.com/littlebeanbean7/docs/blob/master/site/en/tutorials/text/images/text_generation_training.png?raw=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## 試試這個模型\n",
        "\n",
        "現在運行這個模型，看看它是否按預期運行。\n",
        "\n",
        "首先檢查輸出的形狀："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-_70kKAPrPU",
        "colab": {}
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "在上面的例子中，輸入的序列長度為 `100`， 但是這個模型可以在任何長度的輸入上運行："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vPGmAAXmVLGC",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "為了獲得模型的實際預測，我們需要從輸出分佈中抽樣，以獲得實際的字符索引。這個分佈是根據對字符集的邏輯回歸定義的。\n",
        "\n",
        "請注意：從這個分佈中 _抽樣_ 很重要，因為取分佈的 _最大值自變量點集（argmax）_ 很容易使模型卡在循環中。\n",
        "\n",
        "試試這個批次中的第一個樣本："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4V4MfFg0RQJg",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "這使我們得到每個時間步預測的下一個字符的索引。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YqFMUQc_UFgM",
        "colab": {}
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "解碼它們，以查看此未經訓練的模型預測的文本："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWcFwPwLSo05",
        "colab": {}
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## 訓練模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "此時，這個問題可以被視為一個標準的分類問題：給定先前的 RNN 狀態和這一時間步的輸入，預測下一個字符的類別。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### 添加優化器和損失函數"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "標準的 `tf.keras.losses.sparse_categorical_crossentropy` 損失函數在這裡適用，因為它被應用於預測的最後一個維度。\n",
        "\n",
        "因為我們的模型返回邏輯回歸，所以我們需要設定命令行參數 `from_logits`。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4HrXTACTdzY-",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jeOXriLcymww"
      },
      "source": [
        "使用 `tf.keras.Model.compile` 方法配置訓練步驟。我們將使用 `tf.keras.optimizers.Adam` 並採用默認參數，以及損失函數。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDl1_Een6rL0",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### 配置檢查點"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C6XBUUavgF56"
      },
      "source": [
        "使用 `tf.keras.callbacks.ModelCheckpoint` 來確保訓練過程中保存檢查點。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6fWTriUZP-n",
        "colab": {}
      },
      "source": [
        "# 檢查點保存至的目錄\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# 檢查點的文件名\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### 執行訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "為保持訓練時間合理，使用 10 個週期來訓練模型。在 Colab 中，將運行時設置為 GPU 以加速訓練。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7yGBE2zxMMHs",
        "colab": {}
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UK-hmKjYVoll",
        "colab": {}
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## 生成文本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### 恢復最新的檢查點"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LyeYRiuVjodY"
      },
      "source": [
        "為保持此次預測步驟簡單，將批大小設定為 1。\n",
        "\n",
        "由於 RNN 狀態從時間步傳遞到時間步的方式，模型建立好之後只接受固定的批大小。\n",
        "\n",
        "若要使用不同的 `batch_size` 來運行模型，我們需要重建模型並從檢查點中恢復權重。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zk2WJ2-XjkGz",
        "colab": {}
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LycQ-ot_jjyu",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71xa6jnYVrAN",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### 預測循環\n",
        "\n",
        "下面的代碼塊生成文本：\n",
        "\n",
        "* 首先設置起始字符串，初始化 RNN 狀態並設置要生成的字符個數。\n",
        "\n",
        "* 用起始字符串和 RNN 狀態，獲取下一個字符的預測分佈。\n",
        "\n",
        "* 然後，用分類分佈計算預測字符的索引。把這個預測字符當作模型的下一個輸入。\n",
        "\n",
        "* 模型返回的 RNN 狀態被輸送回模型。現在，模型有更多上下文可以學習，而非只有一個字符。在預測出下一個字符後，更改過的 RNN 狀態被再次輸送回模型。模型就是這樣，通過不斷從前面預測的字符獲得更多上下文，進行學習。\n",
        "\n",
        "![為生成文本，模型的輸出被輸送回模型作為輸入](https://github.com/littlebeanbean7/docs/blob/master/site/en/tutorials/text/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "查看生成的文本，你會發現這個模型知道什麼時候使用大寫字母，什麼時候分段，而且模仿出了莎士比亞式的詞彙。由於訓練的周期小，模型尚未學會生成連貫的句子。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # 評估步驟（用學習過的模型生成文本）\n",
        "\n",
        "  # 要生成的字符個數\n",
        "  num_generate = 1000\n",
        "\n",
        "  # 將起始字符串轉換為數字（向量化）\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # 空字符串用於存儲結果\n",
        "  text_generated = []\n",
        "\n",
        "  # 低溫度會生成更可預測的文本\n",
        "  # 較高溫度會生成更令人驚訝的文本\n",
        "  # 可以通過試驗以找到最好的設定\n",
        "  temperature = 1.0\n",
        "\n",
        "  # 這裡批大小為 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # 刪除批次的維度\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # 用分類分佈預測模型返回的字符\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # 把預測字符和前面的隱藏狀態一起傳遞給模型作為下一個輸入\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktovv0RFhrkn",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "若想改進結果，最簡單的方式是延長訓練時間 （試試 `EPOCHS=30`）。\n",
        "\n",
        "你還可以試驗使用不同的起始字符串，或者嘗試增加另一個 RNN 層以提高模型的準確率，亦或調整溫度參數以生成更多或者更少的隨機預測。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## 高級：自定義訓練\n",
        "\n",
        "上面的訓練步驟簡單，但是能控制的地方不多。\n",
        "\n",
        "至此，你已經知道如何手動運行模型。現在，讓我們打開訓練循環，並自己實現它。這是一些任務的起點，例如實現 _課程學習_ 以幫助穩定模型的開環輸出。\n",
        "\n",
        "你將使用 `tf.GradientTape` 跟踪梯度。關於此方法的更多信息請參閱 [eager execution 指南](https://www.tensorflow.org/guide/eager)。\n",
        "\n",
        "步驟如下：\n",
        "\n",
        "* 首先，初始化 RNN 狀態，使用 `tf.keras.Model.reset_states` 方法。\n",
        "\n",
        "* 然後，迭代數據集（逐批次）併計算每次迭代對應的 *預測*。\n",
        "\n",
        "* 打開一個 `tf.GradientTape` 併計算該上下文時的預測和損失。\n",
        "\n",
        "* 使用 `tf.GradientTape.grads` 方法，計算當前模型變量情況下的損失梯度。\n",
        "\n",
        "* 最後，使用優化器的 `tf.train.Optimizer.apply_gradients` 方法向下邁出一步。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_XAm7eCoKULT",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qUKhnZtMVpoJ",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4kH1o0leVIp",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d4tSNwymzf-q",
        "colab": {}
      },
      "source": [
        "# 訓練步驟\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # 在每個訓練週期開始時，初始化隱藏狀態\n",
        "  # 隱藏狀態最初為 None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # 每 5 個訓練週期，保存（檢查點）1 次模型\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
