# Notas de la reunión de colaboradores de TFF del 28/07/2022

- Personas nuevas
- Vayamos todos al [servidor Discord](https://discord.com/invite/5shux83qZ5) para facilitar las conversaciones de manera interactiva.
    - Contactar a Krzys para hacerse colaborador y poder publicar
- [SIG federado](https://github.com/tensorflow/community/blob/master/sigs/federated/CHARTER.md)
- Conversación sobre los "polizones" (<em>free-riders</em>) y el envenenamiento de datos en silos x, debate dirigido por LinkedIn (el contexto son casos de uso identificados por LinkedIn, a menos que se especifique lo contrario):
    - "Polizones": ciertos usuarios que no contribuyen al grupo, entonces, diluyen los beneficios.
        - Podría ser intencional o involuntario.
        - En esta etapa nos centramos en lo involuntario: es el caso que nos interesa principalmente de LinkedIn.
        - Podría ser algo tan simple como un participante que no tiene suficientes datos o datos que no son útiles para entrenamiento.
            - En este momento estamos pensando en modelarlo como un problema de detección de anomalía.
            - Si comparamos, la contribución de la mayoría funciona si es para la menor parte de los datos.
            - Otro métodos: los modelos federados múltiples, creados con o sin contribuciones de un participante dado. Observamos cuáles progresan y excluimos participantes basándonos en esta información.
        - Algunos "polizones" podrían estar contribuyendo con datos basura.
            - Es más difícil modelar como detección de anomalías
            - El mismo método de arriba
    - Envenenamiento (Poisoning)
        - Igual, podría ser intencional o no
        - Nos centramos en lo involuntario: los usuarios más grandes pueden inundar al grupo de datos y sesgar el modelo con tendencia a sus contribuciones.
        - En cuanto a escenarios de interés, comparte similitudes con el problema de los "polizones".
        - Técnicas relevantes en entrenamientos bizantinos distribuidos.
            - P. ej., en vez de un promedio, podríamos adoptar una mediana para agregar algo de solidez contra el "envenenamiento".
    - ¿Vemos que estos problemas ocurren en algún otro lugar? ¿Vale la pena contribuir con una lógica tal en el ecosistema?
        - ¡Sí! Son problemas comunes en entornos adversos, donde los intereses de los silos pueden no estar alineados (las contribuciones afectan el costo del cálculo y requieren de recursos).
    - ¿Cómo podemos medir el impacto del beneficiario gratuito o del envenenamiento?
        - Por contribución vs. agregación: las ideas que figuran arriba apuntan a esto último
    - Observación: una de las características de TFF son las agregaciones parametrizables y con estado que pueden mantener el propio estado interno y actualizarlo, a medida que se producen.
        - P. ej., [federated_aggregate](https://www.tensorflow.org/federated/api_docs/python/tff/federated_aggregate)
    - Qué piensan sobre las compensaciones (<em>tradeoffs</em>) y las sinergias con otros objetivos (p. ej., DP).
        - DP, definitivamente, pude ayudar con el tema del envenenamiento
        - Nos preguntamos sobre DP en el contexto de "beneficiarios gratuitos": todavía es una pregunta abierta.
    - Hallamos ataques de envenenamiento de datos que podrían tener un impacto insignificante.
        - P. ej., ver https://arxiv.org/pdf/2108.10241.pdf
        - Es importante proporcionar una función tal como parte de la plataforma de aprendizaje federado intersilos, independientemente de la magnitud del impacto.
- Redactemos las ideas con más detalles sobre lo escrito arriba y sobre las propuestas de componentes para agregar al ecosistema TFF desde LinkedIn próximamente.
- Veamos más conversaciones al respecto en Discord.
- Próxima reunión, dentro de dos semanas.
