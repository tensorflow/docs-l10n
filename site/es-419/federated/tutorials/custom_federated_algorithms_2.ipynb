{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqrD7Yzlmlsk"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2k8X1C1nmpKv"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32xflLc4NTx-"
      },
      "source": [
        "# <a>Algoritmos federados personalizados, parte 2: implementación del promediado federado</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtATV6DlqPs0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/federated/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/federated/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/federated/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_igJ2sfaNWS8"
      },
      "source": [
        "Este tutorial es la segunda parte de una serie de dos partes que demuestra cómo implementar tipos personalizados de algoritmos federados en TFF con la ayuda de [Federated Core (FC)](../federated_core.md), que sirve como base para la capa de [Aprendizaje Federado (FL)](../federated_learning.md) (`tff.learning`).\n",
        "\n",
        "Le recomendamos que lea primero la [primera parte de esta serie](custom_federated_algorithms_1.ipynb), que presenta algunos de los conceptos clave y las abstracciones de programación que se utilizan aquí.\n",
        "\n",
        "Esta segunda parte de la serie emplea los mecanismos introducidos en la primera parte para implementar una versión simple de algoritmos federados de entrenamiento y evaluación.\n",
        "\n",
        "Le recomendamos que revise los tutoriales de [clasificación de imágenes](federated_learning_for_image_classification.ipynb) y [generación de texto](federated_learning_for_text_generation.ipynb) para obtener una introducción más sencilla y de mayor nivel a las API de aprendizaje federado de TFF, ya que le ayudarán a poner en contexto los conceptos que describimos en este apartado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuJuLEh2TfZG"
      },
      "source": [
        "## Antes de empezar\n",
        "\n",
        "Antes de empezar, intente ejecutar el siguiente ejemplo de \"Hola mundo\" para asegurarse de que su entorno esté configurado correctamente. Si no funciona, consulte la guía de [instalación](../install.md) para acceder a las instrucciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqJnbs_1YKi4"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-skNC6aovM46"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zzXwGnZamIMM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'Hello, World!'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@tff.federated_computation\n",
        "def hello_world():\n",
        "  return 'Hello, World!'\n",
        "\n",
        "hello_world()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu5Gd8D6W33s"
      },
      "source": [
        "## Cómo implementar el promediado federado\n",
        "\n",
        "Al igual que en [Aprendizaje federado para clasificación de imágenes](federated_learning_for_image_classification.ipynb), usaremos el ejemplo MNIST, pero como está pensado como un tutorial de bajo nivel, omitiremos la API de Keras y `tff.simulation`, escribiremos código de modelo sin formato y construiremos un conjunto de datos federados desde cero.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6qCjef350c_"
      },
      "source": [
        "### Cómo preparar conjuntos de datos federados\n",
        "\n",
        "A modo de demostración, vamos a simular un escenario en el que contamos con datos de 10 usuarios y cada uno de los usuarios aporta conocimientos sobre cómo reconocer un dígito diferente. Esto es lo menos [IID](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) posible.\n",
        "\n",
        "En primer lugar, carguemos los datos MNIST estándar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uThZM4Ds-KDQ"
      },
      "outputs": [],
      "source": [
        "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PkJc5rHA2no_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(dtype('uint8'), (60000, 28, 28)), (dtype('uint8'), (60000,))]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[(x.dtype, x.shape) for x in mnist_train]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFET4BKJFbkP"
      },
      "source": [
        "Los datos vienen como arreglos Numpy, uno con imágenes y otro con etiquetas de dígitos, ambos con la primera dimensión rque abarca los ejemplos individuales. Escribamos una función ayudante que les dé un formato compatible con la forma en que cargamos secuencias federadas en los cálculos TFF, es decir, como una lista de listas: la lista externa abarca los usuarios (dígitos), las internas abarcan lotes de datos en la secuencia de cada cliente. Como es habitual, estructuraremos cada lote como un par de tensores denominados `x` e `y`, cada uno con la dimensión principal del lote. Mientras lo hacemos, también aplanaremos cada imagen en un vector de 784 elementos y cambiaremos la escala de los píxeles al intervalo `0..1`, para no tener que saturar la lógica del modelo con conversiones de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XTaTLiq5GNqy"
      },
      "outputs": [],
      "source": [
        "NUM_EXAMPLES_PER_USER = 1000\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "\n",
        "def get_data_for_digit(source, digit):\n",
        "  output_sequence = []\n",
        "  all_samples = [i for i, d in enumerate(source[1]) if d == digit]\n",
        "  for i in range(0, min(len(all_samples), NUM_EXAMPLES_PER_USER), BATCH_SIZE):\n",
        "    batch_samples = all_samples[i:i + BATCH_SIZE]\n",
        "    output_sequence.append({\n",
        "        'x':\n",
        "            np.array([source[0][i].flatten() / 255.0 for i in batch_samples],\n",
        "                     dtype=np.float32),\n",
        "        'y':\n",
        "            np.array([source[1][i] for i in batch_samples], dtype=np.int32)\n",
        "    })\n",
        "  return output_sequence\n",
        "\n",
        "\n",
        "federated_train_data = [get_data_for_digit(mnist_train, d) for d in range(10)]\n",
        "\n",
        "federated_test_data = [get_data_for_digit(mnist_test, d) for d in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpNdBimWaMHD"
      },
      "source": [
        "Como comprobación rápida del estado, observemos el tensor `Y` en el último lote de datos que aportó el quinto cliente (el correspondiente al dígito `5`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bTNuL1W4bcuc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], dtype=int32)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "federated_train_data[5][-1]['y']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgvcwv7Obhat"
      },
      "source": [
        "Para estar seguros, veamos también la imagen correspondiente al último elemento de ese lote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cI4aat1za525"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.imshow(federated_train_data[5][-1]['x'][-1].reshape(28, 28), cmap='gray')\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-ox58PA56f8"
      },
      "source": [
        "### Sobre la combinación de TensorFlow y TFF\n",
        "\n",
        "En este tutorial, para que sea más compacto, decoramos inmediatamente funciones que introducen la lógica de TensorFlow con `tff.tf_computation`. Sin embargo, para una lógica más compleja, no recomendamos ese patrón. Depurar TensorFlow ya de por sí puede ser todo un reto, y depurar TensorFlow después de que se haya serializado completamente para luego reimportarlo implica la pérdida de algunos metadatos y limita la interactividad, lo que hace que la depuración sea un reto aún mayor.\n",
        "\n",
        "Por lo tanto, le **recomendamos enfáticamente que escriba lógica TF compleja como funciones de Python independientes** (es decir, sin el decorador `tff.tf_computation`). De esta manera, la lógica de TensorFlow se puede desarrollar y poner a prueba con las mejores prácticas y herramientas de TF (como el modo eager), antes de serializar el cálculo para TFF (por ejemplo, invocando `tff.tf_computation` con una función de Python como argumento)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSd6UatXbzw-"
      },
      "source": [
        "### Cómo definir una función de pérdida\n",
        "\n",
        "Ahora que tenemos los datos, definamos una función de pérdida que podamos usar para el entrenamiento. Primero, definamos el tipo de entrada como una tupla nombrada de TFF. Dado que el tamaño de los lotes de datos puede variar, configuramos la dimensión del lote en `None` para indicar que se desconoce el tamaño de esta dimensión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "653xv5NXd4fy"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<x=float32[?,784],y=int32[?]>'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BATCH_SPEC = collections.OrderedDict(\n",
        "    x=tf.TensorSpec(shape=[None, 784], dtype=tf.float32),\n",
        "    y=tf.TensorSpec(shape=[None], dtype=tf.int32))\n",
        "BATCH_TYPE = tff.to_type(BATCH_SPEC)\n",
        "\n",
        "str(BATCH_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb6qPUvyh5A1"
      },
      "source": [
        "Quizás se pregunte por qué no podemos definir directamente un tipo de Python normal. Recuerde que, como se explica en [la parte 1](custom_federated_algorithms_1.ipynb), si bien podemos expresar la lógica de los cálculos de TFF con Python, en el fondo los cálculos de TFF *no son* Python. El símbolo `BATCH_TYPE` definido anteriormente representa una especificación de tipo abstracto de TFF. Es importante distinguir este tipo *abstracto* de TFF de los tipos *de representación* concretos de Python, por ejemplo, contenedores como `dict` o `collections.namedtuple` que pueden usarse para representar el tipo TFF en el cuerpo de una función de Python. A diferencia de Python, TFF tiene un único constructor de tipo abstracto `tff.StructType` para contenedores tipo tupla, con elementos que pueden nombrarse individualmente o dejarse sin nombrar. Este tipo también sirve para modelar parámetros formales de cálculos, ya que los cálculos de TFF formalmente solo pueden declarar un parámetro y un resultado; verá ejemplos de esto en breve.\n",
        "\n",
        "Ahora, es momento de definir el tipo TFF de parámetros del modelo, nuevamente como una tupla nombrada TFF de *ponderaciones* y *sesgos*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Og7VViafh-30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<weights=float32[784,10],bias=float32[10]>\n"
          ]
        }
      ],
      "source": [
        "MODEL_SPEC = collections.OrderedDict(\n",
        "    weights=tf.TensorSpec(shape=[784, 10], dtype=tf.float32),\n",
        "    bias=tf.TensorSpec(shape=[10], dtype=tf.float32))\n",
        "MODEL_TYPE = tff.to_type(MODEL_SPEC)\n",
        "print(MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHhdaWSpfQxo"
      },
      "source": [
        "Ahora que ya implementamos esas definiciones, podemos definir la pérdida para un modelo determinado, en un solo lote. Tenga en cuenta el uso del decorador `@tf.function` dentro del decorador `@tff.tf_computation`. Esto nos permite escribir TF con una semántica similar a la de Python, aunque estemos dentro de un contexto `tf.Graph` creado por el decorador `tff.tf_computation`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4EObiz_Ke0uK"
      },
      "outputs": [],
      "source": [
        "# NOTE: `forward_pass` is defined separately from `batch_loss` so that it can \n",
        "# be later called from within another tf.function. Necessary because a\n",
        "# @tf.function  decorated method cannot invoke a @tff.tf_computation.\n",
        "\n",
        "@tf.function\n",
        "def forward_pass(model, batch):\n",
        "  predicted_y = tf.nn.softmax(\n",
        "      tf.matmul(batch['x'], model['weights']) + model['bias'])\n",
        "  return -tf.reduce_mean(\n",
        "      tf.reduce_sum(\n",
        "          tf.one_hot(batch['y'], 10) * tf.math.log(predicted_y), axis=[1]))\n",
        "\n",
        "@tff.tf_computation(MODEL_TYPE, BATCH_TYPE)\n",
        "def batch_loss(model, batch):\n",
        "  return forward_pass(model, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K0UZHGnr8SB"
      },
      "source": [
        "Como se esperaba, el cálculo `batch_loss` devuelve la pérdida `float32` a partir del modelo y un único lote de datos. Observe cómo `MODEL_TYPE` y `BATCH_TYPE` se han agrupado en una tupla doble de parámetros formales; puede reconocer el tipo de `batch_loss` como `(<MODEL_TYPE,BATCH_TYPE> -> float32)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4WXEAY8Nr89V"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(<model=<weights=float32[784,10],bias=float32[10]>,batch=<x=float32[?,784],y=int32[?]>> -> float32)'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(batch_loss.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAnt_UcdnvGa"
      },
      "source": [
        "Como prueba de estado, construyamos un modelo inicial lleno de ceros y calculemos la pérdida en el lote de datos que visualizamos anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "U8Ne8igan3os"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.3025851"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_model = collections.OrderedDict(\n",
        "    weights=np.zeros([784, 10], dtype=np.float32),\n",
        "    bias=np.zeros([10], dtype=np.float32))\n",
        "\n",
        "sample_batch = federated_train_data[5][-1]\n",
        "\n",
        "batch_loss(initial_model, sample_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckigEAyDAWFV"
      },
      "source": [
        "Tenga en cuenta que alimentamos el cálculo de TFF con el modelo inicial definido como `dict`, aunque el cuerpo de la función de Python que lo define consume parámetros del modelo como `model['weight']` y `model['bias']`. Los argumentos de la llamada a `batch_loss` no se pasan simplemente al cuerpo de esa función.\n",
        "\n",
        "¿Qué sucede cuando invocamos `batch_loss`? En la celda anterior donde se definió, ya rastreamos y serializamos el cuerpo de Python de `batch_loss`. TFF actúa como llamador de `batch_loss` en el momento de la definición del cálculo y como objetivo de la invocación en el momento en que se invoca `batch_loss`. En ambas funciones, TFF sirve como puente entre el sistema de tipos de TFF abstractos y los tipos de representación de Python. En el momento de la invocación, TFF aceptará la mayoría de los tipos de contenedores estándar de Python (`dict`, `list`, `tuple`, `collections.namedtuple`, etc.) como representaciones concretas de tuplas abstractas de TFF. Además, aunque como se señaló anteriormente, los cálculos de TFF formalmente solo aceptan un único parámetro, puede usar la sintaxis de llamada familiar de Python con argumentos posicionales o de palabras clave en caso de que el tipo de parámetro sea una tupla; funciona como se esperaba."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB510nILYbId"
      },
      "source": [
        "### Descenso del gradiente en un solo lote\n",
        "\n",
        "Ahora, definamos un cálculo que utilice esta función de pérdida para ejecutar un único paso de descenso del gradiente. Tenga en cuenta que, al definir esta función, utilizamos `batch_loss` como subcomponente. Puede invocar un cálculo construido con `tff.tf_computation` dentro del cuerpo de otro cálculo, aunque normalmente esto no es necesario; como se señaló anteriormente, debido a que la serialización pierde algo de información de depuración, a menudo es preferible para cálculos más complejos escribir y probar todo el código de TensorFlow sin el decorador `tff.tf_computation`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O4uaVxw3AyYS"
      },
      "outputs": [],
      "source": [
        "@tff.tf_computation(MODEL_TYPE, BATCH_TYPE, tf.float32)\n",
        "def batch_train(initial_model, batch, learning_rate):\n",
        "  # Define a group of model variables and set them to `initial_model`. Must\n",
        "  # be defined outside the @tf.function.\n",
        "  model_vars = collections.OrderedDict([\n",
        "      (name, tf.Variable(name=name, initial_value=value))\n",
        "      for name, value in initial_model.items()\n",
        "  ])\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "  @tf.function\n",
        "  def _train_on_batch(model_vars, batch):\n",
        "    # Perform one step of gradient descent using loss from `batch_loss`.\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = forward_pass(model_vars, batch)\n",
        "    grads = tape.gradient(loss, model_vars)\n",
        "    optimizer.apply_gradients(\n",
        "        zip(tf.nest.flatten(grads), tf.nest.flatten(model_vars)))\n",
        "    return model_vars\n",
        "\n",
        "  return _train_on_batch(model_vars, batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y84gQsaohC38"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(<initial_model=<weights=float32[784,10],bias=float32[10]>,batch=<x=float32[?,784],y=int32[?]>,learning_rate=float32> -> <weights=float32[784,10],bias=float32[10]>)'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(batch_train.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID8xg9FCUL2A"
      },
      "source": [
        "Cuando se invoca una función de Python decorada con `tff.tf_computation` dentro del cuerpo de otra función similar, la lógica del cálculo interno de TFF está incorporada (esencialmente, en línea) en la lógica del cálculo externo. Como se señaló anteriormente, si está escribiendo ambos cálculos, probablemente sea preferible hacer que la función interna (`batch_loss` en este caso) sea una Python o `tf.function` normal en lugar de `tff.tf_computation`. Sin embargo, aquí ilustramos que llamar a un `tff.tf_computation` dentro de otro básicamente funciona como se esperaba. Esto puede ser necesario si, por ejemplo, no tiene el código Python que define `batch_loss`, sino solo su representación TFF serializada.\n",
        "\n",
        "Ahora, apliquemos esta función varias veces al modelo inicial para ver si la pérdida disminuye."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8edcJTlXUULm"
      },
      "outputs": [],
      "source": [
        "model = initial_model\n",
        "losses = []\n",
        "for _ in range(5):\n",
        "  model = batch_train(model, sample_batch, 0.1)\n",
        "  losses.append(batch_loss(model, sample_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3n1onojT1zHv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.19690025, 0.13176318, 0.101132266, 0.08273812, 0.0703014]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQk4Ha8PU-3P"
      },
      "source": [
        "### Descenso del gradiente en una secuencia de datos locales\n",
        "\n",
        "Ahora, dado que `batch_train` parece funcionar, escribamos una función de entrenamiento similar `local_train` que consuma la secuencia completa de todos los lotes de un usuario en lugar de solo un lote. Ahora, el nuevo cálculo deberá consumir `tff.SequenceType(BATCH_TYPE)` en lugar de `BATCH_TYPE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EfPD5a6QVNXM"
      },
      "outputs": [],
      "source": [
        "LOCAL_DATA_TYPE = tff.SequenceType(BATCH_TYPE)\n",
        "\n",
        "@tff.federated_computation(MODEL_TYPE, tf.float32, LOCAL_DATA_TYPE)\n",
        "def local_train(initial_model, learning_rate, all_batches):\n",
        "  \n",
        "  # Reduction function to apply to each batch.\n",
        "  @tff.federated_computation((MODEL_TYPE, tf.float32), BATCH_TYPE)\n",
        "  def batch_fn(model_with_lr, batch):\n",
        "    model, lr = model_with_lr\n",
        "    return batch_train(model, batch, lr), lr\n",
        "\n",
        "  trained_model, _ = tff.sequence_reduce(\n",
        "      all_batches, (initial_model, learning_rate), batch_fn\n",
        "  )\n",
        "  return trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sAhkS5yKUgjC"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(<initial_model=<weights=float32[784,10],bias=float32[10]>,learning_rate=float32,all_batches=<x=float32[?,784],y=int32[?]>*> -> <weights=float32[784,10],bias=float32[10]>)'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(local_train.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYT-SiopYBtH"
      },
      "source": [
        "Esta breve sección de código contiene varios detalles ocultos, repasémoslos uno por uno.\n",
        "\n",
        "Primero, si bien podríamos haber implementado esta lógica por completo en TensorFlow y recurrir a `tf.data.Dataset.reduce` para procesar la secuencia de manera similar a como lo hicimos antes, esta vez hemos optado por expresar la lógica en el lenguaje pegamento, como `tff.federated_computation`. Hemos utilizado el operador federado `tff.sequence_reduce` para ejecutar la reducción.\n",
        "\n",
        "El operador `tff.sequence_reduce` se usa de manera similar a `tf.data.Dataset.reduce`. Puede pensar en él como si fuera esencialmente lo mismo que `tf.data.Dataset.reduce`, pero para usar dentro de cálculos federados, que, como recordará, no pueden contener código de TensorFlow. Es un operador de plantilla con un parámetro formal de 3 tuplas que consta de una *secuencia* de elementos tipo `T`, el estado inicial de la reducción (nos referiremos a él de manera abstracta como *cero*) de algún tipo `U` y el *operador de reducción* de tipo `(<U,T> -> U)` que altera el estado de la reducción procesando un solo elemento. El resultado es el estado final de la reducción, después de procesar todos los elementos en orden secuencial. En nuestro ejemplo, el estado de la reducción es el modelo entrenado con un prefijo de datos y los elementos son lotes de datos.\n",
        "\n",
        "En segundo lugar, tenga en cuenta que hemos vuelto a usar un cálculo (`batch_train`) como componente dentro de otro (`local_train`), pero no directamente. No podemos usarlo como operador de reducción porque requiere un parámetro adicional: la tasa de aprendizaje. Para resolver esto, definimos un procesamiento federado integrado `batch_fn` que se vincula al parámetro `learning_rate` de `local_train` en su cuerpo. Está permitido que un cálculo secundario definido de esta manera capture un parámetro formal de su cálculo principal siempre que el cálculo secundario no se invoque fuera del cuerpo de su cálculo principal. Puedes considerar este patrón como un equivalente de `functools.partial` en Python.\n",
        "\n",
        "La implicación práctica de capturar `learning_rate` de esta manera es, por supuesto, que se utiliza el mismo valor de tasa de aprendizaje en todos los lotes.\n",
        "\n",
        "Ahora, probemos la función de entrenamiento local recién definida en toda la secuencia de datos del mismo usuario que contribuyó con el lote de muestra (dígito `5`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EnWFLoZGcSby"
      },
      "outputs": [],
      "source": [
        "locally_trained_model = local_train(initial_model, 0.1, federated_train_data[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0UXUqGk9zoF"
      },
      "source": [
        "¿Funcionó? Para responder a esta pregunta, tenemos que implementar la evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8WDKu6WYy__"
      },
      "source": [
        "### Evaluación local\n",
        "\n",
        "Aquí hay una forma de implementar la evaluación local sumando las pérdidas en todos los lotes de datos (también podríamos haber calculado el promedio; lo dejaremos como ejercicio para el lector)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0RiODuc6z7Ln"
      },
      "outputs": [],
      "source": [
        "@tff.federated_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
        "def local_eval(model, all_batches):\n",
        "\n",
        "  @tff.tf_computation((MODEL_TYPE, tf.float32), BATCH_TYPE)\n",
        "  def accumulate_evaluation(model_and_accumulator, batch):\n",
        "    model, accumulator = model_and_accumulator\n",
        "    return model, accumulator + batch_loss(model, batch)\n",
        "\n",
        "  _, total_loss = tff.sequence_reduce(\n",
        "      all_batches, (model, 0.0), accumulate_evaluation\n",
        "  )\n",
        "  return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pH2XPEAKa4Dg"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(<model=<weights=float32[784,10],bias=float32[10]>,all_batches=<x=float32[?,784],y=int32[?]>*> -> float32)'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(local_eval.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efX81HuE-BcO"
      },
      "source": [
        "Nuevamente, hay algunos elementos nuevos ilustrados en este código; repasémoslos uno por uno.\n",
        "\n",
        "Primero, hemos utilizado dos nuevos operadores federados para procesar secuencias: `tff.sequence_map`, que toma una *función de asignación* `T->U` y una *secuencia* de `T` y emite una secuencia de `U` obtenida mediante la aplicación de la función de asignación puntualmente, y `tff.sequence_sum`, que simplemente agrega todos los elementos. Aquí, asignamos cada lote de datos a un valor de pérdida y luego sumamos los valores de pérdida resultantes para calcular la pérdida total.\n",
        "\n",
        "Tenga en cuenta que podríamos haber usado `tff.sequence_reduce` nuevamente, pero esta no sería la mejor opción: el proceso de reducción es, por definición, secuencial, mientras que la asignación y la suma se pueden calcular en forma paralela. Cuando se tiene la opción, es mejor seguir con operadores que no limiten las opciones de implementación, de modo que, cuando nuestro cálculo de TFF se compile en el futuro para implementarse en un entorno específico, sea posible aprovechar al máximo todas las oportunidades potenciales para una ejecución más rápida, más escalable y más eficiente de los recursos.\n",
        "\n",
        "En segundo lugar, tenga en cuenta que, al igual que en `local_train`, la función del componente que necesitamos (`batch_loss`) toma más parámetros de los que espera el operador federado (`tff.sequence_map`), por lo que volvemos a definir un parcial, esta vez en línea, envolviendo directamente una función `lambda` como `tff.federated_computation`. Usar contenedores en línea con una función como argumento es la forma recomendada de usar `tff.tf_computation` para insertar la lógica de TensorFlow en TFF.\n",
        "\n",
        "Ahora, veamos si nuestro entrenamiento funcionó."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vPw6JSVf5q_x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_model loss = 23.025854\n",
            "locally_trained_model loss = 0.43484688\n"
          ]
        }
      ],
      "source": [
        "print('initial_model loss =', local_eval(initial_model,\n",
        "                                         federated_train_data[5]))\n",
        "print('locally_trained_model loss =',\n",
        "      local_eval(locally_trained_model, federated_train_data[5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tvu70cnBsUf"
      },
      "source": [
        "De hecho, la pérdida disminuyó. Pero ¿qué pasa si lo evaluamos con los datos de otro usuario?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gjF0NYAj5wls"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_model loss = 23.025854\n",
            "locally_trained_model loss = 74.50075\n"
          ]
        }
      ],
      "source": [
        "print('initial_model loss =', local_eval(initial_model,\n",
        "                                         federated_train_data[0]))\n",
        "print('locally_trained_model loss =',\n",
        "      local_eval(locally_trained_model, federated_train_data[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WPumnRTBzUs"
      },
      "source": [
        "Como era de esperar, las cosas empeoraron. El modelo fue entrenado para reconocer `5` y nunca vio un `0`. Esto plantea la pregunta: desde una perspectiva global, ¿cómo influyó el entrenamiento local en la calidad del modelo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJnL2mQRZKTO"
      },
      "source": [
        "### Evaluación federada\n",
        "\n",
        "Este es el punto de nuestro viaje donde finalmente volvemos a los tipos federados y los cálculos federados: el tema con el que comenzamos. A continuación, se muestran un par de definiciones de tipos de TFF para el modelo que se origina en el servidor y los datos que permanecen en los clientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LjGGhpoEBh_6"
      },
      "outputs": [],
      "source": [
        "SERVER_MODEL_TYPE = tff.type_at_server(MODEL_TYPE)\n",
        "CLIENT_DATA_TYPE = tff.type_at_clients(LOCAL_DATA_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gTXV2-jZtE3"
      },
      "source": [
        "Con todas las definiciones que hemos introducido hasta ahora, expresar la evaluación federada en TFF ocupa una sola línea: distribuimos el modelo a los clientes, dejamos que cada cliente invoque la evaluación local en su porción local de datos y luego promediamos la pérdida. A continuación, una manera de escribir esto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2zChEPzEBx4T"
      },
      "outputs": [],
      "source": [
        "@tff.federated_computation(SERVER_MODEL_TYPE, CLIENT_DATA_TYPE)\n",
        "def federated_eval(model, data):\n",
        "  return tff.federated_mean(\n",
        "      tff.federated_map(local_eval, [tff.federated_broadcast(model),  data]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWcNONNWaE0N"
      },
      "source": [
        "Ya hemos visto ejemplos de `tff.federated_mean` y `tff.federated_map` en escenarios más simples y, a nivel intuitivo, funcionan como se esperaba, pero esta sección de código es más compleja de lo que parece, así que repasémoslo detenidamente.\n",
        "\n",
        "Primero, analicemos la *opción de permitir que cada cliente invoque la evaluación local en su parte local de datos*. Como recordará de las secciones anteriores, `local_eval` tiene una firma de tipo del formulario `(<MODEL_TYPE, LOCAL_DATA_TYPE> -> float32)`.\n",
        "\n",
        "El operador federado `tff.federated_map` es una plantilla que acepta como parámetro una tupla doble que consta de la *función de asignación* de algún tipo `T->U` y un valor federado de tipo `{T}@CLIENTS` (es decir, con miembros constituyentes del mismo tipo que el parámetro de la función de asignación) y devuelve un resultado de tipo `{U}@CLIENTS`.\n",
        "\n",
        "Dado que estamos proporcionando `local_eval` como una función de asignación para aplicar por cliente, el segundo argumento debe ser de tipo federado `{<MODEL_TYPE, LOCAL_DATA_TYPE>}@CLIENTS`, es decir, en la nomenclatura de las secciones anteriores, debería ser una tupla federada. Cada cliente debe tener un conjunto completo de argumentos para `local_eval` como miembro constituyente. En lugar de eso, le proporcionamos una `list` de Python de 2 elementos. ¿Qué está sucediendo aquí?\n",
        "\n",
        "Efectivamente, este es un ejemplo de una *conversión de tipos implícita* en TFF, similar a las conversiones de tipos implícitas que puede haber encontrado en otros lugares, por ejemplo, cuando se ingresa un `int` a una función que acepta un `float`. La conversión implícita se utiliza poco en este momento, pero planeamos hacerla más generalizada en TFF como una forma de minimizar el texto repetitivo.\n",
        "\n",
        "La conversión implícita que se aplica en este caso es la equivalencia entre tuplas federadas de la forma `{<X,Y>}@Z` y tuplas de valores federados `<{X}@Z,{Y}@Z>`. Si bien formalmente, se trata de firmas de tipos diferentes, mirándolo desde la perspectiva de los programadores, cada dispositivo en `Z` contiene dos unidades de datos `X` e `Y`. Lo que sucede aquí no es diferente a `zip` en Python y, de hecho, ofrecemos un operador `tff.federated_zip` que le permite hacer dichas conversiones explícitamente. Cuando `tff.federated_map` encuentra una tupla como segundo argumento, simplemente invoca `tff.federated_zip` por usted.\n",
        "\n",
        "Teniendo en cuenta lo anterior, ahora debería poder reconocer la expresión `tff.federated_broadcast(model)` como representación de un valor de tipo TFF `{MODEL_TYPE}@CLIENTS` y `data` como un valor de tipo TFF `{LOCAL_DATA_TYPE}@CLIENTS` (o simplemente `CLIENT_DATA_TYPE`), los dos se filtran juntos a través de un `tff.federated_zip` implícito para formar el segundo argumento de `tff.federated_map`.\n",
        "\n",
        "Como era de esperar, el operador `tff.federated_broadcast` simplemente transfiere datos desde el servidor a los clientes.\n",
        "\n",
        "Ahora, veamos cómo nuestro entrenamiento local afectó la pérdida promedio en el sistema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tbmtJItcn94j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_model loss = 23.025852\n",
            "locally_trained_model loss = 54.43263\n"
          ]
        }
      ],
      "source": [
        "print('initial_model loss =', federated_eval(initial_model,\n",
        "                                             federated_train_data))\n",
        "print('locally_trained_model loss =',\n",
        "      federated_eval(locally_trained_model, federated_train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQi2rGX_fK7i"
      },
      "source": [
        "De hecho, como era de esperar, la pérdida ha aumentado. Para mejorar el modelo para todos los usuarios, tendremos que entrenarlo con los datos de todos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkw9f59qfS7o"
      },
      "source": [
        "### Entrenamiento federado\n",
        "\n",
        "La forma más sencilla de implementar el entrenamiento federado es entrenar localmente y luego promediar los modelos. Esto utiliza los mismos bloques de construcción y patrones que ya hemos discutido, como puede ver a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mBOC4uoG6dd-"
      },
      "outputs": [],
      "source": [
        "SERVER_FLOAT_TYPE = tff.type_at_server(tf.float32)\n",
        "\n",
        "\n",
        "@tff.federated_computation(SERVER_MODEL_TYPE, SERVER_FLOAT_TYPE,\n",
        "                           CLIENT_DATA_TYPE)\n",
        "def federated_train(model, learning_rate, data):\n",
        "  return tff.federated_mean(\n",
        "      tff.federated_map(local_train, [\n",
        "          tff.federated_broadcast(model),\n",
        "          tff.federated_broadcast(learning_rate), data\n",
        "      ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2vACMsQjzO1"
      },
      "source": [
        "Tenga en cuenta que, en la implementación completa de promediado federado que proporciona `tff.learning`, en lugar de promediar los modelos, optamos por promediar los deltas de los modelos, por una serie de razones, por ejemplo, la capacidad de recortar las normas de actualización, para la compresión, etc.\n",
        "\n",
        "Para comprobar si el entrenamiento funciona, hagamos unas cuantas rondas de entrenamiento y comparemos la pérdida media antes y después."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NLx-3rLs9jGY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 0, loss=21.60552406311035\n",
            "round 1, loss=20.365678787231445\n",
            "round 2, loss=19.27480125427246\n",
            "round 3, loss=18.31110954284668\n",
            "round 4, loss=17.457256317138672\n"
          ]
        }
      ],
      "source": [
        "model = initial_model\n",
        "learning_rate = 0.1\n",
        "for round_num in range(5):\n",
        "  model = federated_train(model, learning_rate, federated_train_data)\n",
        "  learning_rate = learning_rate * 0.9\n",
        "  loss = federated_eval(model, federated_train_data)\n",
        "  print('round {}, loss={}'.format(round_num, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0VjSLQzlUIp"
      },
      "source": [
        "Para ser exhaustivos, también vamos a usar los datos de prueba para confirmar si nuestro modelo generaliza bien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZaZT45yFMOaM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_model test loss = 22.795593\n",
            "trained_model test loss = 17.278767\n"
          ]
        }
      ],
      "source": [
        "print('initial_model test loss =',\n",
        "      federated_eval(initial_model, federated_test_data))\n",
        "print('trained_model test loss =', federated_eval(model, federated_test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxlHHwLGlgFB"
      },
      "source": [
        "Esto concluye nuestro tutorial.\n",
        "\n",
        "Es claro que nuestro ejemplo simplificado no refleja una serie de cosas que debería hacer en un escenario más realista; por ejemplo, no hemos calculado otras métricas además de la pérdida. Le recomendamos que estudie la [implementación](https://github.com/tensorflow/federated/blob/main/tensorflow_federated/python/learning/federated_averaging.py) del promediado federado en `tff.learning` como un ejemplo más completo y como una forma de demostrar algunas de las prácticas de codificación que nos gustaría fomentar."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "custom_federated_algorithms_2.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
