{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a930wM_fqUNH"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Federated Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VxVUPYkahDa6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naLrzWx6FbQQ"
      },
      "source": [
        "# Aprendizaje federado de modelos grandes eficiente para el cliente a través de `federated_select` y agregación dispersa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jM4S9YFXamd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/sparse_federated_learning\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/federated/tutorials/sparse_federated_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/federated/tutorials/sparse_federated_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/federated/tutorials/sparse_federated_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saL2ruE25Cx1"
      },
      "source": [
        "Este tutorial muestra cómo se puede usar TFF para entrenar un modelo muy grande donde cada dispositivo cliente solo descarga y actualiza una pequeña parte del modelo, con ayuda de `tff.federated_select` y agregación dispersa. Si bien este tutorial es bastante completo, el <a href=\"https://www.tensorflow.org/federated/tutorials/federated_select\" data-md-type=\"link\">tutorial `tff.federated_select`</a> y el [tutorial de algoritmos de FL personalizados](https://www.tensorflow.org/federated/tutorials/building_your_own_federated_learning_algorithm) sirven de introducción a algunas de las técnicas que se utilizan aquí.\n",
        "\n",
        "Concretamente, en este tutorial consideramos la regresión logística para la clasificación de etiquetas múltiples, prediciendo qué \"etiquetas\" están asociadas con una cadena de texto basada en una representación de características de bolsa de palabras. Es importante destacar que los costos de comunicación y cálculo del lado del cliente están controlados por una constante fija (`MAX_TOKENS_SELECTED_PER_CLIENT`) y *no* escalan con el tamaño general del vocabulario, que podría ser extremadamente grande en la práctica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg3OmMVWp_9D"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89APlnIQCvmp"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from collections.abc import Callable\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92wW8RDBvE9F"
      },
      "source": [
        "Cada cliente usará `federated_select` para seleccionar las filas de las ponderaciones del modelo para, como máximo, esta cantidad de tokens únicos. Esto limita el tamaño del modelo local del cliente y la cantidad de comunicación servidor -&gt; cliente (`federated_select`) y cliente - &gt; servidor `(federated_aggregate`).\n",
        "\n",
        "Este tutorial debería ejecutarse correctamente incluso si se ajusta a un valor tan pequeño como 1 (lo que garantiza que no se seleccionen todas las fichas de cada cliente) o a un valor grande, aunque la convergencia del modelo puede verse afectada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkDumr_6bDtY"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS_SELECTED_PER_CLIENT = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th2PI1FpvaSL"
      },
      "source": [
        "También definimos algunas constantes para varios tipos. Para esta colaboración, un **token** es un identificador entero para una palabra en particular después de analizar el conjunto de datos. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyQ9xmIyvZFO"
      },
      "outputs": [],
      "source": [
        "# There are some constraints on types\n",
        "# here that will require some explicit type conversions:\n",
        "#    - `tff.federated_select` requires int32\n",
        "#    - `tf.SparseTensor` requires int64 indices.\n",
        "TOKEN_DTYPE = tf.int64\n",
        "SELECT_KEY_DTYPE = tf.int32\n",
        "\n",
        "# Type for counts of token occurences.\n",
        "TOKEN_COUNT_DTYPE = tf.int32\n",
        "\n",
        "# A sparse feature vector can be thought of as a map\n",
        "# from TOKEN_DTYPE to FEATURE_DTYPE. \n",
        "# Our features are {0, 1} indicators, so we could potentially\n",
        "# use tf.int8 as an optimization.\n",
        "FEATURE_DTYPE = tf.int32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ej2eY4-zk_a"
      },
      "source": [
        "# Configuración del problema: conjunto de datos y modelo\n",
        "\n",
        "En este tutorial construimos un pequeño conjunto de datos de juguete para facilitar la experimentación. Sin embargo, el formato del conjunto de datos es compatible con [Federated StackOverflow](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data) y la [arquitectura del modelo](https://github.com/google-research/federated/blob/0a558bac8a724fc38175ff4f0ce46c7af3d24be2/utils/datasets/stackoverflow_tag_prediction.py) y el [preprocesamiento](https://github.com/google-research/federated/blob/49a43456aa5eaee3e1749855eed89c0087983541/utils/models/stackoverflow_lr_models.py) se adoptan del problema de predicción de etiquetas StackOverflow de [*Adaptive Federated Optimization*](https://arxiv.org/abs/2003.00295)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ofeMf1phYY"
      },
      "source": [
        "## Parseo y preprocesamiento de conjuntos de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM9Fae0PXSfa"
      },
      "outputs": [],
      "source": [
        "NUM_OOV_BUCKETS = 1\n",
        "\n",
        "BatchType = collections.namedtuple('BatchType', ['tokens', 'tags'])\n",
        "\n",
        "def build_to_ids_fn(word_vocab: list[str],\n",
        "                    tag_vocab: list[str]) -> Callable[[tf.Tensor], tf.Tensor]:\n",
        "  \"\"\"Constructs a function mapping examples to sequences of token indices.\"\"\"\n",
        "  word_table_values = np.arange(len(word_vocab), dtype=np.int64)\n",
        "  word_table = tf.lookup.StaticVocabularyTable(\n",
        "      tf.lookup.KeyValueTensorInitializer(word_vocab, word_table_values),\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS)\n",
        "\n",
        "  tag_table_values = np.arange(len(tag_vocab), dtype=np.int64)\n",
        "  tag_table = tf.lookup.StaticVocabularyTable(\n",
        "      tf.lookup.KeyValueTensorInitializer(tag_vocab, tag_table_values),\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS)\n",
        "\n",
        "  def to_ids(example):\n",
        "    \"\"\"Converts a Stack Overflow example to a bag-of-words/tags format.\"\"\"\n",
        "    sentence = tf.strings.join([example['tokens'], example['title']],\n",
        "                               separator=' ')\n",
        "\n",
        "    # We represent that label (output tags) densely.\n",
        "    raw_tags = example['tags']\n",
        "    tags = tf.strings.split(raw_tags, sep='|')\n",
        "    tags = tag_table.lookup(tags)\n",
        "    tags, _ = tf.unique(tags)\n",
        "    tags = tf.one_hot(tags, len(tag_vocab) + NUM_OOV_BUCKETS)\n",
        "    tags = tf.reduce_max(tags, axis=0)\n",
        "\n",
        "    # We represent the features as a SparseTensor of {0, 1}s.\n",
        "    words = tf.strings.split(sentence)\n",
        "    tokens = word_table.lookup(words)\n",
        "    tokens, _ = tf.unique(tokens)\n",
        "    # Note:  We could choose to use the word counts as the feature vector\n",
        "    # instead of just {0, 1} values (see tf.unique_with_counts).\n",
        "    tokens = tf.reshape(tokens, shape=(tf.size(tokens), 1))\n",
        "    tokens_st = tf.SparseTensor(\n",
        "        tokens,\n",
        "        tf.ones(tf.size(tokens), dtype=FEATURE_DTYPE),\n",
        "        dense_shape=(len(word_vocab) + NUM_OOV_BUCKETS,))\n",
        "    tokens_st = tf.sparse.reorder(tokens_st)\n",
        "\n",
        "    return BatchType(tokens_st, tags)\n",
        "\n",
        "  return to_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv9NMKgbYki9"
      },
      "outputs": [],
      "source": [
        "def build_preprocess_fn(word_vocab, tag_vocab):\n",
        "\n",
        "  @tf.function\n",
        "  def preprocess_fn(dataset):\n",
        "    to_ids = build_to_ids_fn(word_vocab, tag_vocab)\n",
        "    # We *don't* shuffle in order to make this colab deterministic for\n",
        "    # easier testing and reproducibility.\n",
        "    # But real-world training should use `.shuffle()`.\n",
        "    return dataset.map(to_ids, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return preprocess_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxji1x30tZCi"
      },
      "source": [
        "## Un pequeño conjunto de datos de juguete\n",
        "\n",
        "Construimos un pequeño conjunto de datos de juguete con un vocabulario global de 12 palabras y 3 clientes. Este pequeño ejemplo sirve para probar casos extremos (por ejemplo, tenemos dos clientes con menos de `MAX_TOKENS_SELECTED_PER_CLIENT = 6` tokens distintos y uno con más) y desarrollar el código.\n",
        "\n",
        "No obstante, los casos de uso de este enfoque en el mundo real serían vocabularios globales de decenas de millones o más, con quizás miles de tokens distintos en cada cliente. Como el formato de los datos es el mismo, la extensión a problemas de banco de pruebas más realistas, por ejemplo, el conjunto de datos `tff.simulation.datasets.stackoverflow.load_data()`, debería ser sencilla.\n",
        "\n",
        "En primer lugar, definimos nuestros vocabularios de palabras y etiquetas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gtatbx4tdPo"
      },
      "outputs": [],
      "source": [
        "# Features\n",
        "FRUIT_WORDS = ['apple', 'orange', 'pear', 'kiwi']\n",
        "VEGETABLE_WORDS = ['carrot', 'broccoli', 'arugula', 'peas']\n",
        "FISH_WORDS = ['trout', 'tuna', 'cod', 'salmon']\n",
        "WORD_VOCAB = FRUIT_WORDS + VEGETABLE_WORDS + FISH_WORDS\n",
        "\n",
        "# Labels\n",
        "TAG_VOCAB = ['FRUIT', 'VEGETABLE', 'FISH']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQLblngP7gmJ"
      },
      "source": [
        "Ahora, creamos 3 clientes con pequeños conjuntos de datos locales. Si está ejecutando este tutorial en Colab, quizá le resulte útil la característica \"reflejar celda en pestaña\" para fijar esta celda y su salida con el fin de interpretar/comprobar la salida de las funciones que se desarrollan a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr74BLPM1Mxa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word vocab\n",
            " 0 apple\n",
            " 1 orange\n",
            " 2 pear\n",
            " 3 kiwi\n",
            " 4 carrot\n",
            " 5 broccoli\n",
            " 6 arugula\n",
            " 7 peas\n",
            " 8 trout\n",
            " 9 tuna\n",
            "10 cod\n",
            "11 salmon\n",
            "\n",
            "Tag vocab\n",
            " 0 FRUIT\n",
            " 1 VEGETABLE\n",
            " 2 FISH\n"
          ]
        }
      ],
      "source": [
        "preprocess_fn = build_preprocess_fn(WORD_VOCAB, TAG_VOCAB)\n",
        "\n",
        "\n",
        "def make_dataset(raw):\n",
        "  d = tf.data.Dataset.from_tensor_slices(\n",
        "      # Matches the StackOverflow formatting\n",
        "      collections.OrderedDict(\n",
        "          tokens=tf.constant([t[0] for t in raw]),\n",
        "          tags=tf.constant([t[1] for t in raw]),\n",
        "          title=['' for _ in raw]))\n",
        "  d = preprocess_fn(d)\n",
        "  return d\n",
        "\n",
        "\n",
        "# 4 distinct tokens\n",
        "CLIENT1_DATASET = make_dataset([\n",
        "    ('apple orange apple orange', 'FRUIT'),\n",
        "    ('carrot trout', 'VEGETABLE|FISH'),\n",
        "    ('orange apple', 'FRUIT'),\n",
        "    ('orange', 'ORANGE|CITRUS')  # 2 OOV tag\n",
        "])\n",
        "\n",
        "# 6 distinct tokens\n",
        "CLIENT2_DATASET = make_dataset([\n",
        "    ('pear cod', 'FRUIT|FISH'),\n",
        "    ('arugula peas', 'VEGETABLE'),\n",
        "    ('kiwi pear', 'FRUIT'),\n",
        "    ('sturgeon', 'FISH'),  # OOV word\n",
        "    ('sturgeon bass', 'FISH')  # 2 OOV words\n",
        "])\n",
        "\n",
        "# A client with all possible words & tags (13 distinct tokens).\n",
        "# With MAX_TOKENS_SELECTED_PER_CLIENT = 6, we won't download the model\n",
        "# slices for all tokens that occur on this client.\n",
        "CLIENT3_DATASET = make_dataset([\n",
        "    (' '.join(WORD_VOCAB + ['oovword']), '|'.join(TAG_VOCAB)),\n",
        "    # Mathe the OOV token and 'salmon' occur in the largest number\n",
        "    # of examples on this client:\n",
        "    ('salmon oovword', 'FISH|OOVTAG')\n",
        "])\n",
        "\n",
        "print('Word vocab')\n",
        "for i, word in enumerate(WORD_VOCAB):\n",
        "  print(f'{i:2d} {word}')\n",
        "\n",
        "print('\\nTag vocab')\n",
        "for i, tag in enumerate(TAG_VOCAB):\n",
        "  print(f'{i:2d} {tag}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH3T6M0f8Bh2"
      },
      "source": [
        "Defina constantes para los números brutos de características de entrada (tokens/palabras) y etiquetas (etiquetas de publicación). Nuestros espacios de entrada/salida reales son `NUM_OOV_BUCKETS = 1` más grandes porque agregamos un token/etiqueta OOV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBULpAepxloO"
      },
      "outputs": [],
      "source": [
        "NUM_WORDS = len(WORD_VOCAB) \n",
        "NUM_TAGS = len(TAG_VOCAB)\n",
        "\n",
        "WORD_VOCAB_SIZE = NUM_WORDS + NUM_OOV_BUCKETS\n",
        "TAG_VOCAB_SIZE = NUM_TAGS + NUM_OOV_BUCKETS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zF6NF_f8PQR"
      },
      "source": [
        "Cree versiones por lotes de los conjuntos de datos y lotes individuales, que serán útiles para probar el código a medida que avanzamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uxRQAYdJISP"
      },
      "outputs": [],
      "source": [
        "batched_dataset1 = CLIENT1_DATASET.batch(2)\n",
        "batched_dataset2 = CLIENT2_DATASET.batch(3)\n",
        "batched_dataset3 = CLIENT3_DATASET.batch(2)\n",
        "\n",
        "batch1 = next(iter(batched_dataset1))\n",
        "batch2 = next(iter(batched_dataset2))\n",
        "batch3 = next(iter(batched_dataset3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBcObjtlaq8q"
      },
      "source": [
        "## Definición de un modelo con entradas dispersas\n",
        "\n",
        "Utilizamos un modelo de regresión logística independiente y simple para cada etiqueta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTtIkxCKao2Y"
      },
      "outputs": [],
      "source": [
        "def create_logistic_model(word_vocab_size: int, vocab_tags_size: int):\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(word_vocab_size,), sparse=True),\n",
        "      tf.keras.layers.Dense(\n",
        "          vocab_tags_size,\n",
        "          activation='sigmoid',\n",
        "          kernel_initializer=tf.keras.initializers.zeros,\n",
        "          # For simplicity, don't use a bias vector; this means the model\n",
        "          # is a single tensor, and we only need sparse aggregation of\n",
        "          # the per-token slices of the model. Generalizing to also handle\n",
        "          # other model weights that are fully updated \n",
        "          # (non-dense broadcast and aggregate) would be a good exercise.\n",
        "          use_bias=False),\n",
        "  ])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCMScjuWn9t-"
      },
      "source": [
        "Hagamos predicciones para asegurarnos de que funciona:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGQOnvbfa3w4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]]\n"
          ]
        }
      ],
      "source": [
        "model = create_logistic_model(WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\n",
        "p = model.predict(batch1.tokens)\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOT2veElT-ij"
      },
      "source": [
        "Y algún entrenamiento centralizado simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn3lB84WgRgV"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy())\n",
        "model.train_on_batch(batch1.tokens, batch1.tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1aahpVomC4G"
      },
      "source": [
        "# Bloques de creación para el cálculo federado\n",
        "\n",
        "Vamos a implementar una versión simple del algoritmo de [promediado federado](https://arxiv.org/abs/1602.05629) con la diferencia clave de que cada dispositivo solo descarga un subconjunto relevante del modelo y solo aporta actualizaciones a ese subconjunto.\n",
        "\n",
        "Usamos `M` como abreviatura de `MAX_TOKENS_SELECTED_PER_CLIENT`. A un nivel alto, una ronda de entrenamiento implica estos pasos:\n",
        "\n",
        "1. Cada cliente participante escanea su conjunto de datos local, a la vez que parsea las cadenas de entrada y las asigna a los tokens correctos (índices int). Esto requiere acceso al (enorme) diccionario global (esto podría evitarse si se usan técnicas de [hash de características](https://en.wikipedia.org/wiki/Feature_hashing)). A continuación, contamos de forma dispersa cuántas veces aparece cada token. Si aparecen `U` tokens únicos en el dispositivo, elegimos los tokens `num_actual_tokens = min(U, M)` más frecuentes para entrenar.\n",
        "\n",
        "2. Los clientes usan `federated_select` para recuperar los coeficientes del modelo para los tokens `num_actual_tokens` seleccionados del servidor. Cada segmento del modelo es un tensor de forma `(TAG_VOCAB_SIZE, )`, por lo que los datos totales que se transmiten al cliente tienen como máximo el tamaño `TAG_VOCAB_SIZE * M` (consulte la nota a continuación).\n",
        "\n",
        "3. Los clientes construyen una asignación `global_token -> local_token` donde el token local (índice int) es el índice del token global en la lista de tokens seleccionados.\n",
        "\n",
        "4. Los clientes usan una versión \"pequeña\" del modelo global que solo tiene coeficientes para como máximo `M` tokens, del rango `[0, num_actual_tokens)` . La asignación `global -> local` sirve para inicializar los parámetros densos de este modelo a partir de los sectores seleccionados del modelo.\n",
        "\n",
        "5. Los clientes entrenan su modelo local con SGD en datos preprocesados ​​con la asignación `global -> local`.\n",
        "\n",
        "6. Los clientes convierten los parámetros de su modelo local en actualizaciones `IndexedSlices` con ayuda de la asignación `local -> global` para indexar las filas. El servidor agrega estas actualizaciones mediante una agregación de suma dispersa.\n",
        "\n",
        "7. El servidor toma el resultado (denso) de la agregación anterior, lo divide por la cantidad de clientes participantes y aplica la actualización promedio resultante al modelo global.\n",
        "\n",
        "En esta sección, construimos los bloques de creación de estos pasos, que luego se combinarán en un `federated_computation` final que captura la lógica completa de una ronda de entrenamiento.\n",
        "\n",
        "> NOTA: La descripción anterior oculta un detalle técnico: tanto `federated_select` como la construcción del modelo local requieren formas estáticamente conocidas, por lo que no podemos usar el tamaño dinámico de `num_actual_tokens` por cliente. En vez de eso, usamos el valor estático `M` y agregamos amortiguado cuando sea necesario. Esto no afecta la semántica del algoritmo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrB3RWrCYHd_"
      },
      "source": [
        "### Cuente los tokens cliente y decida qué modelo se divide en `federated_select`\n",
        "\n",
        "Cada dispositivo debe decidir qué \"porciones\" del modelo son relevantes para su conjunto de datos de entrenamiento local. Para nuestro problema, hacemos esto contando (¡de forma dispersa!) cuántos ejemplos contiene cada token en el conjunto de datos de entrenamiento del cliente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRr0ip0ja31O"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def token_count_fn(token_counts, batch):\n",
        "  \"\"\"Adds counts from `batch` to the running `token_counts` sum.\"\"\"\n",
        "  # Sum across the batch dimension.\n",
        "  flat_tokens = tf.sparse.reduce_sum(\n",
        "      batch.tokens, axis=0, output_is_sparse=True)\n",
        "  flat_tokens = tf.cast(flat_tokens, dtype=TOKEN_COUNT_DTYPE)\n",
        "  return tf.sparse.add(token_counts, flat_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTZdZZhFgeYz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: [0 1 4 8]\n",
            "counts: [2 3 1 1]\n"
          ]
        }
      ],
      "source": [
        "# Simple tests\n",
        "# Create the initial zero token counts using empty tensors.\n",
        "initial_token_counts = tf.SparseTensor(\n",
        "    indices=tf.zeros(shape=(0, 1), dtype=TOKEN_DTYPE),\n",
        "    values=tf.zeros(shape=(0,), dtype=TOKEN_COUNT_DTYPE),\n",
        "    dense_shape=(WORD_VOCAB_SIZE,))\n",
        "\n",
        "client_token_counts = batched_dataset1.reduce(initial_token_counts,\n",
        "                                              token_count_fn)\n",
        "tokens = tf.reshape(client_token_counts.indices, (-1,)).numpy()\n",
        "print('tokens:', tokens)\n",
        "np.testing.assert_array_equal(tokens, [0, 1, 4, 8])\n",
        "# The count is the number of *examples* in which the token/word\n",
        "# occurs, not the total number of occurences, since we still featurize\n",
        "# multiple occurences in the same example as a \"1\".\n",
        "counts = client_token_counts.values.numpy()\n",
        "print('counts:', counts)\n",
        "np.testing.assert_array_equal(counts, [2, 3, 1, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ghfTtZgce7"
      },
      "source": [
        "Seleccionaremos los parámetros del modelo correspondientes a los tokens `MAX_TOKENS_SELECTED_PER_CLIENT` que aparecen con más frecuencia en el dispositivo. Si hay menos tokens de esta cantidad en el dispositivo, rellenamos la lista para permitir el uso de `federated_select`.\n",
        "\n",
        "Tenga en cuenta que es posible que otras estrategias sean mejores, por ejemplo, seleccionar tokens aleatoriamente (quizás en función de su probabilidad de aparición). Esto garantizaría que todos los sectores del modelo (para los cuales el cliente tiene datos) tengan alguna posibilidad de actualizarse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45YOfL8fh5K8"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def keys_for_client(client_dataset, max_tokens_per_client):\n",
        "  \"\"\"Computes a set of max_tokens_per_client keys.\"\"\"\n",
        "  initial_token_counts = tf.SparseTensor(\n",
        "      indices=tf.zeros((0, 1), dtype=TOKEN_DTYPE),\n",
        "      values=tf.zeros((0,), dtype=TOKEN_COUNT_DTYPE),\n",
        "      dense_shape=(WORD_VOCAB_SIZE,))\n",
        "  client_token_counts = client_dataset.reduce(initial_token_counts,\n",
        "                                              token_count_fn)\n",
        "  # Find the most-frequently occuring tokens\n",
        "  tokens = tf.reshape(client_token_counts.indices, shape=(-1,))\n",
        "  counts = client_token_counts.values\n",
        "  perm = tf.argsort(counts, direction='DESCENDING')\n",
        "  tokens = tf.gather(tokens, perm)\n",
        "  counts = tf.gather(counts, perm)\n",
        "  num_raw_tokens = tf.shape(tokens)[0]\n",
        "  actual_num_tokens = tf.minimum(max_tokens_per_client, num_raw_tokens)\n",
        "  selected_tokens = tokens[:actual_num_tokens]\n",
        "  paddings = [[0, max_tokens_per_client - tf.shape(selected_tokens)[0]]]\n",
        "  padded_tokens = tf.pad(selected_tokens, paddings=paddings)\n",
        "  # Make sure the type is statically determined\n",
        "  padded_tokens = tf.reshape(padded_tokens, shape=(max_tokens_per_client,))\n",
        "\n",
        "  # We will pass these tokens as keys into `federated_select`, which\n",
        "  # requires SELECT_KEY_DTYPE=tf.int32 keys.\n",
        "  padded_tokens = tf.cast(padded_tokens, dtype=SELECT_KEY_DTYPE)\n",
        "  return padded_tokens, actual_num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTyJVDqkjGyG"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "\n",
        "# Case 1: actual_num_tokens > max_tokens_per_client\n",
        "selected_tokens, actual_num_tokens = keys_for_client(batched_dataset1, 3)\n",
        "assert tf.size(selected_tokens) == 3\n",
        "assert actual_num_tokens == 3\n",
        "\n",
        "# Case 2: actual_num_tokens < max_tokens_per_client\n",
        "selected_tokens, actual_num_tokens = keys_for_client(batched_dataset1, 10)\n",
        "assert tf.size(selected_tokens) == 10\n",
        "assert actual_num_tokens == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO0pJ8D-dzp7"
      },
      "source": [
        "### Asignación de tokens globales a tokens locales\n",
        "\n",
        "La selección anterior nos brinda un conjunto denso de tokens en el rango `[0, actual_num_tokens)` que usaremos para el modelo en el dispositivo. Sin embargo, el conjunto de datos que leemos tiene tokens de un rango de vocabulario global mucho más amplio `[0, WORD_VOCAB_SIZE)`.\n",
        "\n",
        "Por lo tanto, necesitamos asignar los tokens globales a sus tokens locales correspondientes. Los identificadores de tokens locales simplemente vienen dados por los índices en el tensor `selected_tokens` calculado en el paso anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG3uLlqJohXg"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def map_to_local_token_ids(client_data, client_keys):\n",
        "  global_to_local = tf.lookup.StaticHashTable(\n",
        "      # Note int32 -> int64 maps are not supported\n",
        "      tf.lookup.KeyValueTensorInitializer(\n",
        "          keys=tf.cast(client_keys, dtype=TOKEN_DTYPE),\n",
        "          # Note we need to use tf.shape, not the static \n",
        "          # shape client_keys.shape[0]\n",
        "          values=tf.range(0, limit=tf.shape(client_keys)[0],\n",
        "                          dtype=TOKEN_DTYPE)),\n",
        "      # We use -1 for tokens that were not selected, which can occur for clients\n",
        "      # with more than MAX_TOKENS_SELECTED_PER_CLIENT distinct tokens.\n",
        "      # We will simply remove these invalid indices from the batch below.\n",
        "      default_value=-1)\n",
        "\n",
        "  def to_local_ids(sparse_tokens):\n",
        "    indices_t = tf.transpose(sparse_tokens.indices)\n",
        "    batch_indices = indices_t[0]  # First column\n",
        "    tokens = indices_t[1]  # Second column\n",
        "    tokens = tf.map_fn(\n",
        "        lambda global_token_id: global_to_local.lookup(global_token_id), tokens)\n",
        "    # Remove tokens that aren't actually available (looked up as -1):\n",
        "    available_tokens = tokens >= 0\n",
        "    tokens = tokens[available_tokens]\n",
        "    batch_indices = batch_indices[available_tokens]\n",
        "\n",
        "    updated_indices = tf.transpose(\n",
        "        tf.concat([[batch_indices], [tokens]], axis=0))\n",
        "    st = tf.sparse.SparseTensor(\n",
        "        updated_indices,\n",
        "        tf.ones(tf.size(tokens), dtype=FEATURE_DTYPE),\n",
        "        # Each client has at most MAX_TOKENS_SELECTED_PER_CLIENT distinct tokens.\n",
        "        dense_shape=[sparse_tokens.dense_shape[0], MAX_TOKENS_SELECTED_PER_CLIENT])\n",
        "    st = tf.sparse.reorder(st)\n",
        "    return st\n",
        "\n",
        "  return client_data.map(lambda b: BatchType(to_local_ids(b.tokens), b.tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Cn60NIBUWf"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "client_keys, actual_num_tokens = keys_for_client(\n",
        "    batched_dataset3, MAX_TOKENS_SELECTED_PER_CLIENT)\n",
        "client_keys = client_keys[:actual_num_tokens]\n",
        "\n",
        "d = map_to_local_token_ids(batched_dataset3, client_keys)\n",
        "batch  = next(iter(d))\n",
        "all_tokens = tf.gather(batch.tokens.indices, indices=1, axis=1)\n",
        "# Confirm we have local indices in the range [0, MAX):\n",
        "assert tf.math.reduce_max(all_tokens) < MAX_TOKENS_SELECTED_PER_CLIENT\n",
        "assert tf.math.reduce_max(all_tokens) >= 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nvZYfzga1yN"
      },
      "source": [
        "### Entrenamiento del (sub)modelo local en cada cliente\n",
        "\n",
        "Observe que `federated_select` devolverá los sectores seleccionados como `tf.data.Dataset` en el mismo orden que las claves de selección. Entonces, primero definimos una función de utilidad para tomar dicho conjunto de datos y convertirlo en un único tensor denso que pueda usarse como ponderaciones del modelo del cliente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFpC_nFjgbtI"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def slices_dataset_to_tensor(slices_dataset):\n",
        "  \"\"\"Convert a dataset of slices to a tensor.\"\"\"\n",
        "  # Use batching to gather all of the slices into a single tensor.\n",
        "  d = slices_dataset.batch(MAX_TOKENS_SELECTED_PER_CLIENT,\n",
        "                           drop_remainder=False)\n",
        "  iter_d = iter(d)\n",
        "  tensor = next(iter_d)\n",
        "  # Make sure we have consumed everything\n",
        "  opt = iter_d.get_next_as_optional()\n",
        "  tf.Assert(tf.logical_not(opt.has_value()), data=[''], name='CHECK_EMPTY')\n",
        "  return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX0gtz5Ylvqf"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "weights = np.random.random(\n",
        "    size=(MAX_TOKENS_SELECTED_PER_CLIENT, TAG_VOCAB_SIZE)).astype(np.float32)\n",
        "model_slices_as_dataset = tf.data.Dataset.from_tensor_slices(weights)\n",
        "weights2 = slices_dataset_to_tensor(model_slices_as_dataset)\n",
        "np.testing.assert_array_equal(weights, weights2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft0ROTYl0laV"
      },
      "source": [
        "Ahora tenemos todos los componentes que necesitamos para definir un ciclo de entrenamiento local simple que se ejecutará en cada cliente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG66d8UR_9Gm"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def client_train_fn(model, client_optimizer,\n",
        "                    model_slices_as_dataset, client_data,\n",
        "                    client_keys, actual_num_tokens):\n",
        "  \n",
        "  initial_model_weights = slices_dataset_to_tensor(model_slices_as_dataset)\n",
        "  assert len(model.trainable_variables) == 1\n",
        "  model.trainable_variables[0].assign(initial_model_weights)\n",
        "\n",
        "  # Only keep the \"real\" (unpadded) keys.\n",
        "  client_keys = client_keys[:actual_num_tokens]\n",
        " \n",
        "  client_data = map_to_local_token_ids(client_data, client_keys)\n",
        "\n",
        "  loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "  for features, labels in client_data:\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(features)\n",
        "      loss = loss_fn(labels, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    client_optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  model_weights_delta = model.trainable_weights[0] - initial_model_weights\n",
        "  model_weights_delta = tf.slice(model_weights_delta, begin=[0, 0], \n",
        "                           size=[actual_num_tokens, -1])\n",
        "  return client_keys, model_weights_delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XOe0thb2TQr"
      },
      "outputs": [],
      "source": [
        "# Simple test\n",
        "# Note if you execute this cell a second time, you need to also re-execute\n",
        "# the preceeding cell to avoid \"tf.function-decorated function tried to \n",
        "# create variables on non-first call\" errors.\n",
        "on_device_model = create_logistic_model(MAX_TOKENS_SELECTED_PER_CLIENT,\n",
        "                                        TAG_VOCAB_SIZE)\n",
        "client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "client_keys, actual_num_tokens = keys_for_client(\n",
        "    batched_dataset2, MAX_TOKENS_SELECTED_PER_CLIENT)\n",
        "\n",
        "model_slices_as_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    np.zeros((MAX_TOKENS_SELECTED_PER_CLIENT, TAG_VOCAB_SIZE),\n",
        "             dtype=np.float32))\n",
        "\n",
        "keys, delta = client_train_fn(\n",
        "    on_device_model,\n",
        "    client_optimizer,\n",
        "    model_slices_as_dataset,\n",
        "    client_data=batched_dataset3,\n",
        "    client_keys=client_keys,\n",
        "    actual_num_tokens=actual_num_tokens)\n",
        "\n",
        "print(delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJFTBczKSYH"
      },
      "source": [
        "### Agregación de IndexedSlices\n",
        "\n",
        "Usamos `tff.federated_aggregate` para construir una suma dispersa federada para `IndexedSlices`. Esta implementación simple tiene la restricción de que la `dense_shape` se conoce estáticamente de antemano. Tenga en cuenta también que esta suma es solo *semidispersa*, en el sentido de que la comunicación cliente -&gt; servidor es dispersa, pero el servidor mantiene una representación densa de la suma en `accumulate` y `merge`, y genera esta representación densa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIG_XYd2OYeO"
      },
      "outputs": [],
      "source": [
        "def federated_indexed_slices_sum(slice_indices, slice_values, dense_shape):\n",
        "  \"\"\"\n",
        "  Sums IndexedSlices@CLIENTS to a dense @SERVER Tensor.\n",
        "\n",
        "  Intermediate aggregation is performed by converting to a dense representation,\n",
        "  which may not be suitable for all applications.\n",
        "\n",
        "  Args:\n",
        "    slice_indices: An IndexedSlices.indices tensor @CLIENTS.\n",
        "    slice_values: An IndexedSlices.values tensor @CLIENTS.\n",
        "    dense_shape: A statically known dense shape.\n",
        "\n",
        "  Returns:\n",
        "    A dense tensor placed @SERVER representing the sum of the client's\n",
        "    IndexedSclies.\n",
        "  \"\"\"\n",
        "  slices_dtype = slice_values.type_signature.member.dtype\n",
        "  zero = tff.tf_computation(\n",
        "      lambda: tf.zeros(dense_shape, dtype=slices_dtype))()\n",
        "\n",
        "  @tf.function\n",
        "  def accumulate_slices(dense, client_value):\n",
        "    indices, slices = client_value\n",
        "    # There is no built-in way to add `IndexedSlices`, but \n",
        "    # tf.convert_to_tensor is a quick way to convert to a dense representation\n",
        "    # so we can add them.\n",
        "    return dense + tf.convert_to_tensor(\n",
        "        tf.IndexedSlices(slices, indices, dense_shape))\n",
        "\n",
        "\n",
        "  return tff.federated_aggregate(\n",
        "      (slice_indices, slice_values),\n",
        "      zero=zero,\n",
        "      accumulate=tff.tf_computation(accumulate_slices),\n",
        "      merge=tff.tf_computation(lambda d1, d2: tf.add(d1, d2, name='merge')),\n",
        "      report=tff.tf_computation(lambda d: d))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "769qwlLhKFA9"
      },
      "source": [
        "Construya un `federated_computation` mínimo como prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_FIvwj3UIAq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({<int64[?],float32[?,2]>}@CLIENTS -> float32[6,2]@SERVER)\n"
          ]
        }
      ],
      "source": [
        "dense_shape = (6, 2)\n",
        "indices_type = tff.TensorType(tf.int64, (None,))\n",
        "values_type = tff.TensorType(tf.float32, (None, 2))\n",
        "client_slice_type = tff.type_at_clients(\n",
        "    (indices_type, values_type))\n",
        "\n",
        "@tff.federated_computation(client_slice_type)\n",
        "def test_sum_indexed_slices(indices_values_at_client):\n",
        "  indices, values = indices_values_at_client\n",
        "  return federated_indexed_slices_sum(indices, values, dense_shape)\n",
        "\n",
        "print(test_sum_indexed_slices.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dulcCZg2S_9f"
      },
      "outputs": [],
      "source": [
        "x = tf.IndexedSlices(\n",
        "    values=np.array([[2., 2.1], [0., 0.1], [1., 1.1], [5., 5.1]],\n",
        "                    dtype=np.float32),\n",
        "    indices=[2, 0, 1, 5],\n",
        "    dense_shape=dense_shape)\n",
        "y = tf.IndexedSlices(\n",
        "    values=np.array([[0., 0.3], [3.1, 3.2]], dtype=np.float32),\n",
        "    indices=[1, 3],\n",
        "    dense_shape=dense_shape)\n",
        "\n",
        "# Sum one.\n",
        "result = test_sum_indexed_slices([(x.indices, x.values)])\n",
        "np.testing.assert_array_equal(tf.convert_to_tensor(x), result)\n",
        "\n",
        "# Sum two.\n",
        "expected = [[0., 0.1], [1., 1.4], [2., 2.1], [3.1, 3.2], [0., 0.], [5., 5.1]]\n",
        "result = test_sum_indexed_slices([(x.indices, x.values), (y.indices, y.values)])\n",
        "np.testing.assert_array_almost_equal(expected, result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sXg8fDQhwB7"
      },
      "source": [
        "# Todo se combina en `federated_computation`\n",
        "\n",
        "Ahora usamos TFF para unir los componentes en un `tff.federated_computation`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yRpGJr_i3XK"
      },
      "outputs": [],
      "source": [
        "DENSE_MODEL_SHAPE = (WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\n",
        "client_data_type = tff.SequenceType(batched_dataset1.element_spec)\n",
        "model_type = tff.TensorType(tf.float32, shape=DENSE_MODEL_SHAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITawWBbodTdE"
      },
      "source": [
        "Usamos una función básica de entrenamiento del servidor basada en el promediado federado, aplicando la actualización con una tasa de aprendizaje del servidor de 1,0. Es importante que apliquemos una actualización (delta) al modelo, en lugar de simplemente promediar los modelos que proporciona el cliente, ya que, de lo contrario, si ningún cliente entrena una porción determinada del modelo en una ronda determinada, sus coeficientes podrían reducirse a cero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGgKTpnscdZI"
      },
      "outputs": [],
      "source": [
        "@tff.tf_computation\n",
        "def server_update(current_model_weights, update_sum, num_clients):\n",
        "  average_update = update_sum / num_clients\n",
        "  return current_model_weights + average_update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSFAmgzqBRHA"
      },
      "source": [
        "Necesitamos un par de componentes `tff.tf_computation` más:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soFQAmktUtYp"
      },
      "outputs": [],
      "source": [
        "# Function to select slices from the model weights in federated_select:\n",
        "select_fn = tff.tf_computation(\n",
        "    lambda model_weights, index: tf.gather(model_weights, index))\n",
        "\n",
        "\n",
        "# We need to wrap `client_train_fn` as a `tff.tf_computation`, making\n",
        "# sure we do any operations that might construct `tf.Variable`s outside\n",
        "# of the `tf.function` we are wrapping.\n",
        "@tff.tf_computation\n",
        "def client_train_fn_tff(model_slices_as_dataset, client_data, client_keys,\n",
        "                        actual_num_tokens):\n",
        "  # Note this is amaller than the global model, using\n",
        "  # MAX_TOKENS_SELECTED_PER_CLIENT which is much smaller than WORD_VOCAB_SIZE.\n",
        "  # We would like a model of size `actual_num_tokens`, but we\n",
        "  # can't build the model dynamically, so we will slice off the padded\n",
        "  # weights at the end.\n",
        "  client_model = create_logistic_model(MAX_TOKENS_SELECTED_PER_CLIENT,\n",
        "                                       TAG_VOCAB_SIZE)\n",
        "  client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "  return client_train_fn(client_model, client_optimizer,\n",
        "                         model_slices_as_dataset, client_data, client_keys,\n",
        "                         actual_num_tokens)\n",
        "\n",
        "@tff.tf_computation\n",
        "def keys_for_client_tff(client_data):\n",
        "  return keys_for_client(client_data, MAX_TOKENS_SELECTED_PER_CLIENT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftI2fCGNBJQ-"
      },
      "source": [
        "¡Ya estamos listos para armar todo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM_2mzkgBOjl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<server_model=float32[13,4]@SERVER,client_data={<tokens=<indices=int64[?,2],values=int32[?],dense_shape=int64[2]>,tags=float32[?,4]>*}@CLIENTS> -> float32[13,4]@SERVER)\n"
          ]
        }
      ],
      "source": [
        "@tff.federated_computation(\n",
        "    tff.type_at_server(model_type), tff.type_at_clients(client_data_type))\n",
        "def sparse_model_update(server_model, client_data):\n",
        "  max_tokens = tff.federated_value(MAX_TOKENS_SELECTED_PER_CLIENT, tff.SERVER)\n",
        "  keys_at_clients, actual_num_tokens = tff.federated_map(\n",
        "      keys_for_client_tff, client_data)\n",
        "\n",
        "  model_slices = tff.federated_select(keys_at_clients, max_tokens, server_model,\n",
        "                                      select_fn)\n",
        "\n",
        "  update_keys, update_slices = tff.federated_map(\n",
        "      client_train_fn_tff,\n",
        "      (model_slices, client_data, keys_at_clients, actual_num_tokens))\n",
        "\n",
        "  dense_update_sum = federated_indexed_slices_sum(update_keys, update_slices,\n",
        "                                                  DENSE_MODEL_SHAPE)\n",
        "  num_clients = tff.federated_sum(tff.federated_value(1.0, tff.CLIENTS))\n",
        "\n",
        "  updated_server_model = tff.federated_map(\n",
        "      server_update, (server_model, dense_update_sum, num_clients))\n",
        "\n",
        "  return updated_server_model\n",
        "\n",
        "\n",
        "print(sparse_model_update.type_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSvveGwdZFCO"
      },
      "source": [
        "# ¡Entrenemos un modelo!\n",
        "\n",
        "Ahora que tenemos nuestra función de entrenamiento, probémosla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn_V-E3FdrLO"
      },
      "outputs": [],
      "source": [
        "server_model = create_logistic_model(WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\n",
        "server_model.compile(  # Compile to make evaluation easy.\n",
        "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.0),  # Unused\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[ \n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.AUC(name='auc'),\n",
        "      tf.keras.metrics.Recall(top_k=2, name='recall_at_2'),\n",
        "  ])\n",
        "\n",
        "def evaluate(model, dataset, name):\n",
        "  metrics = model.evaluate(dataset, verbose=0)\n",
        "  metrics_str = ', '.join([f'{k}={v:.2f}' for k, v in \n",
        "                          (zip(server_model.metrics_names, metrics))])\n",
        "  print(f'{name}: {metrics_str}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw7S9PmfZ3Bw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before training\n",
            "Client 1: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.60\n",
            "Client 2: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.50\n",
            "Client 3: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.40\n",
            "Training on clients [0 1]\n",
            "Training on clients [0 2 1]\n",
            "Training on clients [2 0]\n",
            "Training on clients [1 0 2]\n",
            "Training on clients [2]\n",
            "Training on clients [2 0]\n",
            "Training on clients [1 2 0]\n",
            "Training on clients [0]\n",
            "Training on clients [2]\n",
            "Training on clients [1 2]\n",
            "After training\n",
            "Client 1: loss=0.67, precision=0.80, auc=0.91, recall_at_2=0.80\n",
            "Client 2: loss=0.68, precision=0.67, auc=0.96, recall_at_2=1.00\n",
            "Client 3: loss=0.65, precision=1.00, auc=0.93, recall_at_2=0.80\n"
          ]
        }
      ],
      "source": [
        "print('Before training')\n",
        "evaluate(server_model, batched_dataset1, 'Client 1')\n",
        "evaluate(server_model, batched_dataset2, 'Client 2')\n",
        "evaluate(server_model, batched_dataset3, 'Client 3')\n",
        "\n",
        "model_weights = server_model.trainable_weights[0]\n",
        "\n",
        "client_datasets = [batched_dataset1, batched_dataset2, batched_dataset3]\n",
        "for _ in range(10):  # Run 10 rounds of FedAvg\n",
        "  # We train on 1, 2, or 3 clients per round, selecting\n",
        "  # randomly.\n",
        "  cohort_size = np.random.randint(1, 4)\n",
        "  clients = np.random.choice([0, 1, 2], cohort_size, replace=False)\n",
        "  print('Training on clients', clients)\n",
        "  model_weights = sparse_model_update(\n",
        "      model_weights, [client_datasets[i] for i in clients])\n",
        "server_model.set_weights([model_weights])\n",
        "\n",
        "print('After training')\n",
        "evaluate(server_model, batched_dataset1, 'Client 1')\n",
        "evaluate(server_model, batched_dataset2, 'Client 2')\n",
        "evaluate(server_model, batched_dataset3, 'Client 3')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sparse_federated_learning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
