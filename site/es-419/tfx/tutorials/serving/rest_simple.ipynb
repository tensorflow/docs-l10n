{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhoQ0WE77laV"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Entrenamiento y servicio de un modelo de TensorFlow con TensorFlow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6FwTNtl3S4v"
      },
      "source": [
        "**Advertencia: Este bloc de notas fue diseñado para ejecutarse únicamente en Google Colab**. Instala paquetes en el sistema y requiere acceso de raíz. Si desea ejecutarlo en un bloc de notas Jupyter local, hágalo con cuidado.\n",
        "\n",
        "Nota: Puede ejecutar este ejemplo ahora mismo en un bloc de notas estilo Jupyter, ¡no es necesario configurarlo! Simplemente haga clic en \"Ejecutar en Google Colab\"\n",
        "\n",
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<tr>\n",
        "<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/serving/rest_simple\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tfx/tutorials/serving/rest_simple.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tfx/tutorials/serving/rest_simple.ipynb\"><img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a></td>\n",
        "<td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tfx/tutorials/serving/rest_simple.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</tr>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbVhjPpzn6BM"
      },
      "source": [
        "Esta guía entrena un modelo de red neuronal para clasificar [imágenes de indumentaria, como calzado deportivo y camisas](https://github.com/zalandoresearch/fashion-mnist), guarda el modelo entrenado y luego lo entrega con [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving). La atención se centra en TensorFlow Serving, en lugar del modelado y el entrenamiento en TensorFlow, por lo que para ver un ejemplo completo que se centre en el modelado y el entrenamiento, debe consultar el [ejemplo de clasificación básica](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/keras/basic_classification.ipynb).\n",
        "\n",
        "Esta guía usa [tf.keras](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/keras.ipynb), una API de alto nivel que se usa para compilar y entrenar modelos en TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWkuJabJSKGB"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Confirm that we're using Python 3\n",
        "assert sys.version_info.major == 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "# TensorFlow and tf.keras\n",
        "print(\"Installing dependencies for Colab environment\")\n",
        "!pip install -Uq grpcio==1.26.0\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "print('TensorFlow version: {}'.format(tf.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jAk1ZXqTJqN"
      },
      "source": [
        "## Cómo crear un modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "### Cómo importar el conjunto de datos Fashion MNIST\n",
        "\n",
        "Esta guía usa el conjunto de datos [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) que contiene 70 000 imágenes en escala de grises en 10 categorías. Las imágenes muestran artículos individuales de prendas de vestir en baja resolución (28 x 28 píxeles), como se puede ver a continuación:\n",
        "\n",
        "<table>\n",
        "  <tr><td>     <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\" width=\"600\" alt=\"Fashion MNIST sprite\">\n",
        "</td></tr>\n",
        "  <tr><td align=\"center\">     <b>Figura 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Muestras de Fashion-MNIST</a> (de Zalando, licencia MIT).<br> </td></tr>\n",
        "</table>\n",
        "\n",
        "Fashion MNIST fue pensado como reemplazo directo del clásico conjunto de datos [MNIST](http://yann.lecun.com/exdb/mnist/), a menudo usado como el \"Hola, mundo\" de los programas de aprendizaje automático para visión artificial. Puede acceder a Fashion MNIST directamente desde TensorFlow, simplemente importe y cargue los datos.\n",
        "\n",
        "Nota: Aunque en realidad se trata de imágenes, se cargan como arreglos NumPy y no como objetos binarios de imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MqDQO0KCaWS"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# scale the values to 0.0 to 1.0\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# reshape for feeding into the model\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
        "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDu7OX8Nf5PY"
      },
      "source": [
        "### Cómo entrenar y evaluar el modelo\n",
        "\n",
        "Usemos la CNN más simple posible, ya que no nos centramos en la parte del modelado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTNN0ANGgA36"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
        "                      strides=2, activation='relu', name='Conv1'),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10, name='Dense')\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "testing = False\n",
        "epochs = 5\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "model.fit(train_images, train_labels, epochs=epochs)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('\\nTest accuracy: {}'.format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwGPItyphqXT"
      },
      "source": [
        "## Cómo guardar el modelo\n",
        "\n",
        "Para cargar nuestro modelo entrenado en TensorFlow Serving, primero debemos guardarlo en formato [SavedModel](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model). Esto generará un archivo protobuf en una jerarquía de directorios bien definida e incluirá un número de versión. [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) nos permite seleccionar qué versión de un modelo o \"servible\" queremos usar cuando enviamos solicitudes de inferencia. Cada versión se exportará a un subdirectorio diferente en la ruta indicada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w5Rq8SsgWE6"
      },
      "outputs": [],
      "source": [
        "# Fetch the Keras session and save the model\n",
        "# The signature definition is defined by the input and output tensors,\n",
        "# and stored with the default serving key\n",
        "import tempfile\n",
        "\n",
        "MODEL_DIR = tempfile.gettempdir()\n",
        "version = 1\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "print('export_path = {}\\n'.format(export_path))\n",
        "\n",
        "tf.keras.models.save_model(\n",
        "    model,\n",
        "    export_path,\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        ")\n",
        "\n",
        "print('\\nSaved model:')\n",
        "!ls -l {export_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM7B_RuDYoIj"
      },
      "source": [
        "## Cómo examinar el modelo guardado\n",
        "\n",
        "Usaremos la utilidad de línea de comando `saved_model_cli` para ver [MetaGraphDefs](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef) (los modelos) y [SignatureDefs](https://www.tensorflow.org/tfx/serving/signature_defs) (los métodos que puede llamar) en nuestro SavedModel. Consulte [esta discusión sobre la CLI de SavedModel](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel) en la Guía de TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU4GDF_aYtfQ"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir {export_path} --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSPWuegUb7Eo"
      },
      "source": [
        "¡Eso nos dice mucho sobre nuestro modelo! En este caso, simplemente entrenamos nuestro modelo, por lo que ya conocemos las entradas y salidas, pero si no lo hiciéramos, esta sería información importante. No nos dice todo, como el hecho de que se trata de datos de imágenes en escala de grises, por ejemplo, pero es un gran comienzo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBgsyhytS6KD"
      },
      "source": [
        "## Cómo servir el modelo con TensorFlow Serving\n",
        "\n",
        "**Advertencia: Si NO está ejecutando esto en Google Colab,** las siguientes celdas instalarán paquetes en el sistema con acceso de raíz. Si desea ejecutarlo en un bloc de notas de Jupyter local, hágalo con cuidado.\n",
        "\n",
        "### Cómo agregar el URI de distribución de TensorFlow Serving como fuente del paquete:\n",
        "\n",
        "Nos estamos preparando para instalar TensorFlow Serving con [Aptitude](https://wiki.debian.org/Aptitude) ya que este Colab se ejecuta en un entorno Debian. Agregaremos el paquete `tensorflow-model-server` a la lista de paquetes que Aptitude conoce. Tenga en cuenta que estamos ejecutando como raíz.\n",
        "\n",
        "Nota: Este ejemplo ejecuta TensorFlow Serving de forma nativa, pero [también puede ejecutarlo en un contenedor Docker](https://www.tensorflow.org/tfx/serving/docker), que es una de las formas más sencillas de comenzar a usar TensorFlow Serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2hF_ChoOrEd"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# We need sudo prefix if not on a Google Colab.\n",
        "if 'google.colab' not in sys.modules:\n",
        "  SUDO_IF_NEEDED = 'sudo'\n",
        "else:\n",
        "  SUDO_IF_NEEDED = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWg9X2QHlbGS"
      },
      "outputs": [],
      "source": [
        "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
        "# You would instead do:\n",
        "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
        "\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | {SUDO_IF_NEEDED} tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | {SUDO_IF_NEEDED} apt-key add -\n",
        "!{SUDO_IF_NEEDED} apt update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1ZVp_VOU7Wu"
      },
      "source": [
        "### Cómo instalar TensorFlow Serving\n",
        "\n",
        "Esto es todo lo que necesita: ¡una línea de comando!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygwa9AgRloYy"
      },
      "outputs": [],
      "source": [
        "# TODO: Use the latest model server version when colab supports it.\n",
        "#!{SUDO_IF_NEEDED} apt-get install tensorflow-model-server\n",
        "# We need to install Tensorflow Model server 2.8 instead of latest version\n",
        "# Tensorflow Serving >2.9.0 required `GLIBC_2.29` and `GLIBCXX_3.4.26`. Currently colab environment doesn't support latest version of`GLIBC`,so workaround is to use specific version of Tensorflow Serving `2.8.0` to mitigate issue.\n",
        "!wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-2.8.0/t/tensorflow-model-server/tensorflow-model-server_2.8.0_all.deb'\n",
        "!dpkg -i tensorflow-model-server_2.8.0_all.deb\n",
        "!pip3 install tensorflow-serving-api==2.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5NrYdQeVm52"
      },
      "source": [
        "### Cómo empezar a ejecutar TensorFlow Serving\n",
        "\n",
        "Aquí es donde comenzamos a ejecutar TensorFlow Serving y cargamos nuestro modelo. Después de que se cargue, podemos comenzar a enviar solicitudes de inferencia usando REST. Hay algunos parámetros importantes:\n",
        "\n",
        "- `rest_api_port`: el puerto que se usará para las solicitudes REST.\n",
        "- `model_name`: lo usará en la URL de las solicitudes REST. Puede ser cualquier cosa.\n",
        "- `model_base_path`: esta es la ruta al directorio donde se guardó el modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUgp3vUdU5GS"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJDhHNJVnaLN"
      },
      "outputs": [],
      "source": [
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=fashion_model \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxbeiOCUUs2z"
      },
      "outputs": [],
      "source": [
        "!tail server.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwg1JKaGXWAg"
      },
      "source": [
        "## Cómo hacer una solicitud a su modelo en TensorFlow Serving\n",
        "\n",
        "Primero, veamos un ejemplo aleatorio de nuestros datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luqm_Jyff9iR"
      },
      "outputs": [],
      "source": [
        "def show(idx, title):\n",
        "  plt.figure()\n",
        "  plt.imshow(test_images[idx].reshape(28,28))\n",
        "  plt.axis('off')\n",
        "  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})\n",
        "\n",
        "import random\n",
        "rando = random.randint(0,len(test_images)-1)\n",
        "show(rando, 'An Example Image: {}'.format(class_names[test_labels[rando]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKnEHeTrbh3L"
      },
      "source": [
        "Bien, eso se ve interesante. ¿Qué tan difícil resulta reconocerlo? Ahora creemos el objeto JSON para un lote de tres solicitudes de inferencia y veamos qué tan bien reconoce las cosas nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dsD7KQG1m-R"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()})\n",
        "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReQd4QESIwXN"
      },
      "source": [
        "### Cómo hacer solicitudes REST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT3J-lHrhOYQ"
      },
      "source": [
        "#### La versión más nueva del servible.\n",
        "\n",
        "Enviaremos una solicitud de predicción como POST al punto de conexión REST de nuestro servidor y le pasaremos tres ejemplos. Le pediremos a nuestro servidor que nos proporcione la última versión de nuestro servible sin especificar una versión en particular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGvFyuIzW6n6"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "!pip install -q requests\n",
        "\n",
        "import requests\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://localhost:8501/v1/models/fashion_model:predict', data=data, headers=headers)\n",
        "predictions = json.loads(json_response.text)['predictions']\n",
        "\n",
        "show(0, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
        "  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[0]], test_labels[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJH8LtM4XELp"
      },
      "source": [
        "#### Una versión específica del servible.\n",
        "\n",
        "Ahora definamos una versión específica de nuestro servible. Como solo tenemos uno, seleccionemos la versión 1. También veremos los tres resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRftRxeR1tZx"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://localhost:8501/v1/models/fashion_model/versions/1:predict', data=data, headers=headers)\n",
        "predictions = json.loads(json_response.text)['predictions']\n",
        "\n",
        "for i in range(0,3):\n",
        "  show(i, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
        "    class_names[np.argmax(predictions[i])], np.argmax(predictions[i]), class_names[test_labels[i]], test_labels[i]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rest_simple.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
