{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjUA6S30k52h"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SpNWyqewk8fE"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x1ypzczQCwy"
      },
      "source": [
        "# Validación de datos mediante canalización de TFX y TensorFlow Data Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU9YYythm0dx"
      },
      "source": [
        "Nota: Recomendamos ejecutar este tutorial en un bloc de notas de Colab, ¡no es necesario configurarlo! Simplemente haga clic en \"Ejecutar en Google Colab\".\n",
        "\n",
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tfx/tutorials/tfx/penguin_tfdv.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "<td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tfx/tutorials/tfx/penguin_tfdv.ipynb\"><img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "<td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tfx/tutorials/tfx/penguin_tfdv.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VuwrlnvQJ5k"
      },
      "source": [
        "En este tutorial basado en un bloc de notas, crearemos y ejecutaremos canalizaciones de TFX para validar los datos de entrada y crear un modelo de ML. Este bloc de notas se basa en la canalización de TFX que creamos en el [Tutorial de canalizaciones simples de TFX](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple). Si aún no ha leído ese tutorial, debe leerlo antes de continuar con este bloc de notas.\n",
        "\n",
        "La primera tarea en cualquier proyecto de ciencia de datos o ML es comprender y limpiar los datos, lo que incluye lo siguiente:\n",
        "\n",
        "- Comprender los tipos de datos, las distribuciones y otra información (p. ej., valor medio o número de valores únicos) sobre cada característica.\n",
        "- Generar un esquema preliminar que describa los datos.\n",
        "- Identificar anomalías y valores faltantes en los datos con respecto a un esquema dado.\n",
        "\n",
        "En este tutorial, crearemos dos canalizaciones de TFX.\n",
        "\n",
        "Primero, crearemos una canalización para analizar el conjunto de datos y generar un esquema preliminar del conjunto de datos dado. Este proceso incluirá dos nuevos componentes, `StatisticsGen` y `SchemaGen`.\n",
        "\n",
        "Una vez que tengamos un esquema adecuado de los datos, crearemos una canalización para entrenar un modelo de clasificación de ML basado en la canalización del tutorial anterior. En este canal, usaremos el esquema del primer canal y un nuevo componente, `ExampleValidator`, para validar los datos de entrada.\n",
        "\n",
        "Los tres nuevos componentes, StatisticsGen, SchemaGen y ExampleValidator, son componentes de TFX que se usan para análisis y validación de datos y se implementan a través de la biblioteca [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv).\n",
        "\n",
        "Consulte [Explicación de las canalizaciones de TFX](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) para obtener más información sobre varios conceptos en TFX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmgi8ZvQkScg"
      },
      "source": [
        "## Preparación\n",
        "\n",
        "Primero tenemos que instalar el paquete de Python para TFX y descargar el conjunto de datos que usaremos para nuestro modelo.\n",
        "\n",
        "### Actualización de pip\n",
        "\n",
        "Para evitar actualizar Pip en un sistema cuando se ejecuta localmente, verifique que se esté ejecutando en Colab. Por supuesto, los sistemas locales se pueden actualizar por separado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as4OTe2ukSqm"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZOYTt1RW4TK"
      },
      "source": [
        "### Instalación de TFX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyQtljP-qPHY"
      },
      "outputs": [],
      "source": [
        "!pip install -U tfx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT8fA7f6_OST"
      },
      "source": [
        "### Desinstalación de shapely\n",
        "\n",
        "TODO(b/263441833) Esta es una solución temporal para evitar un ImportError. En última instancia, debería solucionarse admitiendo una versión reciente de Bigquery, en lugar de desinstalar otras dependencias adicionales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NxAIvvg_V-8"
      },
      "outputs": [],
      "source": [
        "!pip uninstall shapely -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwT0nov5QO1M"
      },
      "source": [
        "### ¿Reinició el tiempo de ejecución?\n",
        "\n",
        "Si está usando Google Colab, la primera vez que ejecute la celda anterior, debe hacer clic en el botón \"REINICIAR TIEMPO DE EJECUCIÓN\" o usar el menú \"Tiempo de ejecución &gt; Reiniciar tiempo de ejecución ...\" para reiniciar el tiempo de ejecución. Esto se debe a la forma en que Colab carga los paquetes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDnPgN8UJtzN"
      },
      "source": [
        "Verifique las versiones de TensorFlow y TFX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jh7vKSRqPHb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDtLdSkvqPHe"
      },
      "source": [
        "### Configuración de variables\n",
        "\n",
        "Hay algunas variables que se utilizan para definir una canalización. Puede personalizar estas variables como desee. De forma predeterminada, todas las salidas de la canalización se generarán en el directorio actual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcUseqJaE2XN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# We will create two pipelines. One for schema generation and one for training.\n",
        "SCHEMA_PIPELINE_NAME = \"penguin-tfdv-schema\"\n",
        "PIPELINE_NAME = \"penguin-tfdv\"\n",
        "\n",
        "# Output directory to store artifacts generated from the pipeline.\n",
        "SCHEMA_PIPELINE_ROOT = os.path.join('pipelines', SCHEMA_PIPELINE_NAME)\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
        "# Path to a SQLite DB file to use as an MLMD storage.\n",
        "SCHEMA_METADATA_PATH = os.path.join('metadata', SCHEMA_PIPELINE_NAME,\n",
        "                                    'metadata.db')\n",
        "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
        "\n",
        "# Output directory where created models from the pipeline will be exported.\n",
        "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
        "\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.INFO)  # Set default logging level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsO0l5F3dzOr"
      },
      "source": [
        "### Preparación de datos de ejemplo\n",
        "\n",
        "Descargaremos el conjunto de datos de ejemplo para usarlo en nuestra canalización de TFX. El conjunto de datos que usamos es el [conjunto de datos Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/articles/intro.html), que también se usa en otros [ejemplos de TFX](https://github.com/tensorflow/tfx/tree/master/tfx/examples/penguin).\n",
        "\n",
        "Este conjunto de datos tiene cuatro características numéricas:\n",
        "\n",
        "- culmen_length_mm\n",
        "- culmen_depth_mm\n",
        "- flipper_length_mm\n",
        "- body_mass_g\n",
        "\n",
        "Todas las características ya estaban normalizadas en un rango [0,1]. Compilaremos un modelo de clasificación que prediga las `species` de pingüinos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjE8MkZidzO0"
      },
      "source": [
        "Debido a que TFX ExampleGen lee entradas de un directorio, tenemos que crear un directorio y copiar el conjunto de datos en él."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSfs6qFgdzO1"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import tempfile\n",
        "\n",
        "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
        "_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n",
        "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
        "urllib.request.urlretrieve(_data_url, _data_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5s3wGpndzO1"
      },
      "source": [
        "Veamos rápidamente al archivo CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLn9ith2dzO1"
      },
      "outputs": [],
      "source": [
        "!head {_data_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8EOfCy1dzO2"
      },
      "source": [
        "Debería poder ver cinco columnas de características. `species` es una de 0, 1 o 2, y todas las demás características deben tener valores entre 0 y 1. Crearemos una canalización de TFX para analizar este conjunto de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePhfeYv0fVu1"
      },
      "source": [
        "## Generación de un esquema preliminar\n",
        "\n",
        "Las canalizaciones de TFX se definen mediante las API de Python. Crearemos una canalización para generar un esquema a partir de los ejemplos de entrada automáticamente. Puede revisar este esquema y ajustarlo para que se adapte a sus necesidades. Una vez finalizado el esquema, se puede usar para entrenamiento y validación de ejemplos en tareas posteriores.\n",
        "\n",
        "Además de `CsvExampleGen` que se usa en el [Tutorial de canalizaciones simples de TFX](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple), usaremos `StatisticsGen` y `SchemaGen`:\n",
        "\n",
        "- [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) calcula estadísticas para el conjunto de datos.\n",
        "- [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) examina las estadísticas y crea un esquema de datos inicial.\n",
        "\n",
        "Consulte las guías de cada componente o el [tutorial de componentes de TFX](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras) para obtener más información sobre estos componentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUFq55kCgwsm"
      },
      "source": [
        "### Cómo escribir una definición de canalización\n",
        "\n",
        "Definimos una función para crear una canalización de TFX. Un objeto `Pipeline` representa una canalización de TFX que se puede ejecutar con uno de los sistemas de orquestación de canalizaciones que admite TFX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfQ6FAk9gxJ2"
      },
      "outputs": [],
      "source": [
        "def _create_schema_pipeline(pipeline_name: str,\n",
        "                            pipeline_root: str,\n",
        "                            data_root: str,\n",
        "                            metadata_path: str) -> tfx.dsl.Pipeline:\n",
        "  \"\"\"Creates a pipeline for schema generation.\"\"\"\n",
        "  # Brings data into the pipeline.\n",
        "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
        "\n",
        "  # NEW: Computes statistics over data for visualization and schema generation.\n",
        "  statistics_gen = tfx.components.StatisticsGen(\n",
        "      examples=example_gen.outputs['examples'])\n",
        "\n",
        "  # NEW: Generates schema based on the generated statistics.\n",
        "  schema_gen = tfx.components.SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
        "\n",
        "  components = [\n",
        "      example_gen,\n",
        "      statistics_gen,\n",
        "      schema_gen,\n",
        "  ]\n",
        "\n",
        "  return tfx.dsl.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      metadata_connection_config=tfx.orchestration.metadata\n",
        "      .sqlite_metadata_connection_config(metadata_path),\n",
        "      components=components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuKFLI_Og2xr"
      },
      "source": [
        "### Cómo ejecutar la canalización\n",
        "\n",
        "Usaremos `LocalDagRunner` como en el tutorial anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQspf0ajg9AO"
      },
      "outputs": [],
      "source": [
        "tfx.orchestration.LocalDagRunner().run(\n",
        "  _create_schema_pipeline(\n",
        "      pipeline_name=SCHEMA_PIPELINE_NAME,\n",
        "      pipeline_root=SCHEMA_PIPELINE_ROOT,\n",
        "      data_root=DATA_ROOT,\n",
        "      metadata_path=SCHEMA_METADATA_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD4LsLHBi2O4"
      },
      "source": [
        "Debería ver el mensaje \"INFO:absl:Component SchemaGen is finished\" (INFO:absl:Component SchemaGen finalizó) si la canalización se completó correctamente.\n",
        "\n",
        "Examinaremos la salida de la canalización para comprender el conjunto de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWpckstgg9Zs"
      },
      "source": [
        "### Revisión de la salida de la canalización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL1wWoDh5wkj"
      },
      "source": [
        "Como se explicó en el tutorial anterior, una canalización de TFX produce dos tipos de salidas, artefactos y una [base de datos de metadatos (MLMD)](https://www.tensorflow.org/tfx/guide/mlmd) que contiene metadatos de artefactos y ejecuciones de canalizaciones. Definimos la ubicación de estas salidas en las celdas anteriores. De forma predeterminada, los artefactos se almacenan en el directorio `pipelines` y los metadatos se almacenan como una base de datos sqlite en el directorio `metadata`.\n",
        "\n",
        "Puede utilizar las API de MLMD para localizar estas salidas mediante programación. Primero, definiremos algunas funciones de utilidad para buscar artefactos de salida que se acaban de producir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0i_jTvOI8mv"
      },
      "outputs": [],
      "source": [
        "from ml_metadata.proto import metadata_store_pb2\n",
        "# Non-public APIs, just for showcase.\n",
        "from tfx.orchestration.portable.mlmd import execution_lib\n",
        "\n",
        "# TODO(b/171447278): Move these functions into the TFX library.\n",
        "\n",
        "def get_latest_artifacts(metadata, pipeline_name, component_id):\n",
        "  \"\"\"Output artifacts of the latest run of the component.\"\"\"\n",
        "  context = metadata.store.get_context_by_type_and_name(\n",
        "      'node', f'{pipeline_name}.{component_id}')\n",
        "  executions = metadata.store.get_executions_by_context(context.id)\n",
        "  latest_execution = max(executions,\n",
        "                         key=lambda e:e.last_update_time_since_epoch)\n",
        "  return execution_lib.get_output_artifacts(metadata, latest_execution.id)\n",
        "\n",
        "# Non-public APIs, just for showcase.\n",
        "from tfx.orchestration.experimental.interactive import visualizations\n",
        "\n",
        "def visualize_artifacts(artifacts):\n",
        "  \"\"\"Visualizes artifacts using standard visualization modules.\"\"\"\n",
        "  for artifact in artifacts:\n",
        "    visualization = visualizations.get_registry().get_visualization(\n",
        "        artifact.type_name)\n",
        "    if visualization:\n",
        "      visualization.display(artifact)\n",
        "\n",
        "from tfx.orchestration.experimental.interactive import standard_visualizations\n",
        "standard_visualizations.register_standard_visualizations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CE1dk_3irPL"
      },
      "source": [
        "Ahora podemos examinar las salidas de la ejecución de la canalización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRKSjXzsiqh0"
      },
      "outputs": [],
      "source": [
        "# Non-public APIs, just for showcase.\n",
        "from tfx.orchestration.metadata import Metadata\n",
        "from tfx.types import standard_component_specs\n",
        "\n",
        "metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n",
        "    SCHEMA_METADATA_PATH)\n",
        "\n",
        "with Metadata(metadata_connection_config) as metadata_handler:\n",
        "  # Find output artifacts from MLMD.\n",
        "  stat_gen_output = get_latest_artifacts(metadata_handler, SCHEMA_PIPELINE_NAME,\n",
        "                                         'StatisticsGen')\n",
        "  stats_artifacts = stat_gen_output[standard_component_specs.STATISTICS_KEY]\n",
        "\n",
        "  schema_gen_output = get_latest_artifacts(metadata_handler,\n",
        "                                           SCHEMA_PIPELINE_NAME, 'SchemaGen')\n",
        "  schema_artifacts = schema_gen_output[standard_component_specs.SCHEMA_KEY]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e8i0K-Aiqh-"
      },
      "source": [
        "Es hora de examinar las salidas de cada componente. Como se describió anteriormente, [Tensorflow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started) se usa en `StatisticsGen` y `SchemaGen`, y TFDV también proporciona visualización de las salidas de estos componentes.\n",
        "\n",
        "En este tutorial, usaremos los métodos ayudantes de visualización en TFX que usan TFDV internamente para mostrar la visualización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRGC4X1Ziqh-"
      },
      "source": [
        "#### Análisis de la salida de StatisticsGen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3StnKm04iqh-"
      },
      "outputs": [],
      "source": [
        "# docs-infra: no-execute\n",
        "visualize_artifacts(stats_artifacts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPfVPFTW0Jh2"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\"\n",
        "src=\"images/penguin_tfdv/penguin_tfdv_statistics.png\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS1XXFtfiqh-"
      },
      "source": [
        "Puede ver varias estadísticas para los datos de entrada. Estas estadísticas se proporcionan a `SchemaGen` para construir un esquema inicial de datos automáticamente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20HK9JS7iqh-"
      },
      "source": [
        "#### Análisis de la salida de SchemaGen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVmlot5ziqh_"
      },
      "outputs": [],
      "source": [
        "visualize_artifacts(schema_artifacts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ldXsv2iiqh_"
      },
      "source": [
        "Este esquema se infiere automáticamente a partir de la salida de StatisticsGen. Debería poder ver 4 características FLOAT y 1 característica INT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKpFPwEWhCoB"
      },
      "source": [
        "### Exportación del esquema para uso futuro\n",
        "\n",
        "Tenemos que revisar y refinar el esquema generado. El esquema revisado debe persistir para que podamos usarlo en procesos posteriores para el entrenamiento del modelo de ML. En otras palabras, quizás convenga agregar el archivo de esquema al sistema de control de versiones para casos de uso reales. En este tutorial, simplemente copiaremos el esquema a una ruta del sistema de archivos predefinida para simplificar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Pyi0oaKmRTg"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "_schema_filename = 'schema.pbtxt'\n",
        "SCHEMA_PATH = 'schema'\n",
        "\n",
        "os.makedirs(SCHEMA_PATH, exist_ok=True)\n",
        "_generated_path = os.path.join(schema_artifacts[0].uri, _schema_filename)\n",
        "\n",
        "# Copy the 'schema.pbtxt' file from the artifact uri to a predefined path.\n",
        "shutil.copy(_generated_path, SCHEMA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05U8uQ6dnlB4"
      },
      "source": [
        "El archivo de esquema usa el [formato de texto Protocol Buffer](https://googleapis.dev/python/protobuf/latest/google/protobuf/text_format.html) y una instancia del [protocolo TensorFlow Metadata Schema](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwHO7-HfnlWs"
      },
      "outputs": [],
      "source": [
        "print(f'Schema at {SCHEMA_PATH}-----')\n",
        "!cat {SCHEMA_PATH}/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjKigLTNos4F"
      },
      "source": [
        "Debe asegurarse de revisar y posiblemente editar la definición del esquema según sea necesario. En este tutorial, usaremos el esquema generado sin cambios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH6gizcpSwWV"
      },
      "source": [
        "## Validación de ejemplos de entrada y entrenamiento un modelo de ML\n",
        "\n",
        "Volveremos a la canalización que creamos en el [Tutorial de canalizaciones simples de TFX](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple) para entrenar un modelo de ML y usar el esquema generado para escribir el código de entrenamiento del modelo.\n",
        "\n",
        "También agregaremos un componente [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) que buscará anomalías y valores faltantes en el conjunto de datos entrante con respecto al esquema.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjDv93eS5xV"
      },
      "source": [
        "### Cómo escribir un código de entrenamiento modelo\n",
        "\n",
        "Necesitamos escribir el código del modelo como lo hicimos en el [Tutorial de canalizaciones simples de TFX](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\n",
        "\n",
        "El modelo en sí es el mismo que en el tutorial anterior, pero esta vez usaremos el esquema generado en la canalización anterior en lugar de especificar características manualmente. La mayor parte del código se mantuvo intacta. La única diferencia es que no necesitamos especificar los nombres y tipos de funciones en este archivo. En cambio, los leemos del archivo de *esquema*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aES7Hv5QTDK3"
      },
      "outputs": [],
      "source": [
        "_trainer_module_file = 'penguin_trainer.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnc67uQNTDfW"
      },
      "outputs": [],
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import List\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.public import tfxio\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "# We don't need to specify _FEATURE_KEYS and _FEATURE_SPEC any more.\n",
        "# Those information can be read from the given schema file.\n",
        "\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "_EVAL_BATCH_SIZE = 10\n",
        "\n",
        "def _input_fn(file_pattern: List[str],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              schema: schema_pb2.Schema,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    schema: schema of the input data.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "      schema=schema).repeat()\n",
        "\n",
        "\n",
        "def _build_keras_model(schema: schema_pb2.Schema) -> tf.keras.Model:\n",
        "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model.\n",
        "  \"\"\"\n",
        "  # The model below is built with Functional API, please refer to\n",
        "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
        "\n",
        "  # ++ Changed code: Uses all features in the schema except the label.\n",
        "  feature_keys = [f.name for f in schema.feature if f.name != _LABEL_KEY]\n",
        "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in feature_keys]\n",
        "  # ++ End of the changed code.\n",
        "\n",
        "  d = keras.layers.concatenate(inputs)\n",
        "  for _ in range(2):\n",
        "    d = keras.layers.Dense(8, activation='relu')(d)\n",
        "  outputs = keras.layers.Dense(3)(d)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  model.summary(print_fn=logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  # ++ Changed code: Reads in schema file passed to the Trainer component.\n",
        "  schema = tfx.utils.parse_pbtxt_file(fn_args.schema_path, schema_pb2.Schema())\n",
        "  # ++ End of the changed code.\n",
        "\n",
        "  train_dataset = _input_fn(\n",
        "      fn_args.train_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(\n",
        "      fn_args.eval_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  model = _build_keras_model(schema)\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps)\n",
        "\n",
        "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
        "  # directory.\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blaw0rs-emEf"
      },
      "source": [
        "Ahora ha completado todos los pasos de preparación para compilar una canalización de TFX para entrenamiento de modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3OkNz3gTLwM"
      },
      "source": [
        "### Cómo escribir una definición de canalización\n",
        "\n",
        "Agregaremos dos nuevos componentes, `Importer` y `ExampleValidator`. Importer trae un archivo externo a la canalización de TFX. En este caso, es un archivo que contiene la definición del esquema. ExampleValidator examinará los datos de entrada y validará si todos los datos de entrada se ajustan al esquema de datos que proporcionamos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M49yYVNBTPd4"
      },
      "outputs": [],
      "source": [
        "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
        "                     schema_path: str, module_file: str, serving_model_dir: str,\n",
        "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
        "  \"\"\"Creates a pipeline using predefined schema with TFX.\"\"\"\n",
        "  # Brings data into the pipeline.\n",
        "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
        "\n",
        "  # Computes statistics over data for visualization and example validation.\n",
        "  statistics_gen = tfx.components.StatisticsGen(\n",
        "      examples=example_gen.outputs['examples'])\n",
        "\n",
        "  # NEW: Import the schema.\n",
        "  schema_importer = tfx.dsl.Importer(\n",
        "      source_uri=schema_path,\n",
        "      artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n",
        "          'schema_importer')\n",
        "\n",
        "  # NEW: Performs anomaly detection based on statistics and data schema.\n",
        "  example_validator = tfx.components.ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=schema_importer.outputs['result'])\n",
        "\n",
        "  # Uses user-provided Python function that trains a model.\n",
        "  trainer = tfx.components.Trainer(\n",
        "      module_file=module_file,\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=schema_importer.outputs['result'],  # Pass the imported schema.\n",
        "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
        "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
        "\n",
        "  # Pushes the model to a filesystem destination.\n",
        "  pusher = tfx.components.Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      push_destination=tfx.proto.PushDestination(\n",
        "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)))\n",
        "\n",
        "  components = [\n",
        "      example_gen,\n",
        "\n",
        "      # NEW: Following three components were added to the pipeline.\n",
        "      statistics_gen,\n",
        "      schema_importer,\n",
        "      example_validator,\n",
        "\n",
        "      trainer,\n",
        "      pusher,\n",
        "  ]\n",
        "\n",
        "  return tfx.dsl.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      metadata_connection_config=tfx.orchestration.metadata\n",
        "      .sqlite_metadata_connection_config(metadata_path),\n",
        "      components=components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJbq07THU2GV"
      },
      "source": [
        "### Cómo ejecutar la canalización\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAtfOZTYWJu-"
      },
      "outputs": [],
      "source": [
        "tfx.orchestration.LocalDagRunner().run(\n",
        "  _create_pipeline(\n",
        "      pipeline_name=PIPELINE_NAME,\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      data_root=DATA_ROOT,\n",
        "      schema_path=SCHEMA_PATH,\n",
        "      module_file=_trainer_module_file,\n",
        "      serving_model_dir=SERVING_MODEL_DIR,\n",
        "      metadata_path=METADATA_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ3nTzG8uAzn"
      },
      "source": [
        "Debería ver el mensaje \"INFO:absl:Component Pusher is finished\" (INFO:absl:Component Pusher finalizó) si la canalización se completó correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuD5FRPAcOn8"
      },
      "source": [
        "### Análisis de salidas de la canalización\n",
        "\n",
        "Hemos entrenado el modelo de clasificación de pingüinos y también hemos validado los ejemplos de entrada en el componente ExampleValidator. Podemos analizar la salida de ExampleValidator como lo hicimos con la canalización anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtsrZEUB1-J4"
      },
      "outputs": [],
      "source": [
        "metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n",
        "    METADATA_PATH)\n",
        "\n",
        "with Metadata(metadata_connection_config) as metadata_handler:\n",
        "  ev_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,\n",
        "                                   'ExampleValidator')\n",
        "  anomalies_artifacts = ev_output[standard_component_specs.ANOMALIES_KEY]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U5MNAUIdBtN"
      },
      "source": [
        "También se pueden visualizar ExampleAnomalies desde ExampleValidator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-4oAjGR-IR0"
      },
      "outputs": [],
      "source": [
        "visualize_artifacts(anomalies_artifacts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t026ZzbU0961"
      },
      "source": [
        "Debería ver el mensaje \"No anomalies found\" (No se encontraron anomalías) para cada división de ejemplos. Debido a que utilizamos los mismos datos que se utilizaron para la generación del esquema en esta canalización, no se espera ninguna anomalía en este punto. Si ejecuta esta canalización repetidamente con nuevos datos entrantes, ExampleValidator debería poder encontrar cualquier discrepancia entre los nuevos datos y el esquema existente.\n",
        "\n",
        "Si se encuentra alguna anomalía, puede revisar sus datos para comprobar si algún ejemplo no sigue sus suposiciones. Las salidas de otros componentes como StatisticsGen pueden resultar útiles. Sin embargo, cualquier anomalía que se encuentre NO bloqueará futuras ejecuciones de la canalización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08R8qvweThRf"
      },
      "source": [
        "## Siguientes pasos\n",
        "\n",
        "Puede encontrar más recursos en https://www.tensorflow.org/tfx/tutorials.\n",
        "\n",
        "Consulte [Explicación de las canalizaciones de TFX](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) para obtener más información sobre distintos conceptos en TFX.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DjUA6S30k52h"
      ],
      "name": "penguin_tfdv.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
