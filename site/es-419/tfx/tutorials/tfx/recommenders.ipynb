{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X80i_girFR2o"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bB8gHCR3FVC0"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu4gy3TAw4As"
      },
      "source": [
        "# Recomendación de películas: modelos de recomendación en TFX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z17OmgavQfp4"
      },
      "source": [
        "Nota: Recomendamos ejecutar este tutorial en un bloc de notas de Colab, ¡no es necesario configurarlo! Simplemente haga clic en \"Ejecutar en Google Colab\".\n",
        "\n",
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/recommenders\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tfx/tutorials/tfx/recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tfx/tutorials/tfx/recommenders.ipynb\"><img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tfx/tutorials/tfx/recommenders.ipynb\"><img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeYA79m1DEX"
      },
      "source": [
        "## Tutorial de TFRS trasladado a TFX\n",
        "\n",
        "Esta es una adaptación de un tutorial básico de TensorFlow Recommenders (TFRS) para TFX, que se diseñó para demostrar cómo usar TFRS en una canalización de TFX. Refleja el [tutorial básico](https://www.tensorflow.org/recommenders/examples/basic_retrieval).\n",
        "\n",
        "Por contexto, los sistemas de recomendación del mundo real a menudo se componen de dos etapas:\n",
        "\n",
        "1. La etapa de recuperación es responsable de seleccionar un conjunto inicial de cientos de candidatos entre todos los candidatos posibles. El objetivo principal de este modelo es eliminar de manera eficiente todos los candidatos que no le interesan al usuario. Debido a que el modelo de recuperación puede tratar con millones de candidatos, tiene que ser computacionalmente eficiente.\n",
        "2. La etapa de clasificación toma los resultados del modelo de recuperación y los afina para seleccionar el mejor conjunto posible de recomendaciones. Su tarea consiste en reducir el conjunto de artículos que pueden interesar al usuario a una lista de posibles candidatos.\n",
        "\n",
        "En este tutorial, nos centraremos en la primera etapa, la recuperación. Los modelos de recuperación suelen estar compuestos por dos submodelos:\n",
        "\n",
        "1. Un modelo de consulta que calcula la representación de la consulta (normalmente un vector de incorporación de dimensionalidad fija) a partir de las características de consulta.\n",
        "2. Un modelo candidato que calcula la representación candidata (un vector del mismo tamaño) mediante las características candidatas.\n",
        "\n",
        "Luego, las salidas de los dos modelos se multiplican para obtener una puntuación de afinidad entre la consulta y el candidato, donde las puntuaciones más altas expresan una mejor coincidencia entre el candidato y la consulta.\n",
        "\n",
        "En este tutorial, compilaremos y entrenaremos un modelo de dos torres utilizando el conjunto de datos Movielens.\n",
        "\n",
        "Haremos lo siguiente:\n",
        "\n",
        "1. Ingeriremos e inspeccionaremos el conjunto de datos MovieLens.\n",
        "2. Implementaremos un modelo de recuperación.\n",
        "3. Entrenaremos y exportaremos el modelo.\n",
        "4. Haremos predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7QZ3kkMQo48"
      },
      "source": [
        "## El conjunto de datos\n",
        "\n",
        "El conjunto de datos Movielens es un conjunto de datos clásico del grupo de investigación [GroupLens](https://grouplens.org/datasets/movielens/) de la Universidad de Minnesota. Contiene un conjunto de calificaciones que un conjunto de usuarios otorgó a películas y es un caballo de batalla en la investigación de sistemas de recomendación.\n",
        "\n",
        "Los datos se pueden tratar de dos formas:\n",
        "\n",
        "1. Puede interpretarse como que expresa qué películas vieron (y calificaron) los usuarios y cuáles no. Esta es una forma de retroalimentación implícita, donde las visualizaciones de los usuarios nos dicen qué cosas prefieren ver y cuáles preferirían no ver.\n",
        "2. También se puede considerar que expresa cuánto les gustaron a los usuarios las películas que vieron. Esta es una forma de retroalimentación explícita; como el usuario miró una película, podemos saber aproximadamente cuánto le gustó al observar la calificación que le dio.\n",
        "\n",
        "En este tutorial, nos centramos en un sistema de recuperación: un modelo que predice un conjunto de películas del catálogo que es probable que vea el usuario. A menudo, los datos implícitos son más útiles aquí, por lo que trataremos a Movielens como un sistema implícito. Esto significa que cada película que vio un usuario es un ejemplo positivo y cada película que no vio es un ejemplo negativo implícito."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sawo1x8kQk9b"
      },
      "source": [
        "## Importaciones\n",
        "\n",
        "Primero, eliminemos nuestras importaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtR3txiwrT9w"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq tfx\n",
        "!pip install -Uq tensorflow-recommenders\n",
        "!pip install -Uq tensorflow-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJrgGNTHhzlq"
      },
      "source": [
        "### Desinstalación de shapely\n",
        "\n",
        "TODO(b/263441833) Esta es una solución temporal para evitar un ImportError. En última instancia, debería solucionarse admitiendo una versión reciente de Bigquery, en lugar de desinstalar otras dependencias adicionales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w90AGSpJhz8X"
      },
      "outputs": [],
      "source": [
        "!pip uninstall shapely -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCnUpuaJGKa0"
      },
      "source": [
        "### ¿Reinició el tiempo de ejecución?\n",
        "\n",
        "Si está usando Google Colab, la primera vez que ejecute la celda anterior, debe reiniciar el tiempo de ejecución (Tiempo de ejecución &gt; Reiniciar tiempo de ejecución...). Esto se debe a la forma en que Colab carga los paquetes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZGYDaF-m5wZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import absl\n",
        "import json\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Any, Dict, List, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs\n",
        "import apache_beam as beam\n",
        "\n",
        "from absl import logging\n",
        "\n",
        "from tfx.components.example_gen.base_example_gen_executor import BaseExampleGenExecutor\n",
        "from tfx.components.example_gen.component import FileBasedExampleGen\n",
        "from tfx.components.example_gen import utils\n",
        "from tfx.dsl.components.base import executor_spec\n",
        "\n",
        "from tfx.types import artifact\n",
        "from tfx.types import artifact_utils\n",
        "from tfx.types import channel\n",
        "from tfx.types import standard_artifacts\n",
        "from tfx.types.standard_artifacts import Examples\n",
        "\n",
        "from tfx.dsl.component.experimental.annotations import InputArtifact\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "\n",
        "# Set up logging.\n",
        "tf.get_logger().propagate = False\n",
        "absl.logging.set_verbosity(absl.logging.INFO)\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"TFX version: {tfx.__version__}\")\n",
        "print(f\"TensorFlow Recommenders version: {tfrs.__version__}\")\n",
        "\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znn3FAHAsomB"
      },
      "source": [
        "## Cómo crear un ExampleGen de TFDS\n",
        "\n",
        "Creamos un [componente ExampleGen personalizado](https://www.tensorflow.org/tfx/guide/examplegen#custom_examplegen) que usamos para cargar un conjunto de datos TensorFlow Datasets (TFDS). Esto usa un ejecutor personalizado en FileBasedExampleGen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcVgf7rLsv70"
      },
      "outputs": [],
      "source": [
        "@beam.ptransform_fn\n",
        "@beam.typehints.with_input_types(beam.Pipeline)\n",
        "@beam.typehints.with_output_types(tf.train.Example)\n",
        "def _TFDatasetToExample(  # pylint: disable=invalid-name\n",
        "    pipeline: beam.Pipeline,\n",
        "    exec_properties: Dict[str, Any],\n",
        "    split_pattern: str\n",
        "    ) -> beam.pvalue.PCollection:\n",
        "    \"\"\"Read a TensorFlow Dataset and create tf.Examples\"\"\"\n",
        "    custom_config = json.loads(exec_properties['custom_config'])\n",
        "    dataset_name = custom_config['dataset']\n",
        "    split_name = custom_config['split']\n",
        "\n",
        "    builder = tfds.builder(dataset_name)\n",
        "    builder.download_and_prepare()\n",
        "\n",
        "    return (pipeline\n",
        "            | 'MakeExamples' >> tfds.beam.ReadFromTFDS(builder, split=split_name)\n",
        "            | 'AsNumpy' >> beam.Map(tfds.as_numpy)\n",
        "            | 'ToDict' >> beam.Map(dict)\n",
        "            | 'ToTFExample' >> beam.Map(utils.dict_to_example)\n",
        "            )\n",
        "\n",
        "class TFDSExecutor(BaseExampleGenExecutor):\n",
        "  def GetInputSourceToExamplePTransform(self) -> beam.PTransform:\n",
        "    \"\"\"Returns PTransform for TF Dataset to TF examples.\"\"\"\n",
        "    return _TFDatasetToExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srH6ChVCtR55"
      },
      "source": [
        "## Contexto de canalización de inicio de TFX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM-46D40tW_V"
      },
      "outputs": [],
      "source": [
        "context = InteractiveContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAqjR4a1RR4"
      },
      "source": [
        "## Cómo preparar el conjunto de datos\n",
        "\n",
        "Usaremos nuestro ejecutor personalizado en `FileBasedExampleGen` para cargar nuestros conjuntos de datos desde TFDS. Como tenemos dos conjuntos de datos, crearemos dos componentes `ExampleGen`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaQhqcLGP0jL"
      },
      "outputs": [],
      "source": [
        "# Ratings data.\n",
        "ratings_example_gen = FileBasedExampleGen(\n",
        "    input_base='dummy',\n",
        "    custom_config={'dataset':'movielens/100k-ratings', 'split':'train'},\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor))\n",
        "context.run(ratings_example_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlUFANrRvKDW"
      },
      "outputs": [],
      "source": [
        "# Features of all the available movies.\n",
        "movies_example_gen = FileBasedExampleGen(\n",
        "    input_base='dummy',\n",
        "    custom_config={'dataset':'movielens/100k-movies', 'split':'train'},\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(TFDSExecutor))\n",
        "context.run(movies_example_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRHorm8W1yf3"
      },
      "source": [
        "## Cómo crear la utilidad `inspect_examples`\n",
        "\n",
        "Creamos una utilidad conveniente para inspeccionar conjuntos de datos de TF.Examples. El conjunto de datos de calificaciones devuelve un diccionario de identificación de películas, identificación de usuarios, calificación asignada, marca de tiempo, información de la película e información del usuario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1-KQV2ynMdh"
      },
      "outputs": [],
      "source": [
        "def inspect_examples(component,\n",
        "                     channel_name='examples',\n",
        "                     split_name='train',\n",
        "                     num_examples=1):\n",
        "  # Get the URI of the output artifact, which is a directory\n",
        "  full_split_name = 'Split-{}'.format(split_name)\n",
        "  print('channel_name: {}, split_name: {} (\\\"{}\\\"), num_examples: {}\\n'.format(\n",
        "      channel_name, split_name, full_split_name, num_examples))\n",
        "  train_uri = os.path.join(\n",
        "      component.outputs[channel_name].get()[0].uri, full_split_name)\n",
        "\n",
        "  # Get the list of files in this directory (all compressed TFRecord files)\n",
        "  tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                        for name in os.listdir(train_uri)]\n",
        "\n",
        "  # Create a `TFRecordDataset` to read these files\n",
        "  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "  # Iterate over the records and print them\n",
        "  for tfrecord in dataset.take(num_examples):\n",
        "    serialized_example = tfrecord.numpy()\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(serialized_example)\n",
        "    pp.pprint(example)\n",
        "\n",
        "inspect_examples(ratings_example_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGLGCjSt_q96"
      },
      "source": [
        "El conjunto de datos de películas contiene la identificación de la película, el título de la película y datos sobre los géneros a los que pertenece. Tenga en cuenta que los géneros están codificados con etiquetas de números enteros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHLsIHhw_x1d"
      },
      "outputs": [],
      "source": [
        "inspect_examples(movies_example_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu4XSa_G1nyN"
      },
      "source": [
        "## ExampleGen generó la división\n",
        "\n",
        "Cuando ingerimos el conjunto de datos de la lente de la película, nuestro componente `ExampleGen` dividió los datos en divisiones `train` y `eval`. En realidad, se denominan `Split-train` y `Split-eval`. Por defecto, la división es 66 % para entrenamiento y 34 % para evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk7Y7hWGKH9e"
      },
      "source": [
        "## Cómo generar estadísticas de películas y calificaciones\n",
        "\n",
        "Para una canalización de TFX, necesitamos generar estadísticas para el conjunto de datos. Lo hacemos usando un [componente StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen). Estos serán utilizados por el [componente SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) a continuación cuando generemos un esquema para nuestro conjunto de datos. De todos modos, esta es una buena práctica, porque es importante examinar y analizar los datos de forma continua. Como tenemos dos conjuntos de datos, crearemos dos componentes StatisticsGen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7-ZI8IsKT2P"
      },
      "outputs": [],
      "source": [
        "movies_stats_gen = tfx.components.StatisticsGen(\n",
        "    examples=movies_example_gen.outputs['examples'])\n",
        "context.run(movies_stats_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlKLrrgnKzIe"
      },
      "outputs": [],
      "source": [
        "context.show(movies_stats_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmTThijxKmhA"
      },
      "outputs": [],
      "source": [
        "ratings_stats_gen = tfx.components.StatisticsGen(\n",
        "    examples=ratings_example_gen.outputs['examples'])\n",
        "context.run(ratings_stats_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoRcgChqK62O"
      },
      "outputs": [],
      "source": [
        "context.show(ratings_stats_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Lj0OBwLElb"
      },
      "source": [
        "## Cómo crear esquemas para películas y calificaciones\n",
        "\n",
        "Para una canalización de TFX, necesitamos generar un esquema de datos a partir de nuestro conjunto de datos. Lo hacemos utilizando un [componente SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen). Esto será utilizado por el [componente Transform](https://www.tensorflow.org/tfx/guide/transform) a continuación para realizar nuestra ingeniería de características de una manera que sea altamente escalable a grandes conjuntos de datos y evite el sesgo entrenamiento/servicio. Como tenemos dos conjuntos de datos, crearemos dos componentes SchemaGen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL85CAcILJiw"
      },
      "outputs": [],
      "source": [
        "movies_schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=movies_stats_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(movies_schema_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eMtN1U1Lha1"
      },
      "outputs": [],
      "source": [
        "context.show(movies_schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-DkVUOeLmvX"
      },
      "outputs": [],
      "source": [
        "ratings_schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=ratings_stats_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(ratings_schema_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxD9oAhZLt_Z"
      },
      "outputs": [],
      "source": [
        "context.show(ratings_schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN648jrYLxmH"
      },
      "source": [
        "## Ingeniería de características usando Transform\n",
        "\n",
        "Para un diseño estructurado y repetible de una canalización de TFX, necesitaremos un enfoque escalable para la ingeniería de características. Esto nos permite manejar grandes conjuntos de datos que suelen formar parte de muchos sistemas de recomendación y también evita el sesgo entre entrenamiento y servicio. Lo haremos con ayuda del [componente Transform](https://www.tensorflow.org/tfx/guide/transform).\n",
        "\n",
        "El componente Transform usa un archivo de módulo para proporcionar código de usuario para la ingeniería de características que queremos hacer, por lo que nuestro primer paso es crear ese archivo de módulo. Como tenemos dos conjuntos de datos, crearemos dos de estos archivos de módulo y dos componentes Transform.\n",
        "\n",
        "Una de las cosas que nuestro recomendador necesita son vocabularios para los campos `user_id` y `movie_title`. En el [tutorial de basic_retrieval,](https://www.tensorflow.org/recommenders/examples/basic_retrieval) estos se crean con Numpy en línea, pero aquí usaremos Transform.\n",
        "\n",
        "Nota: La magia de celda `%%writefile {_movies_transform_module_file}` a continuación crea y escribe el contenido de esa celda en un archivo en el servidor de bloc de notas donde se ejecuta este bloc de notas (por ejemplo, Colab VM). Al hacer esto fuera de un bloc de notas, simplemente crearía un archivo Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oqzx5mSI8zm"
      },
      "outputs": [],
      "source": [
        "_movies_transform_module_file = 'movies_transform_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKROCiPo_5LJ"
      },
      "outputs": [],
      "source": [
        "%%writefile {_movies_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  # We only want the movie title\n",
        "  return {'movie_title':inputs['movie_title']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQcQBN9SIzIa"
      },
      "outputs": [],
      "source": [
        "movies_transform = tfx.components.Transform(\n",
        "    examples=movies_example_gen.outputs['examples'],\n",
        "    schema=movies_schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_movies_transform_module_file))\n",
        "context.run(movies_transform, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5oai0TlNWlv"
      },
      "outputs": [],
      "source": [
        "context.show(movies_transform.outputs['post_transform_schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWahQqCiBXqA"
      },
      "outputs": [],
      "source": [
        "inspect_examples(movies_transform, channel_name='transformed_examples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4PmR-a8O-mD"
      },
      "outputs": [],
      "source": [
        "_ratings_transform_module_file = 'ratings_transform_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWXuBqivPDuK"
      },
      "outputs": [],
      "source": [
        "%%writefile {_ratings_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import pdb\n",
        "\n",
        "NUM_OOV_BUCKETS = 1\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  # We only want the user ID and the movie title, but we also need vocabularies\n",
        "  # for both of them.  The vocabularies aren't features, they're only used by\n",
        "  # the lookup.\n",
        "  outputs = {}\n",
        "  outputs['user_id'] = tft.sparse_tensor_to_dense_with_shape(inputs['user_id'], [None, 1], '-1')\n",
        "  outputs['movie_title'] = tft.sparse_tensor_to_dense_with_shape(inputs['movie_title'], [None, 1], '-1')\n",
        "\n",
        "  tft.compute_and_apply_vocabulary(\n",
        "      inputs['user_id'],\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS,\n",
        "      vocab_filename='user_id_vocab')\n",
        "\n",
        "  tft.compute_and_apply_vocabulary(\n",
        "      inputs['movie_title'],\n",
        "      num_oov_buckets=NUM_OOV_BUCKETS,\n",
        "      vocab_filename='movie_title_vocab')\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4NgpBOkPXsj"
      },
      "outputs": [],
      "source": [
        "ratings_transform = tfx.components.Transform(\n",
        "    examples=ratings_example_gen.outputs['examples'],\n",
        "    schema=ratings_schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_ratings_transform_module_file))\n",
        "context.run(ratings_transform, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9Vqby34Dvzd"
      },
      "outputs": [],
      "source": [
        "context.show(ratings_transform.outputs['post_transform_schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_ec39jiaMG-"
      },
      "outputs": [],
      "source": [
        "inspect_examples(ratings_transform, channel_name='transformed_examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCi-seR86qqa"
      },
      "source": [
        "## Implementación de un modelo en TFX\n",
        "\n",
        "En el tutorial de [basic_retrieval](https://www.tensorflow.org/recommenders/examples/basic_retrieval), el modelo se creó en línea en el tiempo de ejecución de Python. En una canalización de TFX, el modelo, la métrica y la pérdida se definen y entrenan en el archivo del módulo para un [componente de canalización conocido como Trainer](https://www.tensorflow.org/tfx/guide/trainer). Esto hace que el modelo, la métrica y la pérdida formen parte de un proceso repetible que se puede automatizar y monitorear.\n",
        "\n",
        "### Arquitectura del modelo de TensorFlow Recommenders\n",
        "\n",
        "Vamos a compilar un modelo de recuperación de dos torres. El concepto de dos torres significa que tendremos una torre de consultas que calculará la representación del usuario a partir de las características del usuario y otra torre de elementos que calculará la representación de la película a partir de las características de la película. Podemos construir cada torre por separado (en los métodos `_build_user_model()` y `_build_movie_model()` siguientes) y luego combinarlas en el modelo final (como en la clase `MobieLensModel`). `MovieLensModel` es una subclase de la clase base `tfrs.Model`, que agiliza la compilación de modelos: todo lo que necesitamos hacer es configurar los componentes en el método `__init__` e implementar el método `compute_loss`, tomando las características sin procesar y devolviendo un valor de pérdida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_mmYhjAJP4g"
      },
      "outputs": [],
      "source": [
        "# We're now going to create the module file for Trainer, which will include the\n",
        "# code above with some modifications for TFX.\n",
        "\n",
        "_trainer_module_file = 'trainer_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHQZJEhXP93N"
      },
      "outputs": [],
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import Dict, List, Text\n",
        "\n",
        "import pdb\n",
        "\n",
        "import os\n",
        "import absl\n",
        "import datetime\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "from absl import logging\n",
        "from tfx.types import artifact_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.coders import example_coder\n",
        "from tfx_bsl.public import tfxio\n",
        "\n",
        "absl.logging.set_verbosity(absl.logging.INFO)\n",
        "\n",
        "EMBEDDING_DIMENSION = 32\n",
        "INPUT_FN_BATCH_SIZE = 1\n",
        "\n",
        "\n",
        "def extract_str_feature(dataset, feature_name):\n",
        "  np_dataset = []\n",
        "  for example in dataset:\n",
        "    np_example = example_coder.ExampleToNumpyDict(example.numpy())\n",
        "    np_dataset.append(np_example[feature_name][0].decode())\n",
        "  return tf.data.Dataset.from_tensor_slices(np_dataset)\n",
        "\n",
        "\n",
        "class MovielensModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, user_model, movie_model, tf_transform_output, movies_uri):\n",
        "    super().__init__()\n",
        "    self.movie_model: tf.keras.Model = movie_model\n",
        "    self.user_model: tf.keras.Model = user_model\n",
        "\n",
        "    movies_artifact = movies_uri.get()[0]\n",
        "    input_dir = artifact_utils.get_split_uri([movies_artifact], 'train')\n",
        "    movie_files = glob.glob(os.path.join(input_dir, '*'))\n",
        "    movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")\n",
        "    movies_dataset = extract_str_feature(movies, 'movie_title')\n",
        "\n",
        "    loss_metrics = tfrs.metrics.FactorizedTopK(\n",
        "        candidates=movies_dataset.batch(128).map(movie_model)\n",
        "        )\n",
        "\n",
        "    self.task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "        metrics=loss_metrics\n",
        "        )\n",
        "\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    try:\n",
        "      user_embeddings = tf.squeeze(self.user_model(features['user_id']), axis=1)\n",
        "      # And pick out the movie features and pass them into the movie model,\n",
        "      # getting embeddings back.\n",
        "      positive_movie_embeddings = self.movie_model(features['movie_title'])\n",
        "\n",
        "      # The task computes the loss and the metrics.\n",
        "      _task = self.task(user_embeddings, positive_movie_embeddings)\n",
        "    except BaseException as err:\n",
        "      logging.error('######## ERROR IN compute_loss:\\n{}\\n###############'.format(err))\n",
        "\n",
        "    return _task\n",
        "\n",
        "\n",
        "# This function will apply the same transform operation to training data\n",
        "# and serving requests.\n",
        "def _apply_preprocessing(raw_features, tft_layer):\n",
        "  try:\n",
        "    transformed_features = tft_layer(raw_features)\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN _apply_preprocessing:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return transformed_features\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size),\n",
        "      tf_transform_output.transformed_metadata.schema)\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN _input_fn:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "  \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
        "  try:\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "      \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "      try:\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        result = model(transformed_features)\n",
        "      except BaseException as err:\n",
        "        logging.error('######## ERROR IN serve_tf_examples_fn:\\n{}\\n###############'.format(err))\n",
        "      return result\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN _get_serve_tf_examples_fn:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _build_user_model(\n",
        "    tf_transform_output: tft.TFTransformOutput, # Specific to ratings\n",
        "    embedding_dimension: int = 32) -> tf.keras.Model:\n",
        "  \"\"\"Creates a Keras model for the query tower.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: [tft.TFTransformOutput], the results of Transform\n",
        "    embedding_dimension: [int], the dimensionality of the embedding space\n",
        "\n",
        "  Returns:\n",
        "    A keras Model.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    unique_user_ids = tf_transform_output.vocabulary_by_name('user_id_vocab')\n",
        "    users_vocab_str = [b.decode() for b in unique_user_ids]\n",
        "\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.StringLookup(\n",
        "             vocabulary=users_vocab_str, mask_token=None),\n",
        "         # We add an additional embedding to account for unknown tokens.\n",
        "         tf.keras.layers.Embedding(len(users_vocab_str) + 1, embedding_dimension)\n",
        "         ])\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN _build_user_model:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def _build_movie_model(\n",
        "    tf_transform_output: tft.TFTransformOutput, # Specific to movies\n",
        "    embedding_dimension: int = 32) -> tf.keras.Model:\n",
        "  \"\"\"Creates a Keras model for the candidate tower.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: [tft.TFTransformOutput], the results of Transform\n",
        "    embedding_dimension: [int], the dimensionality of the embedding space\n",
        "\n",
        "  Returns:\n",
        "    A keras Model.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    unique_movie_titles = tf_transform_output.vocabulary_by_name('movie_title_vocab')\n",
        "    titles_vocab_str = [b.decode() for b in unique_movie_titles]\n",
        "\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.StringLookup(\n",
        "             vocabulary=titles_vocab_str, mask_token=None),\n",
        "         # We add an additional embedding to account for unknown tokens.\n",
        "         tf.keras.layers.Embedding(len(titles_vocab_str) + 1, embedding_dimension)\n",
        "        ])\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN _build_movie_model:\\n{}\\n###############'.format(err))\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n",
        "                              tf_transform_output, INPUT_FN_BATCH_SIZE)\n",
        "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n",
        "                            tf_transform_output, INPUT_FN_BATCH_SIZE)\n",
        "\n",
        "    model = MovielensModel(\n",
        "        _build_user_model(tf_transform_output, EMBEDDING_DIMENSION),\n",
        "        _build_movie_model(tf_transform_output, EMBEDDING_DIMENSION),\n",
        "        tf_transform_output,\n",
        "        fn_args.custom_config['movies']\n",
        "        )\n",
        "\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=fn_args.model_run_dir, update_freq='batch')\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
        "  except BaseException as err:\n",
        "    logging.error('######## ERROR IN run_fn before fit:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  try:\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=fn_args.custom_config['epochs'],\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        callbacks=[tensorboard_callback])\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN run_fn during fit:\\n{}\\n###############'.format(err))\n",
        "\n",
        "  try:\n",
        "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "\n",
        "    movies_artifact = fn_args.custom_config['movies'].get()[0]\n",
        "    input_dir = artifact_utils.get_split_uri([movies_artifact], 'eval')\n",
        "    movie_files = glob.glob(os.path.join(input_dir, '*'))\n",
        "    movies = tf.data.TFRecordDataset(movie_files, compression_type=\"GZIP\")\n",
        "\n",
        "    movies_dataset = extract_str_feature(movies, 'movie_title')\n",
        "\n",
        "    index.index_from_dataset(\n",
        "      tf.data.Dataset.zip((\n",
        "          movies_dataset.batch(100),\n",
        "          movies_dataset.batch(100).map(model.movie_model))\n",
        "      )\n",
        "    )\n",
        "\n",
        "    # Run once so that we can get the right signatures into SavedModel\n",
        "    _, titles = index(tf.constant([\"42\"]))\n",
        "    print(f\"Recommendations for user 42: {titles[0, :3]}\")\n",
        "\n",
        "    signatures = {\n",
        "        'serving_default':\n",
        "            _get_serve_tf_examples_fn(index,\n",
        "                                      tf_transform_output).get_concrete_function(\n",
        "                                          tf.TensorSpec(\n",
        "                                              shape=[None],\n",
        "                                              dtype=tf.string,\n",
        "                                              name='examples')),\n",
        "    }\n",
        "    index.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n",
        "\n",
        "  except BaseException as err:\n",
        "      logging.error('######## ERROR IN run_fn during export:\\n{}\\n###############'.format(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDN_LJGlnRGo"
      },
      "source": [
        "## Cómo entrenar el modelo\n",
        "\n",
        "Después de definir el modelo, podemos ejecutar el [componente Trainer](https://www.tensorflow.org/tfx/guide/trainer) para ejecutar el entrenamiento del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsWC8UpVrngY"
      },
      "outputs": [],
      "source": [
        "trainer = tfx.components.Trainer(\n",
        "    module_file=os.path.abspath(_trainer_module_file),\n",
        "    examples=ratings_transform.outputs['transformed_examples'],\n",
        "    transform_graph=ratings_transform.outputs['transform_graph'],\n",
        "    schema=ratings_transform.outputs['post_transform_schema'],\n",
        "    train_args=tfx.proto.TrainArgs(num_steps=500),\n",
        "    eval_args=tfx.proto.EvalArgs(num_steps=10),\n",
        "    custom_config={\n",
        "        'epochs':5,\n",
        "        'movies':movies_transform.outputs['transformed_examples'],\n",
        "        'movie_schema':movies_transform.outputs['post_transform_schema'],\n",
        "        'ratings':ratings_transform.outputs['transformed_examples'],\n",
        "        'ratings_schema':ratings_transform.outputs['post_transform_schema']\n",
        "        })\n",
        "\n",
        "context.run(trainer, enable_cache=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WXkKKZpRkaj"
      },
      "source": [
        "## Cómo exportar el modelo\n",
        "\n",
        "Después de entrenar el modelo, podemos usar el [componente Pusher](https://www.tensorflow.org/tfx/guide/pusher) para exportarlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hXlzwMTRkaj"
      },
      "outputs": [],
      "source": [
        "_serving_model_dir = os.path.join(tempfile.mkdtemp(), 'serving_model/tfrs_retrieval')\n",
        "\n",
        "pusher = tfx.components.Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    push_destination=tfx.proto.PushDestination(\n",
        "        filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher, enable_cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB2v43NJU3Xf"
      },
      "source": [
        "## Hacer predicciones\n",
        "\n",
        "Ahora que tenemos un modelo, lo volvemos a cargar y hacemos predicciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUwd9QoGRkaj"
      },
      "outputs": [],
      "source": [
        "loaded = tf.saved_model.load(pusher.outputs['pushed_model'].get()[0].uri)\n",
        "scores, titles = loaded([\"42\"])\n",
        "\n",
        "print(f\"Recommendations: {titles[0][:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ipAubFuRkak"
      },
      "source": [
        "## Siguientes pasos\n",
        "\n",
        "En este tutorial, aprendimos a implementar un modelo de recuperación con TensorFlow Recommenders y TFX. Para ampliar lo que se presenta aquí, eche un vistazo al tutorial de [clasificación de TFRS con TFX](https://www.tensorflow.org/recommenders/examples/ranking_tfx)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "X80i_girFR2o"
      ],
      "name": "recommenders.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
