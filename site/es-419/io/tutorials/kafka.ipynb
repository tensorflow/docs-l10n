{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow IO Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Aprendizaje automático robusto en transmisión de datos con Kafka y Tensorflow-IO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/io/tutorials/kafka\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/kafka.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/io/blob/master/docs/tutorials/kafka.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver código fuente en GitHub</a>\n",
        "</td>\n",
        "      <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/io/docs/tutorials/kafka.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Descripción general\n",
        "\n",
        "Este tutorial se centra en la transmisión de datos desde un clúster de [Kafka](https://kafka.apache.org/quickstart) a un `tf.data.Dataset` que luego se usa junto con `tf.keras` para entrenamiento e inferencia.\n",
        "\n",
        "Kafka es principalmente una plataforma distribuida de transmisión de eventos que proporciona transmisión de datos escalable y tolerante a fallas a través de canales de datos. Es un componente técnico esencial de una gran cantidad de empresas importantes donde la entrega de datos de misión crítica es un requisito principal.\n",
        "\n",
        "**NOTA:** Una comprensión básica de los [componentes de Kafka](https://kafka.apache.org/documentation/#intro_concepts_and_terms) le ayudará a seguir el tutorial con facilidad.\n",
        "\n",
        "**NOTA:** Se requiere un entorno de ejecución de Java para ejecutar este tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgCc3gXybsA"
      },
      "source": [
        "### Instalar los paquetes tensorflow-io y kafka necesarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48B9eAMMhAgw"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-io\n",
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjrZNJQRJP-U"
      },
      "source": [
        "### Importar paquetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6KXZuTBWgRm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCgO11GTJaTj"
      },
      "source": [
        "### Validar importaciones tf y tfio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX74RKfZ_TdF"
      },
      "outputs": [],
      "source": [
        "print(\"tensorflow-io version: {}\".format(tfio.__version__))\n",
        "print(\"tensorflow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "## Descargar y configurar instancias de Kafka y Zookeeper\n",
        "\n",
        "Para fines de demostración, las siguientes instancias se configuran localmente:\n",
        "\n",
        "- Kafka (Brokers: 127.0.0.1:9092)\n",
        "- Zookeeper (Node: 127.0.0.1:2181)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUj0878jPyz7"
      },
      "outputs": [],
      "source": [
        "!curl -sSOL https://dlcdn.apache.org/kafka/3.1.0/kafka_2.13-3.1.0.tgz\n",
        "!tar -xzf kafka_2.13-3.1.0.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAzfu_WiEs4F"
      },
      "source": [
        "Se usan las configuraciones predeterminadas (proporcionadas por Apache Kafka) para activar las instancias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9ujlunrWgRx"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.13-3.1.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.1.0/config/zookeeper.properties\n",
        "!./kafka_2.13-3.1.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.1.0/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qxCdypE1DD"
      },
      "source": [
        "Una vez que las instancias se inician como procesos demonio, busque con grep `kafka` en la lista de procesos. Los dos procesos de Java corresponden a las instancias de zookeeper y kafka."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48LqMJ1BEHm5"
      },
      "outputs": [],
      "source": [
        "!ps -ef | grep kafka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3TntBqanQnh"
      },
      "source": [
        "Cree los temas de Kafka con las siguientes especificaciones:\n",
        "\n",
        "- susy de entrenamiento: particiones = 1, factor de replicación = 1\n",
        "- susy de prueba: particiones=2, factor de replicación=1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXJWqMmWnPyP"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic susy-train\n",
        "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic susy-test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxf_NqjnycC"
      },
      "source": [
        "Describa el tema para obtener detalles sobre la configuración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apCf9pfVnwn7"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic susy-train\n",
        "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic susy-test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKVnz3Pjot9t"
      },
      "source": [
        "El factor de replicación 1 indica que los datos no se están replicando. Esto se debe a la presencia de un único corredor en nuestra configuración de Kafka. En los sistemas de producción, la cantidad de servidores de arranque puede estar en el rango de cientos de nodos. Ahí es donde entra en escena la tolerancia a fallos mediante replicación.\n",
        "\n",
        "Consulte los [documentos](https://kafka.apache.org/documentation/#replication) para obtener más detalles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjCy3zaCQJ7-"
      },
      "source": [
        "## Conjunto de datos SUSY\n",
        "\n",
        "Kafka, al ser una plataforma de transmisión de eventos, permite escribir en ella datos de diversas fuentes. Por ejemplo:\n",
        "\n",
        "- Registros de tráfico web\n",
        "- Medidas astronómicas\n",
        "- Datos de sensores de IoT\n",
        "- Reseñas de productos y mucho más.\n",
        "\n",
        "Para los fines de este tutorial, descargaremos el conjunto de datos [SUSY](https://archive.ics.uci.edu/ml/datasets/SUSY#) e introduciremos los datos en Kafka manualmente. El objetivo de este problema de clasificación es distinguir entre un proceso de señal que produce partículas supersimétricas y un proceso de fondo que no lo hace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emslB2EGQMCR"
      },
      "outputs": [],
      "source": [
        "!curl -sSOL https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CfKVmCvwcL7"
      },
      "source": [
        "### Explorar el conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18aR_MsOKToc"
      },
      "source": [
        "La primera columna es la etiqueta de clase (1 para señal, 0 para fondo), seguida de las 18 características (8 características de bajo nivel y luego 10 características de alto nivel). Las primeras 8 características son propiedades cinemáticas que miden los detectores de partículas en el acelerador. Las últimas 10 características son funciones de las primeras 8 características. Estas son características de alto nivel derivadas por los físicos para ayudar a discriminar entre las dos clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkXyocIdKRSB"
      },
      "outputs": [],
      "source": [
        "COLUMNS = [\n",
        "          #  labels\n",
        "           'class',\n",
        "          #  low-level features\n",
        "           'lepton_1_pT',\n",
        "           'lepton_1_eta',\n",
        "           'lepton_1_phi',\n",
        "           'lepton_2_pT',\n",
        "           'lepton_2_eta',\n",
        "           'lepton_2_phi',\n",
        "           'missing_energy_magnitude',\n",
        "           'missing_energy_phi',\n",
        "          #  high-level derived features\n",
        "           'MET_rel',\n",
        "           'axial_MET',\n",
        "           'M_R',\n",
        "           'M_TR_2',\n",
        "           'R',\n",
        "           'MT2',\n",
        "           'S_R',\n",
        "           'M_Delta_R',\n",
        "           'dPhi_r_b',\n",
        "           'cos(theta_r1)'\n",
        "           ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0NBA51_1Ie2"
      },
      "source": [
        "El conjunto de datos completo consta de 5 millones de filas. Sin embargo, para los fines de este tutorial, consideraremos solo una fracción del conjunto de datos (100 000 filas) para dedicar menos tiempo a mover los datos y más tiempo a comprender la funcionalidad de la API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC-yt_c9u0sH"
      },
      "outputs": [],
      "source": [
        "susy_iterator = pd.read_csv('SUSY.csv.gz', header=None, names=COLUMNS, chunksize=100000)\n",
        "susy_df = next(susy_iterator)\n",
        "susy_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlNuW7xbu6o8"
      },
      "outputs": [],
      "source": [
        "# Number of datapoints and columns\n",
        "len(susy_df), len(susy_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6Cg22bU0-na"
      },
      "outputs": [],
      "source": [
        "# Number of datapoints belonging to each class (0: background noise, 1: signal)\n",
        "len(susy_df[susy_df[\"class\"]==0]), len(susy_df[susy_df[\"class\"]==1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF5K9xtmlT2P"
      },
      "source": [
        "### Dividir el conjunto de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ku_X0Wld59"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(susy_df, test_size=0.4, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))\n",
        "\n",
        "x_train_df = train_df.drop([\"class\"], axis=1)\n",
        "y_train_df = train_df[\"class\"]\n",
        "\n",
        "x_test_df = test_df.drop([\"class\"], axis=1)\n",
        "y_test_df = test_df[\"class\"]\n",
        "\n",
        "# The labels are set as the kafka message keys so as to store data\n",
        "# in multiple-partitions. Thus, enabling efficient data retrieval\n",
        "# using the consumer groups.\n",
        "x_train = list(filter(None, x_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_train = list(filter(None, y_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "\n",
        "x_test = list(filter(None, x_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_test = list(filter(None, y_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHXk0x2MXVgL"
      },
      "outputs": [],
      "source": [
        "NUM_COLUMNS = len(x_train_df.columns)\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwP5U4GqmhoL"
      },
      "source": [
        "### Almacenar los datos de entrenamiento y de prueba en Kafka\n",
        "\n",
        "El almacenamiento de datos en Kafka simula un entorno para la recuperación remota continua de datos con fines de entrenamiento e inferencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhwFImSqncLE"
      },
      "outputs": [],
      "source": [
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "def write_to_kafka(topic_name, items):\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8')).add_errback(error_callback)\n",
        "    count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "write_to_kafka(\"susy-train\", zip(x_train, y_train))\n",
        "write_to_kafka(\"susy-test\", zip(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58q52py93jEf"
      },
      "source": [
        "### Definir el conjunto de datos de entrenamiento tfio\n",
        "\n",
        "La clase `IODataset` se usa para transmitir datos desde Kafka a tensorflow. La clase hereda de `tf.data.Dataset` y, por lo tanto, tiene todas las funcionalidades útiles de `tf.data.Dataset` listas para usar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHOcitbW2_d1"
      },
      "outputs": [],
      "source": [
        "def decode_kafka_item(item):\n",
        "  message = tf.io.decode_csv(item.message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(item.key)\n",
        "  return (message, key)\n",
        "\n",
        "BATCH_SIZE=64\n",
        "SHUFFLE_BUFFER_SIZE=64\n",
        "train_ds = tfio.IODataset.from_kafka('susy-train', partition=0, offset=0)\n",
        "train_ds = train_ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "train_ds = train_ds.map(decode_kafka_item)\n",
        "train_ds = train_ds.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x84lZJY164RI"
      },
      "source": [
        "## Construir y entrenar el modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuHtpAMqLqmv"
      },
      "outputs": [],
      "source": [
        "# Set the parameters\n",
        "\n",
        "OPTIMIZER=\"adam\"\n",
        "LOSS=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "METRICS=['accuracy']\n",
        "EPOCHS=10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lBmxxuj63jZ"
      },
      "outputs": [],
      "source": [
        "# design/build the model\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input(shape=(NUM_COLUMNS,)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.4),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.4),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTDFVxpSLfXI"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIJMg-saLgeR"
      },
      "outputs": [],
      "source": [
        "# fit the model\n",
        "model.fit(train_ds, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPy0Ka21QII5"
      },
      "source": [
        "Nota: No confunda el paso de entrenamiento con el entrenamiento en línea. Es un paradigma completamente diferente que se tratará en una sección más adelante.\n",
        "\n",
        "Dado que solo se usa una fracción del conjunto de datos, nuestra precisión se limita a 78 % aproximadamente durante la fase de entrenamiento. Sin embargo, no dude en almacenar datos adicionales en Kafka para mejorar el rendimiento del modelo. Además, dado que el objetivo era simplemente demostrar la funcionalidad de los conjuntos de datos de tfio de kafka, se usó una red neuronal más pequeña y menos complicada. Sin embargo, se puede aumentar la complejidad del modelo, modificar la estrategia de aprendizaje, ajustar los hiperparámetros y más para seguir explorando. Para obtener un enfoque básico, consulte este [artículo](https://www.nature.com/articles/ncomms5308#Sec11)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJW8za2qm4c"
      },
      "source": [
        "## Inferir sobre los datos de prueba\n",
        "\n",
        "Para inferir los datos de prueba adhiriéndose a la semántica de \"exactamente una vez\" junto con la tolerancia a fallas, se puede usar `streaming.KafkaGroupIODataset`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3FZOlSh2pmy"
      },
      "source": [
        "### Definir el conjunto de datos de prueba tfio\n",
        "\n",
        "El parámetro `stream_timeout` se bloquea durante la duración especificada para que se transmitan nuevos puntos de datos al tema. Esto elimina la necesidad de crear nuevos conjuntos de datos si los datos se transmiten al tema de forma intermitente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjnM81lPROen"
      },
      "outputs": [],
      "source": [
        "test_ds = tfio.experimental.streaming.KafkaGroupIODataset(\n",
        "    topics=[\"susy-test\"],\n",
        "    group_id=\"testcg\",\n",
        "    servers=\"127.0.0.1:9092\",\n",
        "    stream_timeout=10000,\n",
        "    configuration=[\n",
        "        \"session.timeout.ms=7000\",\n",
        "        \"max.poll.interval.ms=8000\",\n",
        "        \"auto.offset.reset=earliest\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "def decode_kafka_test_item(raw_message, raw_key):\n",
        "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(raw_key)\n",
        "  return (message, key)\n",
        "\n",
        "test_ds = test_ds.map(decode_kafka_test_item)\n",
        "test_ds = test_ds.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg8j3bZsSF6u"
      },
      "source": [
        "Aunque esta clase se puede usar con fines de entrenamiento, existen advertencias que deben abordarse. Una vez que se leen todos los mensajes de Kafka y se confirman las últimas compensaciones mediante `streaming.KafkaGroupIODataset`, el consumidor no reinicia la lectura de los mensajes desde el principio. Por lo tanto, durante el entrenamiento, solo es posible entrenar para una única época con los datos fluyendo continuamente. Este tipo de funcionalidad tiene casos de uso limitados durante la fase de entrenamiento en los que, una vez que el modelo ha consumido un punto de datos, ya no es requerido y puede ser descartado.\n",
        "\n",
        "Sin embargo, esta funcionalidad brilla cuando se trata de inferencia robusta con semántica de exactamente una vez."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PapN5Q_241k"
      },
      "source": [
        "### Evaluar el rendimiento de los datos de prueba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hMtIe1X215P"
      },
      "outputs": [],
      "source": [
        "res = model.evaluate(test_ds)\n",
        "print(\"test loss, test acc:\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWX9j11bWJGe"
      },
      "source": [
        "Dado que la inferencia se basa en la semántica de \"exactamente una vez\", la evaluación en el conjunto de prueba solo se puede ejecutar una vez. Para volver a ejecutar la inferencia sobre los datos de prueba, se debe usar un nuevo grupo de consumidores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Chcbd9xThl"
      },
      "source": [
        "### Seguimiento del retraso de compensación del grupo de consumidores `testcg`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uz3km0RxUG7"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.13-3.1.0/bin/kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --describe --group testcg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Wg0_eXMKL9"
      },
      "source": [
        "Una vez que el `current-offset` coincide con el `log-end-offset` para todas las particiones, quiere decir que los consumidores han completado la recuperación de todos los mensajes del tema de Kafka."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYwillcxP97z"
      },
      "source": [
        "## Aprendizaje en línea\n",
        "\n",
        "El paradigma del aprendizaje automático en línea es un poco diferente de la forma tradicional/convencional de entrenar modelos de aprendizaje automático. En el primer caso, el modelo continúa aprendiendo/actualizando incrementalmente sus parámetros tan pronto como los nuevos puntos de datos estén disponibles y se espera que este proceso continúe indefinidamente. Esto es diferente a los últimos enfoques, donde el conjunto de datos es fijo y el modelo lo itera `n` veces. En el aprendizaje en línea, es posible que los datos que una vez consumió el modelo no estén disponibles para volver a entrenarlo.\n",
        "\n",
        "Con `streaming.KafkaBatchIODataset`, ahora es posible entrenar los modelos de esta manera. Seguiremos usando nuestro conjunto de datos SUSY para demostrar esta funcionalidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5HyQtUZXi_P"
      },
      "source": [
        "### El conjunto de datos de entrenamiento tfio para el aprendizaje en línea\n",
        "\n",
        "`streaming.KafkaBatchIODataset` es similar a `streaming.KafkaGroupIODataset` en su API. Además, se recomienda usar el parámetro `stream_timeout` para configurar la duración durante la cual el conjunto de datos se bloqueará para recibir nuevos mensajes antes de que expire el tiempo de espera. En el siguiente ejemplo, el conjunto de datos está configurado con un `stream_timeout` de `10000` milisegundos. Esto implica que, después de que se hayan consumido todos los mensajes del tema, el conjunto de datos esperará 10 segundos adicionales antes de agotar el tiempo de espera y desconectarse del clúster de Kafka. Si se transmiten nuevos mensajes al tema antes de que expire el tiempo de espera, el consumo de datos y el entrenamiento del modelo se reanudan para esos puntos de datos recién consumidos. Para bloquearlo indefinidamente, puede establecerlo en `-1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-zCHNOuSJDL"
      },
      "outputs": [],
      "source": [
        "online_train_ds = tfio.experimental.streaming.KafkaBatchIODataset(\n",
        "    topics=[\"susy-train\"],\n",
        "    group_id=\"cgonline\",\n",
        "    servers=\"127.0.0.1:9092\",\n",
        "    stream_timeout=10000, # in milliseconds, to block indefinitely, set it to -1.\n",
        "    configuration=[\n",
        "        \"session.timeout.ms=7000\",\n",
        "        \"max.poll.interval.ms=8000\",\n",
        "        \"auto.offset.reset=earliest\"\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgSCn5dskO0t"
      },
      "source": [
        "Cada elemento que genera `online_train_ds` es un `tf.data.Dataset` en sí mismo. Por lo tanto, todas las transformaciones estándar se pueden aplicar como de costumbre.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cxF0bgGkQJs"
      },
      "outputs": [],
      "source": [
        "def decode_kafka_online_item(raw_message, raw_key):\n",
        "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(raw_key)\n",
        "  return (message, key)\n",
        "  \n",
        "for mini_ds in online_train_ds:\n",
        "  mini_ds = mini_ds.shuffle(buffer_size=32)\n",
        "  mini_ds = mini_ds.map(decode_kafka_online_item)\n",
        "  mini_ds = mini_ds.batch(32)\n",
        "  if len(mini_ds) > 0:\n",
        "    model.fit(mini_ds, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGph8eP9isuW"
      },
      "source": [
        "El modelo entrenado incrementalmente se puede guardar de forma periódica (según casos de uso) y se puede usar para inferir los datos de prueba en modo en línea o fuera de línea.\n",
        "\n",
        "Nota: `streaming.KafkaBatchIODataset` y `streaming.KafkaGroupIODataset` aún se encuentran en fase experimental y puede mejorarse según los comentarios de los usuarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8QAS_3k1y3u"
      },
      "source": [
        "## Referencias:\n",
        "\n",
        "- Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014)\n",
        "\n",
        "- Conjunto de datos SUSY: https://archive.ics.uci.edu/ml/datasets/SUSY#\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "kafka.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
