{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFPyjGqMQ82Q"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aNZ7aEDyQIYU"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMOmzhPEQh7b"
      },
      "source": [
        "# Normalizaciones\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/layers_normalizations\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/addons/tutorials/layers_normalizations.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/addons/tutorials/layers_normalizations.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/addons/tutorials/layers_normalizations.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a>\n",
        "</td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cthm5dovQMJl"
      },
      "source": [
        "## Descripción general\n",
        "\n",
        "En estas anotaciones se proporciona una breve introducción a las [capas de normalización](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py) de TensorFlow. Las capas compatibles actualmente son las siguientes:\n",
        "\n",
        "- **Normalización por grupos** (complementos de TensorFlow)\n",
        "- **Normalización por instancias** (complementos de TensorFlow)\n",
        "- **Normalización por capas** (esencial de TensorFlow)\n",
        "\n",
        "La idea de base detrás de estas capas es la de normalizar la salida de una capa de activación para mejorar la convergencia durante el entrenamiento. En contraste con la [normalización por lotes](https://keras.io/layers/normalization/), estas normalizaciones no funcionan en lotes, sino que normalizan las activaciones de a una sola muestra; por lo tanto, resultan adecuadas también para redes neuronales recurrentes.\n",
        "\n",
        "Por lo general, la normalización se lleva a cabo mediante el cálculo de la media y la desviación estándar de un subgrupo en el tensor de entrada. También es posible aplicar un factor escalar y uno de compensación en este caso.\n",
        "\n",
        "$y_{i} = \\frac{\\gamma ( x_{i} - \\mu )}{\\sigma }+ \\beta$\n",
        "\n",
        "$ y$ : salida\n",
        "\n",
        "$x$ : entrada\n",
        "\n",
        "$\\gamma$ : factor escalar\n",
        "\n",
        "$\\mu$: media\n",
        "\n",
        "$\\sigma$: desviación estándar\n",
        "\n",
        "$\\beta$: factor de desplazamiento (<em>offset</em>)\n",
        "\n",
        "Con la siguiente imagen se demuestra la diferencia entre estas técnicas. Cada subgráfico muestra un tensor de entrada donde N es el eje del lote; C, como eje del canal; y H (alto) y W (ancho), como los ejes espaciales (por ejemplo, el alto y el ancho de una foto). Los pixeles en azul están normalizados por la misma media y varianza, calculada por el agregado de los valores de estos pixeles.\n",
        "\n",
        "![](https://github.com/shaohua0116/Group-Normalization-Tensorflow/raw/master/figure/gn.png)\n",
        "\n",
        "Fuente: (https://arxiv.org/pdf/1803.08494.pdf)\n",
        "\n",
        "Los pesos gamma y beta se pueden entrenar en cualquiera de las capas de normalización para compensar por la posible pérdida de la capacidad de representación. Estos factores se pueden activar estableciendo el código `center` o la marca `scale` como `True`. Por supuesto, se pueden usar `initializers`, `constraints` y `regularizer` para `beta` y `gamma` a fin de ajustar estos valores durante el proceso de entrenamiento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2XlcXf5WBHb"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlbneoEUKrD"
      },
      "source": [
        "### Instalación de los componentes de Tensorflow 2.0 y Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZQGY_ALnirQ"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aGgPZG_WBHg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u82Gz_gOUPDZ"
      },
      "source": [
        "### Preparación del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wso9oidUZZQ"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTQH56j89POZ"
      },
      "source": [
        "## Tutorial para la normalización por grupos\n",
        "\n",
        "### Introducción\n",
        "\n",
        "En la normalización por grupos (GN), los canales de las entradas se dividen en subgrupos más pequeños y se normalizan estos valores, según su media y varianza. Dado que la normalización por grupos funciona con ejemplos únicos, podemos decir que esta técnica es independiente del tamaño del lote.\n",
        "\n",
        "La normalización por grupos a nivel experimental obtuvo una calificación cercana a la de la normalización por lotes en cuanto a tareas de clasificación de imágenes. Por lo tanto, puede ser beneficioso usar la normalización por grupos, en vez de la normalización por lotes, en aquellos casos en que el tamaño del lote en general es pequeño (<em>low</em>), algo que podría derivar en un mal funcionamiento de la normalización por lotes.\n",
        "\n",
        "### Ejemplo en el que se dividen 10 canales de una capa Conv2D en 5 subgrupos en un escenario estándar \"<em>channels last</em>\" (los canales al final):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIGjLwYWAm0v"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # Reshape into \"channels last\" setup.\n",
        "  tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n",
        "  tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"),\n",
        "  # Groupnorm Layer\n",
        "  tfa.layers.GroupNormalization(groups=5, axis=3),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMwUfJUib3ka"
      },
      "source": [
        "## Tutorial de normalización por instancias\n",
        "\n",
        "### Introducción\n",
        "\n",
        "La normalización por instancias es un caso especial de normalización por grupos en el que el tamaño del grupo es igual al tamaño del canal (o del tamaño del eje).\n",
        "\n",
        "Los resultados experimentales muestran que la normalización por instancias funciona bien en la transferencia de estilos cuando se usa en lugar de la normalización por lotes. Recientemente, la normalización por instancias también se ha usado como reemplazo de la normalización por lotes en redes GAN.\n",
        "\n",
        "### Ejemplo\n",
        "\n",
        "Aplicación de InstanceNormalization después de una capa Conv2D y uso de un factor escalar y de compensación inicializado uniforme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sLVv-C8f6Kf"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # Reshape into \"channels last\" setup.\n",
        "  tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n",
        "  tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"),\n",
        "  # LayerNorm Layer\n",
        "  tfa.layers.InstanceNormalization(axis=3, \n",
        "                                   center=True, \n",
        "                                   scale=True,\n",
        "                                   beta_initializer=\"random_uniform\",\n",
        "                                   gamma_initializer=\"random_uniform\"),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYdnEocRUCll"
      },
      "source": [
        "## Tutorial de normalización por capas\n",
        "\n",
        "### Introducción\n",
        "\n",
        "La normalización por capas es un caso especial de normalización por grupos en la que el tamaño del grupo es 1. La media y la desviación estándar se calcula a partir de todas las activaciones de una muestra individual.\n",
        "\n",
        "Los resultados experimentales muestran que la normalización por capas es adecuada para las redes neuronales recurrentes, ya que funcionan con independencia del tamaño del lote.\n",
        "\n",
        "### Ejemplo\n",
        "\n",
        "Aplicación de la normalización por capas después de una capa Conv2D y uso de un factor escalar y de compensación. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh-Pp_e5UB54"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # Reshape into \"channels last\" setup.\n",
        "  tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n",
        "  tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"),\n",
        "  # LayerNorm Layer\n",
        "  tf.keras.layers.LayerNormalization(axis=3 , center=True , scale=True),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shvGfnB0WpQQ"
      },
      "source": [
        "## Bibliografía\n",
        "\n",
        "[Layer norm](https://arxiv.org/pdf/1607.06450.pdf) (Normalización por capas)\n",
        "\n",
        "[Instance norm](https://arxiv.org/pdf/1607.08022.pdf) (Normalización por instancias)\n",
        "\n",
        "[Group Norm](https://arxiv.org/pdf/1803.08494.pdf) (Normalización por grupos)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "layers_normalizations.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
