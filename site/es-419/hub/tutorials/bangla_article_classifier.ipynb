{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDdZSPcLtKx4"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g5By3P4tavy"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS, \n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpaLrN0mteAS"
      },
      "source": [
        "# Clasificación de artículos en bengalí con TF-Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/bangla_article_classifier\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/hub/tutorials/bangla_article_classifier.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/hub/tutorials/bangla_article_classifier.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/hub/tutorials/bangla_article_classifier.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhN2WtIrBQ4y"
      },
      "source": [
        "Precaución: Además de instalar paquetes de Python con pip, en este bloc de notas se usa `sudo apt install` para instalar paquetes del sistema: `unzip`.\n",
        "\n",
        "En esta colaboración se demuestra el uso de [TensorFlow Hub](https://www.tensorflow.org/hub/) para la clasificación de idiomas diferentes del inglés. En este caso, elegimos [bengalí](https://en.wikipedia.org/wiki/Bengali_language) como idioma local y utilizamos incorporaciones de palabras previamente entrenadas para resolver tareas de clasificación multiclase, donde clasificamos artículos de noticias en bengalí de [fastText](https://fasttext.cc/docs/en/crawl-vectors.html), una biblioteca de Facebook con vectores de palabras preentrenados para 157 idiomas.\n",
        "\n",
        "Usaremos el exportador de incorporaciones preentrenadas de TF-Hub para convertir las incorporaciones de palabras en un módulo de incorporación de textos y luego usarlo para entrenar un clasificador con [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras), la API de alto nivel, fácil de usar, de Tensorflow para crear modelos de aprendizaje profundo. Incluso aunque usemos incorporaciones fastText en este caso, es posible exportar cualquier otra incorporación previamente entrenada de otras tareas y obtener rápidamente resultados con Tensorflow Hub. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4DN769E2O_R"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vt-StAAZguA"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# https://github.com/pypa/setuptools/issues/1694#issuecomment-466010982\n",
        "pip install gdown --no-use-pep517"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcBA19FlDPZO"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "sudo apt-get install -y unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSeyZMq-BYsu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import gdown\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FB7gLU4F54l"
      },
      "source": [
        "# Conjunto de datos\n",
        "\n",
        "Utilizaremos [BARD](https://www.researchgate.net/publication/328214545_BARD_Bangla_Article_Classification_Using_a_New_Comprehensive_Dataset) (Bangla Article Dataset, el conjunto de datos de artículos en bengalí) que tiene alrededor de 376 226 artículos recolectados a partir de diferentes portales de noticias en bengalí y están etiquetados según 5 categorías: economía, política, internacionales, deportes y entretenimiento. Descargamos el archivo de Google Drive. Este enlace ([bit.ly/BARD_DATASET](https://bit.ly/BARD_DATASET)) conecta con [este](https://github.com/tanvirfahim15/BARD-Bangla-Article-Classifier) repositorio de GitHub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdQrL_rwa-1K"
      },
      "outputs": [],
      "source": [
        "gdown.download(\n",
        "    url='https://drive.google.com/uc?id=1Ag0jd21oRwJhVFIBohmX_ogeojVtapLy',\n",
        "    output='bard.zip',\n",
        "    quiet=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2YW4GGa9Y5o"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "unzip -qo bard.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js75OARBF_B8"
      },
      "source": [
        "# Exportación de vectores de palabras previamente entrenados para módulo de TF-Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uAicYA6vLsf"
      },
      "source": [
        "TF-Hub ofrece <em>scripts</em> útiles para convertir incorporaciones de palabras en módulos de incorporación de textos para TF-hub [aquí](https://github.com/tensorflow/hub/tree/master/examples/text_embeddings_v2). Para hacer el módulo de bengalí o cualquier otro idioma, simplemente debemos descargar un archivo `.txt` o `.vec` de incorporación de textos en el mismo directorio que `export_v2.py` y ejecutar el <em>script</em>.\n",
        "\n",
        "El exportador lee los vectores de incorporación y los exporta a un [SavedModel](https://www.tensorflow.org/beta/guide/saved_model) de Tensorflow. El SavedModel contiene un programa completo de TensorFlow que incluye los pesos y el grafo. TF-Hub puede cargar el SavedModel como un [módulo](https://www.tensorflow.org/hub/api_docs/python/hub/Module) que usaremos para construir el modelo para clasificación de textos. Como estamos usando `tf.keras` para crear el modelo, utilizaremos [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer), que ofrece un encapsulador (<em>wrapper</em>) para que el módulo de TF-Hub lo use como una capa Keras.\n",
        "\n",
        "Primero, obtendremos las incorporaciones de palabras de fastText y el exportador de incorporaciones del [repositorio](https://github.com/tensorflow/hub) de TF-Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DY5Ze6pO1G5"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "curl -O https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.bn.300.vec.gz\n",
        "curl -O https://raw.githubusercontent.com/tensorflow/hub/master/examples/text_embeddings_v2/export_v2.py\n",
        "gunzip -qf cc.bn.300.vec.gz --k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAzdNZaHmdl1"
      },
      "source": [
        "Después, ejecutaremos el script exportador en nuestro archivo de incorporaciones. Como las incorporaciones de fastText tienen una línea de encabezado y son bastante grandes (alrededor de 3.3 GB para bengalí después de la conversión a un módulo) ignoramos la primera línea y exportamos solamente los primeros 100 000 tokens al módulo de incorporación de textos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tkv5acr_Q9UU"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "python export_v2.py --embedding_file=cc.bn.300.vec --export_path=text_module --num_lines_to_ignore=1 --num_lines_to_use=100000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9WEpmedF_3_"
      },
      "outputs": [],
      "source": [
        "module_path = \"text_module\"\n",
        "embedding_layer = hub.KerasLayer(module_path, trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQHbmS_D4YIo"
      },
      "source": [
        "El módulo de incorporación de textos toma un lote de oraciones en un tensor de 1 D de <em>strings</em> como entrada y emite como salida vectores de incorporación, con la forma (batch_size, embedding_dim), correspondientes a las oraciones. Se preprocesa la entrada dividiendo por los espacios. Las incorporaciones de textos se combinan en incorporaciones de oraciones con el combinador `sqrtn` (ver [aquí](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse)). Para demostrarlo pasamos una lista de palabras en bengalí como entrada y obtuvimos los vectores de incorporación correspondientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1MBnaBUihWn"
      },
      "outputs": [],
      "source": [
        "embedding_layer(['বাস', 'বসবাস', 'ট্রেন', 'যাত্রী', 'ট্রাক']) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KY8LiFOHmcd"
      },
      "source": [
        "# Conversión a conjunto de datos de Tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNguCDNe6bvz"
      },
      "source": [
        "Dado que el conjunto de datos es realmente grande, en vez de cargarlo completo en la memoria, usaremos un generador que produzca muestras en tiempo de ejecución, en lotes. Lo haremos utilizando las funciones del [conjunto de datos de Tensorflow](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). El conjunto de datos también está muy desbalanceado, entonces, antes de usar el generador, lo aleatorizaremos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYv6LqlEChO1"
      },
      "outputs": [],
      "source": [
        "dir_names = ['economy', 'sports', 'entertainment', 'state', 'international']\n",
        "\n",
        "file_paths = []\n",
        "labels = []\n",
        "for i, dir in enumerate(dir_names):\n",
        "  file_names = [\"/\".join([dir, name]) for name in os.listdir(dir)]\n",
        "  file_paths += file_names\n",
        "  labels += [i] * len(os.listdir(dir))\n",
        "  \n",
        "np.random.seed(42)\n",
        "permutation = np.random.permutation(len(file_paths))\n",
        "\n",
        "file_paths = np.array(file_paths)[permutation]\n",
        "labels = np.array(labels)[permutation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b-UtAP5TL-W"
      },
      "source": [
        "Podemos verificar la distribución de etiquetas en los ejemplos de entrenamiento y validación, después de la aleatorización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mimhWVSzzAmS"
      },
      "outputs": [],
      "source": [
        "train_frac = 0.8\n",
        "train_size = int(len(file_paths) * train_frac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BNXFrkotAYu"
      },
      "outputs": [],
      "source": [
        "# plot training vs validation distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(labels[0:train_size])\n",
        "plt.title(\"Train labels\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(labels[train_size:])\n",
        "plt.title(\"Validation labels\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVbHb2I3TUNA"
      },
      "source": [
        "Para crear un [conjunto de datos](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) con un generador, primero escribimos la función del generador que lee cada uno de los artículos de `file_paths` y las etiquetas del arreglo de etiquetas y produce un ejemplo de entrenamiento a cada paso. Pasamos esta función generadora al método [`tf.data.Dataset.from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) y especificamos los tipos de salida. Cada ejemplo de entrenamiento es una tupla que contiene un artículo de tipo de datos `tf.string` y una etiqueta codificada en un solo paso (<em>one-hot</em>). Dividimos el conjunto de datos con una separación para validación de entrenamiento de 80-20 con los métodos [`tf.data.Dataset.skip`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#skip) y [`tf.data.Dataset.take`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZRGTzEhUi7Q"
      },
      "outputs": [],
      "source": [
        "def load_file(path, label):\n",
        "    return tf.io.read_file(path), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g4nRflB7fbF"
      },
      "outputs": [],
      "source": [
        "def make_datasets(train_size):\n",
        "  batch_size = 256\n",
        "\n",
        "  train_files = file_paths[:train_size]\n",
        "  train_labels = labels[:train_size]\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
        "  train_ds = train_ds.map(load_file).shuffle(5000)\n",
        "  train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  test_files = file_paths[train_size:]\n",
        "  test_labels = labels[train_size:]\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((test_files, test_labels))\n",
        "  test_ds = test_ds.map(load_file)\n",
        "  test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "  return train_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PuuN6el8tv9"
      },
      "outputs": [],
      "source": [
        "train_data, validation_data = make_datasets(train_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrdZI6FqPJNP"
      },
      "source": [
        "# Entrenamiento y evaluación del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgr7YScGVS58"
      },
      "source": [
        "Como ya hemos agregado un encapsulador en torno a nuestro módulo para usarlo como cualquier otra capa de Keras, ahora podemos crear un modelo [secuencial](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) pequeño que sea una acumulación (pila) lineal de capas. Podemos agregar nuestro módulo de incorporación de textos con `model.add`, del mismo modo que con cualquier otra capa. Compilamos el modelo mediante la especificación de la pérdida y el optimizador, y lo entrenamos por 10 épocas. La API `tf.keras` puede tratar a los conjuntos de datos de TensorFlow como entradas, lo que nos permite pasar una instancia de un conjunto de datos al método adecuado para el entrenamiento del modelo. Dado que usamos la función del generador, `tf.data` trabajará con las muestras generadoras, agrupará en lotes y las enviará al modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhCqbDK2uUV5"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHUw807XPPM9"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=[], dtype=tf.string),\n",
        "    embedding_layer,\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(5),\n",
        "  ])\n",
        "  model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      optimizer=\"adam\", metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J4EXJUmPVNG"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "# Create earlystopping callback\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ7XJLg2u2No"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoBkN2tAaXWD"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_data, \n",
        "                    validation_data=validation_data, \n",
        "                    epochs=5, \n",
        "                    callbacks=[early_stopping_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoDk8otmMoT7"
      },
      "source": [
        "## Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ZRKGOsXEh4"
      },
      "source": [
        "Podemos visualizar las curvas de exactitud y pérdida para los datos de entrenamiento y validación con el objeto `tf.keras.callbacks.History` devuelto por el método `tf.keras.Model.fit`, que contiene los valores de pérdida y exactitud para cada época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6tOnByIOeGn"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D54IXLqcG8Cq"
      },
      "source": [
        "## Predicción\n",
        "\n",
        "Podemos obtener las predicciones para los datos de validación y controlar la matriz de confusión para ver el desempeño del modelo en cada una de las 5 clases. Porque el método `tf.keras.Model.predict` devuelve un arreglo n-d para las probabilidades de cada clase, que se pueden convertir en etiquetas de clase con `np.argmax`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dptEywzZJk4l"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dzeml6Pk0ub"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(y_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4M3Lzg8jHcB"
      },
      "outputs": [],
      "source": [
        "samples = file_paths[0:3]\n",
        "for i, sample in enumerate(samples):\n",
        "  f = open(sample)\n",
        "  text = f.read()\n",
        "  print(text[0:100])\n",
        "  print(\"True Class: \", sample.split(\"/\")[0])\n",
        "  print(\"Predicted Class: \", dir_names[y_pred[i]])\n",
        "  f.close()\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlDTIpMBu6h-"
      },
      "source": [
        "## Comparación del desempeño\n",
        "\n",
        "Ahora podemos tomar de `labels` las etiquetas correctas para los datos de validación y podemos compararlas con nuestras predicciones para obtener un [classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqrERUCS1Xn7"
      },
      "outputs": [],
      "source": [
        "y_true = np.array(labels[train_size:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX5w-NuTKuVP"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true, y_pred, target_names=dir_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5e9m3bV6oXK"
      },
      "source": [
        "También podemos comparar el desempeño de nuestro modelo con los resultados publicados obtenidos en la [publicación](https://www.researchgate.net/publication/328214545_BARD_Bangla_Article_Classification_Using_a_New_Comprehensive_Dataset) original, que tenían una precisión de 0.96. Los autores originales describen muchos pasos de preprocesamiento efectuados al conjunto de datos; tales como dejar caer puntuaciones y dígitos, quitando las primeras 25 palabras vacías frecuentes. Como podemos ver en el `classification_report`, ¡también logramos obtener una precisión y exactitud del 0.96 después de entrenar solamente durante 5 épocas sin ningún procesamiento previo!\n",
        "\n",
        "En este ejemplo, cuando creamos la capa Keras a partir de nuestro módulo de incorporaciones, preparamos el parámetro `trainable=False`; significa que los pesos de las incorporaciones no se actualizarán durante el entrenamiento. Pruebe definiéndolo como `True` para alcanzar una exactitud de aproximadamente el 97% con este conjunto de datos después de solamente 2 épocas. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IDdZSPcLtKx4"
      ],
      "name": "bangla_article_classifier.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
