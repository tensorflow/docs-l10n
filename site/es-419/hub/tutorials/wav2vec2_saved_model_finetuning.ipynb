{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCs7P9JTMlzV"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqn-HYw-Mkea"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stRetE8gMlmZ"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/wav2vec2_saved_model_finetuning\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/hub/tutorials/wav2vec2_saved_model_finetuning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/hub/tutorials/wav2vec2_saved_model_finetuning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/hub/tutorials/wav2vec2_saved_model_finetuning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "  <td>     <a href=\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelos de TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndG8MjmJeicp"
      },
      "source": [
        "# Ajuste de Wav2Vec2 con un cabezal de modelado de lenguaje\n",
        "\n",
        "En este cuaderno, cargaremos el modelo wav2vec2 preentrenado desde [TFHub](https://tfhub.dev). Para ajustarlo en el [conjunto de datos LibriSpeech](https://huggingface.co/datasets/librispeech_asr), agregaremos el cabezal de modelado de lenguaje (LM) en la parte superior de nuestro modelo preentrenado. La tarea subyacente es generar un modelo para el **reconocimiento de voz automático**, es decir, a partir de ingresar algo mediante la voz, el modelo debería poder transcribirlo a texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWk8nL6Ui-_0"
      },
      "source": [
        "## Configuración\n",
        "\n",
        "Antes de ejecutar este cuaderno, asegúrese de estar en tiempo de ejecución de GPU (`Runtime` &gt; `Change runtime type` &gt; `GPU`). La siguiente celda instalará el paquete [`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2) y sus dependencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seqTlMyeZvM4"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main\n",
        "!sudo apt-get install -y libsndfile1-dev\n",
        "!pip3 install -q SoundFile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuJL8-f0zn5"
      },
      "source": [
        "## Preparar el modelo con `TFHub`\n",
        "\n",
        "Comenzaremos por importar algunas bibliotecas/módulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3_fgx4eZvM7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from wav2vec2 import Wav2Vec2Config\n",
        "\n",
        "config = Wav2Vec2Config()\n",
        "\n",
        "print(\"TF version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0rVUxyWsS5f"
      },
      "source": [
        "Primero, descargaremos nuestro modelo de TFHub y envolveremos la signatura de nuestro modelo con [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) para poder usar este modelo como cualquier otra capa de Keras. Por suerte, `hub.KerasLayer` puede hacer ambas cosas en 1 sola línea.\n",
        "\n",
        "**Nota:** Al cargar el modelo con `hub.KerasLayer`, el modelo se vuelve un poco opaco pero a veces necesitamos controles más precisos sobre el modelo, luego podemos cargar el modelo con `tf.keras.models.load_model(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO6QRC7KZvM9"
      },
      "outputs": [],
      "source": [
        "pretrained_layer = hub.KerasLayer(\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\", trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCputyVBv2e9"
      },
      "source": [
        "Puede consultar este [script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/export2hub.py) en caso de que le interese el script de exportación del modelo. El objeto `pretrained_layer` es la versión inmovilizada de [`Wav2Vec2Model`](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/wav2vec2/modeling.py). Estos pesos preentrenados se convirtieron a partir de [pesos preentrenados en](https://huggingface.co/facebook/wav2vec2-base) HuggingFace PyTorch con [este script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/convert_torch_to_tf.py).\n",
        "\n",
        "Originalmente, wav2vec2 se preentrenó con un enfoque de modelado de lenguaje enmascarado con el objetivo de identificar la verdadera representación del habla latente cuantificada durante un paso de tiempo enmascarado. Puede leer más sobre el objetivo del entrenamiento en el artículo [wav2vec 2.0: Un marco para el aprendizaje autosupervisado de representaciones del habla](https://arxiv.org/abs/2006.11477)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SseDnCr7hyhC"
      },
      "source": [
        "Ahora definiremos algunas constantes e hiperparámetros que serán útiles en las siguientes celdas. `AUDIO_MAXLEN` se establece intencionalmente en `246000` ya que la signatura del modelo solo acepta una longitud de secuencia estática de `246000`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiILuMBERxlO"
      },
      "outputs": [],
      "source": [
        "AUDIO_MAXLEN = 246000\n",
        "LABEL_MAXLEN = 256\n",
        "BATCH_SIZE = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V4gTgGLgXvO"
      },
      "source": [
        "En la siguiente celda, envolveremos `pretrained_layer` y una capa densa (cabezal del modelado de lenguaje) con la [API funcional de Keras](https://www.tensorflow.org/guide/keras/functional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3CUN1KEB10Q"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(AUDIO_MAXLEN,))\n",
        "hidden_states = pretrained_layer(inputs)\n",
        "outputs = tf.keras.layers.Dense(config.vocab_size)(hidden_states)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zDXuoMXhDMo"
      },
      "source": [
        "La capa densa (definida anteriormente) tiene una dimensión de salida de `vocab_size`, ya que queremos predecir las probabilidades de cada token en el vocabulario en cada paso de tiempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPp18ZHRtnq-"
      },
      "source": [
        "## Configurar el estado de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATQy1ZK3vFr7"
      },
      "source": [
        "En TensorFlow, los pesos del modelo se crean solo cuando se llama `model.call` o `model.build` por primera vez, por lo que la siguiente celda creará los pesos del modelo por nosotros. Además, ejecutaremos `model.summary()` para verificar la cantidad total de parámetros entrenables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgL5wyaXZvM-"
      },
      "outputs": [],
      "source": [
        "model(tf.random.uniform(shape=(BATCH_SIZE, AUDIO_MAXLEN)))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxxA4Fevp7m"
      },
      "source": [
        "Ahora, necesitamos definir `loss_fn` y el optimizador para poder entrenar el modelo. La siguiente celda hará eso por nosotros. Usaremos el optimizador `Adam` por simplicidad. `CTCLoss` es un tipo de pérdida común que se usa para tareas (como `ASR`) donde las subpartes de entrada no se pueden alinear fácilmente con las subpartes de salida. Puede leer más sobre la pérdida de CTC en esta increíble [publicación de blog](https://distill.pub/2017/ctc/).\n",
        "\n",
        "`CTCLoss` (del paquete [`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2)) acepta 3 argumentos: `config`, `model_input_shape` y `division_factor`. Si `division_factor=1`, entonces la pérdida simplemente se sumará, así que pase `division_factor` en consecuencia para obtener la media sobre el lote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glDepVEHZvM_"
      },
      "outputs": [],
      "source": [
        "from wav2vec2 import CTCLoss\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "loss_fn = CTCLoss(config, (BATCH_SIZE, AUDIO_MAXLEN), division_factor=BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mvTuOXpwsQe"
      },
      "source": [
        "## Cargar y preprocesar los datos\n",
        "\n",
        "Ahora descargaremos el conjunto de datos LibriSpeech del [sitio web oficial](http://www.openslr.org/12) y lo configuraremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4kIEC77cBCM"
      },
      "outputs": [],
      "source": [
        "!wget https://www.openslr.org/resources/12/dev-clean.tar.gz -P ./data/train/\n",
        "!tar -xf ./data/train/dev-clean.tar.gz -C ./data/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsQpmpn6jrMI"
      },
      "source": [
        "**Nota:** Estamos usando la configuración `dev-clean` ya que este cuaderno es solo para fines de demostración, por lo que necesitamos una pequeña cantidad de datos. Los datos completos del entrenamiento se pueden descargar fácilmente desde [el sitio web de LibriSpeech](http://www.openslr.org/12)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynxAjtGHGFpM"
      },
      "outputs": [],
      "source": [
        "ls ./data/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBMiORo0xJD0"
      },
      "source": [
        "Nuestro conjunto de datos se encuentra en el directorio LibriSpeech. Exploremos estos archivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkIu_Wt4ZvNA"
      },
      "outputs": [],
      "source": [
        "data_dir = \"./data/train/LibriSpeech/dev-clean/2428/83705/\"\n",
        "all_files = os.listdir(data_dir)\n",
        "\n",
        "flac_files = [f for f in all_files if f.endswith(\".flac\")]\n",
        "txt_files = [f for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "print(\"Transcription files:\", txt_files, \"\\nSound files:\", flac_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEObi_Apk3ZD"
      },
      "source": [
        "Muy bien, entonces cada subdirectorio tiene muchos archivos `.flac` y un archivo `.txt`. El archivo `.txt` contiene transcripciones de texto para todas las muestras de voz (es decir, archivos `.flac`) presentes en ese subdirectorio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYW6WKJflO2e"
      },
      "source": [
        "Podemos cargar estos datos de texto de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEBKxQblHPwq"
      },
      "outputs": [],
      "source": [
        "def read_txt_file(f):\n",
        "  with open(f, \"r\") as f:\n",
        "    samples = f.read().split(\"\\n\")\n",
        "    samples = {s.split()[0]: \" \".join(s.split()[1:]) for s in samples if len(s.split()) > 2}\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldkf_ceb0_YW"
      },
      "source": [
        "De manera similar, definiremos una función para cargar una muestra de voz desde un archivo `.flac`.\n",
        "\n",
        "`REQUIRED_SAMPLE_RATE` está configurado en `16000` ya que wav2vec2 fue preentrenado con una frecuencia `16K` y se recomienda ajustarlo sin ningún cambio importante en la distribución de datos debido a la frecuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOJ3OzPsTyXv"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "REQUIRED_SAMPLE_RATE = 16000\n",
        "\n",
        "def read_flac_file(file_path):\n",
        "  with open(file_path, \"rb\") as f:\n",
        "      audio, sample_rate = sf.read(f)\n",
        "  if sample_rate != REQUIRED_SAMPLE_RATE:\n",
        "      raise ValueError(\n",
        "          f\"sample rate (={sample_rate}) of your files must be {REQUIRED_SAMPLE_RATE}\"\n",
        "      )\n",
        "  file_id = os.path.split(file_path)[-1][:-len(\".flac\")]\n",
        "  return {file_id: audio}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sxDN8P4nWkW"
      },
      "source": [
        "Ahora, elegiremos algunas muestras aleatorias e intentaremos visualizarlas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI5J-2Dfm_wT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "import random\n",
        "\n",
        "file_id = random.choice([f[:-len(\".flac\")] for f in flac_files])\n",
        "flac_file_path, txt_file_path = os.path.join(data_dir, f\"{file_id}.flac\"), os.path.join(data_dir, \"2428-83705.trans.txt\")\n",
        "\n",
        "print(\"Text Transcription:\", read_txt_file(txt_file_path)[file_id], \"\\nAudio:\")\n",
        "Audio(filename=flac_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8jJ7Ed81p_A"
      },
      "source": [
        "Ahora, combinaremos todas las muestras de voz y texto y definiremos la función (en la siguiente celda) para ese propósito."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI-5YCzaTsei"
      },
      "outputs": [],
      "source": [
        "def fetch_sound_text_mapping(data_dir):\n",
        "  all_files = os.listdir(data_dir)\n",
        "\n",
        "  flac_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".flac\")]\n",
        "  txt_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "  txt_samples = {}\n",
        "  for f in txt_files:\n",
        "    txt_samples.update(read_txt_file(f))\n",
        "\n",
        "  speech_samples = {}\n",
        "  for f in flac_files:\n",
        "    speech_samples.update(read_flac_file(f))\n",
        "\n",
        "  assert len(txt_samples) == len(speech_samples)\n",
        "\n",
        "  samples = [(speech_samples[file_id], txt_samples[file_id]) for file_id in speech_samples.keys() if len(speech_samples[file_id]) < AUDIO_MAXLEN]\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx95Lxvu0nT4"
      },
      "source": [
        "Llegó la hora de ver algunas muestras..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ls7X_jqIz4R"
      },
      "outputs": [],
      "source": [
        "samples = fetch_sound_text_mapping(data_dir)\n",
        "samples[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjhSWfsnlCL"
      },
      "source": [
        "Nota: Estamos cargando estos datos en la memoria mientras trabajamos con una pequeña cantidad de conjunto de datos en este cuaderno. Pero para entrenar con el conjunto de datos completo (300 GB apróx.), tendrá que cargar los datos de forma diferida. Puede consultar [este script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/data_utils.py) para obtener más información al respecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg8Zia1kzw0J"
      },
      "source": [
        "¡Preprocesemos los datos ya!\n",
        "\n",
        "Primero definiremos el tokenizador y el procesador con el paquete `gsoc-wav2vec2`. Luego, haremos un preprocesamiento muy simple. `processor` normalizará el eje de fotogramas en relación con la voz sin procesar y `tokenizer` convertirá las salidas de nuestro modelo en la cadena de texto (usando el vocabulario definido) y se encargará de eliminar los tokens especiales (según la configuración de su tokenizador)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaat_hMLNVHF"
      },
      "outputs": [],
      "source": [
        "from wav2vec2 import Wav2Vec2Processor\n",
        "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
        "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  label = tokenizer(text)\n",
        "  return tf.constant(label, dtype=tf.int32)\n",
        "\n",
        "def preprocess_speech(audio):\n",
        "  audio = tf.constant(audio, dtype=tf.float32)\n",
        "  return processor(tf.transpose(audio))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyKl8QP-zRFC"
      },
      "source": [
        "Ahora, definiremos el generador de Python para llamar a las funciones de preprocesamiento que definimos en las celdas anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoQrRalwMpQ6"
      },
      "outputs": [],
      "source": [
        "def inputs_generator():\n",
        "  for speech, text in samples:\n",
        "    yield preprocess_speech(speech), preprocess_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vlm3ySFULsG"
      },
      "source": [
        "## Configurar `tf.data.Dataset`\n",
        "\n",
        "En la siguiente celda configuraremos el objeto `tf.data.Dataset` con su método `.from_generator(...)`. Usaremos el objeto `generator` que definimos en la celda anterior.\n",
        "\n",
        "**Nota:** Para el entrenamiento distribuido (especialmente en las unidades de procesamiento de tensores [TPU por sus siglas en inglés]), `.from_generator(...)` no funciona actualmente y se recomienda entrenar con datos almacenados en formato `.tfrecord`. (Nota: Lo ideal es que los archivos  TFRecords se almacenen dentro de un depósito de Google Cloud Storage para que las TPU funcionen al máximo).\n",
        "\n",
        "Puede consultar [este script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/make_tfrecords.py) para obtener más detalles sobre cómo convertir datos de LibriSpeech en tfrecords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbQ_dMwGO62h"
      },
      "outputs": [],
      "source": [
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None),  dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        ")\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(inputs_generator, output_signature=output_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXBbNsRyPyw3"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(flac_files)\n",
        "SEED = 42\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DAUmns3pXfr"
      },
      "source": [
        "Pasaremos el conjunto de datos a varios lotes, así que preparemos los lotes en la siguiente celda. Ahora, todas las secuencias de un lote deben rellenarse hasta una longitud constante. Usaremos el método `.padded_batch(...)` para ese propósito."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okhko1IWRida"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(AUDIO_MAXLEN, LABEL_MAXLEN), padding_values=(0.0, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A45CjQG5qSbV"
      },
      "source": [
        "Los aceleradores (como GPU/TPU) son muy rápidos y, a menudo, la carga de datos (y el preprocesamiento) se convierte en un cuello de botella durante el entrenamiento, ya que la parte de carga de datos ocurre en las CPU. Esto puede aumentar significativamente el tiempo de entrenamiento, especialmente cuando se necesita mucho preprocesamiento en línea o los datos se transmiten en línea desde depósitos de GCS. Para tratar estos problemas, `tf.data.Dataset` ofrece el método `.prefetch(...)`. Este método ayuda a preparar los siguientes lotes en paralelo (en CPU) mientras el modelo realiza predicciones (en GPU/TPU) en el lote actual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-bKu2YjRior"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqk2cs6LxVIh"
      },
      "source": [
        "Dado que este cuaderno está creado con fines de demostración, tomaremos primero `num_train_batches` y solo lo entrenaremos para eso. Sin embargo, le recomendamos entrenar  todo el conjunto de datos. De manera similar, evaluaremos solo `num_val_batches`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6GO5oYUxXtz"
      },
      "outputs": [],
      "source": [
        "num_train_batches = 10\n",
        "num_val_batches = 4\n",
        "\n",
        "train_dataset = dataset.take(num_train_batches)\n",
        "val_dataset = dataset.skip(num_train_batches).take(num_val_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzAOI78tky08"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Para entrenar nuestro modelo, llamaremos directamente al método `.fit(...)` después de compilar nuestro modelo con `.compile(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuBY2sZElgwg"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer, loss=loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qswxafSl0HjO"
      },
      "source": [
        "La celda anterior configurará nuestro estado de entrenamiento. Ahora podemos iniciar el entrenamiento con el método `.fit(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtuSfnj1l-I_"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
        "history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySvp8r2E1q_V"
      },
      "source": [
        "Guardemos nuestro modelo con el método `.save(...)` para poder realizar la inferencia más adelante. También puede exportar este SavedModel a TFHub siguiendo los pasos de [la documentación de TFHub](https://www.tensorflow.org/hub/publish)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0KEYcwydwjF"
      },
      "outputs": [],
      "source": [
        "save_dir = \"finetuned-wav2vec2\"\n",
        "model.save(save_dir, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkOpp9rZ211t"
      },
      "source": [
        "Nota: Estamos configurando `include_optimizer=False` porque queremos usar este modelo solo para inferencias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJfPlTgezD0i"
      },
      "source": [
        "## Evaluación\n",
        "\n",
        "Ahora calcularemos la tasa de error de palabras sobre el conjunto de datos de validación.\n",
        "\n",
        "**La tasa de error de palabras** (WER por sus siglas en inglés) es una métrica común para medir el rendimiento de un sistema de reconocimiento de voz automático. El WER se deriva de la distancia de Levenshtein, trabajando a nivel de palabra. La tasa de error de palabra se puede calcular como: WER = (S + D + I) / N = (S + D + I) / (S + D + C) donde S es el número de sustituciones, D es el número de eliminaciones, I es el número de inserciones, C es el número de palabras correctas, N es el número de palabras en la referencia (N=S+D+C). Este valor indica el porcentaje de palabras que se predijeron incorrectamente.\n",
        "\n",
        "Puede consultar [este documento](https://www.isca-speech.org/archive_v0/interspeech_2004/i04_2765.html) para obtener más información sobre WER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_91Y7-r3xu"
      },
      "source": [
        "Usaremos la función `load_metric(...)` de la biblioteca de [conjuntos de datos HuggingFace](https://huggingface.co/docs/datasets/). Primero instalemos la biblioteca de `datasets` con `pip` y luego definamos el objeto `metric`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW9F_oVDU1TZ"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q datasets\n",
        "\n",
        "from datasets import load_metric\n",
        "metric = load_metric(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssWXWc7CZvNB"
      },
      "outputs": [],
      "source": [
        "@tf.function(jit_compile=True)\n",
        "def eval_fwd(batch):\n",
        "  logits = model(batch, training=False)\n",
        "  return tf.argmax(logits, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFh1myg1x4ua"
      },
      "source": [
        "Ya podemos ejecutar la evaluación de los datos de validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQTFVjZghckJ"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for speech, labels in tqdm(val_dataset, total=num_val_batches):\n",
        "    predictions  = eval_fwd(speech)\n",
        "    predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "    references = [tokenizer.decode(label, group_tokens=False) for label in labels.numpy().tolist()]\n",
        "    metric.add_batch(references=references, predictions=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCc8qBesv3e"
      },
      "source": [
        "Estamos usando el método `tokenizer.decode(...)` para volver a decodificar nuestras predicciones y etiquetas en el texto y las agregaremos a la métrica para el cálculo `WER` más adelante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI_URj8Wtb2g"
      },
      "source": [
        "Ahora, calculemos el valor de la métrica en la siguiente celda:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a83wekLgWMod"
      },
      "outputs": [],
      "source": [
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cD1OgVEjl4"
      },
      "source": [
        "**Nota:** Aquí el valor de la métrica no tiene ningún sentido ya que el modelo se entrena con datos muy pequeños y las tareas como las de reconocimiento de voz automático (ASR por sus siglas en inglés) a menudo requieren una gran cantidad de datos para aprender una asignación de voz a texto. Probablemente debería entrenar con una gran cantidad de datos para obtener buenos resultados. Este cuaderno le proporciona una plantilla para ajustar un modelo de voz preentrenado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G14o706kdTE1"
      },
      "source": [
        "## Inferencia\n",
        "\n",
        "Ahora que estamos satisfechos con el proceso de entrenamiento y hemos guardado el modelo en `save_dir`, veremos cómo se puede usar este modelo para la inferencia.\n",
        "\n",
        "Primero, cargaremos nuestro modelo con `tf.keras.models.load_model(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrTrExiUdaED"
      },
      "outputs": [],
      "source": [
        "finetuned_model = tf.keras.models.load_model(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luodSroz20SR"
      },
      "source": [
        "Descarguemos algunas muestras de voz para realizar inferencias. También puede reemplazar la siguiente muestra con su muestra de voz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUE0shded6Ej"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/SA2.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycBjU_U53FjL"
      },
      "source": [
        "Ahora, leeremos la muestra de voz con `soundfile.read(...)` y la rellenaremos con `AUDIO_MAXLEN` para satisfacer la signatura del modelo. Luego normalizaremos esa muestra de voz con la instancia `Wav2Vec2Processor` y la ingresaremos en el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7CARje4d5_H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "speech, _ = sf.read(\"SA2.wav\")\n",
        "speech = np.pad(speech, (0, AUDIO_MAXLEN - len(speech)))\n",
        "speech = tf.expand_dims(processor(tf.constant(speech)), 0)\n",
        "\n",
        "outputs = finetuned_model(speech)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUSttSPa30qP"
      },
      "source": [
        "Decodifiquemos los números nuevamente en una secuencia de texto con la instancia `Wav2Vec2tokenizer`, que definimos anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYdJqxQ4llgI"
      },
      "outputs": [],
      "source": [
        "predictions = tf.argmax(outputs, axis=-1)\n",
        "predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXC757bztJc"
      },
      "source": [
        "Esta predicción es bastante aleatoria ya que el modelo nunca se entrenó con datos grandes en este cuaderno (pués este cuaderno no está diseñado para realizar un entrenamiento completo). Obtendrá buenas predicciones si entrena este modelo en el conjunto de datos completo de LibriSpeech.\n",
        "\n",
        "Por fin llegamos al final de este cuaderno. Pero no es el final del aprendizaje de TensorFlow para tareas relacionadas con la voz; este [repositorio](https://github.com/tulasiram58827/TTS_TFLite) contiene más tutoriales increíbles. En caso de que encuentre algún error en este cuaderno, cree un problema [aquí](https://github.com/vasudevgupta7/gsoc-wav2vec2/issues)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rWk8nL6Ui-_0",
        "wvuJL8-f0zn5",
        "oPp18ZHRtnq-",
        "1mvTuOXpwsQe",
        "7Vlm3ySFULsG",
        "CzAOI78tky08",
        "SJfPlTgezD0i",
        "G14o706kdTE1"
      ],
      "name": "wav2vec2_saved_model_finetuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
