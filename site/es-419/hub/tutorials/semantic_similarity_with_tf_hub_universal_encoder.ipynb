{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUymE2l9GZfO"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "JMyTNwSJGGWg"
      },
      "outputs": [],
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co7MV6sX7Xto"
      },
      "source": [
        "# Codificador de oraciones universal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "  <td>     <a href=\"https://tfhub.dev/s?q=google%2Funiversal-sentence-encoder%2F4%20OR%20google%2Funiversal-sentence-encoder-large%2F5\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelos de TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAVQGidpL8v5"
      },
      "source": [
        "En este cuaderno, se ilustra cómo acceder al Codificador universal de oraciones y usarlo para tareas de similitud y clasificación de oraciones.\n",
        "\n",
        "El Codificador de oraciones universal hace que obtener incorporaciones a nivel de oración sea tan fácil como lo ha sido históricamente buscar incorporaciones de palabras individuales. Las incorporaciones de oraciones se pueden usar trivialmente para calcular la similitud de significado a nivel de oración, así como para permitir un mejor rendimiento en las tareas de clasificación posteriores con datos de entrenamiento menos supervisados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOTzp8O36CyQ"
      },
      "source": [
        "## Preparación\n",
        "\n",
        "En esta sección, se configura el entorno para acceder al Codificador de oraciones universal en TF Hub y se proporcionan ejemplos de cómo aplicar el codificador a palabras, oraciones y párrafos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVjNK8shFKOC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Pd3nJnTl-i"
      },
      "source": [
        "Puede encontrar información más detallada sobre la instalación de Tensorflow en [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zwty8Z6mAkdV"
      },
      "outputs": [],
      "source": [
        "#@title Load the Universal Sentence Encoder's TF Hub module\n",
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8F4LNGFqOiq"
      },
      "outputs": [],
      "source": [
        "#@title Compute a representation for each message, showing various lengths supported.\n",
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the embedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "# Reduce logging output.\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "message_embeddings = embed(messages)\n",
        "\n",
        "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "  print(\"Message: {}\".format(messages[i]))\n",
        "  print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "  message_embedding_snippet = \", \".join(\n",
        "      (str(x) for x in message_embedding[:3]))\n",
        "  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnvjATdy64eR"
      },
      "source": [
        "# Ejemplo de tarea de similitud textual semántica\n",
        "\n",
        "Las incorporaciones que produce el Codificador de oraciones universal están casa normalizadas. La similitud semántica de dos oraciones puede calcularse trivialmente como el producto interno de las codificaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1FFCTKm7ba4"
      },
      "outputs": [],
      "source": [
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "def run_and_plot(messages_):\n",
        "  message_embeddings_ = embed(messages_)\n",
        "  plot_similarity(messages_, message_embeddings_, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "339tuJ5Pwqqv"
      },
      "source": [
        "## Similitud visualizada\n",
        "\n",
        "Aquí le mostramos la similitud en un mapa térmico. El gráfico final es una matriz de 9x9 donde cada entrada `[i, j]` tiene un color según el producto interno de las codificaciones de las oraciones `i` y `j`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPMCaxrZwp7t"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    # Smartphones\n",
        "    \"I like my phone\",\n",
        "    \"My phone is not good.\",\n",
        "    \"Your cellphone looks great.\",\n",
        "\n",
        "    # Weather\n",
        "    \"Will it snow tomorrow?\",\n",
        "    \"Recently a lot of hurricanes have hit the US\",\n",
        "    \"Global warming is real\",\n",
        "\n",
        "    # Food and health\n",
        "    \"An apple a day, keeps the doctors away\",\n",
        "    \"Eating strawberries is healthy\",\n",
        "    \"Is paleo better than keto?\",\n",
        "\n",
        "    # Asking about age\n",
        "    \"How old are you?\",\n",
        "    \"what is your age?\",\n",
        "]\n",
        "\n",
        "run_and_plot(messages)\n",
        "               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FjdeCqPJeg-"
      },
      "source": [
        "## Evaluación: prueba comparativa de STS (similitud textual semántica)\n",
        "\n",
        "La [**prueba comparativa de STS**](https://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) proporciona una evaluación intrínseca del grado en que las puntuaciones de la similitud que se calculan mediante incorporaciones de oraciones se alinean con el criterio humano. El punto de referencia requiere que los sistemas devuelvan puntuaciones de similitud para una selección diversa de pares de oraciones. Luego se usa la [correlación de Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) para evaluar la calidad de las puntuaciones de similitud de la máquina frente al criterio humano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5nuBbI1iFQR"
      },
      "source": [
        "### Descargar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOs8ZfOnJeBF"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import scipy\n",
        "import math\n",
        "import csv\n",
        "\n",
        "sts_dataset = tf.keras.utils.get_file(\n",
        "    fname=\"Stsbenchmark.tar.gz\",\n",
        "    origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
        "    extract=True)\n",
        "sts_dev = pandas.read_table(\n",
        "    os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"),\n",
        "    skip_blank_lines=True,\n",
        "    usecols=[4, 5, 6],\n",
        "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
        "sts_test = pandas.read_table(\n",
        "    os.path.join(\n",
        "        os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"),\n",
        "    quoting=csv.QUOTE_NONE,\n",
        "    skip_blank_lines=True,\n",
        "    usecols=[4, 5, 6],\n",
        "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
        "# cleanup some NaN values in sts_dev\n",
        "sts_dev = sts_dev[[isinstance(s, str) for s in sts_dev['sent_2']]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OKy8WhnKRe_"
      },
      "source": [
        "### Evaluar incorporaciones de oraciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-q2r7jyZGb7"
      },
      "outputs": [],
      "source": [
        "sts_data = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n",
        "\n",
        "def run_sts_benchmark(batch):\n",
        "  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)\n",
        "  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)\n",
        "  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
        "  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
        "  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\n",
        "  \"\"\"Returns the similarity scores\"\"\"\n",
        "  return scores\n",
        "\n",
        "dev_scores = sts_data['sim'].tolist()\n",
        "scores = []\n",
        "for batch in np.array_split(sts_data, 10):\n",
        "  scores.extend(run_sts_benchmark(batch))\n",
        "\n",
        "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\n",
        "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
        "    pearson_correlation[0], pearson_correlation[1]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RUymE2l9GZfO"
      ],
      "name": "semantic_similarity_with_tf_hub_universal_encoder.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
