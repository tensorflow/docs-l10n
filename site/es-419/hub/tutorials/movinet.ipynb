{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toCy3v03Dwx7"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKe-ubNcDvgv"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# MoViNet para reconocimiento de acciones en <em>streaming</em> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/movinet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/hub/tutorials/movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/hub/tutorials/movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/hub/tutorials/movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/collections/movinet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelos de TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vxk2Kbc_KSP"
      },
      "source": [
        "En este tutorial se demuestra cómo usar un modelo de clasificación de video previamente entrenado para clasificar una actividad (como bailar, nadar, pedalear, etc.) en un video dado.\n",
        "\n",
        "La arquitectura del modelo usada en este tutorial se denomina [MoViNet](https://arxiv.org/pdf/2103.11511.pdf) (Mobile Video Networks, redes móviles de video). Las MoVieNet son una familia de modelos de clasificación de video eficientes entrenados con conjuntos de datos muy grandes ([Kinetics 600](https://deepmind.com/research/open-source/kinetics)).\n",
        "\n",
        "Por el contrario a lo que sucede con los [modelos i3d](https://tfhub.dev/s?q=i3d-kinetics) disponibles en TF Hub, las MoViNet también se pueden utilizar con inferencias cuadro por cuadro en transmisión de videos.\n",
        "\n",
        "Los videos preentrenados se encuentran disponibles en [TF Hub](https://tfhub.dev/google/collections/movinet/1). La colección de TF Hub también incluye modelos cuantificados optimizados para [TFLite](https://tensorflow.org/lite).\n",
        "\n",
        "La fuente para estos modelos se encuentra en [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/official/projects/movinet). Incluye una [versión más extensa de este tutorial](https://colab.sandbox.google.com/github/tensorflow/models/blob/master/official/projects/movinet/movinet_tutorial.ipynb) que también abarca la construcción y el ajuste fino de un modelo MoViNet.\n",
        "\n",
        "Este tutorial sobre MoViNet es parte de una serie de tutoriales en video de TensorFlow. A continuación, compartimos otros tres tutoriales:\n",
        "\n",
        "- [Carga de datos de video](https://www.tensorflow.org/tutorials/load_data/video): en este tutorial se explica cómo cargar y preprocesar datos de video desde cero para una canalización de conjuntos de datos de TensorFlow.\n",
        "- [Creación de un modelo 3D CNN para la clasificación de video](https://www.tensorflow.org/tutorials/video/video_classification): tenga en cuenta que en este tutorial se usa (2+1)D CNN que descompone los aspectos espaciales y temporales de los datos en 3D. Si usa datos volumétricos como un escaneo MRI, considere utilizar un 3D CNN en vez de un (2+1)D CNN.\n",
        "- [Transferencia de aprendizaje para la clasificación de videos con MoViNet](https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet): en este tutorial se explica cómo usar, con el conjunto de datos UCF-101, un modelo de clasificación de videos previamente entrenado en un conjunto de datos diferente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E96e1UKQ8uR"
      },
      "source": [
        "![Corto de saltos tijera](https://storage.googleapis.com/tf_model_garden/vision/movinet/artifacts/jumpingjacks_plot.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_oLnvJy7kz5"
      },
      "source": [
        "## Preparación\n",
        "\n",
        "Para inferir a partir de modelos más pequeños (A0-A2), la CPU es suficiente en este caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUgUMGmY1yq-"
      },
      "outputs": [],
      "source": [
        "!sudo apt install -y ffmpeg\n",
        "!pip install -q mediapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3khsunT7kWa"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -q -y opencv-python-headless\n",
        "!pip install -q \"opencv-python-headless<4.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI_1csl6Q-gH"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pathlib\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tqdm\n",
        "\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 10,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn8K9oWbmREi"
      },
      "source": [
        "Tomamos una lista de 600 etiquetas cinéticas e imprimimos las primeras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VJUAcjhkfb3"
      },
      "outputs": [],
      "source": [
        "labels_path = tf.keras.utils.get_file(\n",
        "    fname='labels.txt',\n",
        "    origin='https://raw.githubusercontent.com/tensorflow/models/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/kinetics_600_labels.txt'\n",
        ")\n",
        "labels_path = pathlib.Path(labels_path)\n",
        "\n",
        "lines = labels_path.read_text().splitlines()\n",
        "KINETICS_600_LABELS = np.array([line.strip() for line in lines])\n",
        "KINETICS_600_LABELS[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9BU5XsOmaq3"
      },
      "source": [
        "Para trabajar en la clasificación con un video de ejemplo simple, podemos cargar un gif corto de una persona haciendo saltos tijera.\n",
        "\n",
        "![saltos tijera](https://github.com/tensorflow/models/raw/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/jumpingjack.gif)\n",
        "\n",
        "Créditos: la filmación ha sido compartida por [Coach Bobby Bluford](https://www.youtube.com/watch?v=-AxHpj-EuPg) en YouTube bajo licencia de CC-BY."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aFKMbr4mfSg"
      },
      "source": [
        "Descargamos el gif."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w62jqXhaSb15"
      },
      "outputs": [],
      "source": [
        "jumpingjack_url = 'https://github.com/tensorflow/models/raw/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/jumpingjack.gif'\n",
        "jumpingjack_path = tf.keras.utils.get_file(\n",
        "    fname='jumpingjack.gif',\n",
        "    origin=jumpingjack_url,\n",
        "    cache_dir='.', cache_subdir='.',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdRS_22PebfB"
      },
      "source": [
        "Definimos una función para leer un archivo gif en un `tf.Tensor`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPhmCu6oSi5f"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Read and process a video\n",
        "def load_gif(file_path, image_size=(224, 224)):\n",
        "  \"\"\"Loads a gif file into a TF tensor.\n",
        "\n",
        "  Use images resized to match what's expected by your model.\n",
        "  The model pages say the \"A2\" models expect 224 x 224 images at 5 fps\n",
        "\n",
        "  Args:\n",
        "    file_path: path to the location of a gif file.\n",
        "    image_size: a tuple of target size.\n",
        "\n",
        "  Returns:\n",
        "    a video of the gif file\n",
        "  \"\"\"\n",
        "  # Load a gif file, convert it to a TF tensor\n",
        "  raw = tf.io.read_file(file_path)\n",
        "  video = tf.io.decode_gif(raw)\n",
        "  # Resize the video\n",
        "  video = tf.image.resize(video, image_size)\n",
        "  # change dtype to a float32\n",
        "  # Hub models always want images normalized to [0,1]\n",
        "  # ref: https://www.tensorflow.org/hub/common_signatures/images#input\n",
        "  video = tf.cast(video, tf.float32) / 255.\n",
        "  return video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx7cZm8vpDJm"
      },
      "source": [
        "El formato de video es `(frames, height, width, colors)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7k_PmbFSkHv"
      },
      "outputs": [],
      "source": [
        "jumpingjack=load_gif(jumpingjack_path)\n",
        "jumpingjack.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcKFy3oedBvF"
      },
      "source": [
        "## Cómo se usa el modelo\n",
        "\n",
        "Esta sección contiene una descripción en la que se muestra paso a paso cómo se usan los [modelos de TensorFlow Hub](https://tfhub.dev/google/collections/movinet/1). Si lo que quiere, solamente, es ver los modelos en acción, puede saltarse la siguiente sección.\n",
        "\n",
        "Hay dos versiones para cada modelo: `base` y `streaming`.\n",
        "\n",
        "- La versión `base` toma un video como entrada y devuelve las probabilidades calculadas en base al promedio de los fotogramas.\n",
        "- La versión `streaming` toma un fotograma de video y un estado de RNN como entrada, y devuelve las predicciones para ese fotograma y el nuevo estado RNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQO6Zb8Hm-9q"
      },
      "source": [
        "### El modelo base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfnYU20JnPqp"
      },
      "source": [
        "Descargue el [modelo previamente entrenado de TensorFlow Hub](https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/3). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnpPo6HSR7qv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "id = 'a2'\n",
        "mode = 'base'\n",
        "version = '3'\n",
        "hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'\n",
        "model = hub.load(hub_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvaFwKhxndmb"
      },
      "source": [
        "Esta versión del modelo tiene una `signature`. Toma un argumento de `image` que es un `tf.float32` con formato `(batch, frames, height, width, colors)`. Devuelve un diccionario que contiene una salida: un tensor `tf.float32` de funciones <em>logit</em> con formato `(batch, classes)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GzZ4Y03T_gH"
      },
      "outputs": [],
      "source": [
        "sig = model.signatures['serving_default']\n",
        "print(sig.pretty_printed_signature())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Xny1ANomi4"
      },
      "source": [
        "Para ejecutar la firma en el video, primero hay que agregar la dimensión del `batch` exterior al video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBOFEDG1XvZE"
      },
      "outputs": [],
      "source": [
        "#warmup\n",
        "sig(image = jumpingjack[tf.newaxis, :1]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCeW3KycVbGn"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "logits = sig(image = jumpingjack[tf.newaxis, ...])\n",
        "logits = logits['classifier_head'][0]\n",
        "\n",
        "print(logits.shape)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE8doqkPpxED"
      },
      "source": [
        "Defina una función `get_top_k` que empaquete el procesamiento de salida que figura arriba para después."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OozPNO6LvZ00"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Get top_k labels and probabilities\n",
        "def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
        "  \"\"\"Outputs the top k model labels and probabilities on the given video.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    k: the number of top predictions to select.\n",
        "    label_map: a list of labels to map logit indices to label strings.\n",
        "\n",
        "  Returns:\n",
        "    a tuple of the top-k labels and probabilities.\n",
        "  \"\"\"\n",
        "  # Sort predictions to find top_k\n",
        "  top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
        "  # collect the labels of top_k predictions\n",
        "  top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
        "  # decode lablels\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "  # top_k probabilities of the predictions\n",
        "  top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
        "  return tuple(zip(top_labels, top_probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfKMT29pP_Z"
      },
      "source": [
        "Convierta las `logits` a probabilidades y busque las 5 clases principales de video. El modelo confirma que el video probablemente sea de `jumping jacks` (saltos tijera)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-SrNGsGV5Mt"
      },
      "outputs": [],
      "source": [
        "probs = tf.nn.softmax(logits, axis=-1)\n",
        "for label, p in get_top_k(probs):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltdijoQpqjxZ"
      },
      "source": [
        "### El modelo de <em>streaming</em>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dqdUPQXq45b"
      },
      "source": [
        "En la sección anterior usamos un modelo que se ejecuta en un video completo. Por lo general, cuando se procesa el video no se pretende lograr una sola predicción al final, lo que se busca es que las predicciones se actualicen fotograma a fotograma. Las versiones `stream` del modelo permiten hacerlo.\n",
        "\n",
        "Cargue la versión `stream` del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxt0hRXFZkAM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "id = 'a2'\n",
        "mode = 'stream'\n",
        "version = '3'\n",
        "hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'\n",
        "model = hub.load(hub_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDswtsGgsYGS"
      },
      "source": [
        "Este modelo es un poco más complejo que el `base`. Hay que controlar el estado interno de las RNN del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fM_Vb1VsbDm"
      },
      "outputs": [],
      "source": [
        "list(model.signatures.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojr1_iYCtPvp"
      },
      "source": [
        "La firma `init_states` toma la **forma** `(batch, frames, height, width, colors)` del video como entrada y devuelve un diccionario grande de tensores que contiene los estados iniciales de RNN: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67loYFGpo_RP"
      },
      "outputs": [],
      "source": [
        "lines = model.signatures['init_states'].pretty_printed_signature().splitlines()\n",
        "lines = lines[:10]\n",
        "lines.append('      ...')\n",
        "print('.\\n'.join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5lG3vejn5df"
      },
      "outputs": [],
      "source": [
        "initial_state = model.init_states(jumpingjack[tf.newaxis, ...].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3DwmyHnuhH_"
      },
      "outputs": [],
      "source": [
        "type(initial_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8SyiEU6tB-e"
      },
      "outputs": [],
      "source": [
        "list(sorted(initial_state.keys()))[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeMCzJMBvwRF"
      },
      "source": [
        "Una vez que cuenta con el estado inicial para las RNN, puede pasar el estado y un fotograma de video como entrada (el fotograma conserva la forma `(batch, frames, height, width, colors)`). El modelo devuelve un par `(logits, state)`.\n",
        "\n",
        "Después de ver el primer fotograma, el modelo no está convencido de que el video sea sobre \"saltos tijera\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McSLdIgtsI3d"
      },
      "outputs": [],
      "source": [
        "inputs = initial_state.copy()\n",
        "\n",
        "# Add the batch axis, take the first frme, but keep the frame-axis.\n",
        "inputs['image'] = jumpingjack[tf.newaxis, 0:1, ...] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlH7PqLPX664"
      },
      "outputs": [],
      "source": [
        "# warmup\n",
        "model(inputs);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uzNXtu7X5sr"
      },
      "outputs": [],
      "source": [
        "logits, new_state = model(inputs)\n",
        "logits = logits[0]\n",
        "probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "for label, p in get_top_k(probs):\n",
        "  print(f'{label:20s}: {p:.3f}')\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLU644FQwXSb"
      },
      "source": [
        "Si el modelo se ejecuta en un ciclo, pasando el estado actualizado en cada fotograma, el modelo, rápidamente, converge y concluye el resultado correcto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzm7T4ImmIEg"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "state = initial_state.copy()\n",
        "all_logits = []\n",
        "\n",
        "for n in range(len(jumpingjack)):\n",
        "  inputs = state\n",
        "  inputs['image'] = jumpingjack[tf.newaxis, n:n+1, ...]\n",
        "  result, state = model(inputs)\n",
        "  all_logits.append(logits)\n",
        "\n",
        "probabilities = tf.nn.softmax(all_logits, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7UtHoSWcOT2"
      },
      "outputs": [],
      "source": [
        "for label, p in get_top_k(probabilities[-1]):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ffV3NhZcsrv"
      },
      "outputs": [],
      "source": [
        "id = tf.argmax(probabilities[-1])\n",
        "plt.plot(probabilities[:, id])\n",
        "plt.xlabel('Frame #')\n",
        "plt.ylabel(f\"p('{KINETICS_600_LABELS[id]}')\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7MZ_AfRW845"
      },
      "source": [
        "Notará que la probabilidad es mucho más certera que en secciones anteriores en las que se ejecutó el modelo `base`. El modelo `base` devuelve un promedio de las predicciones basadas en los fotogramas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wij4tsyW8dR"
      },
      "outputs": [],
      "source": [
        "for label, p in get_top_k(tf.reduce_mean(probabilities, axis=0)):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLUoC9ejggGo"
      },
      "source": [
        "## Animación de las predicciones a lo largo del tiempo\n",
        "\n",
        "En la sección anterior repasamos algunos detalles sobre cómo usar estos modelos. Esta sección toma como base lo descrito allí para producir algunas animaciones inferidas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnFqOXazoWgy"
      },
      "source": [
        "La celda oculta (debajo) define funciones ayudante usadas en esta sección."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dx55NK3ZoZeh"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Get top_k labels and probabilities predicted using MoViNets streaming model\n",
        "def get_top_k_streaming_labels(probs, k=5, label_map=KINETICS_600_LABELS):\n",
        "  \"\"\"Returns the top-k labels over an entire video sequence.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    k: the number of top predictions to select.\n",
        "    label_map: a list of labels to map logit indices to label strings.\n",
        "\n",
        "  Returns:\n",
        "    a tuple of the top-k probabilities, labels, and logit indices\n",
        "  \"\"\"\n",
        "  top_categories_last = tf.argsort(probs, -1, 'DESCENDING')[-1, :1]\n",
        "  # Sort predictions to find top_k\n",
        "  categories = tf.argsort(probs, -1, 'DESCENDING')[:, :k]\n",
        "  categories = tf.reshape(categories, [-1])\n",
        "\n",
        "  counts = sorted([\n",
        "      (i.numpy(), tf.reduce_sum(tf.cast(categories == i, tf.int32)).numpy())\n",
        "      for i in tf.unique(categories)[0]\n",
        "  ], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  top_probs_idx = tf.constant([i for i, _ in counts[:k]])\n",
        "  top_probs_idx = tf.concat([top_categories_last, top_probs_idx], 0)\n",
        "  # find unique indices of categories\n",
        "  top_probs_idx = tf.unique(top_probs_idx)[0][:k+1]\n",
        "  # top_k probabilities of the predictions\n",
        "  top_probs = tf.gather(probs, top_probs_idx, axis=-1)\n",
        "  top_probs = tf.transpose(top_probs, perm=(1, 0))\n",
        "  # collect the labels of top_k predictions\n",
        "  top_labels = tf.gather(label_map, top_probs_idx, axis=0)\n",
        "  # decode the top_k labels\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "\n",
        "  return top_probs, top_labels, top_probs_idx\n",
        "\n",
        "# Plot top_k predictions at a given time step\n",
        "def plot_streaming_top_preds_at_step(\n",
        "    top_probs,\n",
        "    top_labels,\n",
        "    step=None,\n",
        "    image=None,\n",
        "    legend_loc='lower left',\n",
        "    duration_seconds=10,\n",
        "    figure_height=500,\n",
        "    playhead_scale=0.8,\n",
        "    grid_alpha=0.3):\n",
        "  \"\"\"Generates a plot of the top video model predictions at a given time step.\n",
        "\n",
        "  Args:\n",
        "    top_probs: a tensor of shape (k, num_frames) representing the top-k\n",
        "      probabilities over all frames.\n",
        "    top_labels: a list of length k that represents the top-k label strings.\n",
        "    step: the current time step in the range [0, num_frames].\n",
        "    image: the image frame to display at the current time step.\n",
        "    legend_loc: the placement location of the legend.\n",
        "    duration_seconds: the total duration of the video.\n",
        "    figure_height: the output figure height.\n",
        "    playhead_scale: scale value for the playhead.\n",
        "    grid_alpha: alpha value for the gridlines.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of the output numpy image, figure, and axes.\n",
        "  \"\"\"\n",
        "  # find number of top_k labels and frames in the video\n",
        "  num_labels, num_frames = top_probs.shape\n",
        "  if step is None:\n",
        "    step = num_frames\n",
        "  # Visualize frames and top_k probabilities of streaming video\n",
        "  fig = plt.figure(figsize=(6.5, 7), dpi=300)\n",
        "  gs = mpl.gridspec.GridSpec(8, 1)\n",
        "  ax2 = plt.subplot(gs[:-3, :])\n",
        "  ax = plt.subplot(gs[-3:, :])\n",
        "  # display the frame\n",
        "  if image is not None:\n",
        "    ax2.imshow(image, interpolation='nearest')\n",
        "    ax2.axis('off')\n",
        "  # x-axis (frame number)\n",
        "  preview_line_x = tf.linspace(0., duration_seconds, num_frames)\n",
        "  # y-axis (top_k probabilities)\n",
        "  preview_line_y = top_probs\n",
        "\n",
        "  line_x = preview_line_x[:step+1]\n",
        "  line_y = preview_line_y[:, :step+1]\n",
        "\n",
        "  for i in range(num_labels):\n",
        "    ax.plot(preview_line_x, preview_line_y[i], label=None, linewidth='1.5',\n",
        "            linestyle=':', color='gray')\n",
        "    ax.plot(line_x, line_y[i], label=top_labels[i], linewidth='2.0')\n",
        "\n",
        "\n",
        "  ax.grid(which='major', linestyle=':', linewidth='1.0', alpha=grid_alpha)\n",
        "  ax.grid(which='minor', linestyle=':', linewidth='0.5', alpha=grid_alpha)\n",
        "\n",
        "  min_height = tf.reduce_min(top_probs) * playhead_scale\n",
        "  max_height = tf.reduce_max(top_probs)\n",
        "  ax.vlines(preview_line_x[step], min_height, max_height, colors='red')\n",
        "  ax.scatter(preview_line_x[step], max_height, color='red')\n",
        "\n",
        "  ax.legend(loc=legend_loc)\n",
        "\n",
        "  plt.xlim(0, duration_seconds)\n",
        "  plt.ylabel('Probability')\n",
        "  plt.xlabel('Time (s)')\n",
        "  plt.yscale('log')\n",
        "\n",
        "  fig.tight_layout()\n",
        "  fig.canvas.draw()\n",
        "\n",
        "  data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close()\n",
        "\n",
        "  figure_width = int(figure_height * data.shape[1] / data.shape[0])\n",
        "  image = PIL.Image.fromarray(data).resize([figure_width, figure_height])\n",
        "  image = np.array(image)\n",
        "\n",
        "  return image\n",
        "\n",
        "# Plotting top_k predictions from MoViNets streaming model\n",
        "def plot_streaming_top_preds(\n",
        "    probs,\n",
        "    video,\n",
        "    top_k=5,\n",
        "    video_fps=25.,\n",
        "    figure_height=500,\n",
        "    use_progbar=True):\n",
        "  \"\"\"Generates a video plot of the top video model predictions.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    video: the video to display in the plot.\n",
        "    top_k: the number of top predictions to select.\n",
        "    video_fps: the input video fps.\n",
        "    figure_fps: the output video fps.\n",
        "    figure_height: the height of the output video.\n",
        "    use_progbar: display a progress bar.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array representing the output video.\n",
        "  \"\"\"\n",
        "  # select number of frames per second\n",
        "  video_fps = 8.\n",
        "  # select height of the image\n",
        "  figure_height = 500\n",
        "  # number of time steps of the given video\n",
        "  steps = video.shape[0]\n",
        "  # estimate duration of the video (in seconds)\n",
        "  duration = steps / video_fps\n",
        "  # estiamte top_k probabilities and corresponding labels\n",
        "  top_probs, top_labels, _ = get_top_k_streaming_labels(probs, k=top_k)\n",
        "\n",
        "  images = []\n",
        "  step_generator = tqdm.trange(steps) if use_progbar else range(steps)\n",
        "  for i in step_generator:\n",
        "    image = plot_streaming_top_preds_at_step(\n",
        "        top_probs=top_probs,\n",
        "        top_labels=top_labels,\n",
        "        step=i,\n",
        "        image=video[i],\n",
        "        duration_seconds=duration,\n",
        "        figure_height=figure_height,\n",
        "    )\n",
        "    images.append(image)\n",
        "\n",
        "  return np.array(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLgFBslcZOQO"
      },
      "source": [
        "Comience por ejecutar el modelo de streaming con los fotogramas de video y recolecte las funciones logit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXWR13wthnK5"
      },
      "outputs": [],
      "source": [
        "init_states = model.init_states(jumpingjack[tf.newaxis].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqSkt7l8ltwt"
      },
      "outputs": [],
      "source": [
        "# Insert your video clip here\n",
        "video = jumpingjack\n",
        "images = tf.split(video[tf.newaxis], video.shape[0], axis=1)\n",
        "\n",
        "all_logits = []\n",
        "\n",
        "# To run on a video, pass in one frame at a time\n",
        "states = init_states\n",
        "for image in tqdm.tqdm(images):\n",
        "  # predictions for each frame\n",
        "  logits, states = model({**states, 'image': image})\n",
        "  all_logits.append(logits)\n",
        "\n",
        "# concatinating all the logits\n",
        "logits = tf.concat(all_logits, 0)\n",
        "# estimating probabilities\n",
        "probs = tf.nn.softmax(logits, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOGcCMMJyuPl"
      },
      "outputs": [],
      "source": [
        "final_probs = probs[-1]\n",
        "print('Top_k predictions and their probablities\\n')\n",
        "for label, p in get_top_k(final_probs):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaybT0rbZct-"
      },
      "source": [
        "Convierta la secuencia de probabilidades en un video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdox556CtMRb"
      },
      "outputs": [],
      "source": [
        "# Generate a plot and output to a video tensor\n",
        "plot_video = plot_streaming_top_preds(probs, video, video_fps=8.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSStKE9klCs3"
      },
      "outputs": [],
      "source": [
        "# For gif format, set codec='gif'\n",
        "media.show_video(plot_video, fps=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCImgZ3OdJw7"
      },
      "source": [
        "## Recursos\n",
        "\n",
        "Los videos preentrenados se encuentran disponibles en [TF Hub](https://tfhub.dev/google/collections/movinet/1). La colección de TF Hub también incluye modelos cuantificados optimizados para [TFLite](https://tensorflow.org/lite).\n",
        "\n",
        "La fuente para estos modelos se encuentra en [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/official/projects/movinet). Incluye una [versión más extensa de este tutorial](https://colab.sandbox.google.com/github/tensorflow/models/blob/master/official/projects/movinet/movinet_tutorial.ipynb) que también abarca la construcción y el ajuste fino de un modelo MoViNet. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh5lLAo-HpVF"
      },
      "source": [
        "## Próximos pasos\n",
        "\n",
        "Para obtener más información sobre cómo trabajar con datos de video en TensorFlow, consulte los siguientes tutoriales:\n",
        "\n",
        "- [Carga de datos de video](https://www.tensorflow.org/tutorials/load_data/video)\n",
        "- [Creación de un modelo de CNN 3D para clasificación de video](https://www.tensorflow.org/tutorials/video/video_classification)\n",
        "- [Aprendizaje por transferencia para clasificación de video con MoViNet](https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "movinet.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
