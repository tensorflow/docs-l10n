{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DADUIVuPIqYi"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Neural Structured Learning Authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Cu1zPez8Ip1S"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlIh_TPLI54s"
      },
      "source": [
        "# Regularización de grafos para clasificación de documentos con grafos naturales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL9fF9FWI-Q1"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/neural_structured_learning/tutorials/graph_keras_mlp_cora.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/neural_structured_learning/tutorials/graph_keras_mlp_cora.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/neural_structured_learning/tutorials/graph_keras_mlp_cora.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJlN8oxhNGto"
      },
      "source": [
        "## Descripción general\n",
        "\n",
        "La regularización de grafos es una técnica específica dentro del paradigma, mucho más amplio, del aprendizaje con grafos neuronales ([Bui et al., 2018](https://research.google/pubs/pub46568.pdf)). La idea central es la de entrenar modelos de redes neuronales con objetivos con grafos regularizados, aprovechando tanto los datos etiquetados como los no etiquetados.\n",
        "\n",
        "En este tutorial, exploraremos el uso de la regularización de grafos para clasificar documentos que forman un grafo (orgánico) natural.\n",
        "\n",
        "La receta general para crear un modelo de grafo regularizado con un aprendizaje estructurado neuronal (NSL) es la siguiente:\n",
        "\n",
        "1. Generar datos de entrenamiento a partir del grafo de entrada y las características de la muestra. Los nodos del grafo corresponden a las muestras y las aristas, a la similitud entre pares de muestras. Los datos de entrenamiento resultantes contendrán las características vecinas, además de las características del nodo original.\n",
        "2. Crear con una API secuencial, funcional o de subclase de `Keras` una red neuronal como modelo de base.\n",
        "3. Encapsular el modelo de base con la clase del encapsulador **`GraphRegularization`**, provista por el marco de trabajo NSL, para crear un nuevo modelo `Keras` de grafo. El modelo nuevo incluirá tanto una pérdida por regularización de grafo, como el término de regularización en su objetivo de entrenamiento.\n",
        "4. Entrenar y evaluar el modelo `Keras` de grafo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOFbB34KY1R"
      },
      "source": [
        "## Preparación\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgSLDi0SyBuO"
      },
      "source": [
        "Instalar el paquete de aprendizaje estructurado neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVnjPmOaQlnH"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet neural-structured-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AiNrPJaX_Lb"
      },
      "source": [
        "## Dependencias e importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sEamf-wZJkX"
      },
      "outputs": [],
      "source": [
        "import neural_structured_learning as nsl\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Resets notebook state\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\n",
        "    \"GPU is\",\n",
        "    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtbcGZ_N6Ll9"
      },
      "source": [
        "## Conjunto de datos Cora\n",
        "\n",
        "El [conjunto de datos Cora](https://linqs.soe.ucsc.edu/data) es un grafo de citas en el que los nodos representan a las publicaciones de aprendizaje automático, y las aristas, a las citas entre pares de publicaciones. En esta clasificación de documentos la tarea se realiza con el objetivo de categorizar cada una de las publicaciones en alguna de las 7 categorías. En otras palabras, es un problema de clasificación en múltiples clases con 7 clases.\n",
        "\n",
        "### Grafo\n",
        "\n",
        "El grafo original está dirigido. Pero a efectos de este ejemplo, consideramos la versión no dirigida de este grafo. Entonces, si en la publicación A se cita la publicación B, también consideramos que la publicación B tiene una cita de A. A pesar de que no sea necesariamente cierto, en este ejemplo, consideramos a las citas como sustituto (proxy) para similitud, que, por lo común, es una propiedad conmutativa.\n",
        "\n",
        "### Características\n",
        "\n",
        "Cada publicación de la entrada, efectivamente, contiene dos características:\n",
        "\n",
        "1. **Palabras**: una representación de bolsa de palabras multi-hot densa del texto de la publicación. El vocabulario para el conjunto de datos Cora contiene 1433 palabras únicas. Entonces, la longitud de esta característica es de 1433 y el valor en la posición 'i' es 0/1, lo que indica si la palabra 'i' del vocabulario existe en la publicación o no.\n",
        "\n",
        "2. **Etiqueta**: un entero solo representa el ID de clase (la categoría) de la publicación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2-FVpVHEyIS"
      },
      "source": [
        "### Descarga del conjunto de datos Cora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nSZjKqwE6Rn"
      },
      "outputs": [],
      "source": [
        "!wget --quiet -P /tmp https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
        "!tar -C /tmp -xvzf /tmp/cora.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylYP32_IoqZI"
      },
      "source": [
        "### Conversión de los datos de Cora al formato de NSL\n",
        "\n",
        "Para preprocesar el conjunto de datos Cora y convertirlo al formato requerido para el aprendizaje estructurado neuronal, ejecutaremos el script **'preprocess_cora_dataset.py'**, incluido en el repositorio de GitHub para NSL. Este script hace lo siguiente:\n",
        "\n",
        "1. Genera características vecinas utilizando el grafo y las características del nodo original.\n",
        "2. Genera divisiones entre los datos de entrenamiento y prueba que contienen instancias de `tf.train.Example`.\n",
        "3. Hace que los datos de entrenamiento y prueba persistan en el formato `TFRecord`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myz01LVZQ8Uh"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/tensorflow/neural-structured-learning/master/neural_structured_learning/examples/preprocess/cora/preprocess_cora_dataset.py\n",
        "\n",
        "!python preprocess_cora_dataset.py \\\n",
        "--input_cora_content=/tmp/cora/cora.content \\\n",
        "--input_cora_graph=/tmp/cora/cora.cites \\\n",
        "--max_nbrs=5 \\\n",
        "--output_train_data=/tmp/cora/train_merged_examples.tfr \\\n",
        "--output_test_data=/tmp/cora/test_examples.tfr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXoyHIdV5hEe"
      },
      "source": [
        "## Variables globales\n",
        "\n",
        "Las rutas de archivos para los datos de entrenamiento y prueba se basan en los valores de indicadores (bandera) de la línea de comandos utilizados para invocar el script **'preprocess_cora_dataset.py'** que vimos arriba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKAmzKIH6I9f"
      },
      "outputs": [],
      "source": [
        "### Experiment dataset\n",
        "TRAIN_DATA_PATH = '/tmp/cora/train_merged_examples.tfr'\n",
        "TEST_DATA_PATH = '/tmp/cora/test_examples.tfr'\n",
        "\n",
        "### Constants used to identify neighbor features in the input.\n",
        "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
        "NBR_WEIGHT_SUFFIX = '_weight'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gYWAqJqZ76I"
      },
      "source": [
        "## Hiperparámetros\n",
        "\n",
        "Utilizaremos una instancia de `HParams` para incluir varios hiperparámetros y contantes que se usan para el entrenamiento y la evaluación. A continuación, describimos brevemente cada una de ellas:\n",
        "\n",
        "- **num_classes**: son un total de 7 clases diferentes\n",
        "\n",
        "- **max_seq_length**: es el tamaño del vocabulario y todas las instancias de la entrada que tienen una representación de bolsa de palabras multi-hot densa. En otras palabras, un valor de 1 para una palabra indica que esa palabra está presente en la entrada y un valor de 0 indica que no lo está.\n",
        "\n",
        "- **distance_type**: es la métrica de distancia utilizada para regularizar la muestra con sus vecinos.\n",
        "\n",
        "- **graph_regularization_multiplier**: controla el peso relativo del término de regularización del grafo en la función de pérdida general.\n",
        "\n",
        "- **num_neighbors**: la cantidad de vecinos utilizados para la regularización del grafo. Este valor tiene que ser menor o igual que el argumento de línea de comandos `max_nbrs` utilizado arriba cuando ejecutamos `preprocess_cora_dataset.py`.\n",
        "\n",
        "- **num_fc_units**: la cantidad de capas totalmente conectadas de nuestra red neuronal.\n",
        "\n",
        "- **train_epochs**: la cantidad de épocas de entrenamiento.\n",
        "\n",
        "- **batch_size**: el tamaño del lote para entrenamiento y evaluación.\n",
        "\n",
        "- **dropout_rate**: controla la tasa de dilución que sigue a cada capa totalmente conectada.\n",
        "\n",
        "- **eval_steps**: la cantidad de lotes a procesar antes de que la evaluación se complete. Si se determina como `None`, se evalúan todas las instancias del conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N03EFEkcOBaJ"
      },
      "outputs": [],
      "source": [
        "class HParams(object):\n",
        "  \"\"\"Hyperparameters used for training.\"\"\"\n",
        "  def __init__(self):\n",
        "    ### dataset parameters\n",
        "    self.num_classes = 7\n",
        "    self.max_seq_length = 1433\n",
        "    ### neural graph learning parameters\n",
        "    self.distance_type = nsl.configs.DistanceType.L2\n",
        "    self.graph_regularization_multiplier = 0.1\n",
        "    self.num_neighbors = 1\n",
        "    ### model architecture\n",
        "    self.num_fc_units = [50, 50]\n",
        "    ### training parameters\n",
        "    self.train_epochs = 100\n",
        "    self.batch_size = 128\n",
        "    self.dropout_rate = 0.5\n",
        "    ### eval parameters\n",
        "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
        "\n",
        "HPARAMS = HParams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMp34x0MfMMZ"
      },
      "source": [
        "## Carga de los datos de entrenamiento y prueba\n",
        "\n",
        "Tal como se ha descripto antes en este bloc de notas, los datos de entrenamiento y prueba han sido creados por **'preprocess_cora_dataset.py'**. Los cargaremos en dos objetos `tf.data.Dataset`, uno para entrenamiento y otro para prueba.\n",
        "\n",
        "En la capa de entrada de nuestro modelo, extraeremos no solo las características de las \"palabras\" y \"etiquetas\" de cada muestra, sino que además obtendremos las características vecinas correspondientes, basándonos en el valor `hparams.num_neighbors`. A las instancias con menos vecinas que `hparams.num_neighbors`, se les asignarán valores ficticios en los casos en los que no existan características vecinas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCKQVKGee1ST"
      },
      "outputs": [],
      "source": [
        "def make_dataset(file_path, training=False):\n",
        "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
        "\n",
        "  Args:\n",
        "    file_path: Name of the file in the `.tfrecord` format containing\n",
        "      `tf.train.Example` objects.\n",
        "    training: Boolean indicating if we are in training mode.\n",
        "\n",
        "  Returns:\n",
        "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
        "    objects.\n",
        "  \"\"\"\n",
        "\n",
        "  def parse_example(example_proto):\n",
        "    \"\"\"Extracts relevant fields from the `example_proto`.\n",
        "\n",
        "    Args:\n",
        "      example_proto: An instance of `tf.train.Example`.\n",
        "\n",
        "    Returns:\n",
        "      A pair whose first value is a dictionary containing relevant features\n",
        "      and whose second value contains the ground truth label.\n",
        "    \"\"\"\n",
        "    # The 'words' feature is a multi-hot, bag-of-words representation of the\n",
        "    # original raw text. A default value is required for examples that don't\n",
        "    # have the feature.\n",
        "    feature_spec = {\n",
        "        'words':\n",
        "            tf.io.FixedLenFeature([HPARAMS.max_seq_length],\n",
        "                                  tf.int64,\n",
        "                                  default_value=tf.constant(\n",
        "                                      0,\n",
        "                                      dtype=tf.int64,\n",
        "                                      shape=[HPARAMS.max_seq_length])),\n",
        "        'label':\n",
        "            tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
        "    }\n",
        "    # We also extract corresponding neighbor features in a similar manner to\n",
        "    # the features above during training.\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i,\n",
        "                                         NBR_WEIGHT_SUFFIX)\n",
        "        feature_spec[nbr_feature_key] = tf.io.FixedLenFeature(\n",
        "            [HPARAMS.max_seq_length],\n",
        "            tf.int64,\n",
        "            default_value=tf.constant(\n",
        "                0, dtype=tf.int64, shape=[HPARAMS.max_seq_length]))\n",
        "\n",
        "        # We assign a default value of 0.0 for the neighbor weight so that\n",
        "        # graph regularization is done on samples based on their exact number\n",
        "        # of neighbors. In other words, non-existent neighbors are discounted.\n",
        "        feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
        "            [1], tf.float32, default_value=tf.constant([0.0]))\n",
        "\n",
        "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "\n",
        "    label = features.pop('label')\n",
        "    return features, label\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset([file_path])\n",
        "  if training:\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.map(parse_example)\n",
        "  dataset = dataset.batch(HPARAMS.batch_size)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = make_dataset(TRAIN_DATA_PATH, training=True)\n",
        "test_dataset = make_dataset(TEST_DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEWEGhtVzI2p"
      },
      "source": [
        "Echemos un vistazo al conjunto de datos de entrenamiento para observar su contenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx-YFaBoCOcl"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in train_dataset.take(1):\n",
        "  print('Feature list:', list(feature_batch.keys()))\n",
        "  print('Batch of inputs:', feature_batch['words'])\n",
        "  nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, 0, 'words')\n",
        "  nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, 0, NBR_WEIGHT_SUFFIX)\n",
        "  print('Batch of neighbor inputs:', feature_batch[nbr_feature_key])\n",
        "  print('Batch of neighbor weights:',\n",
        "        tf.reshape(feature_batch[nbr_weight_key], [-1]))\n",
        "  print('Batch of labels:', label_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7B30hRPzOBE"
      },
      "source": [
        "Ahora, echemos un vistazo al conjunto de datos de prueba para observar su contenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNJuM9yJiFtj"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in test_dataset.take(1):\n",
        "  print('Feature list:', list(feature_batch.keys()))\n",
        "  print('Batch of inputs:', feature_batch['words'])\n",
        "  print('Batch of labels:', label_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZhsTo8yio8i"
      },
      "source": [
        "## Definición del modelo\n",
        "\n",
        "Con el objetivo de demostrar el uso de la regularización de grafos, primero, creamos un modelo de base para este problema. Usaremos la red neuronal prealimentada simple con 2 capas ocultas y dilución entre capas. Ilustramos la creación del modelo base usando todos los tipos de modelos compatibles con el marco de trabajo `tf.Keras`, secuencial, funcional y de subclase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_kBDDfFiuyI"
      },
      "source": [
        "### Modelo base secuencial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pecJmegfWijx"
      },
      "outputs": [],
      "source": [
        "def make_mlp_sequential_model(hparams):\n",
        "  \"\"\"Creates a sequential multi-layer perceptron model.\"\"\"\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(\n",
        "      tf.keras.layers.InputLayer(\n",
        "          input_shape=(hparams.max_seq_length,), name='words'))\n",
        "  # Input is already one-hot encoded in the integer format. We cast it to\n",
        "  # floating point format here.\n",
        "  model.add(\n",
        "      tf.keras.layers.Lambda(lambda x: tf.keras.backend.cast(x, tf.float32)))\n",
        "  for num_units in hparams.num_fc_units:\n",
        "    model.add(tf.keras.layers.Dense(num_units, activation='relu'))\n",
        "    # For sequential models, by default, Keras ensures that the 'dropout' layer\n",
        "    # is invoked only during training.\n",
        "    model.add(tf.keras.layers.Dropout(hparams.dropout_rate))\n",
        "  model.add(tf.keras.layers.Dense(hparams.num_classes))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfZWxqVPiz_f"
      },
      "source": [
        "### Modelo base funcional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU-YE4NXi7PK"
      },
      "outputs": [],
      "source": [
        "def make_mlp_functional_model(hparams):\n",
        "  \"\"\"Creates a functional API-based multi-layer perceptron model.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(hparams.max_seq_length,), dtype='int64', name='words')\n",
        "\n",
        "  # Input is already one-hot encoded in the integer format. We cast it to\n",
        "  # floating point format here.\n",
        "  cur_layer = tf.keras.layers.Lambda(\n",
        "      lambda x: tf.keras.backend.cast(x, tf.float32))(\n",
        "          inputs)\n",
        "\n",
        "  for num_units in hparams.num_fc_units:\n",
        "    cur_layer = tf.keras.layers.Dense(num_units, activation='relu')(cur_layer)\n",
        "    # For functional models, by default, Keras ensures that the 'dropout' layer\n",
        "    # is invoked only during training.\n",
        "    cur_layer = tf.keras.layers.Dropout(hparams.dropout_rate)(cur_layer)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(hparams.num_classes)(cur_layer)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs=outputs)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LmAhITRi8M0"
      },
      "source": [
        "### Modelo base de subclase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l1aK9b_jAw6"
      },
      "outputs": [],
      "source": [
        "def make_mlp_subclass_model(hparams):\n",
        "  \"\"\"Creates a multi-layer perceptron subclass model in Keras.\"\"\"\n",
        "\n",
        "  class MLP(tf.keras.Model):\n",
        "    \"\"\"Subclass model defining a multi-layer perceptron.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "      super(MLP, self).__init__()\n",
        "      # Input is already one-hot encoded in the integer format. We create a\n",
        "      # layer to cast it to floating point format here.\n",
        "      self.cast_to_float_layer = tf.keras.layers.Lambda(\n",
        "          lambda x: tf.keras.backend.cast(x, tf.float32))\n",
        "      self.dense_layers = [\n",
        "          tf.keras.layers.Dense(num_units, activation='relu')\n",
        "          for num_units in hparams.num_fc_units\n",
        "      ]\n",
        "      self.dropout_layer = tf.keras.layers.Dropout(hparams.dropout_rate)\n",
        "      self.output_layer = tf.keras.layers.Dense(hparams.num_classes)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "      cur_layer = self.cast_to_float_layer(inputs['words'])\n",
        "      for dense_layer in self.dense_layers:\n",
        "        cur_layer = dense_layer(cur_layer)\n",
        "        cur_layer = self.dropout_layer(cur_layer, training=training)\n",
        "\n",
        "      outputs = self.output_layer(cur_layer)\n",
        "\n",
        "      return outputs\n",
        "\n",
        "  return MLP()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbGpIbscjIAo"
      },
      "source": [
        "## Creación de modelos base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzMBxiMGjCO4"
      },
      "outputs": [],
      "source": [
        "# Create a base MLP model using the functional API.\n",
        "# Alternatively, you can also create a sequential or subclass base model using\n",
        "# the make_mlp_sequential_model() or make_mlp_subclass_model() functions\n",
        "# respectively, defined above. Note that if a subclass model is used, its\n",
        "# summary cannot be generated until it is built.\n",
        "base_model_tag, base_model = 'FUNCTIONAL', make_mlp_functional_model(HPARAMS)\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1uEboimjiW7"
      },
      "source": [
        "## Entrenamiento de modelo MLP base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JALapM4PoCvi"
      },
      "outputs": [],
      "source": [
        "# Compile and train the base MLP model\n",
        "base_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "base_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPRioqydQD_8"
      },
      "source": [
        "## Evaluación de modelo MLP base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NcsJVt6FSmZ"
      },
      "outputs": [],
      "source": [
        "# Helper function to print evaluation metrics.\n",
        "def print_metrics(model_desc, eval_metrics):\n",
        "  \"\"\"Prints evaluation metrics.\n",
        "\n",
        "  Args:\n",
        "    model_desc: A description of the model.\n",
        "    eval_metrics: A dictionary mapping metric names to corresponding values. It\n",
        "      must contain the loss and accuracy metrics.\n",
        "  \"\"\"\n",
        "  print('\\n')\n",
        "  print('Eval accuracy for ', model_desc, ': ', eval_metrics['accuracy'])\n",
        "  print('Eval loss for ', model_desc, ': ', eval_metrics['loss'])\n",
        "  if 'graph_loss' in eval_metrics:\n",
        "    print('Eval graph loss for ', model_desc, ': ', eval_metrics['graph_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-myfttwIQAwc"
      },
      "outputs": [],
      "source": [
        "eval_results = dict(\n",
        "    zip(base_model.metrics_names,\n",
        "        base_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
        "print_metrics('Base MLP model', eval_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGwSzS9Spaiu"
      },
      "source": [
        "## Entrenamiento del modelo MLP con regularización de grafo\n",
        "\n",
        "Con solo unas líneas de código es posible incorporar la regularización del grafo en el término de pérdida de un `tf.Keras.Model` existente. El modelo base se encapsula para crear un nuevo modelo de subclase `tf.Keras`, cuya pérdida incluye la regularización del grafo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIGN8KQH0jR"
      },
      "source": [
        "Para evaluar el beneficio incremental de la regularización de grafos, crearemos una nueva instancia del modelo de base. El motivo es que el `base_model` ya ha sido entrenado para unas pocas iteraciones y reutilizarlo para crear un modelo de grafo regularizado no ofrecería un elemento de comparación verdadero para el `base_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fvLei9dLCH0"
      },
      "outputs": [],
      "source": [
        "# Build a new base MLP model.\n",
        "base_reg_model_tag, base_reg_model = 'FUNCTIONAL', make_mlp_functional_model(\n",
        "    HPARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT3mpC8Lo1UZ"
      },
      "outputs": [],
      "source": [
        "# Wrap the base MLP model with graph regularization.\n",
        "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
        "    max_neighbors=HPARAMS.num_neighbors,\n",
        "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
        "    distance_type=HPARAMS.distance_type,\n",
        "    sum_over_axis=-1)\n",
        "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
        "                                                graph_reg_config)\n",
        "graph_reg_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "graph_reg_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6409ylRVQS7q"
      },
      "source": [
        "## Evaluación del modelo MLP con regularización de grafo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TsOE1bAQTqD"
      },
      "outputs": [],
      "source": [
        "eval_results = dict(\n",
        "    zip(graph_reg_model.metrics_names,\n",
        "        graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
        "print_metrics('MLP + graph regularization', eval_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adc-r84EOSQi"
      },
      "source": [
        "La exactitud del modelo de grafo regularizado es de entre un 2 y un 3% superior a la del modelo base (`base_model`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEXQHFNvJQJe"
      },
      "source": [
        "## Conclusión\n",
        "\n",
        "Hemos demostrado el uso de la regularización de grafos para la clasificación de documentación en un grafo de citas naturales (Cora) con aprendizaje estructurado neuronal (NSL). En nuestro [tutorial avanzado](graph_keras_lstm_imdb.ipynb) se incluye la sintetización de grafos basada en incorporaciones de muestras antes de entrenar una red neuronal con regularización de grafos. Este enfoque resulta útil si la entrada no contiene un grafo explícito.\n",
        "\n",
        "Les recomendamos a los usuarios seguir experimentando con la variación de la cantidad de supervisión y seguir probando diferentes arquitecturas neuronales para la regularización de grafos."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "graph_keras_mlp_cora.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
