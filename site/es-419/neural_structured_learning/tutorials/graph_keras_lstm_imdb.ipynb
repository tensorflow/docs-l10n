{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24gYiJcWNlpA"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Neural Structured Learning Authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ioaprt5q5US7"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# Regularización de grafos para clasificación de sentimientos con grafos sintetizados\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/neural_structured_learning/tutorials/graph_keras_lstm_imdb.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://tfhub.dev/\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelo de TF Hub</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3otbdCMmJiJ"
      },
      "source": [
        "## Descripción general"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "En este bloc de notas se clasifican reseñas de películas como *positiva* o *negativa* a partir del texto de la reseña. Este es un ejemplo de clasificación *binaria*, un tipo de problema de aprendizaje automático importante y ampliamente aplicable.\n",
        "\n",
        "Demostraremos el uso de la regularización de grafos mediante la creación de un grafo a partir de una entrada dada. A continuación, compartimos la receta general para crear un modelo de grafo regularizado con el marco de trabajo del aprendizaje estructurado neuronal (NSL) cuando la entrada no contenga un grafo explícito:\n",
        "\n",
        "1. Crear incorporaciones para cada muestra de texto de la entrada. Se puede hacer con modelos previamente entrenados como [word2vec](https://arxiv.org/pdf/1310.4546.pdf), [Swivel](https://arxiv.org/abs/1602.02215), [BERT](https://arxiv.org/abs/1810.04805), etc.\n",
        "2. Crear un grafo basado en estas incorporaciones usando una métrica de similitud, como las de distancia 'L2', distancia 'coseno', etc. Los nodos del grafo corresponden a las muestras del grafo, y las aristas, a la similitud entre pares de muestras.\n",
        "3. Generar datos de entrenamiento a partir del grafo anterior sintetizado y de las características de la muestra. Los datos de entrenamiento que resulten de esta acción contendrán las características vecinas, además de las características del nodo original.\n",
        "4. Crear con una API secuencial, funcional o de subclase de Keras una red neuronal como modelo de base.\n",
        "5. Encapsular el modelo de base con la clase del encapsulador GraphRegularization, provista por el marco de trabajo NSL, para crear un nuevo modelo Keras de grafo. El modelo nuevo incluirá una pérdida por regularización de grafo, como el término de regularización en su objetivo de entrenamiento.\n",
        "6. Entrenar y evaluar el modelo Keras de grafo.\n",
        "\n",
        "**Nota**: Previmos que a los lectores les tomará alrededor de una hora leer este tutorial completo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOFbB34KY1R"
      },
      "source": [
        "## Requisitos\n",
        "\n",
        "1. Instalar el paquete de aprendizaje estructurado neuronal.\n",
        "2. Instalar tensorflow-hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVnjPmOaQlnH"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet neural-structured-learning\n",
        "!pip install --quiet tensorflow-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6FJ64qMNLez"
      },
      "source": [
        "## Dependencias e importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ew7HTbPpCJH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import neural_structured_learning as nsl\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Resets notebook state\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\n",
        "    \"GPU is\",\n",
        "    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGwwFd99n42P"
      },
      "source": [
        "## Conjunto de datos de IMDB\n",
        "\n",
        "El [conjunto de datos de IMDB](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) contiene el texto de 50 000 reseñas de películas de [Internet Movie Database](https://www.imdb.com/). Se divide en 25 000 reseñas para entrenamiento y 25 000 para prueba. Los conjuntos de entrenamiento y prueba están *equilibrados*, lo que significa que contienen la misma cantidad de reseñas positivas y negativas.\n",
        "\n",
        "En este tutorial usaremos una versión previamente procesada del conjunto de datos IMDB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "source": [
        "### Descargar el conjunto de datos de IMDB preprocesado\n",
        "\n",
        "El conjunto de datos de IMDB viene empaquetado con TensorFlow. Ya ha sido previamente procesado, de modo tal que las reseñas (secuencias de palabras) se han convertido en secuencias de enteros, donde cada entero representa a una palabra específica del diccionario.\n",
        "\n",
        "Con el siguiente código se descarga el conjunto de datos de IMDB (o, si ya se ha descargado, se usa una copia en caché):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXXx5Oc3pOmN"
      },
      "outputs": [],
      "source": [
        "imdb = tf.keras.datasets.imdb\n",
        "(pp_train_data, pp_train_labels), (pp_test_data, pp_test_labels) = (\n",
        "    imdb.load_data(num_words=10000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odr-KlzO-lkL"
      },
      "source": [
        "El argumento `num_words=10000` contiene las 10 000 palabras más frecuentes en los datos de entrenamiento. Las palabras menos frecuentes se descartan para que el vocabulario mantenga un tamaño que pueda seguir siendo gestionable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l50X3GfjpU4r"
      },
      "source": [
        "### Exploración de los datos\n",
        "\n",
        "Tomémonos un momento para comprender el formato de los datos. El conjunto de datos viene previamente procesado: cada ejemplo es un arreglo de enteros que representan las palabras de la reseña de una película. La etiqueta es un valor de número entero que puede ser 0 o 1, donde 0 corresponde a una reseña negativa y 1 corresponde a una positiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8qCnve_-lkO"
      },
      "outputs": [],
      "source": [
        "print('Training entries: {}, labels: {}'.format(\n",
        "    len(pp_train_data), len(pp_train_labels)))\n",
        "training_samples_count = len(pp_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnKvHWW4-lkW"
      },
      "source": [
        "El texto de las reseñas se ha convertido en enteros, donde cada entero representa a una palabra específica del diccionario. A continuación, compartimos cómo se ve la primera reseña:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtTS4kpEpjbi"
      },
      "outputs": [],
      "source": [
        "print(pp_train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIE4l_72x7DP"
      },
      "source": [
        "Las reseñas de películas pueden tener longitudes diferentes. El siguiente código muestra la cantidad de palabras que contienen la primera revisión y la segunda. Como las entradas de una red neuronal deben tener la misma longitud, más adelante deberemos resolver este aspecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-6Ii9Pfx6Nr"
      },
      "outputs": [],
      "source": [
        "len(pp_train_data[0]), len(pp_train_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wJg2FiYpuoX"
      },
      "source": [
        "### Conversión de enteros de vuelta en palabras\n",
        "\n",
        "Puede resultar útil saber cómo volver a convertir los enteros en el texto correspondiente. Lo que haremos será crear una función ayudante para consultar un objeto diccionario que contenga el entero para el mapeo de la <em>string</em>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr5s_1alpzop"
      },
      "outputs": [],
      "source": [
        "def build_reverse_word_index():\n",
        "  # A dictionary mapping words to an integer index\n",
        "  word_index = imdb.get_word_index()\n",
        "\n",
        "  # The first indices are reserved\n",
        "  word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "  word_index['<PAD>'] = 0\n",
        "  word_index['<START>'] = 1\n",
        "  word_index['<UNK>'] = 2  # unknown\n",
        "  word_index['<UNUSED>'] = 3\n",
        "  return dict((value, key) for (key, value) in word_index.items())\n",
        "\n",
        "reverse_word_index = build_reverse_word_index()\n",
        "\n",
        "def decode_review(text):\n",
        "  return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3CNRvEZVppl"
      },
      "source": [
        "Ahora, podemos usar la función `decode_review` para mostrar el texto para la primera reseña:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_OqxmH6-lkn"
      },
      "outputs": [],
      "source": [
        "decode_review(pp_train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVmqL-zcWm8v"
      },
      "source": [
        "## Construcción de grafos\n",
        "\n",
        "La construcción de grafos consiste en crear incorporaciones (<em>embeddings</em>) para muestras de texto y después, usar una función de similitud para comparar dichas incorporaciones.\n",
        "\n",
        "Antes de continuar, crearemos un directorio para compartir los artefactos creados en este tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZicFxFOeL2J"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /tmp/imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUyHEa-3TB2X"
      },
      "source": [
        "### Creación de incorporaciones de muestra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCe9vOy7-Br9"
      },
      "source": [
        "Usaremos las incorporaciones Swivel previamente entrenadas para crear otras incorporaciones con el formato `tf.train.Example` para cada muestra de la entrada. Almacenaremos las incorporaciones resultantes con el formato `TFRecord` junto con una característica adicional que representa al ID de cada muestra. Es importante y nos permitirá hacer coincidir, más adelante, a las incorporaciones de la muestra con los nodos correspondientes del grafo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq2Ohd9CuZv_"
      },
      "outputs": [],
      "source": [
        "pretrained_embedding = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\n",
        "\n",
        "hub_layer = hub.KerasLayer(\n",
        "    pretrained_embedding, input_shape=[], dtype=tf.string, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXJ3RaboTSKQ"
      },
      "outputs": [],
      "source": [
        "def _int64_feature(value):\n",
        "  \"\"\"Returns int64 tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value.tolist()))\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns bytes tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(\n",
        "      bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
        "\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns float tf.train.Feature.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=value.tolist()))\n",
        "\n",
        "\n",
        "def create_embedding_example(word_vector, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's embedding and its ID.\"\"\"\n",
        "\n",
        "  text = decode_review(word_vector)\n",
        "\n",
        "  # Shape = [batch_size,].\n",
        "  sentence_embedding = hub_layer(tf.reshape(text, shape=[-1,]))\n",
        "\n",
        "  # Flatten the sentence embedding back to 1-D.\n",
        "  sentence_embedding = tf.reshape(sentence_embedding, shape=[-1])\n",
        "\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'embedding': _float_feature(sentence_embedding.numpy())\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "\n",
        "def create_embeddings(word_vectors, output_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(output_path) as writer:\n",
        "    for word_vector in word_vectors:\n",
        "      example = create_embedding_example(word_vector, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "\n",
        "# Persist TF.Example features containing embeddings for training data in\n",
        "# TFRecord format.\n",
        "create_embeddings(pp_train_data, '/tmp/imdb/embeddings.tfr', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8s06RuI_vKs"
      },
      "source": [
        "### Creación de un grafo\n",
        "\n",
        "Ahora que tenemos las incorporaciones de la muestra, las usaremos para crear un grafo de similitud. Es decir, los nodos del grafo corresponderán a las muestras, y las aristas, a la similitud entre pares de nodos.\n",
        "\n",
        "El aprendizaje estructurado neuronal proporciona una biblioteca de creación de grafos, que permite crear un grafo en base a las incorporaciones de las muestras. Utiliza la [**similitud coseno**](https://en.wikipedia.org/wiki/Cosine_similarity) como medida de similitud para comparar las incorporaciones y crear aristas entre ellas. También sirve para descartar aristas distintas del grafo final. En este ejemplo, al utilizar 0.99 como umbral de similitud y 12345 como semilla aleatoria, terminamos con un grafo que tiene 429 415 aristas bidireccionales. Aquí usamos el soporte del \"constructor\" de grafos para [locality-sensitive hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) (LSH), a fin de acelerar la creación del grafo. Para más detalles sobre cómo utilizar el soporte LSH del constructor de grafos, consulte la documentación de la API [`build_graph_from_config`](https://www.tensorflow.org/neural_structured_learning/api_docs/python/nsl/tools/build_graph_from_config)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY6lqhNkBh2Q"
      },
      "outputs": [],
      "source": [
        "graph_builder_config = nsl.configs.GraphBuilderConfig(\n",
        "    similarity_threshold=0.99, lsh_splits=32, lsh_rounds=15, random_seed=12345)\n",
        "nsl.tools.build_graph_from_config(['/tmp/imdb/embeddings.tfr'],\n",
        "                                  '/tmp/imdb/graph_99.tsv',\n",
        "                                  graph_builder_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dk9xfQcK553"
      },
      "source": [
        "Cada arista bidireccional está representada, en el archivo TSV de salida, por dos aristas dirigidas; de modo tal, que contiene un total de 429 415 × 2 = 858 830 líneas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDPwTpZcJ3zF"
      },
      "outputs": [],
      "source": [
        "!wc -l /tmp/imdb/graph_99.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06QrEVCIlTvV"
      },
      "source": [
        "**Nota:** La calidad del grafo y, por extensión, la calidad de la incorporación son muy importantes para la regularización del grafo. Si bien hemos usado incorporaciones Swivel en este bloc de notas, si usamos incorporaciones BERT, por ejemplo, probablemente capturaremos con mucha mayor precisión la semántica de las reseñas. Recomendamos a los usuarios utilizar las incorporaciones que prefieran y las que sean adecuadas a sus necesidades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USkfut69gNW"
      },
      "source": [
        "## Características de muestra\n",
        "\n",
        "Creamos características de muestra para nuestro problema con el formato `tf.train.Example` y persistimos con ellas en el formato `TFRecord`. Cada muestra incluirá las siguientes tres características:\n",
        "\n",
        "1. Un **id** (ID): el ID del nodo de la muestra.\n",
        "2. **words** (Palabras): una lista int64 que contiene los ID de las palabras.\n",
        "3. Una **label** (etiqueta): un patrón singleton int64 con el que se identifica la clase de destino de la reseña."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PcUF4_B9grB"
      },
      "outputs": [],
      "source": [
        "def create_example(word_vector, label, record_id):\n",
        "  \"\"\"Create tf.Example containing the sample's word vector, label, and ID.\"\"\"\n",
        "  features = {\n",
        "      'id': _bytes_feature(str(record_id)),\n",
        "      'words': _int64_feature(np.asarray(word_vector)),\n",
        "      'label': _int64_feature(np.asarray([label])),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "def create_records(word_vectors, labels, record_path, starting_record_id):\n",
        "  record_id = int(starting_record_id)\n",
        "  with tf.io.TFRecordWriter(record_path) as writer:\n",
        "    for word_vector, label in zip(word_vectors, labels):\n",
        "      example = create_example(word_vector, label, record_id)\n",
        "      record_id = record_id + 1\n",
        "      writer.write(example.SerializeToString())\n",
        "  return record_id\n",
        "\n",
        "# Persist TF.Example features (word vectors and labels) for training and test\n",
        "# data in TFRecord format.\n",
        "next_record_id = create_records(pp_train_data, pp_train_labels,\n",
        "                                '/tmp/imdb/train_data.tfr', 0)\n",
        "create_records(pp_test_data, pp_test_labels, '/tmp/imdb/test_data.tfr',\n",
        "               next_record_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhFO9sZ8Aa_g"
      },
      "source": [
        "## Datos de entrenamiento aumentado con grafos vecinos\n",
        "\n",
        "Como tenemos las características de las muestras y el grafo sintetizado, podemos generar datos de entrenamiento aumentado para aprendizaje estructurado neuronal. El marco NSL ofrece una biblioteca que permite combinar las características del grafo y de la muestra para producir los datos finales de entrenamiento necesarios para la regularización del grafo. Los datos resultantes del entrenamiento incluirán las características de la muestra original y las de sus correspondientes vecinos.\n",
        "\n",
        "Para este tutorial, consideramos aristas sin dirección y usamos un máximo de 3 vecinos por muestra, para datos de entrenamiento aumentado con vecinos de grafos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSCHj4rIBj_A"
      },
      "outputs": [],
      "source": [
        "nsl.tools.pack_nbrs(\n",
        "    '/tmp/imdb/train_data.tfr',\n",
        "    '',\n",
        "    '/tmp/imdb/graph_99.tsv',\n",
        "    '/tmp/imdb/nsl_train_data.tfr',\n",
        "    add_undirected_edges=True,\n",
        "    max_nbrs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzBWdWkBqlMy"
      },
      "source": [
        "## Modelo base\n",
        "\n",
        "Ahora, está todo listo para crear un modelo base sin regularización de grafos. Para construir este modelo, podemos usar las incorporaciones que se utilizaron para la creación del grafo o, aprender incorporaciones nuevas junto con la tarea de clasificación. Para cumplir con lo propuesto para este bloc de notas, haremos lo último."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSbRFguBUNl"
      },
      "source": [
        "### Variables globales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsA8HuvvwGri"
      },
      "outputs": [],
      "source": [
        "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
        "NBR_WEIGHT_SUFFIX = '_weight'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8gMVBw6t6CI"
      },
      "source": [
        "### Hiperparámetros\n",
        "\n",
        "Utilizaremos una instancia de `HParams` para incluir varios hiperparámetros y contantes que se usan para el entrenamiento y la evaluación. A continuación, describimos brevemente cada una de ellas:\n",
        "\n",
        "- **num_classes**: hay 2 clases: una *positiva* y otra *negativa*.\n",
        "\n",
        "- **max_seq_length**: es la cantidad máxima de palabras consideradas de cada reseña de película, en este ejemplo.\n",
        "\n",
        "- **vocab_size**: es el tamaño del vocabulario considerado para este ejemplo.\n",
        "\n",
        "- **distance_type**: es la métrica de distancia utilizada para regularizar la muestra con sus vecinas.\n",
        "\n",
        "- **graph_regularization_multiplier**: controla el peso relativo del término de regularización del grafo en la función de pérdida general.\n",
        "\n",
        "- **num_neighbors**: la cantidad de vecinos utilizados para la regularización del grafo. Este valor tiene que ser menor o igual que el argumento `max_nbrs` utilizado arriba cuando invocamos `nsl.tools.pack_nbrs`.\n",
        "\n",
        "- **num_fc_units**: la cantidad de unidades de una capa totalmente conectada de la red neuronal.\n",
        "\n",
        "- **train_epochs**: la cantidad de épocas de entrenamiento.\n",
        "\n",
        "- **batch_size**: el tamaño del lote para entrenamiento y evaluación.\n",
        "\n",
        "- **eval_steps**: la cantidad de lotes a procesar antes de que la evaluación se complete. Si se determina como `None`, se evalúan todas las instancias del conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlTmug7auQ2r"
      },
      "outputs": [],
      "source": [
        "class HParams(object):\n",
        "  \"\"\"Hyperparameters used for training.\"\"\"\n",
        "  def __init__(self):\n",
        "    ### dataset parameters\n",
        "    self.num_classes = 2\n",
        "    self.max_seq_length = 256\n",
        "    self.vocab_size = 10000\n",
        "    ### neural graph learning parameters\n",
        "    self.distance_type = nsl.configs.DistanceType.L2\n",
        "    self.graph_regularization_multiplier = 0.1\n",
        "    self.num_neighbors = 2\n",
        "    ### model architecture\n",
        "    self.num_embedding_dims = 16\n",
        "    self.num_lstm_dims = 64\n",
        "    self.num_fc_units = 64\n",
        "    ### training parameters\n",
        "    self.train_epochs = 10\n",
        "    self.batch_size = 128\n",
        "    ### eval parameters\n",
        "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
        "\n",
        "HPARAMS = HParams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFP_XKVRp4_S"
      },
      "source": [
        "### Preparación de los datos\n",
        "\n",
        "Las reseñas (arreglos de los enteros) deben convertirse a tensores antes de ingresar a la red neuronal. Esta conversión se puede hacer de dos maneras:\n",
        "\n",
        "- Convirtiendo los arreglos en vectores de los `0` y los `1`, indicando la ocurrencia de palabras, en forma similar a una codificación en un solo paso <em>one-hot</em>. Por ejemplo, la secuencia `[3, 5]` se convertiría en un vector `10000`-dimensional que tiene todos ceros, excepto por los índices `3` y `5`, que son unos. Después, usemos esto como la primera capa de nuestra red, una capa `Dense` que puede trabajar con datos de vectores de punto flotante. Esta opción es intensiva con respecto a la memoria, a pesar de que requiere una matriz con el tamaño `num_words * num_reviews`.\n",
        "\n",
        "- Como alternativa, podemos rellenar (<em>pad</em>) los arreglos para que todos tengan la misma longitud y después, crear un tensor entero de forma `max_length * num_reviews`. Podemos usar una capa de incorporación capaz de operar con esta forma como primera capa de nuestra red.\n",
        "\n",
        "En este tutorial, usaremos la segunda opción.\n",
        "\n",
        "Dado que las reseñas de películas deben tener la misma longitud, usaremos la función `pad_sequence` definida a continuación para estandarizar las longitudes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5lkZVynuHWs"
      },
      "outputs": [],
      "source": [
        "def make_dataset(file_path, training=False):\n",
        "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
        "\n",
        "  Args:\n",
        "    file_path: Name of the file in the `.tfrecord` format containing\n",
        "      `tf.train.Example` objects.\n",
        "    training: Boolean indicating if we are in training mode.\n",
        "\n",
        "  Returns:\n",
        "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
        "    objects.\n",
        "  \"\"\"\n",
        "\n",
        "  def pad_sequence(sequence, max_seq_length):\n",
        "    \"\"\"Pads the input sequence (a `tf.SparseTensor`) to `max_seq_length`.\"\"\"\n",
        "    pad_size = tf.maximum([0], max_seq_length - tf.shape(sequence)[0])\n",
        "    padded = tf.concat(\n",
        "        [sequence.values,\n",
        "         tf.fill((pad_size), tf.cast(0, sequence.dtype))],\n",
        "        axis=0)\n",
        "    # The input sequence may be larger than max_seq_length. Truncate down if\n",
        "    # necessary.\n",
        "    return tf.slice(padded, [0], [max_seq_length])\n",
        "\n",
        "  def parse_example(example_proto):\n",
        "    \"\"\"Extracts relevant fields from the `example_proto`.\n",
        "\n",
        "    Args:\n",
        "      example_proto: An instance of `tf.train.Example`.\n",
        "\n",
        "    Returns:\n",
        "      A pair whose first value is a dictionary containing relevant features\n",
        "      and whose second value contains the ground truth labels.\n",
        "    \"\"\"\n",
        "    # The 'words' feature is a variable length word ID vector.\n",
        "    feature_spec = {\n",
        "        'words': tf.io.VarLenFeature(tf.int64),\n",
        "        'label': tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
        "    }\n",
        "    # We also extract corresponding neighbor features in a similar manner to\n",
        "    # the features above during training.\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i,\n",
        "                                         NBR_WEIGHT_SUFFIX)\n",
        "        feature_spec[nbr_feature_key] = tf.io.VarLenFeature(tf.int64)\n",
        "\n",
        "        # We assign a default value of 0.0 for the neighbor weight so that\n",
        "        # graph regularization is done on samples based on their exact number\n",
        "        # of neighbors. In other words, non-existent neighbors are discounted.\n",
        "        feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
        "            [1], tf.float32, default_value=tf.constant([0.0]))\n",
        "\n",
        "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "\n",
        "    # Since the 'words' feature is a variable length word vector, we pad it to a\n",
        "    # constant maximum length based on HPARAMS.max_seq_length\n",
        "    features['words'] = pad_sequence(features['words'], HPARAMS.max_seq_length)\n",
        "    if training:\n",
        "      for i in range(HPARAMS.num_neighbors):\n",
        "        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "        features[nbr_feature_key] = pad_sequence(features[nbr_feature_key],\n",
        "                                                 HPARAMS.max_seq_length)\n",
        "\n",
        "    labels = features.pop('label')\n",
        "    return features, labels\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset([file_path])\n",
        "  if training:\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.map(parse_example)\n",
        "  dataset = dataset.batch(HPARAMS.batch_size)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = make_dataset('/tmp/imdb/nsl_train_data.tfr', True)\n",
        "test_dataset = make_dataset('/tmp/imdb/test_data.tfr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "### Creación del modelo\n",
        "\n",
        "Una red neuronal se crea apilando capas, lo que implica tomar dos decisiones principales en términos de arquitectura:\n",
        "\n",
        "- ¿Cuántas capas se usarán en el modelo?\n",
        "- ¿Cuántas *unidades ocultas* se usarán para cada capa?\n",
        "\n",
        "En este ejemplo, los datos de entrada son un arreglo de índices de palabras. Las etiquetas que se deben predecir son 0 o 1.\n",
        "\n",
        "Para este tutorial utilizaremos una LSTM bidireccional como modelo base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpKOoWgu-llD"
      },
      "outputs": [],
      "source": [
        "# This function exists as an alternative to the bi-LSTM model used in this\n",
        "# notebook.\n",
        "def make_feed_forward_model():\n",
        "  \"\"\"Builds a simple 2 layer feed forward neural network.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "  embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size, 16)(inputs)\n",
        "  pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
        "  dense_layer = tf.keras.layers.Dense(16, activation='relu')(pooling_layer)\n",
        "  outputs = tf.keras.layers.Dense(1)(dense_layer)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "def make_bilstm_model():\n",
        "  \"\"\"Builds a bi-directional LSTM model.\"\"\"\n",
        "  inputs = tf.keras.Input(\n",
        "      shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "  embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size,\n",
        "                                              HPARAMS.num_embedding_dims)(\n",
        "                                                  inputs)\n",
        "  lstm_layer = tf.keras.layers.Bidirectional(\n",
        "      tf.keras.layers.LSTM(HPARAMS.num_lstm_dims))(\n",
        "          embedding_layer)\n",
        "  dense_layer = tf.keras.layers.Dense(\n",
        "      HPARAMS.num_fc_units, activation='relu')(\n",
        "          lstm_layer)\n",
        "  outputs = tf.keras.layers.Dense(1)(dense_layer)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "# Feel free to use an architecture of your choice.\n",
        "model = make_bilstm_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "Las capas se apilan secuencialmente con efectividad para generar el clasificador:\n",
        "\n",
        "1. La primera es una capa de `Input` que toma el vocabulario codificado con enteros.\n",
        "2. La primera capa es una capa `Embedding`. Esta capa toma las reseñas cifradas con números enteros y busca el vector de incorporación para cada índice de palabra. Estos vectores se aprenden a medida que se entrena el modelo. Los vectores agregan una dimensión al arreglo de salida. Las dimensiones resultantes son las siguientes: `(batch, sequence, embedding)`.\n",
        "3. Después, una capa LSTM bidireccional devuelve un vector de salida con longitud fija para cada ejemplo.\n",
        "4. Este vector de salida de longitud fija se canaliza a través de una capa (`Dense`) completamente conectada con 64 unidades ocultas.\n",
        "5. La última capa está conectada densamente con un nodo de salida único. Cuando utilizamos la función de activación `sigmoid`, este valor es un flotante de entre 0 y 1 que representa una probabilidad o un nivel de confianza."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XMwnDOp-llH"
      },
      "source": [
        "### Unidades ocultas\n",
        "\n",
        "El modelo anterior tiene dos capas intermedias u \"ocultas\" entre la entrada y la salida, excluyendo la capa `Embedding`. La cantidad de salidas (unidades, nodos o neuronas) es la dimensión del espacio representativo para la capa. En otras palabras, la cantidad de libertad que se le permite a la capa cuando aprende una representación interna.\n",
        "\n",
        "Si un modelo tiene más unidades ocultas (un espacio de representación con más dimensiones) o más capas, la red puede aprender representaciones más complejas. Sin embargo, la red se vuelve computacionalmente más cara y puede derivar en patrones indeseados de aprendizaje; patrones que mejoren el desempeño de los datos de entrenamiento pero no de los datos de prueba. A esto se lo denomina \"sobreajuste\" (*overfitting*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### Función de pérdida y optimizador\n",
        "\n",
        "Un modelo necesita una función de pérdida y un optimizador para el entrenamiento. Dado que este es un problema de clasificación binaria y el modelo genera una probabilidad (una capa de una sola unidad con una activación sigmoide), usaremos la función de pérdida `binary_crossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCWYwkug-llQ"
      },
      "source": [
        "### Crear un conjunto de validación\n",
        "\n",
        "Durante el entrenamiento, nos conviene comprobar la exactitud del modelo con datos que no haya visto antes. Creemos un *conjunto de validación*. Para crearlo separemos una fracción de los datos de entrenamiento originales. (¿Por qué no usamos el conjunto de prueba ahora? Nuestro objetivo es desarrollar y ajustar el modelo usando solamente los datos de entrenamiento y después, usar los datos de prueba una sola vez para evaluar la exactitud).\n",
        "\n",
        "En este tutorial, tomamos aproximadamente el 10 % de las muestras de entrenamiento iniciales (10% de 25000) como datos etiquetados para el entrenamiento y, el resto como datos de validación. Como la separación inicial entre entrenamiento o prueba se hizo 50/50 (25000 muestras cada opción), ahora, una separación efectiva entre entrenamiento, prueba y validación tiene la siguiente proporción: 5/45/50.\n",
        "\n",
        "Tenga en cuenta que el 'train_dataset' ya ha sido agrupado en lotes y aleatorizado. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYTf7zkZQ-Dl"
      },
      "outputs": [],
      "source": [
        "validation_fraction = 0.9\n",
        "validation_size = int(validation_fraction *\n",
        "                      int(training_samples_count / HPARAMS.batch_size))\n",
        "print(validation_size)\n",
        "validation_dataset = train_dataset.take(validation_size)\n",
        "train_dataset = train_dataset.skip(validation_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "### Entrenamiento del modelo\n",
        "\n",
        "Entrenemos el modelo en minilotes. Mientras se entrena, supervisemos la pérdida y la exactitud del modelo en el conjunto de validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLWzgfF1xpDu"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "### Evaluación del modelo\n",
        "\n",
        "Ahora, veamos el desempeño del modelo. Nos devolverá dos valores; la pérdida (un número que representa nuestro error, los valores bajos son mejores) y la exactitud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q7CoDfoCJ5h"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KggXVeL-llZ"
      },
      "source": [
        "### Creación de un grafo de exactitud y pérdida a lo largo del tiempo\n",
        "\n",
        "`model.fit()` devuelve un objeto `History` que contiene un diccionario con todo lo que pasó durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcvSXvhp-llb"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRKsqL40-lle"
      },
      "source": [
        "Hay cuatro entradas: una por cada métrica que se monitoreó durante el entrenamiento y la validación. Podemos usarlas para trazar la pérdida y validación del entrenamiento, para compararlas. Podemos hacer lo mismo con la exactitud de entrenamiento y validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGoYf2Js-lle"
      },
      "outputs": [],
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hXx-xOv-llh"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEmZ5zq-llk"
      },
      "source": [
        "Como puede ver, la pérdida del entrenamiento *se reduce* época tras época y la exactitud del entrenamiento *aumenta* a medida que pasan las épocas. Esto es lo que suele pasar cuando se usa una optimización con descenso de gradiente, debe reducir al mínimo la cantidad deseada en cada iteración."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SymtYWWiMUum"
      },
      "source": [
        "## Regularización de grafos\n",
        "\n",
        "No estamos listos para probar la regularización de grafos con el modelo de base que acabamos de crear. Usaremos la clase del encapsulador `GraphRegularization` proporcionada por el marco de aprendizaje estructurado neuronal para encapsular el modelo (bi-LSTM) de base, a fin de incluir la regularización del grafo. El resto de los pasos para entrenamiento y evaluación del modelo del grafo regularizado son similares a los del modelo de base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCIkVe_QFX38"
      },
      "source": [
        "### Creación de un modelo con grafo regularizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIGN8KQH0jR"
      },
      "source": [
        "Para evaluar el beneficio incremental de la regularización de grafos, crearemos una nueva instancia del modelo de base. El motivo es que el `model` ya ha sido entrenado para unas pocas iteraciones y reusarlo para crear un modelo de grafo regularizado no ofrecería un elemento de comparación verdadero para el `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOEElnbtPzSr"
      },
      "outputs": [],
      "source": [
        "# Build a new base LSTM model.\n",
        "base_reg_model = make_bilstm_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGaDeyjEOMLC"
      },
      "outputs": [],
      "source": [
        "# Wrap the base model with graph regularization.\n",
        "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
        "    max_neighbors=HPARAMS.num_neighbors,\n",
        "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
        "    distance_type=HPARAMS.distance_type,\n",
        "    sum_over_axis=-1)\n",
        "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
        "                                                graph_reg_config)\n",
        "graph_reg_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSZSqJOKFdgX"
      },
      "source": [
        "### Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aONZhwc9FWoo"
      },
      "outputs": [],
      "source": [
        "graph_reg_history = graph_reg_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD1oHiGHFjPB"
      },
      "source": [
        "### Evaluación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdFMEfe2e5JY"
      },
      "outputs": [],
      "source": [
        "graph_reg_results = graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(graph_reg_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BshURAbF49R"
      },
      "source": [
        "### Creación de un grafo de exactitud y pérdida a lo largo del tiempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHxshrYLah9v"
      },
      "outputs": [],
      "source": [
        "graph_reg_history_dict = graph_reg_history.history\n",
        "graph_reg_history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBrp0Y0jHu5k"
      },
      "source": [
        "En el diccionario, hay cinco entradas en total: la pérdida del entrenamiento, la exactitud del entrenamiento, la pérdida del grafo de entrenamiento, la pérdida de validación y la exactitud de validación. Para compararlas, podemos trazarlas todas juntas. Tenga en cuenta que la pérdida del grafo solamente se computa durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhjhH4n_aprb"
      },
      "outputs": [],
      "source": [
        "acc = graph_reg_history_dict['accuracy']\n",
        "val_acc = graph_reg_history_dict['val_accuracy']\n",
        "loss = graph_reg_history_dict['loss']\n",
        "graph_loss = graph_reg_history_dict['scaled_graph_loss']\n",
        "val_loss = graph_reg_history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.clf()   # clear figure\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-gD\" is for solid green line with diamond markers.\n",
        "plt.plot(epochs, graph_loss, '-gD', label='Training graph loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE0vcDiqa1Id"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su1TOgT3mgrk"
      },
      "source": [
        "## El poder del aprendizaje semisupervisado\n",
        "\n",
        "El aprendizaje semisupervisado y, más específicamente, la regularización del grafo en el contexto de este tutorial puede ser realmente potente cuando la cantidad de datos de entrenamiento es poca. La falta de datos de entrenamiento se compensa aprovechando la similitud que haya entre las muestras de entrenamiento; algo que no es posible con el entrenamiento supervisado tradicional.\n",
        "\n",
        "Definimos a la ***relación de supervisión*** como la relación de las muestras de entrenamiento con respecto a la cantidad total de muestras (incluidas las muestras de entrenamiento, validación y prueba). En este bloc de notas hemos aplicado una relación de supervisión de 0.05 (es decir, del 5% de los datos etiquetados) para entrenar ambos modelos, el de base y el de grafo regularizado. En la celda a continuación, ilustramos el impacto que tiene la relación de supervisión en la exactitud del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWWa384R5vSm"
      },
      "outputs": [],
      "source": [
        "# Accuracy values for both the Bi-LSTM model and the feed forward NN model have\n",
        "# been precomputed for the following supervision ratios.\n",
        "\n",
        "supervision_ratios = [0.3, 0.15, 0.05, 0.03, 0.02, 0.01, 0.005]\n",
        "\n",
        "model_tags = ['Bi-LSTM model', 'Feed Forward NN model']\n",
        "base_model_accs = [[84, 84, 83, 80, 65, 52, 50], [87, 86, 76, 74, 67, 52, 51]]\n",
        "graph_reg_model_accs = [[84, 84, 83, 83, 65, 63, 50],\n",
        "                        [87, 86, 80, 75, 67, 52, 50]]\n",
        "\n",
        "plt.clf()  # clear figure\n",
        "\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "fig.set_size_inches((12, 5))\n",
        "\n",
        "for ax, model_tag, base_model_acc, graph_reg_model_acc in zip(\n",
        "    axes, model_tags, base_model_accs, graph_reg_model_accs):\n",
        "\n",
        "  # \"-r^\" is for solid red line with triangle markers.\n",
        "  ax.plot(base_model_acc, '-r^', label='Base model')\n",
        "  # \"-gD\" is for solid green line with diamond markers.\n",
        "  ax.plot(graph_reg_model_acc, '-gD', label='Graph-regularized model')\n",
        "  ax.set_title(model_tag)\n",
        "  ax.set_xlabel('Supervision ratio')\n",
        "  ax.set_ylabel('Accuracy(%)')\n",
        "  ax.set_ylim((25, 100))\n",
        "  ax.set_xticks(range(len(supervision_ratios)))\n",
        "  ax.set_xticklabels(supervision_ratios)\n",
        "  ax.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tukoIryKugX_"
      },
      "source": [
        "Se puede observar que a medida que la relación de supervisión disminuye, la exactitud del modelo también baja. Esto les sucede tanto al modelo de base como al de grafo regularizado, independientemente de la arquitectura que se aplique para cada modelo. Sin embargo, es de destacar que el modelo de grafo regularizado se desempeña mejor que el base en ambas arquitecturas. En particular, en el caso del modelo Bi-LSTM cuando la relación de supervisión es de 0.01, la exactitud del modelo de grafo regularizado es **~20 %** superior a la del modelo de base. Esto se debe, principalmente, al aprendizaje semisupervisado del modelo de grafo regularizado; donde, además de las muestras de entrenamiento, se usa la similitud estructural entre las mismas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4zCEyPhIp-"
      },
      "source": [
        "## Conclusión\n",
        "\n",
        "Hemos demostrado el uso de la regularización de grafos con el marco de aprendizaje estructurado neuronal (NSL), incluso cuando la entrada no contiene un grafo explícito. Para el caso, consideramos la tarea de clasificación de sentimientos de reseñas sobre películas de IMDB para la que sintetizamos un grafo de similitud basado en incorporaciones de reseñas. A los usuarios, les recomendamos que practiquen con más experiencias, variando los hiperparámetros, la cantidad de supervisión y las diferentes arquitecturas de los modelos."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "24gYiJcWNlpA"
      ],
      "name": "graph_keras_lstm_imdb.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
