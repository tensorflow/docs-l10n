# Marco de trabajo de aprendizaje estructurado neuronal

El aprendizaje estructurado neuronal (NSL) se centra en el entrenamiento de redes neuronales profundas aprovechando las señales estructuradas (cuando hay) junto con las entradas de las características. Tal como lo presenta [Bui et al. (WSDM'18)](https://research.google/pubs/pub46568.pdf), estas señales estructuradas se usan para regularizar el entrenamiento de una red neuronal, forzando al modelo a aprender predicciones exactas (al minimizar la pérdida supervisada); mientras, a la vez, se mantiene la similitud estructural de la entrada (al minimizar, esta vez, la pérdida vecina; consulte la imagen a continuación). Esta técnica es genérica y se puede aplicar en arquitecturas neuronales arbitrarias (como las redes neuronales prealimentadas, las convolucionales o las recurrentes).

![Concepto de NSL](images/nlink_figure.png)

Tenga en cuenta que la ecuación de pérdida por vecino generalizada es flexible y puede tener otras formas además de la ilustrada arriba en estas notas. Por ejemplo, también podemos seleccionar $$\sum_{x_j \in \mathcal{N}(x_i)}\mathcal{E}(y_i,g_\theta(x_j))$$ para que sea la pérdida por vecino, que calcula la distancia entre la verdad fundamental $$y_i$$ y la predicción del vecino $$g_\theta(x_j)$$. Por lo común, esto se usa en el aprendizaje adversativo [(Goodfellow et al., ICLR'15)](https://arxiv.org/pdf/1412.6572.pdf). Por lo tanto, el NSL generaliza en **aprendizaje con redes neuronales de grafos**, si los vecinos están explícitamente representados por grafos; y como **aprendizaje adversativo**, si a los vecinos los induce implícitamente la perturbación adversativa.

El flujo de trabajo general para el aprendizaje estructurado neuronal se ilustra a continuación. Las flechas negras representan al flujo de entrenamiento convencional y la rojas, al nuevo flujo de trabajo introducido por las señales estructuradas aprovechadas del NSL. Primero, las muestras de entrenamiento se aumentan para incluir las señales estructuradas. Cuando estas señales no son explícitamente provistas, se pueden construir o inducir (esto último en aprendizaje adversativo). Después, las muestras aumentadas de entrenamiento (incluidas tanto las muestras originales como sus correspondientes muestras vecinas) se envían a la red neuronal para calcular sus incorporaciones. Se calcula la distancia entre la incorporación de una muestra y la de su vecina y se usa como pérdida por vecino, lo que se trata como un término de regularización y se agrega a la pérdida final. Para la regularización basada en vecinos explícitos, normalmente calculamos la pérdida por vecino como la distancia entre la incorporación de la muestra y la incorporación de la vecina. Pero, en realidad, se puede utilizar cualquier capa de red neuronal para calcular la pérdida por vecino. Además, por otra parte, calculamos la pérdida por vecino como la distancia entre la predicción de salida del vecino adversativo inducido y la etiqueta de verdad fundamental.

![Flujo del NSL](images/workflow_overview.png)

## Por qué conviene usar NSL

El NSL aporta las siguientes ventajas:

- **Mayor exactitud**: las señales estructuradas entre las muestras aportan información que no siempre está disponible en las entradas de características; por lo tanto, el entrenamiento conjunto (con las características y las señales estructuradas) ha demostrado tener un mayor rendimiento que muchos otros métodos (que dependen del entrenamiento solo con características) en una amplia gama de tareas, como la clasificación de documentación y la clasificación de intención semántica ([Bui et al., WSDM'18](https://research.google/pubs/pub46568.pdf) y [Kipf et al., ICLR'17](https://arxiv.org/pdf/1609.02907.pdf)).
- **Robustez**: los modelos entrenados con ejemplos adversativos han demostrado ser robustos ante las perturbaciones adversativas diseñadas para engañar a las predicciones o clasificaciones del modelo ([Goodfellow et al., ICLR'15](https://arxiv.org/pdf/1412.6572.pdf) y [Miyato et al., ICLR'16](https://arxiv.org/pdf/1704.03976.pdf)). Cuando la cantidad de muestras para entrenamiento es pequeña, el entrenamiento con ejemplos adversativos también ayuda a mejorar la exactitud del modelo ([Tsipras et al., ICLR'19](https://arxiv.org/pdf/1805.12152.pdf)).
- **Se requieren menos datos etiquetados**: el NSL permite que las redes neuronales admitan tanto los datos etiquetados como no etiquetados. Esta facilidad extiende el paradigma del aprendizaje hasta convertirlo en [aprendizaje semisupervisado](https://en.wikipedia.org/wiki/Semi-supervised_learning). Específicamente, el NSL permite que la red haga el entrenamiento con datos etiquetados como en el entorno supervisado y, a la vez, hace que la red aprenda representaciones ocultas similares para las "muestras vecinas" que pueden o no tener etiquetas. Esta técnica ha demostrado ser muy prometedora para mejorar la exactitud del modelo cuando la cantidad de datos etiquetados es relativamente pequeña ([Bui et al., WSDM'18](https://research.google/pubs/pub46568.pdf) y [Miyato et al., ICLR'16](https://arxiv.org/pdf/1704.03976.pdf)).

## Tutoriales paso a paso

Para obtener experiencia práctica con el aprendizaje estructurado neuronal, tenemos tutoriales que abarcan varios escenarios posibles en los cuales las señales estructuradas pueden ser dadas explícitamente, construidas o inducidas. A continuación, compartimos algunos:

- [Regularización de grafos para clasificación de documentos con grafos naturales](tutorials/graph_keras_mlp_cora.ipynb). En este tutorial, exploramos el uso de la regularización de grafos para clasificar documentos que forman un grafo (orgánico) natural.

- [Regularización de grafos para clasificación de sentimientos con grafos sintetizados](tutorials/graph_keras_lstm_imdb.ipynb). En este tutorial, demostramos el uso de la regularización de grafos para clasificar sentimientos de reseñas de películas mediante la construcción (sintetización) de señales estructuradas.

- [Aprendizaje adversativo para clasificación de imágenes](tutorials/adversarial_keras_cnn_mnist.ipynb). En este tutorial, exploramos el uso del aprendizaje adversativo (con señales estructuradas inducidas) para clasificación de imágenes con dígitos numéricos.

Puede encontrar más ejemplos y tutoriales en el directorio de [ejemplos](https://github.com/tensorflow/neural-structured-learning/tree/master/neural_structured_learning/examples) de nuestro repositorio de GitHub.
