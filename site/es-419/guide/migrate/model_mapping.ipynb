{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bYaCABobL5q"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FlUw7tSKbtg4"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-fogOi3K7nR"
      },
      "source": [
        "# Usar modelos TF1.x en flujos de trabajo TF2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/migrate/model_mapping\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/migrate/model_mapping.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/migrate/model_mapping.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/migrate/model_mapping.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-GwECUqrkqT"
      },
      "source": [
        "Esta guía proporciona una visión general y ejemplos de un [shim de código de modelado](https://en.wikipedia.org/wiki/Shim_(computing)) que puede emplear para usar sus modelos TF1.x existentes en flujos de trabajo TF2 como ejecución eager, `tf.function` y estrategias de distribución con cambios mínimos en su código de modelado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_ezCbogxaqt"
      },
      "source": [
        "## Ámbito de uso\n",
        "\n",
        "La shim descrita en esta guía está diseñada para los modelos TF1.x que se basan en:\n",
        "\n",
        "1. `tf.compat.v1.get_variable` y `tf.compat.v1.variable_scope` para controlar la creación y reutilización de variables, y\n",
        "2. APIs basadas en la recopilación de grafos como `tf.compat.v1.global_variables()`, `tf.compat.v1.trainable_variables`, `tf.compat.v1.losses.get_regularization_losses()`, y `tf.compat.v1.get_collection()` para conservar un registro de las ponderaciones y las pérdidas de regularización.\n",
        "\n",
        "Esto incluye la mayoría de los modelos construidos usando como base las APIs `tf.compat.v1.layer`, `tf.contrib.layers`, y [TensorFlow-Slim](https://github.com/google-research/tf-slim).\n",
        "\n",
        "La shim **NO** es necesaria para los siguientes modelos TF1.x:\n",
        "\n",
        "1. Modelos Keras autónomos que ya hacen un seguimiento de todas sus ponderaciones entrenables y pérdidas de regularización a través de `model.trainable_weights` y `model.losses` respectivamente.\n",
        "2. `tf.Module`s que ya hacen un seguimiento de todas sus ponderaciones entrenables a través de `module.trainable_variables`, y sólo crean ponderaciones si aún no se han creado.\n",
        "\n",
        "Es probable que estos modelos funcionen en TF2 con ejecución eager y `tf.function`s de forma inmediata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OQNFp8zgV0C"
      },
      "source": [
        "## Preparación\n",
        "\n",
        "Importe TensorFlow y otras dependencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG2n3-qlD5mA"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y -q tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVfR3MBvD9Sc"
      },
      "outputs": [],
      "source": [
        "# Install tf-nightly as the DeterministicRandomTestTool is available only in\n",
        "# Tensorflow 2.8\n",
        "\n",
        "!pip install -q tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzkV-2cna823"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as v1\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from contextlib import contextmanager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox4kn0DK8H0f"
      },
      "source": [
        "## El decorador `track_tf1_style_variables`\n",
        "\n",
        "La shim clave descrita en esta guía es `tf.compat.v1.keras.utils.track_tf1_style_variables`, un decorador que puede usar dentro de métodos pertenecientes a `tf.keras.layers.Layer` y `tf.Module` para hacer un seguimiento de las ponderaciones de estilo TF1.x y capturar las pérdidas de regularización.\n",
        "\n",
        "Decorar los métodos de llamada de un `tf.keras.layers.Layer` o `tf.Module` con `tf.compat.v1.keras.utils.track_tf1_style_variables` permite que la creación y reutilización de variables a través de `tf.compat.v1.get_variable` (y por extensión `tf.compat.v1.layers`) trabajen correctamente dentro del método decorado en lugar de hacerlo dentro del método decorado. También hará que la capa o módulo haga un seguimiento implícito de cualquier ponderación creada o a la que se acceda mediante `get_variable` dentro del método decorado.\n",
        "\n",
        "Además de hacer un seguimiento de las propias ponderaciones bajo las propiedades estándares `layer.variable`/`module.variable`/etc., si el método pertenece a una `tf.keras.layers. Layer`, entonces cualquier pérdida de regularización especificada mediante los argumentos de regularizador `get_variable` o `tf.compat.v1.layers` será seguida por la capa bajo la propiedad estándar `layer.losses`.\n",
        "\n",
        "Este mecanismo de seguimiento permite usar grandes clases de código TF1.x-style model-forward-pass dentro de capas Keras o `tf.Module`s en TF2 incluso con los comportamientos TF2 habilitados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq6IqZILmGmO"
      },
      "source": [
        "## Ejemplos de uso\n",
        "\n",
        "Los ejemplos de uso que aparecen a continuación muestran las shims de modelado utilizadas para decorar los métodos `tf.keras.layers.Layer`, pero excepto en los casos en los que interactúan específicamente con características de Keras, también son aplicables al decorar los métodos `tf.Module`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWGPh6KmkHq6"
      },
      "source": [
        "### Capa construida con tf.compat.v1.get_variable\n",
        "\n",
        "Imagine que tiene una capa implementada directamente sobre `tf.compat.v1.get_variable` de la siguiente manera:\n",
        "\n",
        "```python\n",
        "def dense(self, inputs, units):\n",
        "  out = inputs\n",
        "  with tf.compat.v1.variable_scope(\"dense\"):\n",
        "    # The weights are created with a `regularizer`,\n",
        "    kernel = tf.compat.v1.get_variable(\n",
        "        shape=[out.shape[-1], units],\n",
        "        regularizer=tf.keras.regularizers.L2(),\n",
        "        initializer=tf.compat.v1.initializers.glorot_normal,\n",
        "        name=\"kernel\")\n",
        "    bias = tf.compat.v1.get_variable(\n",
        "        shape=[units,],\n",
        "        initializer=tf.compat.v1.initializers.zeros,\n",
        "        name=\"bias\")\n",
        "    out = tf.linalg.matmul(out, kernel)\n",
        "    out = tf.compat.v1.nn.bias_add(out, bias)\n",
        "  return out\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sZWU7JSok2n"
      },
      "source": [
        "Use la shim para convertirla en una capa y llámela sobre las entradas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3eKkcKtS_N4"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    out = inputs\n",
        "    with tf.compat.v1.variable_scope(\"dense\"):\n",
        "      # The weights are created with a `regularizer`,\n",
        "      # so the layer should track their regularization losses\n",
        "      kernel = tf.compat.v1.get_variable(\n",
        "          shape=[out.shape[-1], self.units],\n",
        "          regularizer=tf.keras.regularizers.L2(),\n",
        "          initializer=tf.compat.v1.initializers.glorot_normal,\n",
        "          name=\"kernel\")\n",
        "      bias = tf.compat.v1.get_variable(\n",
        "          shape=[self.units,],\n",
        "          initializer=tf.compat.v1.initializers.zeros,\n",
        "          name=\"bias\")\n",
        "      out = tf.linalg.matmul(out, kernel)\n",
        "      out = tf.compat.v1.nn.bias_add(out, bias)\n",
        "    return out\n",
        "\n",
        "layer = DenseLayer(10)\n",
        "x = tf.random.normal(shape=(8, 20))\n",
        "layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqXAlWnYgwcq"
      },
      "source": [
        "Acceda a las variables de seguimiento y a las pérdidas de regularización capturadas como una capa Keras estándar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNz5HmkXg0B5"
      },
      "outputs": [],
      "source": [
        "layer.trainable_variables\n",
        "layer.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0z9GmRlhM9X"
      },
      "source": [
        "Para comprobar que las ponderaciones se reutilizan cada vez que llama a la capa, ponga todas las ponderaciones a cero y vuelva a llamar a la capa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ4vOu2Rf-I2"
      },
      "outputs": [],
      "source": [
        "print(\"Resetting variables to zero:\", [var.name for var in layer.trainable_variables])\n",
        "\n",
        "for var in layer.trainable_variables:\n",
        "  var.assign(var * 0.0)\n",
        "\n",
        "# Note: layer.losses is not a live view and\n",
        "# will get reset only at each layer call\n",
        "print(\"layer.losses:\", layer.losses)\n",
        "print(\"calling layer again.\")\n",
        "out = layer(x)\n",
        "print(\"layer.losses: \", layer.losses)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwEprtA-lOh6"
      },
      "source": [
        "También puede usar la capa convertida directamente en la construcción de modelos funcionales Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E7ZCINHlaHU"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(20))\n",
        "outputs = DenseLayer(10)(inputs)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "x = tf.random.normal(shape=(8, 20))\n",
        "model(x)\n",
        "\n",
        "# Access the model variables and regularization losses\n",
        "model.weights\n",
        "model.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew5TTEyZkZGU"
      },
      "source": [
        "### Modelo construido con `tf.compat.v1.layers`\n",
        "\n",
        "Imagine que tiene una capa o modelo implementado directamente sobre `tf.compat.v1.layers` como sigue:\n",
        "\n",
        "```python\n",
        "def model(self, inputs, units):\n",
        "  with tf.compat.v1.variable_scope('model'):\n",
        "    out = tf.compat.v1.layers.conv2d(\n",
        "        inputs, 3, 3,\n",
        "        kernel_regularizer=\"l2\")\n",
        "    out = tf.compat.v1.layers.flatten(out)\n",
        "    out = tf.compat.v1.layers.dense(\n",
        "        out, units,\n",
        "        kernel_regularizer=\"l2\")\n",
        "    return out\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZolXllfpVx6"
      },
      "source": [
        "Use la shim para convertirla en una capa y llámela sobre las entradas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBpfSHWTTTCv"
      },
      "outputs": [],
      "source": [
        "class CompatV1LayerModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          inputs, 3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      out = tf.compat.v1.layers.flatten(out)\n",
        "      out = tf.compat.v1.layers.dense(\n",
        "          out, self.units,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n",
        "\n",
        "layer = CompatV1LayerModel(10)\n",
        "x = tf.random.normal(shape=(8, 5, 5, 5))\n",
        "layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkG9oLlblfK_"
      },
      "source": [
        "Advertencia: Por motivos de seguridad, asegúrese de poner todas las `tf.compat.v1.layers` dentro de una `variable_scope` de cadena no vacía. Esto se debe a que `tf.compat.v1.layers` con nombres autogenerados siempre autoincrementarán el nombre fuera de cualquier ámbito de variable. Esto significa que los nombres de las variables solicitadas no coincidirán cada vez que llame a la capa/módulo. Así, en lugar de reutilizar las ponderaciones ya hechas, creará un nuevo conjunto de variables en cada llamada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAVN6dy3p7ik"
      },
      "source": [
        "Acceda a las variables de seguimiento y a las pérdidas de regularización capturadas como una capa Keras estándar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTRF99vJp7ik"
      },
      "outputs": [],
      "source": [
        "layer.trainable_variables\n",
        "layer.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkNuEcyIp7ik"
      },
      "source": [
        "Para comprobar que las ponderaciones se reutilizan cada vez que llama a la capa, ponga todas las ponderaciones a cero y vuelva a llamar a la capa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dk4XScdp7il"
      },
      "outputs": [],
      "source": [
        "print(\"Resetting variables to zero:\", [var.name for var in layer.trainable_variables])\n",
        "\n",
        "for var in layer.trainable_variables:\n",
        "  var.assign(var * 0.0)\n",
        "\n",
        "out = layer(x)\n",
        "print(\"layer.losses: \", layer.losses)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zD3a8PKzU7S"
      },
      "source": [
        "También puede usar la capa convertida directamente en la construcción de modelos funcionales Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q88BgBCup7il"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(5, 5, 5))\n",
        "outputs = CompatV1LayerModel(10)(inputs)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "x = tf.random.normal(shape=(8, 5, 5, 5))\n",
        "model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cioB6Zap7il"
      },
      "outputs": [],
      "source": [
        "# Access the model variables and regularization losses\n",
        "model.weights\n",
        "model.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBNODOx9ly6r"
      },
      "source": [
        "### Capturar las actualizaciones de normalización por lotes y los args de `training` del modelo\n",
        "\n",
        "En TF1.x, la normalización por lotes se realiza así:\n",
        "\n",
        "```python\n",
        "  x_norm = tf.compat.v1.layers.batch_normalization(x, training=training)\n",
        "\n",
        "  # ...\n",
        "\n",
        "  update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "  train_op = optimizer.minimize(loss)\n",
        "  train_op = tf.group([train_op, update_ops])\n",
        "```\n",
        "\n",
        "Tenga en cuenta que:\n",
        "\n",
        "1. Las actualizaciones promedio móviles de normalización por lotes son rastreadas por `get_collection` que se llamó por separado de la capa\n",
        "2. `tf.compat.v1.layers.batch_normalization` requiere un argumento `training` (generalmente llamado `is_training` cuando se usan capas de normalización por lotes TF-Slim).\n",
        "\n",
        "En TF2, debido a [ejecución eager](https://www.tensorflow.org/guide/eager) y a las dependencias de control automáticas, las actualizaciones promedio móviles de normalización por lotes se ejecutarán de inmediato. No es necesario recopilarlas por separado de la recolección de actualizaciones y añadirlas como dependencias de control explícitas.\n",
        "\n",
        "Además, si le da un argumento `training` al método de pasada hacia adelante de su `tf.keras.layers.Layer`, Keras podrá pasarle la fase de entrenamiento actual y cualquier capa anidada al igual que lo hace para cualquier otra capa. Consulte la documentación de la API de `tf.keras.Model` para saber cómo maneja Keras el argumento `training`.\n",
        "\n",
        "Si está decorando métodos `tf.Module`, deberá asegurarse de pasar manualmente todos los argumentos `training` que sean necesarios. Sin embargo, las actualizaciones promedio móviles de normalización por lotes se seguirán aplicando automáticamente sin necesidad de dependencias de control explícitas.\n",
        "\n",
        "Los siguientes fragmentos de código demuestran cómo incrustar capas de normalización por lotes en el shim y cómo funciona su uso en un modelo Keras (aplicable a `tf.keras.layers.Layer`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjZE-J7mkS9p"
      },
      "outputs": [],
      "source": [
        "class CompatV1BatchNorm(tf.keras.layers.Layer):\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    print(\"Forward pass called with `training` =\", training)\n",
        "    with v1.variable_scope('batch_norm_layer'):\n",
        "      return v1.layers.batch_normalization(x, training=training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGuvvElmY-fu"
      },
      "outputs": [],
      "source": [
        "print(\"Constructing model\")\n",
        "inputs = tf.keras.Input(shape=(5, 5, 5))\n",
        "outputs = CompatV1BatchNorm()(inputs)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "print(\"Calling model in inference mode\")\n",
        "x = tf.random.normal(shape=(8, 5, 5, 5))\n",
        "model(x, training=False)\n",
        "\n",
        "print(\"Moving average variables before training: \",\n",
        "      {var.name: var.read_value() for var in model.non_trainable_variables})\n",
        "\n",
        "# Notice that when running TF2 and eager execution, the batchnorm layer directly\n",
        "# updates the moving averages while training without needing any extra control\n",
        "# dependencies\n",
        "print(\"calling model in training mode\")\n",
        "model(x, training=True)\n",
        "\n",
        "print(\"Moving average variables after training: \",\n",
        "      {var.name: var.read_value() for var in model.non_trainable_variables})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gai4ikpmeRqR"
      },
      "source": [
        "### Reutilización de variables basada en el ámbito variable\n",
        "\n",
        "Las creaciones de variables en la pasada hacia adelante basadas en `get_variable` mantendrán la misma semántica de nomenclatura y reutilización de variables que tienen los ámbitos de variables en TF1.x. Esto es cierto siempre que tenga al menos un ámbito externo no vacío para cualquier `tf.compat.v1.layers` con nombres autogenerados, como se mencionó anteriormente.\n",
        "\n",
        "Nota: La asignación de nombres y la reutilización tendrán un alcance dentro de una única instancia de capa/módulo. Las llamadas a `get_variable` dentro de una capa o módulo decorado con shim no podrán consultar variables creadas dentro de capas o módulos. Puede evitar esto usando referencias Python a otras variables directamente si es necesario, en lugar de acceder a las variables a través de `get_variable`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PzYZdX2nMVt"
      },
      "source": [
        "### Ejecución eager y `tf.function`\n",
        "\n",
        "Como se ha visto anteriormente, los métodos decorados para `tf.keras.layers.Layer` y `tf.Module` se ejecutan dentro de ejecución eager y también son compatibles con `tf.function`. Esto significa que puede usar [pdb](https://docs.python.org/3/library/pdb.html) y otras herramientas interactivas para recorrer su pasada hacia adelante mientras se ejecuta.\n",
        "\n",
        "Advertencia: Aunque es perfectamente seguro llamar a sus métodos de capa/módulo decorados con shim desde *dentro* de un `tf.function`, no es seguro poner `tf.function`s dentro de sus métodos decorados con shim si esos `tf.functions` contienen llamadas `get_variable`. Introducir una `tf.function` restablece `variable_scope`s, lo que significa que la reutilización de variables basada en el alcance de variables al estilo TF1.x que imita el shim no funcionará con esta configuración."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPytVgZWnShe"
      },
      "source": [
        "### Estrategias de distribución\n",
        "\n",
        "Las llamadas a `get_variable` dentro de métodos de capas o módulos decorados con `@track_tf1_style_variables` usan creaciones de variables estándares `tf.Variable` de forma oculta. Esto significa que puede usarlas con las distintas estrategias de distribución disponibles con `tf.distribute`, como `MirroredStrategy` y `TPUStrategy`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DcK24FOA8A2"
      },
      "source": [
        "## Anidar `tf.Variable`s, `tf.Module`s, `tf.keras.layers` y `tf.keras.models` en llamadas decoradas\n",
        "\n",
        "Decorar su llamada de capa en `tf.compat.v1.keras.utils.track_tf1_style_variables` sólo añadirá un seguimiento implícito automático de las variables creadas (y reutilizadas) mediante `tf.compat.v1.get_variable`. No capturará ponderaciones creadas directamente por llamadas a `tf.Variable`, como las que usan las capas Keras típicas y la mayoría de `tf.Module`. Esta sección describe cómo manejar estos casos anidados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azxza3bVOZlv"
      },
      "source": [
        "### (Usos preexistentes) `tf.keras.layers` y `tf.keras.models`\n",
        "\n",
        "Para usos preexistentes de capas y modelos Keras anidados, use `tf.compat.v1.keras.utils.get_or_create_layer`. Esto sólo se recomienda para facilitar la migración de los usos anidados de Keras existentes en TF1.x; el código nuevo debe usar la configuración explícita de atributos como se describe a continuación para tf.Variables y tf.Modules.\n",
        "\n",
        "Para usar `tf.compat.v1.keras.utils.get_or_create_layer`, envuelva el código que construye su modelo anidado en un método y páselo al método. Ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN15TcRgHKsq"
      },
      "outputs": [],
      "source": [
        "class NestedModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  def build_model(self):\n",
        "    inp = tf.keras.Input(shape=(5, 5))\n",
        "    dense_layer = tf.keras.layers.Dense(\n",
        "        10, name=\"dense\", kernel_regularizer=\"l2\",\n",
        "        kernel_initializer=tf.compat.v1.ones_initializer())\n",
        "    model = tf.keras.Model(inputs=inp, outputs=dense_layer(inp))\n",
        "    return model\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    # Get or create a nested model without assigning it as an explicit property\n",
        "    model = tf.compat.v1.keras.utils.get_or_create_layer(\n",
        "        \"dense_model\", self.build_model)\n",
        "    return model(inputs)\n",
        "\n",
        "layer = NestedModel(10)\n",
        "layer(tf.ones(shape=(5,5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgsKlltPHI8z"
      },
      "source": [
        "Este método garantiza que estas capas anidadas sean reutilizadas y seguidas correctamente por tensorflow. Tenga en cuenta que el decorador `@track_tf1_style_variables` sigue siendo necesario en el método apropiado. El método constructor de modelos pasado a `get_or_create_layer` (en este caso, `self.build_model`), no debe tomar argumentos.\n",
        "\n",
        "Se hace seguimiento de las ponderaciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zO5A78MJsqO"
      },
      "outputs": [],
      "source": [
        "assert len(layer.weights) == 2\n",
        "weights = {x.name: x for x in layer.variables}\n",
        "\n",
        "assert set(weights.keys()) == {\"dense/bias:0\", \"dense/kernel:0\"}\n",
        "\n",
        "layer.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Xsi-JbKTuj"
      },
      "source": [
        "Y también la pérdida de regularización:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdK5RGm5KW5C"
      },
      "outputs": [],
      "source": [
        "tf.add_n(layer.losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_VRycQYJrXu"
      },
      "source": [
        "### Migración incremental: `tf.Variables` y `tf. Modules`\n",
        "\n",
        "Si necesita incrustar llamadas `tf.Variable` o `tf. Module` en sus métodos decorados (por ejemplo, si está siguiendo la migración incremental a las API de TF2 no heredadas que se describe más adelante en esta guía), aún deberá realizar un seguimiento explícito de las mismas, con los siguientes requisitos:\n",
        "\n",
        "- Asegúrese explícitamente de que la variable/módulo/capa sólo se crea una vez\n",
        "- Adjúntelos explícitamente como atributos de instancia del mismo modo que lo haría al definir un [módulo o capa típicos](https://www.tensorflow.org/guide/intro_to_modules#defining_models_and_layers_in_tensorflow)\n",
        "- Reutilice explícitamente el objeto ya creado en las llamadas que sigan\n",
        "\n",
        "Esto garantiza que las ponderaciones no se creen nuevas en cada llamada y se reutilicen correctamente. Además, esto también garantiza el seguimiento de las ponderaciones existentes y de las pérdidas de regularización.\n",
        "\n",
        "Aquí tiene un ejemplo de cómo puede quedar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrRPPoJ5ap5U"
      },
      "outputs": [],
      "source": [
        "class NestedLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def __call__(self, inputs):\n",
        "    out = inputs\n",
        "    with tf.compat.v1.variable_scope(\"inner_dense\"):\n",
        "      # The weights are created with a `regularizer`,\n",
        "      # so the layer should track their regularization losses\n",
        "      kernel = tf.compat.v1.get_variable(\n",
        "          shape=[out.shape[-1], self.units],\n",
        "          regularizer=tf.keras.regularizers.L2(),\n",
        "          initializer=tf.compat.v1.initializers.glorot_normal,\n",
        "          name=\"kernel\")\n",
        "      bias = tf.compat.v1.get_variable(\n",
        "          shape=[self.units,],\n",
        "          initializer=tf.compat.v1.initializers.zeros,\n",
        "          name=\"bias\")\n",
        "      out = tf.linalg.matmul(out, kernel)\n",
        "      out = tf.compat.v1.nn.bias_add(out, bias)\n",
        "    return out\n",
        "\n",
        "class WrappedDenseLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.units = units\n",
        "    # Only create the nested tf.variable/module/layer/model\n",
        "    # once, and then reuse it each time!\n",
        "    self._dense_layer = NestedLayer(self.units)\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    with tf.compat.v1.variable_scope('outer'):\n",
        "      outputs = tf.compat.v1.layers.dense(inputs, 3)\n",
        "      outputs = tf.compat.v1.layers.dense(inputs, 4)\n",
        "      return self._dense_layer(outputs)\n",
        "\n",
        "layer = WrappedDenseLayer(10)\n",
        "\n",
        "layer(tf.ones(shape=(5, 5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo9h6wc6bmEF"
      },
      "source": [
        "Tenga en cuenta que el seguimiento explícito del módulo anidado es necesario aunque esté decorado con el decorador `track_tf1_style_variables`. Esto se debe a que cada módulo/capa con métodos decorados tiene asociado su propio almacén de variables.\n",
        "\n",
        "Se hace un seguimiento correcto de las ponderaciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt6USaTVbauM"
      },
      "outputs": [],
      "source": [
        "assert len(layer.weights) == 6\n",
        "weights = {x.name: x for x in layer.variables}\n",
        "\n",
        "assert set(weights.keys()) == {\"outer/inner_dense/bias:0\",\n",
        "                               \"outer/inner_dense/kernel:0\",\n",
        "                               \"outer/dense/bias:0\",\n",
        "                               \"outer/dense/kernel:0\",\n",
        "                               \"outer/dense_1/bias:0\",\n",
        "                               \"outer/dense_1/kernel:0\"}\n",
        "\n",
        "layer.trainable_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHn-bJoNJw7l"
      },
      "source": [
        "Así como de la pérdida de regularización:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq5GFtXjJyut"
      },
      "outputs": [],
      "source": [
        "layer.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7VKJj3JOCEk"
      },
      "source": [
        "Tenga en cuenta que si la `NestedLayer` fuera un `tf.Module` no Keras en su lugar, las variables seguirían siendo objeto de seguimiento, pero las pérdidas de regularización no serían objeto de seguimiento automático, por lo que tendría que hacer un seguimiento explícito por separado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsTgnydkdezQ"
      },
      "source": [
        "### Orientación sobre los nombres de variables\n",
        "\n",
        "Las llamadas explícitas `tf.Variable` y las capas Keras usan un mecanismo de autogeneración de nombre de capa/nombre de variable diferente al que puede estar acostumbrado por la combinación de `get_variable` y `variable_scopes`. Aunque la shim hará que sus nombres de variables coincidan para las variables creadas por `get_variable` incluso al pasar de grafos TF1.x a ejecución eager y `tf.function` de TF2, no puede garantizar lo mismo para los nombres de variables generados para las llamadas `tf.Variable` y las capas Keras que incruste dentro de sus decoradores de métodos. Incluso es posible que varias variables compartan el mismo nombre en la eager execution y `tf.function` de TF2.\n",
        "\n",
        "Debe tener especial cuidado con esto cuando siga las secciones sobre validación de la corrección y mapeado de los puntos de verificación TF1.x más adelante en esta guía."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaP7fxoUWfMm"
      },
      "source": [
        "### Usar `tf.compat.v1.make_template` en el método decorado\n",
        "\n",
        "**Es muy recomendable usar directamente `tf.compat.v1.keras.utils.track_tf1_style_variables` en lugar de usar `tf.compat.v1.make_template`, ya que es una capa más delgada encima de TF2**.\n",
        "\n",
        "Siga la orientación de esta sección para el código TF1.x anterior que ya dependía de `tf.compat.v1.make_template`.\n",
        "\n",
        "Dado que `tf.compat.v1.make_template` envuelve el código que utiliza `get_variable`, el decorador `track_tf1_style_variables` le permite usar estas plantillas en las llamadas a capas y realizar con éxito el seguimiento de las ponderaciones y las pérdidas de regularización.\n",
        "\n",
        "Sin embargo, asegúrese de llamar a `make_template` sólo una vez y luego reutilizar la misma plantilla en cada llamada a la capa. De lo contrario, se creará una nueva plantilla cada vez que llame a la capa junto con un nuevo conjunto de variables.\n",
        "\n",
        "Por ejemplo,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHEQN8z44dbK"
      },
      "outputs": [],
      "source": [
        "class CompatV1TemplateScaleByY(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    def my_op(x, scalar_name):\n",
        "      var1 = tf.compat.v1.get_variable(scalar_name,\n",
        "                            shape=[],\n",
        "                            regularizer=tf.compat.v1.keras.regularizers.L2(),\n",
        "                            initializer=tf.compat.v1.constant_initializer(1.5))\n",
        "      return x * var1\n",
        "    self.scale_by_y = tf.compat.v1.make_template('scale_by_y', my_op, scalar_name='y')\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    with tf.compat.v1.variable_scope('layer'):\n",
        "      # Using a scope ensures the `scale_by_y` name will not be incremented\n",
        "      # for each instantiation of the layer.\n",
        "      return self.scale_by_y(inputs)\n",
        "\n",
        "layer = CompatV1TemplateScaleByY()\n",
        "\n",
        "out = layer(tf.ones(shape=(2, 3)))\n",
        "print(\"weights:\", layer.weights)\n",
        "print(\"regularization loss:\", layer.losses)\n",
        "print(\"output:\", out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKTJ7IsTEe8"
      },
      "source": [
        "Advertencia: Evite compartir la misma plantilla `make_template` creada a través de múltiples instancias de capa ya que puede romper los mecanismos de seguimiento de pérdidas de variables y regularización del decorador shim. Además, si planea usar el mismo nombre de `make_template` dentro de múltiples instancias de capa, entonces debería anidar el uso de la plantilla creada dentro de un `variable_scope`. De lo contrario, el nombre generado para el `variable_scope` de la plantilla se incrementará con cada nueva instancia de la capa. Esto puede alterar los nombres de las ponderaciones de forma inesperada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4E3-XPhWD2N"
      },
      "source": [
        "## Migración incremental al TF2 nativo\n",
        "\n",
        "Como ya se ha mencionado, `track_tf1_style_variables` permite mezclar el estilo TF2 orientado a objetos `tf.Variable`/`tf.keras.layers.Layer`/`tf. Module` con el uso heredado `tf.compat.v1.get_variable`/`tf.compat.v1.layers` dentro del mismo módulo/capa decorado.\n",
        "\n",
        "Esto significa que después de haber hecho su modelo TF1.x totalmente compatible con TF2, puede escribir todos los nuevos componentes del modelo con las API nativas de TF2 (que no sean `tf.compat.v1`) y hacer que interoperen con su código más antiguo.\n",
        "\n",
        "Sin embargo, si sigue modificando sus componentes modelo más antiguos, también puede seleccionar cambiar gradualmente su uso de estilo heredado `tf.compat.v1` a las API orientadas a objetos puramente nativas que se recomiendan para el código TF2 recién escrito.\n",
        "\n",
        "El uso de `tf.compat.v1.get_variable` puede ser reemplazado por llamadas a `self.add_weight` si está decorando una capa/modelo Keras, o por llamadas a `tf.Variable` si está decorando objetos Keras o `tf.Module`s.\n",
        "\n",
        "Tanto la capa de estilo funcional como la orientada a objetos `tf.compat.v1.layers` pueden ser generalmente reemplazadas por la capa equivalente `tf.keras.layers` sin que se requieran cambios en los argumentos.\n",
        "\n",
        "También puede considerar fraccionar partes de su modelo o patrones comunes en capas/módulos individuales durante su paso incremental a APIs puramente nativas, que pueden usar por sí mismas `track_tf1_style_variables`.\n",
        "\n",
        "### Una nota sobre Slim y contrib.layers\n",
        "\n",
        "Una gran cantidad de código anterior de TF 1.x usa la librería [Slim](https://ai.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html), que se empaquetó con TF 1.x como `tf.contrib.layers`. Convertir el código que usa Slim a TF 2 nativo es más complicado que convertir `v1.layers`. De hecho, quizá tenga sentido convertir primero su código Slim a `v1.layers` y después convertirlo a Keras. A continuación encontrará una guía general para convertir el código Slim.\n",
        "\n",
        "- Asegúrese de que todos los argumentos son explícitos. Elimine `arg_scopes` si es posible. Si aún necesita usarlos, divida `normalizer_fn` y `activation_fn` en sus propias capas.\n",
        "- Las capas conv separables mapean una o varias capas Keras diferentes (capas Keras a profundidad, enfocadas a puntos y separables).\n",
        "- Slim y `v1.layers` tienen diferentes nombres de argumentos y valores por defecto.\n",
        "- Tenga en cuenta que algunos argumentos tienen escalas diferentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFoULo-gazit"
      },
      "source": [
        "### Migración a TF2 nativo ignorando la compatibilidad de puntos de verificación\n",
        "\n",
        "El siguiente ejemplo de código demuestra un paso incremental de un modelo a APIs puramente nativas sin tener en cuenta la compatibilidad de los puntos de verificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPO9YJsb6r-D"
      },
      "outputs": [],
      "source": [
        "class CompatModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          inputs, 3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      out = tf.compat.v1.layers.flatten(out)\n",
        "      out = tf.compat.v1.layers.dropout(out, training=training)\n",
        "      out = tf.compat.v1.layers.dense(\n",
        "          out, self.units,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp16xK6Oa8k9"
      },
      "source": [
        "A continuación, sustituya las APIs `compat.v1` con sus equivalentes nativos orientados a objetos por partes. Empiece por cambiar la capa de convolución por un objeto Keras creado en el constructor de capas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOj1Swe16so3"
      },
      "outputs": [],
      "source": [
        "class PartiallyMigratedModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "    self.conv_layer = tf.keras.layers.Conv2D(\n",
        "      3, 3,\n",
        "      kernel_regularizer=\"l2\")\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_layer(inputs)\n",
        "      out = tf.compat.v1.layers.flatten(out)\n",
        "      out = tf.compat.v1.layers.dropout(out, training=training)\n",
        "      out = tf.compat.v1.layers.dense(\n",
        "          out, self.units,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzJF0H0sbce8"
      },
      "source": [
        "Use la clase [`v1.keras.utils.DeterministicRandomTestTool`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/utils/DeterministicRandomTestTool) para verificar que este cambio incremental deja el modelo con el mismo comportamiento que antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTJq0qW9_Tz2"
      },
      "outputs": [],
      "source": [
        "random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')\n",
        "with random_tool.scope():\n",
        "  tf.keras.utils.set_random_seed(42)\n",
        "  layer = CompatModel(10)\n",
        "\n",
        "  inputs = tf.random.normal(shape=(10, 5, 5, 5))\n",
        "  original_output = layer(inputs)\n",
        "\n",
        "  # Grab the regularization loss as well\n",
        "  original_regularization_loss = tf.math.add_n(layer.losses)\n",
        "\n",
        "print(original_regularization_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4Wq3wuaHjEV"
      },
      "outputs": [],
      "source": [
        "random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')\n",
        "with random_tool.scope():\n",
        "  tf.keras.utils.set_random_seed(42)\n",
        "  layer = PartiallyMigratedModel(10)\n",
        "\n",
        "  inputs = tf.random.normal(shape=(10, 5, 5, 5))\n",
        "  migrated_output = layer(inputs)\n",
        "\n",
        "  # Grab the regularization loss as well\n",
        "  migrated_regularization_loss = tf.math.add_n(layer.losses)\n",
        "\n",
        "print(migrated_regularization_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMMXS7EHjvCy"
      },
      "outputs": [],
      "source": [
        "# Verify that the regularization loss and output both match\n",
        "np.testing.assert_allclose(original_regularization_loss.numpy(), migrated_regularization_loss.numpy())\n",
        "np.testing.assert_allclose(original_output.numpy(), migrated_output.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMxiMVFwbiQy"
      },
      "source": [
        "Ahora ha sustituido todas las `compat.v1.layers` individuales por capas Keras nativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dFCnyYc9DrX"
      },
      "outputs": [],
      "source": [
        "class NearlyFullyNativeModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "    self.conv_layer = tf.keras.layers.Conv2D(\n",
        "      3, 3,\n",
        "      kernel_regularizer=\"l2\")\n",
        "    self.flatten_layer = tf.keras.layers.Flatten()\n",
        "    self.dense_layer = tf.keras.layers.Dense(\n",
        "      self.units,\n",
        "      kernel_regularizer=\"l2\")\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_layer(inputs)\n",
        "      out = self.flatten_layer(out)\n",
        "      out = self.dense_layer(out)\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGPqEjkGHgar"
      },
      "outputs": [],
      "source": [
        "random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')\n",
        "with random_tool.scope():\n",
        "  tf.keras.utils.set_random_seed(42)\n",
        "  layer = NearlyFullyNativeModel(10)\n",
        "\n",
        "  inputs = tf.random.normal(shape=(10, 5, 5, 5))\n",
        "  migrated_output = layer(inputs)\n",
        "\n",
        "  # Grab the regularization loss as well\n",
        "  migrated_regularization_loss = tf.math.add_n(layer.losses)\n",
        "\n",
        "print(migrated_regularization_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAs60eCdj6x_"
      },
      "outputs": [],
      "source": [
        "# Verify that the regularization loss and output both match\n",
        "np.testing.assert_allclose(original_regularization_loss.numpy(), migrated_regularization_loss.numpy())\n",
        "np.testing.assert_allclose(original_output.numpy(), migrated_output.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA6viSo3bo3y"
      },
      "source": [
        "Por último, elimine tanto cualquier uso restante (que ya no sea necesario) de `variable_scope` como el propio decorador `track_tf1_style_variables`.\n",
        "\n",
        "Ahora solo le queda una versión del modelo que usa APIs totalmente nativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIHpHWIRDunU"
      },
      "outputs": [],
      "source": [
        "class FullyNativeModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "    self.conv_layer = tf.keras.layers.Conv2D(\n",
        "      3, 3,\n",
        "      kernel_regularizer=\"l2\")\n",
        "    self.flatten_layer = tf.keras.layers.Flatten()\n",
        "    self.dense_layer = tf.keras.layers.Dense(\n",
        "      self.units,\n",
        "      kernel_regularizer=\"l2\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    out = self.conv_layer(inputs)\n",
        "    out = self.flatten_layer(out)\n",
        "    out = self.dense_layer(out)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttAmiCvLHW54"
      },
      "outputs": [],
      "source": [
        "random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')\n",
        "with random_tool.scope():\n",
        "  tf.keras.utils.set_random_seed(42)\n",
        "  layer = FullyNativeModel(10)\n",
        "\n",
        "  inputs = tf.random.normal(shape=(10, 5, 5, 5))\n",
        "  migrated_output = layer(inputs)\n",
        "\n",
        "  # Grab the regularization loss as well\n",
        "  migrated_regularization_loss = tf.math.add_n(layer.losses)\n",
        "\n",
        "print(migrated_regularization_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym5DYtT4j7e3"
      },
      "outputs": [],
      "source": [
        "# Verify that the regularization loss and output both match\n",
        "np.testing.assert_allclose(original_regularization_loss.numpy(), migrated_regularization_loss.numpy())\n",
        "np.testing.assert_allclose(original_output.numpy(), migrated_output.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX4pdrzycIsa"
      },
      "source": [
        "### Mantener la compatibilidad de los puntos de verificación durante la migración al TF2 nativo\n",
        "\n",
        "El proceso de migración anterior a las APIs nativas de TF2 cambió tanto los nombres de las variables (ya que las APIs de Keras producen nombres de ponderación muy diferentes), como las rutas orientadas a objetos que apuntan a diferentes ponderaciones en el modelo. Estos cambios dejarán inservibles tanto los puntos de verificación basados en nombres al estilo TF1 como los puntos de verificación orientados a objetos al estilo TF2.\n",
        "\n",
        "Sin embargo, en algunos casos, es posible que pueda tomar su punto de verificación original basado en nombres y encontrar un mapeado de las variables a sus nuevos nombres con enfoques como el que se detalla en la guía [Reutilización de puntos de verificación TF1.x](./migrating_checkpoints.ipynb).\n",
        "\n",
        "Algunos consejos para que esto sea factible son los siguientes:\n",
        "\n",
        "- Las variables siguen teniendo todas un argumento `name` que puede configurar.\n",
        "- Los modelos Keras también toman un argumento `name` que configuran como prefijo para sus variables.\n",
        "- La función `v1.name_scope` puede usarse para configurar los prefijos de los nombres de las variables. Esto es muy diferente de `tf.variable_scope`. Sólo afecta a los nombres, y no hace un seguimiento de las variables y su reutilización.\n",
        "\n",
        "Con las indicaciones anteriores en mente, los ejemplos de código siguientes demuestran un flujo de trabajo que puede adaptar a su código para actualizar de forma incremental parte de un modelo mientras se actualizan simultáneamente los puntos de verificación.\n",
        "\n",
        "Nota: Debido a la complejidad de la denominación de variables con las capas Keras, no se garantiza que esto funcione para todos los casos de uso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFmMY3dcx3mR"
      },
      "source": [
        "1. Empiece por cambiar las capas `tf.compat.v1.layers` de estilo funcional por sus versiones orientadas a objetos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRxCFmNjl2ta"
      },
      "outputs": [],
      "source": [
        "class FunctionalStyleCompatModel(tf.keras.layers.Layer):\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          inputs, 3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          out, 4, 4,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          out, 5, 5,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n",
        "\n",
        "layer = FunctionalStyleCompatModel()\n",
        "layer(tf.ones(shape=(10, 10, 10, 10)))\n",
        "[v.name for v in layer.weights]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvzUyXxjydAd"
      },
      "source": [
        "1. A continuación, asigne los objetos compat.v1.layer y cualquier variable creada por `compat.v1.get_variable` como propiedades del objeto `tf.keras.layers.Layer`/`tf.Module` cuyo método está decorado con `track_tf1_style_variables` (tenga en cuenta que cualquier punto de verificación del estilo TF2 orientado a objetos guardará ahora tanto una ruta por nombre de variable como la nueva ruta orientada a objetos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02jMQkJFmFwl"
      },
      "outputs": [],
      "source": [
        "class OOStyleCompatModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.conv_1 = tf.compat.v1.layers.Conv2D(\n",
        "          3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "    self.conv_2 = tf.compat.v1.layers.Conv2D(\n",
        "          4, 4,\n",
        "          kernel_regularizer=\"l2\")\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_1(inputs)\n",
        "      out = self.conv_2(out)\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          out, 5, 5,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n",
        "\n",
        "layer = OOStyleCompatModel()\n",
        "layer(tf.ones(shape=(10, 10, 10, 10)))\n",
        "[v.name for v in layer.weights]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8evFpd8Nq63v"
      },
      "source": [
        "1. Vuelva a guardar un punto de verificación cargado en este punto para guardar las rutas tanto por el nombre de la variable (para compat.v1.layers), como por el grafo de objetos orientado a objetos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7neFr-9pqmJX"
      },
      "outputs": [],
      "source": [
        "weights = {v.name: v for v in layer.weights}\n",
        "assert weights['model/conv2d/kernel:0'] is layer.conv_1.kernel\n",
        "assert weights['model/conv2d_1/bias:0'] is layer.conv_2.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvsi743Xh9wn"
      },
      "source": [
        "1. Ahora puede cambiar las `compat.v1.layers` orientadas a objetos por capas Keras nativas sin dejar de poder cargar el punto de verificación guardado recientemente. Asegúrese de conservar los nombres de las variables para las `compat.v1.layers` restantes registrando aún los `variable_scopes` autogenerados de las capas sustituidas. Estas capas/variables sustituidas ahora sólo usarán la ruta del atributo del objeto a las variables en el punto de verificación en lugar de la ruta del nombre de la variable.\n",
        "\n",
        "En general, puede reemplazar el uso de `compat.v1.get_variable` en variables adjuntas a propiedades por:\n",
        "\n",
        "- Cambiarlas para usar `tf.Variable`, **O**\n",
        "- Actualizarlas usando [`tf.keras.layers.Layer.add_weight`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight). Tenga en cuenta que si no está cambiando todas las capas de una sola vez, esto puede cambiar la nomenclatura autogenerada de capas/variables para las `compat.v1.layers` restantes a las que les falta un argumento `name`. Si ese es el caso, debe conservar los nombres de las variables para las `compat.v1.layers` restantes igual abriendo y cerrando manualmente un `ámbito_variable` correspondiente al nombre de ámbito generado de la `compat.v1.layer` eliminada. De lo contrario, las rutas de los puntos de verificación existentes pueden entrar en conflicto y la carga de puntos de verificación se comportará de forma incorrecta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbixtIW-maoH"
      },
      "outputs": [],
      "source": [
        "def record_scope(scope_name):\n",
        "  \"\"\"Record a variable_scope to make sure future ones get incremented.\"\"\"\n",
        "  with tf.compat.v1.variable_scope(scope_name):\n",
        "    pass\n",
        "\n",
        "class PartiallyNativeKerasLayersModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.conv_1 = tf.keras.layers.Conv2D(\n",
        "          3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "    self.conv_2 = tf.keras.layers.Conv2D(\n",
        "          4, 4,\n",
        "          kernel_regularizer=\"l2\")\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_1(inputs)\n",
        "      record_scope('conv2d') # Only needed if follow-on compat.v1.layers do not pass a `name` arg\n",
        "      out = self.conv_2(out)\n",
        "      record_scope('conv2d_1') # Only needed if follow-on compat.v1.layers do not pass a `name` arg\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          out, 5, 5,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n",
        "\n",
        "layer = PartiallyNativeKerasLayersModel()\n",
        "layer(tf.ones(shape=(10, 10, 10, 10)))\n",
        "[v.name for v in layer.weights]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eaPpevGs3dA"
      },
      "source": [
        "Guardar un punto de verificación en este paso después de construir las variables hará que contenga ***sólo*** las rutas de objetos disponibles en ese momento.\n",
        "\n",
        "Asegúrese de registrar los ámbitos de las `compat.v1.layers` eliminadas para conservar los nombres de ponderación autogenerados para las `compat.v1.layers` restantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK7vtWBprObA"
      },
      "outputs": [],
      "source": [
        "weights = set(v.name for v in layer.weights)\n",
        "assert 'model/conv2d_2/kernel:0' in weights\n",
        "assert 'model/conv2d_2/bias:0' in weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ5-SfmWFTvY"
      },
      "source": [
        "1. Repita los pasos anteriores hasta que haya sustituido todas las `compat.v1.layers` y `compat.v1.get_variable` de su modelo por equivalentes totalmente nativos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA1d2POtnTQa"
      },
      "outputs": [],
      "source": [
        "class FullyNativeKerasLayersModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.conv_1 = tf.keras.layers.Conv2D(\n",
        "          3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "    self.conv_2 = tf.keras.layers.Conv2D(\n",
        "          4, 4,\n",
        "          kernel_regularizer=\"l2\")\n",
        "    self.conv_3 = tf.keras.layers.Conv2D(\n",
        "          5, 5,\n",
        "          kernel_regularizer=\"l2\")\n",
        "\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_1(inputs)\n",
        "      out = self.conv_2(out)\n",
        "      out = self.conv_3(out)\n",
        "      return out\n",
        "\n",
        "layer = FullyNativeKerasLayersModel()\n",
        "layer(tf.ones(shape=(10, 10, 10, 10)))\n",
        "[v.name for v in layer.weights]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZejG7rTsTb6"
      },
      "source": [
        "Recuerde realizar pruebas para asegurarse de que el punto de verificación recién actualizado sigue comportándose como espera. Aplique las técnicas descritas en la [guía de validación de la corrección numérica](./validate_correctness.ipynb) en cada paso incremental de este proceso para asegurarse de que su código migrado se ejecuta correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewi_h-cs6n-I"
      },
      "source": [
        "## Manejar los cambios de comportamiento de TF1.x a TF2 no cubiertos por los shim de modelado\n",
        "\n",
        "Los shim de modelado descritos en esta guía pueden garantizar que las variables, las capas y las pérdidas de regularización creadas con las semánticas `get_variable`, `tf.compat.v1.layers` y `variable_scope` sigan funcionando como antes al usar ejecución eager y `tf.function`, sin tener que depender de las recopilaciones.\n",
        "\n",
        "Esto no cubre ***toda*** la semántica específica de TF1.x en la que pueden estar basándose sus modelos de pase hacia adelante. En algunos casos, los shim pueden ser insuficientes para que su pase hacia delante modelo entre en TF2 por sí solo. Lea la [Guía de comportamientos de TF1.x frente a TF2](./tf1_vs_tf2) para saber más sobre las diferencias de comportamiento entre TF1.x y TF2."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "model_mapping.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
