{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bYaCABobL5q"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FlUw7tSKbtg4"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61dp4Hg5ksTC"
      },
      "source": [
        "# Migración de los puntos de verificación del modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/migrate/migrating_checkpoints\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/migrate/migrating_checkpoints.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/migrate/migrating_checkpoints.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/migrate/migrating_checkpoints.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avuMwzscPnHh"
      },
      "source": [
        "Nota: Se suele llamar a los puntos de verificación guardados con `tf.compat.v1.Saver` como puntos de verificación *TF1 o basados en nombres*. A los puntos de verificación guardados con `tf.train.Checkpoint` se les llama puntos de verificación *TF2 o basados en objetos.*\n",
        "\n",
        "## Descripción general\n",
        "\n",
        "Esta guía asume que usted tiene un modelo que guarda y carga puntos de verificación con [`tf.compat.v1.Saver`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver), y desea migrar el código para usar la API [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) de TF2, o usar puntos de verificación preexistentes en su modelo TF2.\n",
        "\n",
        "A continuación le presentamos algunos escenarios comunes con los que puede encontrarse:\n",
        "\n",
        "**Escenario 1**\n",
        "\n",
        "Existen puntos de verificación TF1 de entrenamientos anteriores que deben cargarse o convertirse a TF2.\n",
        "\n",
        "- Para cargar el punto de verificación TF1 en TF2, consulte el recorte [*Cargar un punto de verificación TF1 en TF2*](#load-tf1-in-tf2).\n",
        "- Para convertir el punto de verificación a TF2, consulte [*Conversión de puntos de verificación*](#checkpoint-conversion).\n",
        "\n",
        "**Escenario 2**\n",
        "\n",
        "Está ajustando su modelo de forma que corre el riesgo de cambiar los nombres y las rutas de las variables (como cuando migra incrementalmente de `get_variable` a la creación explícita de `tf.Variable`), y le gustaría mantener el guardado/carga de los puntos de verificación existentes a lo largo del camino.\n",
        "\n",
        "Consulte la sección sobre [*Cómo mantener la compatibilidad de puntos de verificación durante la migración de modelos*](#maintain-checkpoint-compat)\n",
        "\n",
        "**Escenario 3**\n",
        "\n",
        "Usted está migrando su código de entrenamiento y sus puntos de verificación a TF2, pero su canalización de inferencia sigue necesitando por ahora puntos de verificación de TF1 (para la estabilidad de la producción).\n",
        "\n",
        "*Option 1*\n",
        "\n",
        "Guarde los puntos de verificación de TF1 y TF2 durante el entrenamiento.\n",
        "\n",
        "- vea [*Cómo guardar un punto de verificación TF1 en TF2*](#save-tf1-in-tf2)\n",
        "\n",
        "*Option 2*\n",
        "\n",
        "Convierta el punto de verificación TF2 en TF1.\n",
        "\n",
        "- vea [*Conversión de puntos de verificación*](#checkpoint-conversion)\n",
        "\n",
        "---\n",
        "\n",
        "Los ejemplos siguientes muestran todas las combinaciones de guardado y carga de puntos de verificación en TF1/TF2, para que tenga cierta flexibilidad para determinar cómo migrar su modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaYgaekzOAHf"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcvTd5QhZ78L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "\n",
        "def print_checkpoint(save_path):\n",
        "  reader = tf.train.load_checkpoint(save_path)\n",
        "  shapes = reader.get_variable_to_shape_map()\n",
        "  dtypes = reader.get_variable_to_dtype_map()\n",
        "  print(f\"Checkpoint at '{save_path}':\")\n",
        "  for key in shapes:\n",
        "    print(f\"  (key='{key}', shape={shapes[key]}, dtype={dtypes[key].name}, \"\n",
        "          f\"value={reader.get_tensor(key)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO8Q6QkulJlj"
      },
      "source": [
        "## Cambios de TF1 a TF2\n",
        "\n",
        "Esta sección se incluye por si tiene curiosidad sobre lo que ha cambiado entre el TF1 y el TF2, y lo que entendemos por puntos de verificación \"basados en nombres\" (TF1) frente a \"basados en objetos\" (TF2).\n",
        "\n",
        "Los dos tipos de puntos de verificación se guardan en realidad en el mismo formato, que es esencialmente una tabla clave-valor. La diferencia radica en cómo se generan las claves.\n",
        "\n",
        "Las claves en los puntos de verificación basados en nombres son los **nombres de las variables**. Las claves en los puntos de verificación basados en objetos se refieren a la **ruta desde el objeto raíz hasta la variable** (para comprender mejor lo que esto significa, vea los ejemplos siguientes).\n",
        "\n",
        "En primer lugar, guarde algunos puntos de verificación:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YXzbXvOWvdF"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    saver = tf1.train.Saver()\n",
        "    sess.run(a.assign(1))\n",
        "    sess.run(b.assign(2))\n",
        "    sess.run(c.assign(3))\n",
        "    saver.save(sess, 'tf1-ckpt')\n",
        "\n",
        "print_checkpoint('tf1-ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raOych1UaJzl"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(5.0, name='a')\n",
        "b = tf.Variable(6.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(7.0, name='c')\n",
        "\n",
        "ckpt = tf.train.Checkpoint(variables=[a, b, c])\n",
        "save_path_v2 = ckpt.save('tf2-ckpt')\n",
        "print_checkpoint(save_path_v2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYyLhTYszcpl"
      },
      "source": [
        "Si observa las claves en `tf2-ckpt`, todas ellas se refieren a las rutas de los objetos de cada variable. Por ejemplo, la variable `a` es el primer elemento de la lista `variables`, por lo que su clave pasa a ser `variables/0/...` (siéntase libre de ignorar la constante .ATTRIBUTES/VARIABLE_VALOR).\n",
        "\n",
        "A continuación, una inspección más detallada del objeto `Checkpoint`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLOxvoosg4Al"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "c = tf.Variable(0.)\n",
        "root = ckpt = tf.train.Checkpoint(variables=[a, b, c])\n",
        "print(\"root type =\", type(root).__name__)\n",
        "print(\"root.variables =\", root.variables)\n",
        "print(\"root.variables[0] =\", root.variables[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qaed1yAm3Ar"
      },
      "source": [
        "Experimente con el siguiente fragmento y vea cómo cambian las claves del punto de verificación con la estructura del objeto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdHJXlZOyDnn"
      },
      "outputs": [],
      "source": [
        "module = tf.Module()\n",
        "module.d = tf.Variable(0.)\n",
        "test_ckpt = tf.train.Checkpoint(v={'a': a, 'b': b}, \n",
        "                                c=c,\n",
        "                                module=module)\n",
        "test_ckpt_path = test_ckpt.save('root-tf2-ckpt')\n",
        "print_checkpoint(test_ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iWitEsayDWs"
      },
      "source": [
        "*¿Por qué TF2 usa este mecanismo?*\n",
        "\n",
        "Como ya no hay grafo global en TF2, los nombres de las variables no son fiables y pueden ser inconsistentes entre programas. TF2 fomenta el enfoque de modelado orientado a objetos, en el que las variables son propiedad de capas, y las capas son propiedad de un modelo:\n",
        "\n",
        "```\n",
        "variable = tf.Variable(...)\n",
        "layer.variable_name = variable\n",
        "model.layer_name = layer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kv9SmyVjGLA"
      },
      "source": [
        "## Cómo mantener la compatibilidad de los puntos de verificación durante la migración de modelos\n",
        "\n",
        "<a name=\"maintain-checkpoint-compat\"></a>\n",
        "\n",
        "Un paso importante en el proceso de migración es *asegurarse de que todas las variables se inicializan con los valores correctos*, que le permitirá a su vez validar que las op/funciones realizan los cálculos correctos. Para lograrlo, debe tener en cuenta la **compatibilidad de puntos de verificación** entre los modelos en las distintas etapas de la migración. Básicamente, esta sección responde a la pregunta *cómo conservo el mismo punto de verificación mientras cambio el modelo*.\n",
        "\n",
        "A continuación se presentan tres formas de mantener la compatibilidad de los puntos de verificación, por orden de flexibilidad creciente:\n",
        "\n",
        "1. El modelo tiene los **mismos nombres de variables** que antes.\n",
        "2. El modelo tiene diferentes nombres de variables, y mantiene un **mapa de asignación** que esquematiza los nombres de las variables en el punto de verificación a los nuevos nombres.\n",
        "3. El modelo tiene diferentes nombres de variables y conserva un **objeto Checkpoint TF2** que almacena todas las variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5JhCyPZDx43"
      },
      "source": [
        "### Cuando los nombres de las variables coinciden\n",
        "\n",
        "Título largo: Cómo reutilizar los puntos de verificación cuando los nombres de las variables coinciden.\n",
        "\n",
        "Respuesta corta: Puede cargar directamente el punto de verificación preexistente con `tf1.train.Saver` o `tf.train.Checkpoint`.\n",
        "\n",
        "---\n",
        "\n",
        "Si está usando `tf.compat.v1.keras.utils.track_tf1_style_variables`, entonces se asegurará de que los nombres de las variables de su modelo son los mismos que antes. También puede asegurarse manualmente de que los nombres de las variables coinciden.\n",
        "\n",
        "Cuando los nombres de las variables coinciden en los modelos migrados, puede usar directamente `tf.train.Checkpoint` o `tf.compat.v1.train.Saver` para cargar el punto de verificación. Ambas API son compatibles con eager mode y modo grafo, por lo que puede usarlas en cualquier fase de la migración.\n",
        "\n",
        "Nota: Puede usar `tf.train.Checkpoint` para cargar puntos de verificación TF1, pero no puede usar `tf.compat.v1.Saver` para cargar puntos de verificación TF2 sin una complicada coincidencia de nombres.\n",
        "\n",
        "A continuación se muestran ejemplos de cómo usar el mismo punto de verificación con diferentes modelos. En primer lugar, guarde un punto de verificación TF1 con `tf1.train.Saver`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijlHS96URsfR"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    saver = tf1.train.Saver()\n",
        "    sess.run(a.assign(1))\n",
        "    sess.run(b.assign(2))\n",
        "    sess.run(c.assign(3))\n",
        "    save_path = saver.save(sess, 'tf1-ckpt')\n",
        "print_checkpoint(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg7nWZphQD9u"
      },
      "source": [
        "El ejemplo siguiente usa `tf.compat.v1.Saver` para cargar el punto de verificación mientras está en eager mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4K16m0PPncQ"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.0, name='a')\n",
        "b = tf.Variable(0.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(0.0, name='c')\n",
        "\n",
        "# With the removal of collections in TF2, you must pass in the list of variables\n",
        "# to the Saver object:\n",
        "saver = tf1.train.Saver(var_list=[a, b, c])\n",
        "saver.restore(sess=None, save_path=save_path)\n",
        "print(f\"loaded values of [a, b, c]:  [{a.numpy()}, {b.numpy()}, {c.numpy()}]\")\n",
        "\n",
        "# Saving also works in eager (sess must be None).\n",
        "path = saver.save(sess=None, save_path='tf1-ckpt-saved-in-eager')\n",
        "print_checkpoint(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWnq1f5yAPkq"
      },
      "source": [
        "El siguiente recorte de código carga el punto de verificación usando la API`tf.train.Checkpoint` de TF2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StyrzwGvW1YZ"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.0, name='a')\n",
        "b = tf.Variable(0.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(0.0, name='c')\n",
        "\n",
        "# Without the name_scope, name=\"scoped/c\" works too:\n",
        "c_2 = tf.Variable(0.0, name='scoped/c')\n",
        "\n",
        "print(\"Variable names: \")\n",
        "print(f\"  a.name = {a.name}\")\n",
        "print(f\"  b.name = {b.name}\")\n",
        "print(f\"  c.name = {c.name}\")\n",
        "print(f\"  c_2.name = {c_2.name}\")\n",
        "\n",
        "# Restore the values with tf.train.Checkpoint\n",
        "ckpt = tf.train.Checkpoint(variables=[a, b, c, c_2])\n",
        "ckpt.restore(save_path)\n",
        "print(f\"loaded values of [a, b, c, c_2]:  [{a.numpy()}, {b.numpy()}, {c.numpy()}, {c_2.numpy()}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYYgbj8F7Yb7"
      },
      "source": [
        "#### Nombres de variables en TF2\n",
        "\n",
        "- Las variables siguen teniendo todas un argumento `name` que puede configurar.\n",
        "- Los modelos Keras también toman un argumento `name` que configuran como prefijo para sus variables.\n",
        "- La función `v1.name_scope` puede usarse para configurar los prefijos de los nombres de las variables. Esto es muy diferente de `tf.variable_scope`. Sólo afecta a los nombres, y no hace un seguimiento de las variables y su reutilización.\n",
        "\n",
        "El decorador `tf.compat.v1.keras.utils.track_tf1_style_variables` es un shim que le ayuda a conservar los nombres de las variables y la compatibilidad con el punto de verificación TF1, conservando sin cambios la semántica de denominación y reutilización de `tf.variable_scope` y `tf.compat.v1.get_variable`. Consulte la [Guía para mapear modelos](./model_mapping.ipynb) para saber más.\n",
        "\n",
        "**Nota 1: Si está usando el shim, use las API del TF2 para cargar sus puntos de verificación (incluso cuando use puntos de verificación del TF1 preentrenados).**\n",
        "\n",
        "Consulte la sección *Keras de punto de verificación*.\n",
        "\n",
        "**Nota 2: Al migrar a `tf.Variable` desde `get_variable`:**\n",
        "\n",
        "Si su capa o módulo decorado con shim consta de algunas variables (o capas/modelos Keras) que usan `tf.Variable` en lugar de `tf.compat.v1.get_variable` y se anexan como propiedades/seguimiento según la orientación a objetos, pueden tener una semántica de nomenclatura de variables diferente en los grafos/sesiones TF1.x versus durante la ejecución eager.\n",
        "\n",
        "En resumen, *los nombres pueden no ser lo que usted espera que sean* cuando se ejecuta en TF2.\n",
        "\n",
        "Advertencia: Las variables pueden tener nombres duplicados en ejecución eager, lo que puede causar problemas si es necesario mapear varias variables del punto de verificación basado en nombres con el mismo nombre. Puede ajustar explícitamente la capa y los nombres de las variables usando `tf.name_scope` y el constructor de capa o los argumentos `name` de `tf.Variable` para ajustar los nombres de las variables y asegurarse de que no hay duplicados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkUQJUUyjOJz"
      },
      "source": [
        "### Actualizar los mapas de asignación\n",
        "\n",
        "Los mapas de asignación se suelen usar para transferir ponderaciones entre modelos TF1, y también se pueden usar durante la migración de su modelo si cambian los nombres de las variables.\n",
        "\n",
        "Puede usar estos mapas con [`tf.compat.v1.train.init_from_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/init_from_checkpoint), [`tf.compat.v1.train.Saver`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver), y [`tf.train.load_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/load_checkpoint) para cargar ponderaciones en modelos en los que los nombres de las variables o el ámbito pueden haber cambiado.\n",
        "\n",
        "Los ejemplos de esta sección usarán un punto de verificación previamente guardado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PItyo7DdJ6Ek"
      },
      "outputs": [],
      "source": [
        "print_checkpoint('tf1-ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPryV_WBJrI3"
      },
      "source": [
        "#### Carga con `init_from_checkpoint`\n",
        "\n",
        "[`tf1.train.init_from_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/init_from_checkpoint) debe llamarse mientras se está en un grafo/sesión, porque coloca los valores en los inicializadores de variables en lugar de crear una op de asignación.\n",
        "\n",
        "Puede usar el argumento `assignment_map` para configurar cómo se cargan las variables. De la documentación:\n",
        "\n",
        "> El mapa de asignación admite la siguiente sintaxis:\n",
        "\n",
        "- `'checkpoint_scope_name/': 'scope_name/'`: cargará todas las variables del actual `scope_name` desde `checkpoint_scope_name` con nombres de tensor coincidentes.\n",
        "- `'checkpoint_scope_name/some_other_variable': 'scope_name/variable_name'`: inicializará la variable `scope_name/variable_name` de `checkpoint_scope_name/some_other_variable`.\n",
        "- `'scope_variable_name': variable`: inicializará el objeto `tf.Variable` dado con el tensor 'scope_variable_name' del punto de verificación.\n",
        "- `'scope_variable_name': list(variable)`: inicializará la lista de variables particionadas con el tensor 'scope_variable_name' desde el punto de verificación.\n",
        "- `'/': 'scope_name/'`: cargará todas las variables del `scope_name` actual desde la raíz del punto de verificación (por ejemplo, sin ámbito).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM_7OzRpdH0A"
      },
      "outputs": [],
      "source": [
        "# Restoring with tf1.train.init_from_checkpoint:\n",
        "\n",
        "# A new model with a different scope for the variables.\n",
        "with tf.Graph().as_default() as g:\n",
        "  with tf1.variable_scope('new_scope'):\n",
        "    a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    # The assignment map will remap all variables in the checkpoint to the\n",
        "    # new scope:\n",
        "    tf1.train.init_from_checkpoint(\n",
        "        'tf1-ckpt',\n",
        "        assignment_map={'/': 'new_scope/'})\n",
        "    # `init_from_checkpoint` adds the initializers to these variables.\n",
        "    # Use `sess.run` to run these initializers.\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za_8xhFWKVlH"
      },
      "source": [
        "#### Carga con `tf1.train.Saver`\n",
        "\n",
        "A diferencia de `init_from_checkpoint`, [`tf.compat.v1.train.Saver`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver) se ejecuta tanto en modo grafo como en eager mode. El argumento `var_list` acepta opcionalmente un diccionario, con la salvedad de que debe mapear los nombres de las variables al objeto `tf.Variable`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiKNmdGJgoX9"
      },
      "outputs": [],
      "source": [
        "# Restoring with tf1.train.Saver (works in both graph and eager):\n",
        "\n",
        "# A new model with a different scope for the variables.\n",
        "with tf1.variable_scope('new_scope'):\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                      initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                      initializer=tf1.zeros_initializer())\n",
        "  c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "# Initialize the saver with a dictionary with the original variable names:\n",
        "saver = tf1.train.Saver({'a': a, 'b': b, 'scoped/c': c})\n",
        "saver.restore(sess=None, save_path='tf1-ckpt')\n",
        "print(\"Restored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JsgCXt3Ly-h"
      },
      "source": [
        "#### Carga con `tf.train.load_checkpoint`\n",
        "\n",
        "Esta opción es para usted si necesita un control preciso sobre los valores de las variables. Nuevamente, esto funciona tanto en modo grafo como en eager mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc39Bh6JMso6"
      },
      "outputs": [],
      "source": [
        "# Restoring with tf.train.load_checkpoint (works in both graph and eager):\n",
        "\n",
        "# A new model with a different scope for the variables.\n",
        "with tf.Graph().as_default() as g:\n",
        "  with tf1.variable_scope('new_scope'):\n",
        "    a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    # It may be easier writing a loop if your model has a lot of variables.\n",
        "    reader = tf.train.load_checkpoint('tf1-ckpt')\n",
        "    sess.run(a.assign(reader.get_tensor('a')))\n",
        "    sess.run(b.assign(reader.get_tensor('b')))\n",
        "    sess.run(c.assign(reader.get_tensor('scoped/c')))\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBSTJVCNDKed"
      },
      "source": [
        "### Actualizar un objeto de punto de verificación de TF2\n",
        "\n",
        "Si los nombres de las variables y del ámbito pueden cambiar mucho durante la migración, entonces usa `tf.train.Checkpoint` y puntos de verificación TF2. TF2 usa la **estructura de objetos** en lugar de nombres de variables (más información en *Cambios de TF1 a TF2*).\n",
        "\n",
        "En resumen, cuando crees un `tf.train.Checkpoint` para guardar o restaurar puntos de verificación, asegúrate de que usa el mismo **ordenamiento** (para listas) y **claves** (para diccionarios y argumentos de palabras clave para el inicializador `Checkpoint`). Algunos ejemplos de compatibilidad de puntos de verificación:\n",
        "\n",
        "```\n",
        "ckpt = tf.train.Checkpoint(foo=[var_a, var_b])\n",
        "\n",
        "# compatible with ckpt\n",
        "tf.train.Checkpoint(foo=[var_a, var_b])\n",
        "\n",
        "# not compatible with ckpt\n",
        "tf.train.Checkpoint(foo=[var_b, var_a])\n",
        "tf.train.Checkpoint(bar=[var_a, var_b])\n",
        "```\n",
        "\n",
        "Las muestras de código que aparecen a continuación muestran cómo usar el \"mismo\" `tf.train.Checkpoint` para cargar variables con nombres diferentes. Primero, guarda un punto de verificación TF2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCSkz_-Tbct6"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(1))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(2))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(3))\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    print(\"[a, b, c]: \", sess.run([a, b, c]))\n",
        "\n",
        "    # Save a TF2 checkpoint\n",
        "    ckpt = tf.train.Checkpoint(unscoped=[a, b], scoped=[c])\n",
        "    tf2_ckpt_path = ckpt.save('tf2-ckpt')\n",
        "    print_checkpoint(tf2_ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62MWdZMxezeP"
      },
      "source": [
        "Puede conservar el uso de `tf.train.Checkpoint` aunque cambien los nombres de las variables/ámbitos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh61SGeqb27b"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a_different_name', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b_different_name', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  with tf1.variable_scope('different_scope'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    print(\"Initialized [a, b, c]: \", sess.run([a, b, c]))\n",
        "\n",
        "    ckpt = tf.train.Checkpoint(unscoped=[a, b], scoped=[c])\n",
        "    # `assert_consumed` validates that all checkpoint objects are restored from\n",
        "    # the checkpoint. `run_restore_ops` is required when running in a TF1\n",
        "    # session.\n",
        "    ckpt.restore(tf2_ckpt_path).assert_consumed().run_restore_ops()\n",
        "\n",
        "    # Removing `assert_consumed` is fine if you want to skip the validation.\n",
        "    # ckpt.restore(tf2_ckpt_path).run_restore_ops()\n",
        "\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unDPmL-kldr2"
      },
      "source": [
        "Y en eager mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79S0zMAnfzx7"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "c = tf.Variable(0.)\n",
        "print(\"Initialized [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])\n",
        "\n",
        "# The keys \"scoped\" and \"unscoped\" are no longer relevant, but are used to\n",
        "# maintain compatibility with the saved checkpoints.\n",
        "ckpt = tf.train.Checkpoint(unscoped=[a, b], scoped=[c])\n",
        "\n",
        "ckpt.restore(tf2_ckpt_path).assert_consumed().run_restore_ops()\n",
        "print(\"Restored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKfNAr8l3aFg"
      },
      "source": [
        "## Puntos de verificación de TF2 en Estimator\n",
        "\n",
        "Las secciones anteriores describen cómo mantener la compatibilidad de los puntos de verificación mientras migra su modelo. Estos conceptos también se aplican a los modelos Estimator, aunque la forma en que se guarda/carga el punto de verificación es ligeramente diferente. Al migrar su modelo Estimator para usar las API TF2, es posible que desee cambiar los puntos de verificación *{nbsp}de TF1 a TF2 mientras el modelo sigue usando Estimator*. Esta sección muestra cómo hacerlo.\n",
        "\n",
        "[`tf.estimator.Estimator`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) y [`MonitoredSession`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MonitoredSession) tienen un mecanismo de guardado llamado `scaffold`, un objeto [`tf.compat.v1.train.Scaffold`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Scaffold). El `Scaffold` puede contener un `tf1.train.Saver` o `tf.train.Checkpoint`, que permite a `Estimator` y `MonitoredSession` guardar puntos de verificación del estilo TF1 o TF2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8AT_oO-5TXU"
      },
      "outputs": [],
      "source": [
        "# A model_fn that saves a TF1 checkpoint\n",
        "def model_fn_tf1_ckpt(features, labels, mode):\n",
        "  # This model adds 2 to the variable `v` in every train step.\n",
        "  train_step = tf1.train.get_or_create_global_step()\n",
        "  v = tf1.get_variable('var', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode,\n",
        "      predictions=v,\n",
        "      train_op=tf.group(v.assign_add(2), train_step.assign_add(1)),\n",
        "      loss=tf.constant(1.),\n",
        "      scaffold=None\n",
        "  )\n",
        "\n",
        "!rm -rf est-tf1\n",
        "est = tf.estimator.Estimator(model_fn_tf1_ckpt, 'est-tf1')\n",
        "\n",
        "def train_fn():\n",
        "  return tf.data.Dataset.from_tensor_slices(([1,2,3], [4,5,6]))\n",
        "est.train(train_fn, steps=1)\n",
        "\n",
        "latest_checkpoint = tf.train.latest_checkpoint('est-tf1')\n",
        "print_checkpoint(latest_checkpoint)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttH6cUrl7jK2"
      },
      "outputs": [],
      "source": [
        "# A model_fn that saves a TF2 checkpoint\n",
        "def model_fn_tf2_ckpt(features, labels, mode):\n",
        "  # This model adds 2 to the variable `v` in every train step.\n",
        "  train_step = tf1.train.get_or_create_global_step()\n",
        "  v = tf1.get_variable('var', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  ckpt = tf.train.Checkpoint(var_list={'var': v}, step=train_step)\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode,\n",
        "      predictions=v,\n",
        "      train_op=tf.group(v.assign_add(2), train_step.assign_add(1)),\n",
        "      loss=tf.constant(1.),\n",
        "      scaffold=tf1.train.Scaffold(saver=ckpt)\n",
        "  )\n",
        "\n",
        "!rm -rf est-tf2\n",
        "est = tf.estimator.Estimator(model_fn_tf2_ckpt, 'est-tf2',\n",
        "                             warm_start_from='est-tf1')\n",
        "\n",
        "def train_fn():\n",
        "  return tf.data.Dataset.from_tensor_slices(([1,2,3], [4,5,6]))\n",
        "est.train(train_fn, steps=1)\n",
        "\n",
        "latest_checkpoint = tf.train.latest_checkpoint('est-tf2')\n",
        "print_checkpoint(latest_checkpoint)  \n",
        "\n",
        "assert est.get_variable_value('var_list/var/.ATTRIBUTES/VARIABLE_VALUE') == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYVYgahE8daL"
      },
      "source": [
        "El valor final de `v` debería ser `16`, después de haber sido templado desde `est-tf1`, y luego entrenado durante 5 pasos adicionales. El valor del paso de entrenamiento no se arrastra desde el punto de verificación `warm_start`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq8EjblQUIA2"
      },
      "source": [
        "## Usar puntos de verificación de Keras\n",
        "\n",
        "Los modelos creados con Keras siguen usando `tf1.train.Saver` y `tf.train.Checkpoint` para cargar las ponderaciones preexistentes. Cuando tu modelo esté totalmente migrado, pasa a usar `model.save_weights` y `model.load_weights`, especialmente si estás usando la retrollamada `ModelCheckpoint` durante el entrenamiento.\n",
        "\n",
        "Algunas cosas que debes saber sobre los puntos de verificación y Keras:\n",
        "\n",
        "**Inicialización vs Construcción**\n",
        "\n",
        "Los modelos Keras y las capas deben pasar por **dos pasos** antes de crearse por completo. El primero es la *inicialización* del objeto Python: `layer = tf.keras.capas.Dense(x)`. El segundo es el paso *build*, en el que se crean realmente la mayoría de las ponderaciones: <code>layer.build(input_shape)</code>. También puede construir un modelo llamándolo o ejecutando un único paso `train`, `eval` o `predict` (sólo la primera vez).\n",
        "\n",
        "Si encuentras que `model.load_weights(path).assert_consumed()` produce un error, es probable que el modelo/las capas no se hayan construido.\n",
        "\n",
        "**Keras usa los puntos de verificación de TF2**\n",
        "\n",
        "`tf.train.Checkpoint(model).write` es equivalente a `model.save_weights`. Lo mismo ocurre con `tf.train.Checkpoint(model).read` y `model.load_weights`. Ten en cuenta que `Checkpoint(model) != Checkpoint(model=model)`.\n",
        "\n",
        "**Los puntos de verificación TF2 funcionan con el paso `build()` de Keras**\n",
        "\n",
        "`tf.train.Checkpoint.restore` tiene un mecanismo llamado *restauración diferida* que permite a `tf.Module` y a los objetos Keras almacenar valores de variables si éstas aún no se han creado. Esto permite a los modelos *inicializados* cargar ponderaciones y *construir* después.\n",
        "\n",
        "```\n",
        "m = YourKerasModel()\n",
        "status = m.load_weights(path)\n",
        "\n",
        "# This call builds the model. The variables are created with the restored\n",
        "# values.\n",
        "m.predict(inputs)\n",
        "\n",
        "status.assert_consumed()\n",
        "```\n",
        "\n",
        "Debido a este mecanismo, te recomendamos encarecidamente que uses las API de carga de puntos de verificación TF2 con modelos Keras (incluso al restaurar puntos de verificación TF1 preexistentes en las [shims de mapeo de modelos](./model_mapping.ipynb)). Más información en la [guía de puntos de verificación](https://www.tensorflow.org/guide/checkpoint#delayed_restorations).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO2NucRtqMm6"
      },
      "source": [
        "## Recortes de código\n",
        "\n",
        "Los recortes siguientes muestran la compatibilidad de las versiones TF1/TF2 en las API de guardado de puntos de verificación. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3SSc74olkX3"
      },
      "source": [
        "### Guardar un punto de verificación de TF1 en TF2\n",
        "\n",
        "<a name=\"save-tf1-in-tf2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2ZPk8BPloE1"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(1.0, name='a')\n",
        "b = tf.Variable(2.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(3.0, name='c')\n",
        "\n",
        "saver = tf1.train.Saver(var_list=[a, b, c])\n",
        "path = saver.save(sess=None, save_path='tf1-ckpt-saved-in-eager')\n",
        "print_checkpoint(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxyN5khVjhmA"
      },
      "source": [
        "### Cargar un punto de verificación de TF1 en TF2\n",
        "\n",
        "<a name=\"load-tf1-in-tf2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5kSXy3FmA79"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0., name='a')\n",
        "b = tf.Variable(0., name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(0., name='c')\n",
        "print(\"Initialized [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])\n",
        "saver = tf1.train.Saver(var_list=[a, b, c])\n",
        "saver.restore(sess=None, save_path='tf1-ckpt-saved-in-eager')\n",
        "print(\"Restored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul3V4pEwloeN"
      },
      "source": [
        "### Guardar un punto de verificación de TF2 en TF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhuP_2EIlRm4"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(1))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(2))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(3))\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    ckpt = tf.train.Checkpoint(\n",
        "        var_list={v.name.split(':')[0]: v for v in tf1.global_variables()})\n",
        "    tf2_in_tf1_path = ckpt.save('tf2-ckpt-saved-in-session')\n",
        "    print_checkpoint(tf2_in_tf1_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiViCjCDgxhz"
      },
      "source": [
        "### Cargar un punto de verificación de TF2 en TF1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-4hIPZvmXlb"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(0))\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    print(\"Initialized [a, b, c]: \", sess.run([a, b, c]))\n",
        "    ckpt = tf.train.Checkpoint(\n",
        "        var_list={v.name.split(':')[0]: v for v in tf1.global_variables()})\n",
        "    ckpt.restore('tf2-ckpt-saved-in-session-1').run_restore_ops()\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRrSE2X6sgAM"
      },
      "source": [
        "## Conversión de puntos de verificación\n",
        "\n",
        "<a name=\"checkpoint-conversion\"></a>\n",
        "\n",
        "Puede convertir los puntos de verificación entre TF1 y TF2 cargándolos y volviéndolos a guardar. Una alternativa es `tf.train.load_checkpoint`, que se muestra en el código siguiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9KByaLous4q"
      },
      "source": [
        "### Convertir el punto de verificación TF1 a TF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG8grCv2smAb"
      },
      "outputs": [],
      "source": [
        "def convert_tf1_to_tf2(checkpoint_path, output_prefix):\n",
        "  \"\"\"Converts a TF1 checkpoint to TF2.\n",
        "\n",
        "  To load the converted checkpoint, you must build a dictionary that maps\n",
        "  variable names to variable objects.\n",
        "  ```\n",
        "  ckpt = tf.train.Checkpoint(vars={name: variable})  \n",
        "  ckpt.restore(converted_ckpt_path)\n",
        "  ```\n",
        "\n",
        "  Args:\n",
        "    checkpoint_path: Path to the TF1 checkpoint.\n",
        "    output_prefix: Path prefix to the converted checkpoint.\n",
        "\n",
        "  Returns:\n",
        "    Path to the converted checkpoint.\n",
        "  \"\"\"\n",
        "  vars = {}\n",
        "  reader = tf.train.load_checkpoint(checkpoint_path)\n",
        "  dtypes = reader.get_variable_to_dtype_map()\n",
        "  for key in dtypes.keys():\n",
        "    vars[key] = tf.Variable(reader.get_tensor(key))\n",
        "  return tf.train.Checkpoint(vars=vars).save(output_prefix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyvqK6Sb3dad"
      },
      "source": [
        "Convierta el punto de verificación guardado en el recorte `Save a TF1 checkpoint in TF2`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcHLN4lPvYvw"
      },
      "outputs": [],
      "source": [
        "# Make sure to run the snippet in `Save a TF1 checkpoint in TF2`.\n",
        "print_checkpoint('tf1-ckpt-saved-in-eager')\n",
        "converted_path = convert_tf1_to_tf2('tf1-ckpt-saved-in-eager', \n",
        "                                     'converted-tf1-to-tf2')\n",
        "print(\"\\n[Converted]\")\n",
        "print_checkpoint(converted_path)\n",
        "\n",
        "# Try loading the converted checkpoint.\n",
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "c = tf.Variable(0.)\n",
        "ckpt = tf.train.Checkpoint(vars={'a': a, 'b': b, 'scoped/c': c})\n",
        "ckpt.restore(converted_path).assert_consumed()\n",
        "print(\"\\nRestored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fokg6ybZvE20"
      },
      "source": [
        "### Convertir el punto de verificación TF2 a TF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPQsXQveuQiC"
      },
      "outputs": [],
      "source": [
        "def convert_tf2_to_tf1(checkpoint_path, output_prefix):\n",
        "  \"\"\"Converts a TF2 checkpoint to TF1.\n",
        "\n",
        "  The checkpoint must be saved using a \n",
        "  `tf.train.Checkpoint(var_list={name: variable})`\n",
        "\n",
        "  To load the converted checkpoint with `tf.compat.v1.Saver`:\n",
        "  ```\n",
        "  saver = tf.compat.v1.train.Saver(var_list={name: variable}) \n",
        "\n",
        "  # An alternative, if the variable names match the keys:\n",
        "  saver = tf.compat.v1.train.Saver(var_list=[variables]) \n",
        "  saver.restore(sess, output_path)\n",
        "  ```\n",
        "  \"\"\"\n",
        "  vars = {}\n",
        "  reader = tf.train.load_checkpoint(checkpoint_path)\n",
        "  dtypes = reader.get_variable_to_dtype_map()\n",
        "  for key in dtypes.keys():\n",
        "    # Get the \"name\" from the \n",
        "    if key.startswith('var_list/'):\n",
        "      var_name = key.split('/')[1]\n",
        "      # TF2 checkpoint keys use '/', so if they appear in the user-defined name,\n",
        "      # they are escaped to '.S'.\n",
        "      var_name = var_name.replace('.S', '/')\n",
        "      vars[var_name] = tf.Variable(reader.get_tensor(key))\n",
        "  \n",
        "  return tf1.train.Saver(var_list=vars).save(sess=None, save_path=output_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjZD_OSf1mKX"
      },
      "source": [
        "Convierta el punto de verificación guardado en el recorte `Save a TF2 checkpoint in TF1`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc1MVeV6z2DB"
      },
      "outputs": [],
      "source": [
        "# Make sure to run the snippet in `Save a TF2 checkpoint in TF1`.\n",
        "print_checkpoint('tf2-ckpt-saved-in-session-1')\n",
        "converted_path = convert_tf2_to_tf1('tf2-ckpt-saved-in-session-1',\n",
        "                                    'converted-tf2-to-tf1')\n",
        "print(\"\\n[Converted]\")\n",
        "print_checkpoint(converted_path)\n",
        "\n",
        "# Try loading the converted checkpoint.\n",
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(0))\n",
        "  with tf1.Session() as sess:\n",
        "    saver = tf1.train.Saver([a, b, c])\n",
        "    saver.restore(sess, converted_path)\n",
        "    print(\"\\nRestored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBMfArLQ0jb-"
      },
      "source": [
        "## Guías relacionadas\n",
        "\n",
        "- [Validar la equivalencia numérica y la corrección](./validate_correctness.ipynb)\n",
        "- [Guía para mapear modelos](./model_mapping.ipynb) y `tf.compat.v1.keras.utils.track_tf1_style_variables`\n",
        "- [Guía de Checkpoint de TF2](https://www.tensorflow.org/guide/checkpoint)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "migrating_checkpoints.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
