{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljvLya59ep5"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcQIa1uG86Wh"
      },
      "source": [
        "# Conceptos de DTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dWNQEum9AfY"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/dtensor_overview\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/dtensor_overview.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/dtensor_overview.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/dtensor_overview.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGZuakHVlVQf"
      },
      "source": [
        "## Descripción general\n",
        "\n",
        "En este colaboratorio se presenta DTensor, una extensión de TensorFlow para cálculos distribuidos sincrónicos.\n",
        "\n",
        "DTensor ofrece un modelo de programación global que les permite a los programadores componer aplicaciones que operan globalmente con Tensores, mientras que, a la vez, permiten gestionar la distribución entre distintos dispositivos a nivel interno. DTensor distribuye el programa y los tensores según las directivas del particionamiento horizontal (sharding) a través de un procedimiento llamado *expansión [Un programa, múltiples datos (SPMD)](https://en.wikipedia.org/wiki/SPMD)*.\n",
        "\n",
        "Al desacoplar la aplicación de las directivas de particionamiento horizontal (sharding), DTensor permite la ejecución de la misma aplicación en un solo dispositivo, en varios o, incluso, en múltiples clientes, sin dejar de preservar la semántica global.\n",
        "\n",
        "En esta guía se presentan los conceptos de DTensor para cálculos distribuidos y se muestra cómo DTensor se integra con TensorFlow. Para acceder a una demostración en la que se usa DTensor en entrenamiento de modelos, consulte el tutorial sobre [entrenamiento distribuido con DTensor](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ZTDq7KngwA"
      },
      "source": [
        "## Preparación\n",
        "\n",
        "DTensor es parte de la versión de lanzamiento \"TensorFlow 2.9.0\", que también se ha incluido en las construcciones nocturnas de TensorFlow desde el 09/04/2022."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKaPw8vwwZAC"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade --pre tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3pG29uZIWYO"
      },
      "source": [
        "Una vez instalado, importe `tensorflow` y `tf.experimental.dtensor`. Después, configure TensorFlow para usar 6 CPU virtuales.\n",
        "\n",
        "Aunque este ejemplo usa CPUs, DTensor funciona igual en dispositivos con CPU, GPU o TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q92lo0zjwej8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.experimental import dtensor\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "def configure_virtual_cpus(ncpu):\n",
        "  phy_devices = tf.config.list_physical_devices('CPU')\n",
        "  tf.config.set_logical_device_configuration(phy_devices[0], [\n",
        "        tf.config.LogicalDeviceConfiguration(),\n",
        "    ] * ncpu)\n",
        "\n",
        "configure_virtual_cpus(6)\n",
        "DEVICES = [f'CPU:{i}' for i in range(6)]\n",
        "\n",
        "tf.config.list_logical_devices('CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-lsrxUnlsCC"
      },
      "source": [
        "## El modelo de tensores distribuidos de DTensor\n",
        "\n",
        "DTensor presenta dos conceptos: `dtensor.Mesh` y `dtensor.Layout`. Son abstracciones del modelo de particionamiento horizontal (sharding) de tensores en dispositivos topológicamente relacionados.\n",
        "\n",
        "- `Mesh` define la lista de dispositivos para calcular.\n",
        "- `Layout` define cómo particionar horizontalmente la dimensión del tensor en una `Mesh`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjiHaH0ql9yo"
      },
      "source": [
        "### Malla (mesh)\n",
        "\n",
        "`Mesh` representa a una topología cartesiana lógica de un conjunto de dispositivos. A cada dimensión de la cuadrícula cartesiana se la denomina **dimensión de malla** y nos referimos a ella por un nombre. Los nombres de una dimensión de malla dentro de la misma `Mesh` deben ser únicos.\n",
        "\n",
        "Los nombres de medidas mesh son referenciados por `Layout` para describir el comportamiento del particionamiento horizontal (sharding) de un `tf.Tensor` a lo largo de cada uno de sus ejes. Esto se describe más detalladamente, más adelante, en la sección sobre `Layout`.\n",
        "\n",
        "`Mesh` se puede pensar como un arreglo multidimensional de dispositivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J6cOieEbaUw"
      },
      "source": [
        "En una `Mesh` monodimensional, todos los dispositivos forman una lista que se encuentran en una sola dimensión de malla. En el siguiente ejemplo se usa `dtensor.create_mesh` para crear una malla de 6 dispositivos con CPU a lo largo de una dimensión de malla `'x'` con el tamaño de 6 dispositivos:\n",
        "\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_mesh_1d.png\" class=\"no-filter\" alt=\"Una malla monodimensional con 6 CPU\"> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLH5fgdBmA58"
      },
      "outputs": [],
      "source": [
        "mesh_1d = dtensor.create_mesh([('x', 6)], devices=DEVICES)\n",
        "print(mesh_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSZwaUwnEgXB"
      },
      "source": [
        "Una `Mesh` también puede ser multidimensional. En el siguiente ejemplo, 6 dispositivos CPU forman una malla de `3x2`, donde la dimensión `'x'` de la malla tiene el tamaño de 3 dispositivos, y la dimensión `'y'` de la malla, el tamaño de 2 dispositivos:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_mesh_2d.png\" alt=\"A 2 dimensional mesh with 6 CPUs\" class=\"\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op6TmKUQE-sZ"
      },
      "outputs": [],
      "source": [
        "mesh_2d = dtensor.create_mesh([('x', 3), ('y', 2)], devices=DEVICES)\n",
        "print(mesh_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deAqdrDPFn2f"
      },
      "source": [
        "### Diseño (layout)\n",
        "\n",
        "El **`Layout`** especifica cómo se distribuye o particiona horizontalmente un tensor en una `Mesh`.\n",
        "\n",
        "Nota: Para evitar confusiones entre `Mesh` y `Layout`, en esta guía, el término *dimensión* siempre se asocia con `Mesh`, y el término *eje* con `Tensor` y `Layout`.\n",
        "\n",
        "El rango u orden de `Layout` debería ser igual al del `Tensor`, donde se aplique el `Layout`. Para cada eje del `Tensor`, el `Layout` puede especificar una dimensión de malla para particionar horizontalmente el tensor, o especificar el eje como \"no particionado\". El tensor se replica en cualquiera de las dimensiones de malla que no esté horizontalmente particionada.\n",
        "\n",
        "El rango de un `Layout` y la cantidad de dimensiones de una `Mesh` no necesariamente deben coincidir. Los ejes `unsharded` de un `Layout` no necesitan estar asociados a una dimensión de malla y tampoco es necesario que las dimensiones de malla `unsharded` estén asociadas con un eje de `layout`.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_components_diag.png\" alt=\"Diagram of dtensor components.\" class=\"\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px_bF1c-bQ7e"
      },
      "source": [
        "Analicemos algunos ejemplos de `Layout` para lo creado para la `Mesh` en la sección anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqzCNlWAbm-c"
      },
      "source": [
        "En una malla monodimensional como `[(\"x\", 6)]` (`mesh_1d` en la sección anterior), `Layout([\"unsharded\", \"unsharded\"], mesh_1d)` es un diseño para un tensor de orden 2 replicado en 6 dispositivos. <img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_replicated.png\" class=\"no-filter\" alt=\"Un tensor replicado en una malla de orden 1\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a3EnmZag6x1"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywRJwuLDt2Qq"
      },
      "source": [
        "Con el mismo tensor y la misma malla el diseño `Layout(['unsharded', 'x'])` particionaría horizontalmente el segundo eje del tensor en los 6 dispositivos.\n",
        "\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_rank1.png\" class=\"no-filter\" alt=\"Un tensor horizontalmente particionado en una malla de orden 1\"> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BgqL0jUvV5a"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgciDNmK76l9"
      },
      "source": [
        "Dada una malla de 3x2 bidimensional como `[(\"x\", 3), (\"y\", 2)]`, (`mesh_2d` de la sección anterior), `Layout([\"y\", \"x\"], mesh_2d)` es un diseño para un `Tensor` de orden 2 cuyo primer eje se particiona horizontalmente en la dimensión `\"y\"` de la malla y cuyo segundo eje se particiona horizontalmente en la dimensión `\"x\"` de la malla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyp_qOSyvieo"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_rank2.png\" class=\"no-filter\" alt=\"Un tensor con su primer eje particionado horizontalmente en la dimensión 'y' de la malla y con su segundo eje particionado horizontalmente en la dimensión 'x' de la malla\"> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8OrehEuhPbS"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout(['y', 'x'], mesh_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kyg0V3ehMNJ"
      },
      "source": [
        "Para la misma `mesh_2d`, el diseño `Layout([\"x\", dtensor.UNSHARDED], mesh_2d)` es un diseño para un `Tensor` de segundo orden que se replica por `\"y\"` y cuyo primer eje se particiona horizontalmente en la dimensión `x` de la malla.\n",
        "\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_hybrid.png\" class=\"no-filter\" alt=\"Un tensor replicado en la dimensión 'y' de la malla, con su primer eje particionado horizontalmente por la dimensión 'x' de la malla\"> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkWe6mVl7uRb"
      },
      "outputs": [],
      "source": [
        "layout = dtensor.Layout([\"x\", dtensor.UNSHARDED], mesh_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTalu6M-ISYb"
      },
      "source": [
        "### Aplicaciones monocliente y multicliente\n",
        "\n",
        "DTensor es compatible tanto con aplicaciones monocliente como multicliente. El núcleo (kernel) para Python en Colab es un ejemplo de una aplicación DTensor monocliente, donde hay un solo proceso de Python.\n",
        "\n",
        "En una aplicación DTensor multicliente, múltiples procesos Python funcionan colectivamente como una aplicación coherente. La cuadrícula cartesiana de una `Mesh` en una aplicación DTensor multicliente se puede extender por los dispositivos, independientemente de si están vinculados localmente al cliente actual o si lo están remotamente a otro cliente. El conjunto de todos los dispositivos usados por una `Mesh` se denomina *lista de dispositivos globales*.\n",
        "\n",
        "La creación de una `Mesh` en una aplicación DTensor multicliente es una operación colectiva en la que la *lista de dispositivos globales* es idéntica para todos los clientes involucrados y la creación de la `Mesh` sirve como barrera global.\n",
        "\n",
        "Durante la creación de la `Mesh`, cada cliente proporciona su *lista de dispositivos locales* junto con la *lista de dispositivos globales* esperada. DTensor valida que ambas listas sean consistentes. Para más información sobre la creación de mallas multicliente y sobre la *lista de dispositivos globales*, consulte la documentación de la API para `dtensor.create_mesh` y `dtensor.create_distributed_mesh`.\n",
        "\n",
        "La opción monocliente se puede pensar como un caso especial de multicliente, pero con un cliente. En una aplicación monocliente la *lista de dispositivos globales* es idéntica a la *lista de dispositivos locales*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_F7DWkXkB4w"
      },
      "source": [
        "## DTensor como tensor particionado horizontalmente\n",
        "\n",
        "Ahora, empecemos a codificar con `DTensor`. La función ayudante, `dtensor_from_array`, demuestra la creación de tensores d a partir de algo que se ve como un `tf.Tensor`. La función lleva a cabo 2 pasos:\n",
        "\n",
        "- Replica el tensor en cada dispositivo de la malla.\n",
        "- Particiona horizontalmente la copia según el diseño solicitado en sus argumentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6aws-b8dN9L"
      },
      "outputs": [],
      "source": [
        "def dtensor_from_array(arr, layout, shape=None, dtype=None):\n",
        "  \"\"\"Convert a DTensor from something that looks like an array or Tensor.\n",
        "\n",
        "  This function is convenient for quick doodling DTensors from a known,\n",
        "  unsharded data object in a single-client environment. This is not the\n",
        "  most efficient way of creating a DTensor, but it will do for this\n",
        "  tutorial.\n",
        "  \"\"\"\n",
        "  if shape is not None or dtype is not None:\n",
        "    arr = tf.constant(arr, shape=shape, dtype=dtype)\n",
        "\n",
        "  # replicate the input to the mesh\n",
        "  a = dtensor.copy_to_mesh(arr,\n",
        "          layout=dtensor.Layout.replicated(layout.mesh, rank=layout.rank))\n",
        "  # shard the copy to the desirable layout\n",
        "  return dtensor.relayout(a, layout=layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3o6IysrlGMu"
      },
      "source": [
        "### Anatomía de un DTensor\n",
        "\n",
        "Un DTensor es un objeto `tf.Tensor`, pero aumentado con la anotación `Layout` que define su comportamiento para el particionamiento horizontal. Un DTensor está compuesto por lo siguiente:\n",
        "\n",
        "- Metadatos de tensor global, incluida la forma global y el tipo d de tensor.\n",
        "- Un `Layout`, que define la `Mesh` a la que pertenece el `Tensor` y cómo el `Tensor` se particiona horizontalmente en la `Mesh`.\n",
        "- Una lista de **tensores componentes**, un elemento por dispositivo local en la `Mesh`.\n",
        "\n",
        "Con `dtensor_from_array`, puede crear su primer DTensor, `my_first_dtensor` y examinar su contenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQu_nScGUvYH"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
        "layout = dtensor.Layout([dtensor.UNSHARDED], mesh)\n",
        "\n",
        "my_first_dtensor = dtensor_from_array([0, 1], layout)\n",
        "\n",
        "# Examine the dtensor content\n",
        "print(my_first_dtensor)\n",
        "print(\"global shape:\", my_first_dtensor.shape)\n",
        "print(\"dtype:\", my_first_dtensor.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8LQy1nqmvFy"
      },
      "source": [
        "#### Diseño (layout) y `fetch_layout`\n",
        "\n",
        "El diseño de un DTensor no es un atributo regular de `tf.Tensor`. Sino que, DTensor brinda una función, `dtensor.fetch_layout` para acceder al diseño de un DTensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCSFyaAjmzGu"
      },
      "outputs": [],
      "source": [
        "print(dtensor.fetch_layout(my_first_dtensor))\n",
        "assert layout == dtensor.fetch_layout(my_first_dtensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7i3l2lmatm"
      },
      "source": [
        "#### Tensores componentes, `pack` y `unpack`\n",
        "\n",
        "Un DTensor está compuesto por una lista de **tensores componentes**. El tensor componente de un dispositivo en la `Mesh` es el objeto `Tensor` que representa a la parte del DTensor global que se almacena en este dispositivo.\n",
        "\n",
        "Un DTensor se puede separar en tensores componentes con `dtensor.unpack`. Y `dtensor.unpack` también se puede usar para inspeccionar los componentes del DTensor y confirmar que están en todos los dispositivos de la `Mesh`.\n",
        "\n",
        "Tenga en cuenta que las posiciones de los tensores componentes pueden superponerse en la vista global. Por ejemplo, en el caso de un diseño totalmente replicado, todos los componentes serán réplicas idénticas del tensor global."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGbjqVAOnXMk"
      },
      "outputs": [],
      "source": [
        "for component_tensor in dtensor.unpack(my_first_dtensor):\n",
        "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tqIQM52k788"
      },
      "source": [
        "Tal como se muestra, `my_first_dtensor` es un tensor de `[0, 1]` replicado para los 6 dispositivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6By3k-CGn3yv"
      },
      "source": [
        "La operación inversa de `dtensor.unpack` es `dtensor.pack`. Los tensores componentes se pueden volver a agrupar en un DTensor.\n",
        "\n",
        "Los componentes deben tener el mismo orden y tipo d, que será el orden y tipo d del DTensor. Sin embargo, no hay ningún requerimiento estricto para la colocación de tensores componentes en dispositivos como entradas de `dtensor.unpack`: la función copiará automáticamente los tensores componentes en sus correspondientes y respectivos dispositivos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lT-6qQwxOgf"
      },
      "outputs": [],
      "source": [
        "packed_dtensor = dtensor.pack(\n",
        "    [[0, 1], [0, 1], [0, 1],\n",
        "     [0, 1], [0, 1], [0, 1]],\n",
        "     layout=layout\n",
        ")\n",
        "print(packed_dtensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvS3autrpK2U"
      },
      "source": [
        "### Particionamiento horizontal (sharding) de un DTensor para una malla\n",
        "\n",
        "Hasta aquí, hemos trabajado con `my_first_dtensor`, que es un DTensor de orden 1 totalmente replicado en una `Mesh` monodimensional.\n",
        "\n",
        "A continuación, cree e inspeccione tensores d que estén horizontalmente particionados en una `Mesh` bidimensional. En el siguiente ejemplo, esto se logra con una `Mesh` de 3x2 en 6 dispositivos CPU, donde el tamaño de la dimensión `'x'` de la malla es de 3 dispositivos y el tamaño de la dimensión `'y'` de la malla es de 2 dispositivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWb9Ae0VJ-Rc"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndSeQSFWKQk9"
      },
      "source": [
        "#### Tensor de orden 2 particionado horizontalmente por completo en una malla bidimensional\n",
        "\n",
        "Cree un DTensor de orden 2, de 3x2, particionando horizontalmente su primer eje por la dimensión `'x'` de la malla y su segundo eje por la dimensión `'y'` de la malla.\n",
        "\n",
        "- Debido a que el tamaño del tensor es igual a la dimensión de la malla por todos los ejes particionados horizontalmente, cada dispositivo recibe un único elemento del DTensor.\n",
        "- El orden del tensor componente es siempre igual al del tamaño global. El DTensor adopta esta convención como una manera simple de preservar la información para localizar la relación entre un tensor componente y el DTensor global."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax_ZHouJp1MX"
      },
      "outputs": [],
      "source": [
        "fully_sharded_dtensor = dtensor_from_array(\n",
        "    tf.reshape(tf.range(6), (3, 2)),\n",
        "    layout=dtensor.Layout([\"x\", \"y\"], mesh))\n",
        "\n",
        "for raw_component in dtensor.unpack(fully_sharded_dtensor):\n",
        "  print(\"Device:\", raw_component.device, \",\", raw_component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhsLC-NgrC2p"
      },
      "source": [
        "#### Tensor de orden 2 replicado por completo en una malla bidimensional\n",
        "\n",
        "A modo de comparación, cree un DTensor de orden 2 de 3x2 totalmente replicado en la misma malla bidimensional.\n",
        "\n",
        "- Dado que el DTensor está totalmente replicado, cada dispositivo recibe una réplica completa del DTensor de 3x2.\n",
        "- Los órdenes de los tensores componentes son iguales a los de la forma global. De todos modos, este hecho es trivial, porque en este caso la forma de los tensores componentes es igual a la forma global."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmyC6H6Ec90P"
      },
      "outputs": [],
      "source": [
        "fully_replicated_dtensor = dtensor_from_array(\n",
        "    tf.reshape(tf.range(6), (3, 2)),\n",
        "    layout=dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh))\n",
        "# Or, layout=tensor.Layout.fully_replicated(mesh, rank=2)\n",
        "\n",
        "for component_tensor in dtensor.unpack(fully_replicated_dtensor):\n",
        "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWoyv_oHMzk1"
      },
      "source": [
        "#### Tensor de orden 2 híbrido en una malla bidimensional\n",
        "\n",
        "Y qué sucede cuando queda en medio de completamente particionado y completamente replicado.\n",
        "\n",
        "El DTensor permite que un `Layout` sea híbrido, particionado horizontalmente a lo largo de alguno de los ejes, pero replicado en otros.\n",
        "\n",
        "Por ejemplo, es posible particionar horizontalmente el mismo DTensor de orden 2 de 3x2 de la siguiente manera:\n",
        "\n",
        "- El primer eje, a lo largo de la dimensión `'x'` de la malla.\n",
        "- El segundo eje, a lo largo de la dimensión `'y'` de la malla.\n",
        "\n",
        "Para lograr este esquema de particionamiento horizontal, simplemente debe reemplazar las especificaciones de particionamiento horizontal del segundo eje de `'y'` a `dtensor.UNSHARDED`, para indicar la intención de replicar a lo largo del segundo eje. El objeto de diseño se verá similar a este: `Layout(['x', dtensor.UNSHARDED], mesh)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DygnbkQ1Lu42"
      },
      "outputs": [],
      "source": [
        "hybrid_sharded_dtensor = dtensor_from_array(\n",
        "    tf.reshape(tf.range(6), (3, 2)),\n",
        "    layout=dtensor.Layout(['x', dtensor.UNSHARDED], mesh))\n",
        "\n",
        "for component_tensor in dtensor.unpack(hybrid_sharded_dtensor):\n",
        "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7FtZ9kQRZgE"
      },
      "source": [
        "Puede inspeccionar los tensores componentes del DTensor creado y verificar que realmente están horizontalmente particionados según el esquema deseado. Puede resultar útil ilustrar la situación con un gráfico:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_hybrid_mesh.png\" alt=\"A 3x2 hybrid mesh with 6 CPUs\" class=\"\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auAkA38XjL-q"
      },
      "source": [
        "#### Tensor.numpy() y tensor D particionado horizontalmente\n",
        "\n",
        "Tenga en cuenta que al llamar al método `.numpy()` en un DTensor particionado horizontalmente, surge un error. La base racional del error es que este se genera como protección contra el agrupamiento no intencionado de datos de múltiples dispositivos de cálculo a la CPU host que sirve de respaldo del arreglo numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNdwmnL0jAXS"
      },
      "outputs": [],
      "source": [
        "print(fully_replicated_dtensor.numpy())\n",
        "\n",
        "try:\n",
        "  fully_sharded_dtensor.numpy()\n",
        "except tf.errors.UnimplementedError:\n",
        "  print(\"got an error as expected for fully_sharded_dtensor\")\n",
        "\n",
        "try:\n",
        "  hybrid_sharded_dtensor.numpy()\n",
        "except tf.errors.UnimplementedError:\n",
        "  print(\"got an error as expected for hybrid_sharded_dtensor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WcMkiagPF_6"
      },
      "source": [
        "## API TensorFlow en DTensor\n",
        "\n",
        "Los tensores d (DTensor) aspiran a convertirse en el reemplazo de los tensores actuales de los programas. Las API Python TensorFlow que consumen `tf.Tensor`, como las funciones de librería de Ops (operaciones), `tf.function`, `tf.GradientTape`, también funcionan con DTensor.\n",
        "\n",
        "A fin de lograrlo, para cada [gráfico de TensorFlow](https://www.tensorflow.org/guide/intro_to_graphs), DTensor produce y ejecuta un gráfico [SPMD](https://en.wikipedia.org/wiki/SPMD) equivalente en un procedimiento llamado *expansión SPMD*. Algunos pasos críticos de la expansión SPMD de DTensor son:\n",
        "\n",
        "- Propagar el `Layout` del particionamiento horizontal del DTensor en el gráfico de TensorFlow\n",
        "- Reescribir operaciones de TensorFlow en el DTensor global con operaciones de TensorFlow equivalentes en los tensores componentes, insertando operaciones colectivas y de comunicaciones siempre que sea necesario.\n",
        "- Bajar las operaciones de TensorFlow neutrales de backend a operaciones de TensorFlow específicas de backend.\n",
        "\n",
        "El resultado final es que **DTensor es un reemplazo de Tensor**.\n",
        "\n",
        "Nota: DTensor todavía es una API experimental, lo que significa que las fronteras y los límites del modelo de programación DTensor se seguirán explorando y corriendo.\n",
        "\n",
        "Hay 2 formas de disparar la ejecución de DTensor:\n",
        "\n",
        "- Los DTensor como operandos de una función Python, p. ej., `tf.matmul(a, b)` se ejecutará a través de DTensor si `a`, `b` o ambos son DTensors.\n",
        "- Mediante la solicitud del resultado de una función Python para ser un tensor d; p. ej., `dtensor.call_with_layout(tf.ones, layout, shape=(3, 2))` correrá a través de DTensor porque solicitamos que la salida de tf.ones se particione horizontalmente según un `layout`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urKzmqAoPssT"
      },
      "source": [
        "### DTensor como operandos\n",
        "\n",
        "Muchas funciones de la API TensorFlow toman `tf.Tensor` como su operando y devuelven `tf.Tensor` como sus resultados. Para estas funciones, se puede expresar la intención de ejecutar una función a través de DTensor pasando los DTensor como operandos. En esta sección se usa `tf.matmul(a, b)` como ejemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LO8ZT7iWVga"
      },
      "source": [
        "#### Entrada y salida totalmente replicada\n",
        "\n",
        "En este caso, los tensores d se replican por completo. En cada uno de los dispositivos de la `Mesh`,\n",
        "\n",
        "- el componente tensor para el operando `a` es `[[1, 2, 3], [4, 5, 6]]` (2x3)\n",
        "- el componente tensor para el operando `b` es `[[6, 5], [4, 3], [2, 1]]` (3x2)\n",
        "- el cálculo consiste de un solo `MatMul` de `(2x3, 3x2) -> 2x2`,\n",
        "- el componente tensor para el resultado `c` es `[[20, 14], [56,41]]` (2x2)\n",
        "\n",
        "El número total de operaciones mul de punto flotante es `6 device * 4 result * 3 mul = 72`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiZf2J9JNd2D"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
        "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)\n",
        "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=layout)\n",
        "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=layout)\n",
        "\n",
        "c = tf.matmul(a, b) # runs 6 identical matmuls in parallel on 6 devices\n",
        "\n",
        "# `c` is a DTensor replicated on all devices (same as `a` and `b`)\n",
        "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)\n",
        "print(\"components:\")\n",
        "for component_tensor in dtensor.unpack(c):\n",
        "  print(component_tensor.device, component_tensor.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXtR9qgKWgWV"
      },
      "source": [
        "#### Particionamiento horizontal (sharding) de operandos a lo largo del eje contraído\n",
        "\n",
        "Se puede reducir la cantidad de cálculos por dispositivos mediante el particionamiento horizontal de los operandos `a` y `b`. Un esquema popular de particionamiento horizontal para `tf.matmul` consiste en particionar los operandos a lo largo del eje de contracción, lo que significa particionar horizontalmente `a` a lo largo del segundo eje y `b` a lo largo del primero.\n",
        "\n",
        "El producto de la matriz global particionado horizontalmente bajo este esquema se puede llevar a cabo con eficiencia mediante matmul local que funciona concurrentemente, seguido de una reducción colectiva para agregar los resultados locales. Este es también el [modo canónico](https://github.com/open-mpi/ompi/blob/ee87ec391f48512d3718fc7c8b13596403a09056/docs/man-openmpi/man3/MPI_Reduce.3.rst?plain=1#L265) de implementar un <em>producto punto</em> de matriz distribuida.\n",
        "\n",
        "El número total de operaciones mul de punto flotante es `6 devices * 4 result * 1 = 24`, un factor de reducción 3 comparado con el caso replicado por completo (72) anterior. El factor de 3 se debe al particionamiento horizontal a lo largo de la dimensión `x` de la malla con un tamaño de `3` dispositivos.\n",
        "\n",
        "La reducción del número de operaciones que funciona secuencialmente es el principal mecanismo con el que el paralelismo del modelo sincrónico acelera el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyVAUvMePbms"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "a_layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh)\n",
        "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=a_layout)\n",
        "b_layout = dtensor.Layout(['x', dtensor.UNSHARDED], mesh)\n",
        "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=b_layout)\n",
        "\n",
        "c = tf.matmul(a, b)\n",
        "# `c` is a DTensor replicated on all devices (same as `a` and `b`)\n",
        "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhD8yYgJiCEh"
      },
      "source": [
        "#### Particionamiento horizontal adicional\n",
        "\n",
        "Se puede realizar un particionamiento horizontal adicional en las entradas y se trasladan apropiadamente a los resultados. Por ejemplo, se puede aplicar un particionamiento horizontal adicional del operando `a` a lo largo de su primer eje a la dimensión `'y'` de la malla. El particionamiento horizontal adicional se trasladará al primer eje del resultado `c`.\n",
        "\n",
        "El número total de operaciones mul de punto flotante es `6 devices * 2 result * 1 = 12`, un factor adicional de reducción 2 comparado con el caso (24) anterior. El factor de 2 se debe al particionamiento horizontal a lo largo de la dimensión `y` de la malla con un tamaño de `2` dispositivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PYqe0neiOpR"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "\n",
        "a_layout = dtensor.Layout(['y', 'x'], mesh)\n",
        "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=a_layout)\n",
        "b_layout = dtensor.Layout(['x', dtensor.UNSHARDED], mesh)\n",
        "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=b_layout)\n",
        "\n",
        "c = tf.matmul(a, b)\n",
        "# The sharding of `a` on the first axis is carried to `c'\n",
        "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)\n",
        "print(\"components:\")\n",
        "for component_tensor in dtensor.unpack(c):\n",
        "  print(component_tensor.device, component_tensor.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-1NazCVmLWZ"
      },
      "source": [
        "### DTensor como salida\n",
        "\n",
        "Qué sucede con las funciones de Python en las que no se usan operandos pero devuelven un resultado tensor que se puede particionar horizontalmente. Algunos ejemplos de tales funciones son los siguientes:\n",
        "\n",
        "- `tf.ones`, `tf.zeros`, `tf.random.stateless_normal`,\n",
        "\n",
        "Para estas funciones de Python, DTensor proporciona `dtensor.call_with_layout` que ejecuta, mediante ejecución <em>eager</em>, una función de Python con DTensor y garantiza que el tensor devuelto sea un DTensor con el `Layout` solicitado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0jo_8NPtJiO"
      },
      "outputs": [],
      "source": [
        "help(dtensor.call_with_layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-YdLvfytM7g"
      },
      "source": [
        "La función de Python ejecutada con el modo <em>eager</em>, por lo general, solamente contiene una operación de TensorFlow no trivial.\n",
        "\n",
        "Para usar una función de Python que emite múltiples operaciones de TensorFlow con `dtensor.call_with_layout`, la función debería convertirse a una `tf.function`. Llamar a una `tf.function` es una sola operación de TensorFlow. Cuando se llama a la función `tf.function`, DTensor puede realizar la propagación del diseño cuando analiza el gráfico de cálculos de la `tf.function`, antes de que se materialice cualquier tensor intermedio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLrksgFjqRLS"
      },
      "source": [
        "#### API que emiten una sola operación de TensorFlow\n",
        "\n",
        "Si una función emite una sola operación de TensorFlow, se puede aplicar directamente `dtensor.call_with_layout` a la función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1CuKYSFtFeM"
      },
      "outputs": [],
      "source": [
        "help(tf.ones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m_EAwy-ozOh"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "ones = dtensor.call_with_layout(tf.ones, dtensor.Layout(['x', 'y'], mesh), shape=(6, 4))\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx-7Xo8Cpb8S"
      },
      "source": [
        "#### API que emiten múltiples operaciones de TensorFlow\n",
        "\n",
        "Si la API emite múltiples operaciones de TensorFlow, convierta la función en una sola operación con `tf.function`. Por ejemplo, `tf.random.stateleess_normal`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8BQSTRFtCih"
      },
      "outputs": [],
      "source": [
        "help(tf.random.stateless_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvP81eYopSPm"
      },
      "outputs": [],
      "source": [
        "ones = dtensor.call_with_layout(\n",
        "    tf.function(tf.random.stateless_normal),\n",
        "    dtensor.Layout(['x', 'y'], mesh),\n",
        "    shape=(6, 4),\n",
        "    seed=(1, 1))\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKoojp9ZyWzW"
      },
      "source": [
        "Se puede encapsular (wrapear) una función de Python que emita una sola operación de TensorFlow con `tf.function`. El único inconveniente es el de pagar el costo asociado y la complejidad que ofrece crear una `tf.function` de una función de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbAtKrSkpOaq"
      },
      "outputs": [],
      "source": [
        "ones = dtensor.call_with_layout(\n",
        "    tf.function(tf.ones),\n",
        "    dtensor.Layout(['x', 'y'], mesh),\n",
        "    shape=(6, 4))\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-m1816JP3CE"
      },
      "source": [
        "### De `tf.Variable` a `dtensor.DVariable`\n",
        "\n",
        "En Tensorflow, `tf.Variable` es el portador para un valor de `Tensor` mutable. Con DTensor, la semántica variable correspondiente es provista por `dtensor.DVariable`.\n",
        "\n",
        "La razón por la que se introdujo un tipo nuevo de `DVariable` para la variable DTensor es que las DVariables tienen un requerimiento adicional, que el diseño no pueda cambiar su valor inicial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awRPuR26P0Sc"
      },
      "outputs": [],
      "source": [
        "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
        "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)\n",
        "\n",
        "v = dtensor.DVariable(\n",
        "    initial_value=dtensor.call_with_layout(\n",
        "        tf.function(tf.random.stateless_normal),\n",
        "        layout=layout,\n",
        "        shape=tf.TensorShape([64, 32]),\n",
        "        seed=[1, 1],\n",
        "        dtype=tf.float32))\n",
        "\n",
        "print(v.handle)\n",
        "assert layout == dtensor.fetch_layout(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb9jn473prC_"
      },
      "source": [
        "Aparte del requerimiento sobre la correspondencia con el `layout`, una `DVariable` se comporta de la misma manera que una `tf.Variable`. Por ejemplo, una DVariable se puede agregar a un DTensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adxFw9wJpqQQ"
      },
      "outputs": [],
      "source": [
        "a = dtensor.call_with_layout(tf.ones, layout=layout, shape=(64, 32))\n",
        "b = v + a # add DVariable and DTensor\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxBdNHWSu-kV"
      },
      "source": [
        "También se puede asignar un DTensor a una DVariable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYwfiyw5P94U"
      },
      "outputs": [],
      "source": [
        "v.assign(a) # assign a DTensor to a DVariable\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fvSk_VUvGnj"
      },
      "source": [
        "Si se intenta mutar el diseño de una `DVariable` mediante la asignación de un DTensor con un diseño incompatible, se produce un error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pckUugYP_r-"
      },
      "outputs": [],
      "source": [
        "# variable's layout is immutable.\n",
        "another_mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
        "b = dtensor.call_with_layout(tf.ones,\n",
        "                     layout=dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], another_mesh),\n",
        "                     shape=(64, 32))\n",
        "try:\n",
        "  v.assign(b)\n",
        "except:\n",
        "  print(\"exception raised\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LadIcwRvR6f"
      },
      "source": [
        "## ¿Qué sigue?\n",
        "\n",
        "En este colaboratorio, aprendió sobre DTensor, una extensión de TensorFlow para cálculos distribuidos. Para probar estos conceptos en un tutorial, consulte [Entrenamiento distribuido con DTensor](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "dtensor_overview.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
