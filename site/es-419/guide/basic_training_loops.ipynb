{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rmpybwysXGV"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m8y3rGtQsYP2"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrXv0rU9sIma"
      },
      "source": [
        "# Bucles de entrenamiento básico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S0BwJ_8sLu7"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/basic_training_loops\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/basic_training_loops.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/basic_training_loops.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver código fuente en GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/basic_training_loops.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2o3TTG4TFpt"
      },
      "source": [
        "En los guías anteriores, ha aprendido sobre [tensores](./tensor.ipynb), [variables](./variable.ipynb), [cinta de gradiente](autodiff.ipynb), y [módulos](./intro_to_modules.ipynb).  En esta guía, los combinará todos para entrenar modelos.\n",
        "\n",
        "TensorFlow también incluye la [API tf.Keras](https://www.tensorflow.org/guide/keras/overview), una API de redes neuronales de alto nivel que ofrece abstracciones útiles para reducir el lenguaje repetitivo. Sin embargo, en esta guía, usted usará clases básicas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LXMVuV0VhDr"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiolgWMPgpwI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKD__8kFCKNt"
      },
      "source": [
        "## Cómo resolver problemas de aprendizaje automático\n",
        "\n",
        "Para resolver un problema de aprendizaje automático se suelen seguir los siguientes pasos:\n",
        "\n",
        "- Obtener datos de entrenamiento.\n",
        "- Definir el modelo.\n",
        "- Definir una función de pérdida.\n",
        "- Recorrer los datos del entrenamiento, calculando la pérdida con respecto al valor ideal\n",
        "- Calcular gradientes para esa pérdida y usar un *optimizador* para ajustar las variables a los datos.\n",
        "- Evaluar sus resultados.\n",
        "\n",
        "A modo de ilustración, en esta guía desarrollará un modelo lineal sencillo, $f(x) = x * W + b$, que tiene dos variables: $W$ (ponderaciones) y $b$ (sesgo).\n",
        "\n",
        "Este es el más básico de los problemas de aprendizaje automático: Dados $x$ e $y$, intente encontrar la pendiente y el desplazamiento de una línea mediante [regresión lineal simple](https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qutT_fkl_CBc"
      },
      "source": [
        "## Datos\n",
        "\n",
        "El aprendizaje supervisado usa *entradas* (normalmente denotadas como *x*) y *salidas* (denotadas como *y*, a menudo llamadas *etiquetas*). La meta es aprender a partir de entradas y salidas asociadas para poder predecir el valor de una salida a partir de una entrada.\n",
        "\n",
        "Cada entrada de sus datos, en TensorFlow, está casi siempre representada por un tensor, y a menudo es un vector. En el entrenamiento supervisado, la salida (o valor que le gustaría predecir) es también un tensor.\n",
        "\n",
        "He aquí algunos datos sintetizados añadiendo ruido gaussiano (normal) a los puntos a lo largo de una línea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzivK2ATByOz"
      },
      "outputs": [],
      "source": [
        "# The actual line\n",
        "TRUE_W = 3.0\n",
        "TRUE_B = 2.0\n",
        "\n",
        "NUM_EXAMPLES = 201\n",
        "\n",
        "# A vector of random x values\n",
        "x = tf.linspace(-2,2, NUM_EXAMPLES)\n",
        "x = tf.cast(x, tf.float32)\n",
        "\n",
        "def f(x):\n",
        "  return x * TRUE_W + TRUE_B\n",
        "\n",
        "# Generate some noise\n",
        "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "\n",
        "# Calculate y\n",
        "y = f(x) + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlFd_HVBFGIF"
      },
      "outputs": [],
      "source": [
        "# Plot all the data\n",
        "plt.plot(x, y, '.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH95XUzhL99d"
      },
      "source": [
        "Los tensores suelen reunirse en *lotes*, o grupos de entradas y salidas apiladas. El agrupamiento por lotes puede suponer beneficios para el entrenamiento y funciona bien con aceleradores y computación vectorizada. Dado lo pequeño que es este conjunto de datos, puede tratarlo en su totalidad como un único lote."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFzH64Jn9PIm"
      },
      "source": [
        "## Definir el modelo\n",
        "\n",
        "Use `tf.Variable` para representar todas las ponderaciones de un modelo. Una `tf.Variable` almacena un valor y lo muestra en forma de tensor según sea necesario. Consulte la [guía de variables](./variable.ipynb) para saber más.\n",
        "\n",
        "Use `tf.Module` para encapsular las variables y el cálculo. Usted podría usar cualquier objeto de Python, pero de esta manera se puede guardar fácilmente.\n",
        "\n",
        "Aquí se definen *w* y *b* como variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WRu7Pze7wk8"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # Initialize the weights to `5.0` and the bias to `0.0`\n",
        "    # In practice, these should be randomly initialized\n",
        "    self.w = tf.Variable(5.0)\n",
        "    self.b = tf.Variable(0.0)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.w * x + self.b\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "# List the variables tf.modules's built-in variable aggregation.\n",
        "print(\"Variables:\", model.variables)\n",
        "\n",
        "# Verify the model works\n",
        "assert model(3.0).numpy() == 15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdpN_3ssG9D5"
      },
      "source": [
        "Las variables iniciales se configuran aquí de forma fija, pero Keras viene con cualquiera de un número de [inicializadores](https://www.tensorflow.org/api_docs/python/tf/keras/initializers) que usted puede usar, con o sin el resto de Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa6j_yXa-j79"
      },
      "source": [
        "### Definir una función de pérdida\n",
        "\n",
        "Una función de pérdida mide en qué medida la salida de un modelo para una entrada dada coincide con la salida objetivo. La meta es minimizar esta diferencia durante el entrenamiento. Defina la pérdida estándar L2, también conocida como error \"cuadrático medio\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0ysUFGY924U"
      },
      "outputs": [],
      "source": [
        "# This computes a single loss value for an entire batch\n",
        "def loss(target_y, predicted_y):\n",
        "  return tf.reduce_mean(tf.square(target_y - predicted_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-50nq-wPBsAW"
      },
      "source": [
        "Antes de entrenar el modelo, puede visualizar el valor de la pérdida trazando las predicciones del modelo en rojo y los datos de entrenamiento en azul:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eb83LtrB4nt"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, '.', label=\"Data\")\n",
        "plt.plot(x, f(x), label=\"Ground truth\")\n",
        "plt.plot(x, model(x), label=\"Predictions\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Current loss: %1.6f\" % loss(y, model(x)).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSDP-yeq_4jE"
      },
      "source": [
        "### Definir un bucle de entrenamiento\n",
        "\n",
        "El bucle de entrenamiento consiste en realizar repetidamente tres tareas en orden:\n",
        "\n",
        "- Envío de un lote de entradas a través del modelo para generar salidas\n",
        "- Calcular la pérdida comparando las salidas con la salida (o etiqueta)\n",
        "- Usar cinta de gradiente para encontrar los gradientes\n",
        "- Optimizar las variables con esos gradientes\n",
        "\n",
        "Para este ejemplo, puede entrenar el modelo usando [descenso de gradiente](https://en.wikipedia.org/wiki/Gradient_descent).\n",
        "\n",
        "Hay muchas variantes del esquema de descenso de gradiente que se capturan en `tf.keras.optimizers`. Pero con el ánimo de construir desde los fundamentos, aquí implementará usted mismo las matemáticas básicas con la ayuda de `tf.GradientTape` para la diferenciación automática y `tf.assign_sub` para decrementar un valor (que combina `tf.assign` y `tf.sub`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBIACgdnA55X"
      },
      "outputs": [],
      "source": [
        "# Given a callable model, inputs, outputs, and a learning rate...\n",
        "def train(model, x, y, learning_rate):\n",
        "\n",
        "  with tf.GradientTape() as t:\n",
        "    # Trainable variables are automatically tracked by GradientTape\n",
        "    current_loss = loss(y, model(x))\n",
        "\n",
        "  # Use GradientTape to calculate the gradients with respect to W and b\n",
        "  dw, db = t.gradient(current_loss, [model.w, model.b])\n",
        "\n",
        "  # Subtract the gradient scaled by the learning rate\n",
        "  model.w.assign_sub(learning_rate * dw)\n",
        "  model.b.assign_sub(learning_rate * db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwWPaJryD2aN"
      },
      "source": [
        "Para echar un vistazo al entrenamiento, puede enviar el mismo lote de *x* y *y* a través del bucle de entrenamiento, y ver cómo evolucionan `W` y `b`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdfkR223D9dW"
      },
      "outputs": [],
      "source": [
        "model = MyModel()\n",
        "\n",
        "# Collect the history of W-values and b-values to plot later\n",
        "weights = []\n",
        "biases = []\n",
        "epochs = range(10)\n",
        "\n",
        "# Define a training loop\n",
        "def report(model, loss):\n",
        "  return f\"W = {model.w.numpy():1.2f}, b = {model.b.numpy():1.2f}, loss={loss:2.5f}\"\n",
        "\n",
        "\n",
        "def training_loop(model, x, y):\n",
        "\n",
        "  for epoch in epochs:\n",
        "    # Update the model with the single giant batch\n",
        "    train(model, x, y, learning_rate=0.1)\n",
        "\n",
        "    # Track this before I update\n",
        "    weights.append(model.w.numpy())\n",
        "    biases.append(model.b.numpy())\n",
        "    current_loss = loss(y, model(x))\n",
        "\n",
        "    print(f\"Epoch {epoch:2d}:\")\n",
        "    print(\"    \", report(model, current_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dKKLU4KkQEq"
      },
      "source": [
        "Realizar el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRuNUghs1lHY"
      },
      "outputs": [],
      "source": [
        "current_loss = loss(y, model(x))\n",
        "\n",
        "print(f\"Starting:\")\n",
        "print(\"    \", report(model, current_loss))\n",
        "\n",
        "training_loop(model, x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPJgimg8kSA4"
      },
      "source": [
        "Grafique la evolución de las ponderaciones a lo largo del tiempo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND1fQw8sbTNr"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, weights, label='Weights', color=colors[0])\n",
        "plt.plot(epochs, [TRUE_W] * len(epochs), '--',\n",
        "         label = \"True weight\", color=colors[0])\n",
        "\n",
        "plt.plot(epochs, biases, label='bias', color=colors[1])\n",
        "plt.plot(epochs, [TRUE_B] * len(epochs), \"--\",\n",
        "         label=\"True bias\", color=colors[1])\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhlwj1ojkcUP"
      },
      "source": [
        "Visualice el rendimiento del modelo entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpTEjWWex568"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, '.', label=\"Data\")\n",
        "plt.plot(x, f(x), label=\"Ground truth\")\n",
        "plt.plot(x, model(x), label=\"Predictions\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Current loss: %1.6f\" % loss(model(x), y).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DODMMmfLIiOC"
      },
      "source": [
        "## La misma solución, pero con Keras\n",
        "\n",
        "Es útil contrastar el código anterior con el equivalente en Keras.\n",
        "\n",
        "La definición del modelo es exactamente la misma si hace una subclase a `tf.keras.Model`. Recuerde que los modelos Keras heredan del módulo de forma definitiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z86hCI0x1YX3"
      },
      "outputs": [],
      "source": [
        "class MyModelKeras(tf.keras.Model):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # Initialize the weights to `5.0` and the bias to `0.0`\n",
        "    # In practice, these should be randomly initialized\n",
        "    self.w = tf.Variable(5.0)\n",
        "    self.b = tf.Variable(0.0)\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.w * x + self.b\n",
        "\n",
        "keras_model = MyModelKeras()\n",
        "\n",
        "# Reuse the training loop with a Keras model\n",
        "training_loop(keras_model, x, y)\n",
        "\n",
        "# You can also save a checkpoint using Keras's built-in support\n",
        "keras_model.save_weights(\"my_checkpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kw5P4jt2Az8"
      },
      "source": [
        "En lugar de escribir nuevos bucles de entrenamiento cada vez que cree un modelo, puede usar las funciones integradas de Keras como atajo. Puede ser útil cuando no desee escribir o depurar bucles de entrenamiento en Python.\n",
        "\n",
        "Si lo hace, tendrá que usar `model.compile()` para configurar los parámetros, y `model.fit()` para el entrenamiento. Puede ser menos código usar las implementaciones Keras de la pérdida L2 y el descenso de gradiente, también como atajo.  Las pérdidas y los optimizadores de Keras también pueden usarse fuera de estas funciones de conveniencia, y el ejemplo anterior podría haberlos usado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nbLLfPE2pEl"
      },
      "outputs": [],
      "source": [
        "keras_model = MyModelKeras()\n",
        "\n",
        "# compile sets the training parameters\n",
        "keras_model.compile(\n",
        "    # By default, fit() uses tf.function().  You can\n",
        "    # turn that off for debugging, but it is on now.\n",
        "    run_eagerly=False,\n",
        "\n",
        "    # Using a built-in optimizer, configuring as an object\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "\n",
        "    # Keras comes with built-in MSE error\n",
        "    # However, you could use the loss function\n",
        "    # defined above\n",
        "    loss=tf.keras.losses.mean_squared_error,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrlHODiZccu2"
      },
      "source": [
        "`fit` de Keras espera datos por lotes o un conjunto de datos completo como un arreglo NumPy. Los arreglos NumPy se trocean en lotes y por defecto tienen un tamaño de lote de 32.\n",
        "\n",
        "En este caso, para que coincida con el comportamiento del bucle escrito a mano, debe pasar `x` como un único lote de tamaño 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfAYqtu136PO"
      },
      "outputs": [],
      "source": [
        "print(x.shape[0])\n",
        "keras_model.fit(x, y, epochs=10, batch_size=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zKZIO9P5s1G"
      },
      "source": [
        "Tenga en cuenta que Keras imprime la pérdida después del entrenamiento, no antes, por lo que la primera pérdida parece menor, pero en lo demás esto muestra esencialmente el mismo rendimiento en el entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPnIVuaSJwWz"
      },
      "source": [
        "## Siguientes pasos\n",
        "\n",
        "En esta guía, ha visto cómo usar las clases básicas de tensores, variables, módulos y cinta de gradiente para construir y entrenar un modelo, y además cómo esas ideas se mapean a Keras.\n",
        "\n",
        "Sin embargo, se trata de un problema extremadamente sencillo. Para una introducción más práctica, véase [Recorrido de entrenamiento personalizado](../tutorials/customization/custom_training_walkthrough.ipynb).\n",
        "\n",
        "Para más información sobre cómo usar los bucles de entrenamiento incorporados de Keras, consulte [esta guía](https://www.tensorflow.org/guide/keras/train_and_evaluate). Para más información sobre los bucles de entrenamiento y Keras, consulte [esta guía](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch). Para escribir bucles de entrenamiento distribuidos personalizados, consulte [esta guía](distributed_training.ipynb#using_tfdistributestrategy_with_basic_training_loops_loops)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5rmpybwysXGV",
        "iKD__8kFCKNt"
      ],
      "name": "basic_training_loops.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
