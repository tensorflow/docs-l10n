{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# Cómo usar una GPU\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/gpu\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/gpu.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/gpu.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/gpu.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoYIwe40vEPI"
      },
      "source": [
        "El código de TensorFlow y los modelos `tf.keras` se ejecutarán de forma transparente en una única GPU sin necesidad de modificar el código.\n",
        "\n",
        "Nota: Use `tf.config.list_physical_devices('GPU')` para confirmar que TensorFlow esté usando la GPU.\n",
        "\n",
        "La forma más sencilla de ejecutar en múltiples GPU, en una o varias máquinas, es usar [Estrategias de distribución](distributed_training.ipynb).\n",
        "\n",
        "Esta guía es para usuarios que han probado estos planteamientos y han descubierto que necesitan controlar con precisión la forma en que TensorFlow utiliza la GPU. Para aprender a depurar problemas de rendimiento en escenarios con una o varias GPU, consulte la guía [Optimice el rendimiento de la GPU de TensorFlow](gpu_performance_analysis.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Preparación\n",
        "\n",
        "Asegúrese de tener instalada la última versión de GPU de TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZELutYNetv-v"
      },
      "source": [
        "## Descripción general\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "TensorFlow admite la ejecución de computaciones en una variedad de tipos de dispositivos, incluidos CPU y GPU. Estos se representan con identificadores de cadena, por ejemplo:\n",
        "\n",
        "- `\"/device:CPU:0\"`: la CPU de su máquina.\n",
        "- `\"/GPU:0\"`: anotación abreviada de la primera GPU de su máquina que es visible para TensorFlow.\n",
        "- `\"/job:localhost/replica:0/task:0/device:GPU:1\"`: nombre completamente cualificado de la segunda GPU de su máquina que es visible para TensorFlow.\n",
        "\n",
        "Si una operación de TensorFlow tiene implementaciones tanto de CPU como de GPU, por defecto, el dispositivo GPU tiene prioridad a la hora de asignar la operación. Por ejemplo, `tf.matmul` tiene tanto núcleos de CPU como de GPU y, en un sistema con dispositivos `CPU:0` y `GPU:0`, se selecciona el dispositivo `GPU:0` para ejecutar `tf.matmul` a menos que le solicite explícitamente que se ejecute en otro dispositivo.\n",
        "\n",
        "Si una operación de TensorFlow no tiene una implementación de GPU correspondiente, entonces la operación vuelve al dispositivo CPU. Por ejemplo, dado que `tf.cast` solo tiene un núcleo de CPU, en un sistema con dispositivos `CPU:0` y `GPU:0`, se selecciona el dispositivo `CPU:0` para ejecutar `tf.cast`, incluso si se le solicita que se ejecute en el dispositivo `GPU:0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhNtHfuxCGVy"
      },
      "source": [
        "## Cómo registrar la colocación en el dispositivo\n",
        "\n",
        "Para saber a qué dispositivos se asignan tus operaciones y tensores, introduce `tf.debugging.set_log_device_placement(True)` como primera instrucción de tu programa. Activar el registro de colocación en el dispositivo hace que se impriman todas las asignaciones u operaciones de los tensores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dbw0tpEirCd"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Create some tensors\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKhmFeraTdEI"
      },
      "source": [
        "El código de arriba imprimirá una indicación de que la operación `MatMul` se ejecutó en `GPU:0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U88FspwGjB7W"
      },
      "source": [
        "## Colocación manual en el dispositivo\n",
        "\n",
        "Si desea que una operación específica se ejecute en un dispositivo de su elección en lugar del que se selecciona automáticamente, puede usar `with tf.device` para crear un contexto de dispositivo, y todas las operaciones dentro de ese contexto se ejecutarán en el mismo dispositivo designado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wqaQfEhjHit"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Place tensors on the CPU\n",
        "with tf.device('/CPU:0'):\n",
        "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "# Run on the GPU\n",
        "c = tf.matmul(a, b)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ixO89gRjJUu"
      },
      "source": [
        "Notará que ahora se asignan los identificadores `a` y `b` a `CPU:0`. Como no se especificó explícitamente un dispositivo para la operación `MatMul`, el tiempo de ejecución de TensorFlow elegirá uno en función de la operación y los dispositivos disponibles (`GPU:0` en este ejemplo) y, si es necesario, copiará automáticamente los tensores entre dispositivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARrRhwqijPzN"
      },
      "source": [
        "## Cómo limitar el crecimiento de la memoria de la GPU\n",
        "\n",
        "De forma predeterminada, TensorFlow asigna casi toda la memoria de la GPU de todas las GPU (sujetas a [`CUDA_VISIBLE_DEVICES`](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars)) visibles al proceso. Esto se hace para hacer un uso más eficiente de los recursos de memoria relativamente valiosos de la GPU en los dispositivos mediante la reducción de la fragmentación de la memoria. Para limitar TensorFlow a un conjunto específico de GPU, use el método `tf.config.set_visible_devices`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPI--n_jhZhv"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3x4M55DhYk9"
      },
      "source": [
        "En algunos casos resulta conveniente que el proceso asigne solo un subconjunto de la memoria disponible, o que solo aumente el uso de memoria a medida que lo requiera el proceso. TensorFlow ofrece dos métodos para controlar esto.\n",
        "\n",
        "La primera consiste en activar el crecimiento de la memoria llamando a `tf.config.experimental.set_memory_growth`, que intenta asignar únicamente tanta memoria de la GPU como sea necesaria para las asignaciones en tiempo de ejecución: al principio, asigna muy poca memoria y, a medida que el programa se ejecuta y se necesita más memoria de la GPU, la región de memoria de la GPU se amplía para el proceso de TensorFlow. La memoria no se libera, ya que esto podría provocar su fragmentación. Para activar el crecimiento de memoria para una GPU específica, use el siguiente código antes de asignar un tensor o ejecutar una operación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr3Kf1boFnCO"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1o8t51QFnmv"
      },
      "source": [
        "Otra forma de activar esta opción consiste en establecer la variable de entorno `TF_FORCE_GPU_ALLOW_GROWTH` en `true`. Esta configuración es específica de cada plataforma.\n",
        "\n",
        "La segunda opción consiste en configurar un dispositivo GPU virtual con `tf.config.set_logical_device_configuration` y establecer un límite estricto para la memoria total que se asignará a la GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qO2cS9QFn42"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
        "  try:\n",
        "    tf.config.set_logical_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Virtual devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsg1iLuHFoLW"
      },
      "source": [
        "Esto resulta útil si desea limitar realmente la cantidad de memoria de la GPU disponible para el proceso de TensorFlow. Esta práctica es habitual en el desarrollo local cuando la GPU se comparte con otras aplicaciones, como la interfaz gráfica de una estación de trabajo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B27_-1gyjf-t"
      },
      "source": [
        "## Cómo utilizar una sola GPU en un sistema con múltiples GPU\n",
        "\n",
        "Si tienes más de una GPU en tu sistema, se seleccionará por defecto la GPU con el ID más bajo. Si desea que se ejecute en otra GPU, tendrá que especificar esta preferencia de forma explícita:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wep4iteljjG1"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "try:\n",
        "  # Specify an invalid GPU device\n",
        "  with tf.device('/device:GPU:2'):\n",
        "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "    c = tf.matmul(a, b)\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy-4cCO_jn4G"
      },
      "source": [
        "Si el dispositivo que especificó no existe, obtendrá el mensaje `RuntimeError`: `.../device:GPU:2 unknown device`.\n",
        "\n",
        "Si quiere que TensorFlow elija automáticamente un dispositivo existente y compatible para ejecutar las operaciones en caso de que el dispositivo especificado no exista, puede llamar `tf.config.set_soft_device_placement(True)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sut_UHlkjvWd"
      },
      "outputs": [],
      "source": [
        "tf.config.set_soft_device_placement(True)\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Creates some tensors\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYTYPrQZj2d9"
      },
      "source": [
        "## Cómo usar múltiples GPU\n",
        "\n",
        "Desarrollar para múltiples GPU permite escalar un modelo con los recursos adicionales. Si se desarrolla en un sistema con una sola GPU, es posible emular varias GPU con dispositivos virtuales. Esto permite probar fácilmente configuraciones de múltiples GPU sin necesidad de contar con recursos adicionales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EMGuGKbNkc6"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Create 2 virtual GPUs with 1GB memory each\n",
        "  try:\n",
        "    tf.config.set_logical_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
        "         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Virtual devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmNzO0FxNkol"
      },
      "source": [
        "Una vez que el tiempo de ejecución disponga de varias GPU lógicas, puede utilizarlas con `tf.distribute.Strategy` o con colocación manual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDZmEGq4j6kG"
      },
      "source": [
        "#### Con `tf.distribute.Strategy`\n",
        "\n",
        "La mejor práctica para usar múltiples GPU consiste en usar `tf.distribute.Strategy`. Veamos un ejemplo sencillo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KgzY8V2AvRv"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.list_logical_devices('GPU')\n",
        "strategy = tf.distribute.MirroredStrategy(gpus)\n",
        "with strategy.scope():\n",
        "  inputs = tf.keras.layers.Input(shape=(1,))\n",
        "  predictions = tf.keras.layers.Dense(1)(inputs)\n",
        "  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy7nxlKsAxkK"
      },
      "source": [
        "Este programa ejecutará una copia de su modelo en cada GPU, dividiendo los datos de entrada entre ellas, lo que también se conoce como \"[paralelismo de datos](https://en.wikipedia.org/wiki/Data_parallelism)\".\n",
        "\n",
        "Para obtener más información sobre estrategias de distribución, consulte la guía [aquí](./distributed_training.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8phxM5TVkAY_"
      },
      "source": [
        "#### Colocación manual\n",
        "\n",
        "`tf.distribute.Strategy` funciona de forma interna al replicar la computación entre dispositivos. Puede implementar la replicación manualmente si construye su modelo en cada GPU. Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqPo9ltUA_EY"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "gpus = tf.config.list_logical_devices('GPU')\n",
        "if gpus:\n",
        "  # Replicate your computation on multiple GPUs\n",
        "  c = []\n",
        "  for gpu in gpus:\n",
        "    with tf.device(gpu.name):\n",
        "      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "      c.append(tf.matmul(a, b))\n",
        "\n",
        "  with tf.device('/CPU:0'):\n",
        "    matmul_sum = tf.add_n(c)\n",
        "\n",
        "  print(matmul_sum)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "gpu.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
