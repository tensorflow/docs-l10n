{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bd329a4bbca"
      },
      "source": [
        "# Enmascaramiento y amortiguación con Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b208d0913b8"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/masking_and_padding\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/masking_and_padding.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/masking_and_padding.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver el código fuente en GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/keras/masking_and_padding.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4ac441b1fc"
      },
      "source": [
        "## Configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec52be14e686"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e94d7a46bda8"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "El **enmascaramiento** es una forma de indicarle a las capas que se encargan del procesamiento de las secuencias que faltan ciertos pasos temporales en una entrada y que, por lo tanto, estos deben omitirse durante el procesamiento de los datos.\n",
        "\n",
        "La **amortiguación** es una forma especial de enmascaramiento, donde los pasos enmascarados se encuentran al principio o al final de una secuencia. El amortiguamiento surge por la necesidad de codificar secuencias de datos en lotes contiguos: para que todas las secuencias de un lote se ajusten a una cierta longitud estándar es necesario amortiguar o truncar algunas secuencias.\n",
        "\n",
        "Veamos cómo funciona esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac6b121d6be0"
      },
      "source": [
        "## Amortiguación de una secuencia de datos\n",
        "\n",
        "Cuando se procesan secuencias de datos, es muy común que las muestras individuales tengan diferentes longitudes. Considere el siguiente ejemplo (el texto se convirtió en tokens que funcionan como palabras):\n",
        "\n",
        "```\n",
        "[\n",
        "  [\"Hello\", \"world\", \"!\"],\n",
        "  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
        "  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
        "]\n",
        "```\n",
        "\n",
        "Después de la búsqueda de vocabulario, los datos pueden vectorizarse como números enteros, por ejemplo:\n",
        "\n",
        "```\n",
        "[\n",
        "  [71, 1331, 4231]\n",
        "  [73, 8, 3215, 55, 927],\n",
        "  [83, 91, 1, 645, 1253, 927],\n",
        "]\n",
        "```\n",
        "\n",
        "Los datos consisten en una lista anidada en donde las muestras individuales tienen una longiud de 3, 5, y 6 datos, respectivamente. Dado que los datos de entrada para un modelo de aprendizaje profundo deben ser un tensor único (con forma, por ejemplo, `(batch_size, 6, vocab_size)` en este caso), las muestras más cortas que el elemento más largo deben amortiguarse con algún valor del marcador de posición (o en su defecto, también es posible truncar las muestras largas antes de amortiguar las muestras cortas).\n",
        "\n",
        "Keras proporciona una función que es útil para truncar y amortiguar las listas de Python a una longitud habitual: `tf.keras.preprocessing.sequence.pad_sequences`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb64fb185a05"
      },
      "outputs": [],
      "source": [
        "raw_inputs = [\n",
        "    [711, 632, 71],\n",
        "    [73, 8, 3215, 55, 927],\n",
        "    [83, 91, 1, 645, 1253, 927],\n",
        "]\n",
        "\n",
        "# By default, this will pad using 0s; it is configurable via the\n",
        "# \"value\" parameter.\n",
        "# Note that you could \"pre\" padding (at the beginning) or\n",
        "# \"post\" padding (at the end).\n",
        "# We recommend using \"post\" padding when working with RNN layers\n",
        "# (in order to be able to use the\n",
        "# CuDNN implementation of the layers).\n",
        "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    raw_inputs, padding=\"post\"\n",
        ")\n",
        "print(padded_inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03092b2da690"
      },
      "source": [
        "## Enmascaramiento\n",
        "\n",
        "Ahora que todas las muestras tienen una longitud uniforme, es necesario informarle al modelo que una parte de los datos en realidad es un amortiguado y debe ignorarse. Este mecanismo es el **enmascaramiento**.\n",
        "\n",
        "En los modelos de Keras, hay tres formas de establecer máscaras de entrada:\n",
        "\n",
        "- Agregar una capa `keras.layers.Masking`.\n",
        "- Configurar una capa `keras.layers.Embedding` mediante `mask_zero=True`.\n",
        "- Pasar un argumento `mask` manualmente cuando se llama a las capas que sustentan dicho argumento (por ejemplo, las capas RNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6103601e5fff"
      },
      "source": [
        "## Capas que generan el enmascaramiento: `Embedding` y `Masking`\n",
        "\n",
        "En el trasfondo, estas capas crearán una máscara para el tensor (un tensor con forma 2D `(batch, sequence_length)`), y adjunto a este un tensor de salida devuelto por las capas `Masking` o `Embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2363b293483"
      },
      "outputs": [],
      "source": [
        "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
        "masked_output = embedding(padded_inputs)\n",
        "\n",
        "print(masked_output._keras_mask)\n",
        "\n",
        "masking_layer = layers.Masking()\n",
        "# Simulate the embedding lookup by expanding the 2D input to 3D,\n",
        "# with embedding dimension of 10.\n",
        "unmasked_embedding = tf.cast(\n",
        "    tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), tf.float32\n",
        ")\n",
        "\n",
        "masked_embedding = masking_layer(unmasked_embedding)\n",
        "print(masked_embedding._keras_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17e4bdb563b2"
      },
      "source": [
        "Como puede ver en el resultado impreso, la máscara es un tensor booleano en 2D con forma de `(batch_size, sequence_length)`, donde cada entrada individual `False` indica que el paso temporal correspondiente debe ignorarse durante el procesamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf11a0399fcf"
      },
      "source": [
        "## Propagación de la máscara en la API funcional y en la API secuencial\n",
        "\n",
        "Cuando se utilizan ya sea la API funcional o la API secuencial, una máscara generada por una capa `Embedding` o `Masking` se propagará a través de la red para cualquier capa que sea capaz de utilizarlas (por ejemplo, las capas RNN). Entonces, Keras obtendrá automáticamente la máscara correspondiente a una entrada y la pasará a cualquier capa que sepa utilizarla.\n",
        "\n",
        "Por ejemplo, en el siguiente modelo secuencial, la capa `LSTM` recibirá una máscara automáticamente, lo cual significa que ignorará los valores amortiguados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0adbecda288a"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential(\n",
        "    [layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True), layers.LSTM(32),]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ac6481a1d5"
      },
      "source": [
        "Lo mismo ocurre con el siguiente modelo de una API funcional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f374ab06743d"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
        "outputs = layers.LSTM(32)(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f2c4b96ecb5"
      },
      "source": [
        "## Cómo pasar los tensores de la máscara directamente a las capas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11dccb581014"
      },
      "source": [
        "Las capas que pueden soportar máscaras (como la capa `LSTM`) tienen un argumento de tipo `mask` en su método `__call__`.\n",
        "\n",
        "En cambio, las capas que generan una máscara (por ejemplo, `Embedding`) exhiben un método `compute_mask(input, previous_mask)` al que usted puede llamar.\n",
        "\n",
        "Por lo tanto, puede traspasar la salida del método `compute_mask()` desde una capa que genera máscaras al método `__call__` de una capa que utiliza máscaras, de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1955aa63896b"
      },
      "outputs": [],
      "source": [
        "class MyLayer(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyLayer, self).__init__(**kwargs)\n",
        "        self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
        "        self.lstm = layers.LSTM(32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        # Note that you could also prepare a `mask` tensor manually.\n",
        "        # It only needs to be a boolean tensor\n",
        "        # with the right shape, i.e. (batch_size, timesteps).\n",
        "        mask = self.embedding.compute_mask(inputs)\n",
        "        output = self.lstm(x, mask=mask)  # The layer will ignore the masked values\n",
        "        return output\n",
        "\n",
        "\n",
        "layer = MyLayer()\n",
        "x = np.random.random((32, 10)) * 100\n",
        "x = x.astype(\"int32\")\n",
        "layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b04dd330f848"
      },
      "source": [
        "## Cómo brindar asistencia a las máscaras en sus capas personalizadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8451a1a8ff27"
      },
      "source": [
        "Algunas veces, es posible que necesite escribir capas que generen una máscara (como `Embedding`), o capas que necesiten modificar la máscara actual.\n",
        "\n",
        "Por ejemplo, cualquier capa que genere un tensor cuya dimensión temporal sea diferente a la de su entrada, como una capa `Concatenate` a fin de que concatene en la dimensión temporal, necesitará modificar la máscara actual para que las capas posteriores consideren correctamente los pasos temporales enmascarados.\n",
        "\n",
        "Para ello, su nueva capa debe implementar el método `layer.compute_mask()`, el cual produce una nueva máscara cuando se proporcionan la entrada y la máscara actual.\n",
        "\n",
        "A continuación, se muestra el ejemplo de una capa `TemporalSplit` donde se necesita modificar la máscara actual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a06fb2194c0d"
      },
      "outputs": [],
      "source": [
        "class TemporalSplit(keras.layers.Layer):\n",
        "    \"\"\"Split the input tensor into 2 tensors along the time dimension.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Expect the input to be 3D and mask to be 2D, split the input tensor into 2\n",
        "        # subtensors along the time axis (axis 1).\n",
        "        return tf.split(inputs, 2, axis=1)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        # Also split the mask into 2 if it presents.\n",
        "        if mask is None:\n",
        "            return None\n",
        "        return tf.split(mask, 2, axis=1)\n",
        "\n",
        "\n",
        "first_half, second_half = TemporalSplit()(masked_embedding)\n",
        "print(first_half._keras_mask)\n",
        "print(second_half._keras_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "282b867dcd95"
      },
      "source": [
        "El siguiente es otro ejemplo de una capa `CustomEmbedding` que es capaz de generar una máscara a partir de los valores de entrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e760655cd39c"
      },
      "outputs": [],
      "source": [
        "class CustomEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n",
        "        super(CustomEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.mask_zero = mask_zero\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            dtype=\"float32\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        if not self.mask_zero:\n",
        "            return None\n",
        "        return tf.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "layer = CustomEmbedding(10, 32, mask_zero=True)\n",
        "x = np.random.random((3, 10)) * 9\n",
        "x = x.astype(\"int32\")\n",
        "\n",
        "y = layer(x)\n",
        "mask = layer.compute_mask(x)\n",
        "\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb34149eb837"
      },
      "source": [
        "## Cómo elegir la propagación de máscaras en capas que sean compatibles\n",
        "\n",
        "En la mayoría de las capas no se modifica la dimensión temporal, de modo que no es necesario modificar la máscara actual. Sin embargo, es posible que se desee **propagar** la máscara actual, sin ningún cambio, a la siguiente capa. **Esta es una acción opcional.** De forma predeterminada, una capa personalizada destruirá la máscara actual (ya que la estructura no tiene forma de saber si es seguro propagar la máscara).\n",
        "\n",
        "Si tiene una capa personalizada en la que no se modifique la dimensión temporal, y desea que pueda propagarse la máscara de la entrada actual, debe establecer `self.supports_masking = True` en el generador de la capa. En este caso, la acción predeterminada de `compute_mask()` es simplemente traspasar la máscara actual.\n",
        "\n",
        "A continuación, se muestra el ejemplo de una capa que está en la lista blanca para la propagación de máscaras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "895c35534d06"
      },
      "outputs": [],
      "source": [
        "class MyActivation(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyActivation, self).__init__(**kwargs)\n",
        "        # Signal that the layer is safe for mask propagation\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.relu(inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2e1e0a81995"
      },
      "source": [
        "Ahora puede usar esta capa personalizada que se encuentra entre una capa generadora de máscaras (como `Embedding`) y una capa que utiliza máscaras (como `LSTM`), esto traspasará la máscara para que llegue a la capa que utiliza las máscaras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "486e39e9a9a7"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
        "x = MyActivation()(x)  # Will pass the mask along\n",
        "print(\"Mask found:\", x._keras_mask)\n",
        "outputs = layers.LSTM(32)(x)  # Will receive the mask\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ab9c02f4ba"
      },
      "source": [
        "## Cómo escribir capas que necesitan información sobre la máscara\n",
        "\n",
        "Algunas capas son *consumers* de máscaras: aceptan un argumento `mask` en la `call`y lo usan para determinar si deben saltarse ciertos pasos temporales.\n",
        "\n",
        "Para escribir una capa de este tipo, puede simplemente añadir un argumento `mask=None` en la firma de su `call` signature. La máscara asociada a las entradas se pasará a su capa siempre que esté disponible.\n",
        "\n",
        "A continuación, veremos un ejemplo sencillo: una capa que calcula un softmax sobre la dimensión temporal (eje 1) de una secuencia de entrada, mientras descarta los pasos temporales enmascarados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9774bcd59908"
      },
      "outputs": [],
      "source": [
        "class TemporalSoftmax(keras.layers.Layer):\n",
        "    def call(self, inputs, mask=None):\n",
        "        broadcast_float_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
        "        inputs_exp = tf.exp(inputs) * broadcast_float_mask\n",
        "        inputs_sum = tf.reduce_sum(\n",
        "            inputs_exp * broadcast_float_mask, axis=-1, keepdims=True\n",
        "        )\n",
        "        return inputs_exp / inputs_sum\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs)\n",
        "x = layers.Dense(1)(x)\n",
        "outputs = TemporalSoftmax()(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "y = model(np.random.randint(0, 10, size=(32, 100)), np.random.random((32, 100, 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6373f43bbe18"
      },
      "source": [
        "## Resumen\n",
        "\n",
        "Esto es todo lo que necesita saber sobre la amortiguación y el enmascaramiento en Keras. Para recapitular, recuerde lo siguiente:\n",
        "\n",
        "- El \"enmascaramiento\" es la forma en que las capas son capaces de saber cuándo omitir / ignorar ciertos pasos temporales en las entradas de la secuencia.\n",
        "- Algunas capas son generadoras de máscaras: `Embedding` puede generar una máscara a partir de los valores de entrada (si `mask_zero=True`), y lo mismo puede hacer la capa `Masking`.\n",
        "- Algunas capas utilizan máscaras: exhiben un argumento `mask` en su método `__call__`. Este es el caso de las capas RNN.\n",
        "- Tanto en la API funcional como en la API secuencial, la información sobre la máscara se propaga automáticamente.\n",
        "- Cuando las capas se usan de forma independiente, puede traspasar los argumentos `mask` a las capas de forma manual.\n",
        "- Puede escribir fácilmente capas que modifiquen la máscara actual, ya sea que generen una nueva máscara o que utilicen la máscara asociada a las entradas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "masking_and_padding.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
