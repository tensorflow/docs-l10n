{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e083398b477"
      },
      "source": [
        "# Cómo trabajar con capas de preprocesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64010bd23c2e"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/preprocessing_layers\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver el código fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/keras/preprocessing_layers.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1d403f04693"
      },
      "source": [
        "## El preprocesamiento en Keras\n",
        "\n",
        "La API con capas para preprocesamiento de Keras permite a los desarrolladores crear canalizaciones de procesamiento para las entradas nativas de Keras. Estas canalizaciones para el procesamiento de las entradas se pueden utilizar como un código de preprocesamiento independiente en flujos de trabajo que no sean parte de Keras, combinarse directamente con modelos de Keras y exportarse como parte de un SavedModel de Keras.\n",
        "\n",
        "Con las capas para preprocesamiento de Keras, puede crear y exportar modelos que sean realmente integrales: modelos que acepten como entrada imágenes o datos estructurados sin procesar, modelos que controlen la normalización de características o la indexación de los valores de dichas características por su cuenta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313360fa9024"
      },
      "source": [
        "## Preprocesamiento disponible\n",
        "\n",
        "### Preprocesamiento de texto\n",
        "\n",
        "- `tf.keras.layers.TextVectorization`: convierte las cadenas sin procesar en una representación codificada que puede leerse por una capa `Embedding` o una capa de tipo `Dense`.\n",
        "\n",
        "### Preprocesamiento de funciones numéricas\n",
        "\n",
        "- `tf.keras.layers.Normalization`: realiza la normalización de las funciones de entrada.\n",
        "- `tf.keras.layers.Discretization`: convierte a las funciones numéricas continuas en funciones categóricas enteras.\n",
        "\n",
        "### Preprocesamiento de funciones categóricas\n",
        "\n",
        "- `tf.keras.layers.CategoryEncoding`: convierte a las funciones categóricas enteras en representaciones densas de one-hot, multi-hot o un número de ellas.\n",
        "- `tf.keras.layers.Hashing`: realiza el hashing de las funciones categóricas, también conocido como el \"truco del hashing\".\n",
        "- `tf.keras.layers.StringLookup`: convierte a los valores categóricos de la cadena en una representación cifrada que puede leerse por una capa `Embedding` o una capa `Dense`.\n",
        "- `tf.keras.layers.IntegerLookup`: convierte a los valores categóricos enteros en una representación cifrada que puede leerse por una capa `Embedding` o una capa `Dense`.\n",
        "\n",
        "### Preprocesamiento de imágenes\n",
        "\n",
        "Estas capas sirven para estandarizar las entradas en un modelo de imagen.\n",
        "\n",
        "- `tf.keras.layers.Resizing`: redimensiona un lote de imágenes a un tamaño objetivo.\n",
        "- `tf.keras.layers.Rescaling`: redimensiona y compensa los valores de un lote de imágenes (por ejemplo, pasar las entradas de un rango `[0, 255]` a las entradas en el rango `[0, 1]`.\n",
        "- `tf.keras.layers.CenterCrop`: devuelve el recorte central de un lote de imágenes.\n",
        "\n",
        "### Aumentar los datos de la imagen\n",
        "\n",
        "Estas capas aplican aumento en las transformaciones de forma aleatoria a un lote de imágenes. Solo estarán activas durante el entrenamiento.\n",
        "\n",
        "- `tf.keras.layers.RandomCrop`\n",
        "- `tf.keras.layers.RandomFlip`\n",
        "- `tf.keras.layers.RandomTranslation`\n",
        "- `tf.keras.layers.RandomRotation`\n",
        "- `tf.keras.layers.RandomZoom`\n",
        "- `tf.keras.layers.RandomHeight`\n",
        "- `tf.keras.layers.RandomWidth`\n",
        "- `tf.keras.layers.RandomContrast`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c923e41fb1b4"
      },
      "source": [
        "## El método `adapt()`\n",
        "\n",
        "Algunas capas de preprocesamiento tienen un estado interno que puede calcularse a partir de una muestra de los datos de entrenamiento. La lista de capas de preprocesamiento con estado es:\n",
        "\n",
        "- `TextVectorization`: contiene un mapeo entre tokens de cadena e índices enteros\n",
        "- `StringLookup` y `IntegerLookup`: contienen una correspondencia entre valores de entrada e índices enteros.\n",
        "- `Normalization`: contiene el promedio y la desviación estándar de las características.\n",
        "- `Discretization`: contiene información sobre los límites de los cubos de valores.\n",
        "\n",
        "Estas capas **no son entrenables**. Su estado no se establece durante el entrenamiento; debe establecerse **antes del entrenamiento**, ya sea inicializándolas a partir de una constante precalculada o \"adaptándolas\" a los datos.\n",
        "\n",
        "Se establece el estado de una capa de preprocesamiento exponiéndola a los datos de entrenamiento, mediante el método `adapt()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cac6bd80812"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data = np.array([[0.1, 0.2, 0.3], [0.8, 0.9, 1.0], [1.5, 1.6, 1.7],])\n",
        "layer = layers.Normalization()\n",
        "layer.adapt(data)\n",
        "normalized_data = layer(data)\n",
        "\n",
        "print(\"Features mean: %.2f\" % (normalized_data.numpy().mean()))\n",
        "print(\"Features std: %.2f\" % (normalized_data.numpy().std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d43b8246b8a3"
      },
      "source": [
        "El método `adapt()` toma un array Numpy o un objeto `tf.data.Dataset`. En el caso de `StringLookup` y `TextVectorization`, también se puede pasar una lista de cadenas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48d95713348a"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    \"ξεῖν᾽, ἦ τοι μὲν ὄνειροι ἀμήχανοι ἀκριτόμυθοι\",\n",
        "    \"γίγνοντ᾽, οὐδέ τι πάντα τελείεται ἀνθρώποισι.\",\n",
        "    \"δοιαὶ γάρ τε πύλαι ἀμενηνῶν εἰσὶν ὀνείρων:\",\n",
        "    \"αἱ μὲν γὰρ κεράεσσι τετεύχαται, αἱ δ᾽ ἐλέφαντι:\",\n",
        "    \"τῶν οἳ μέν κ᾽ ἔλθωσι διὰ πριστοῦ ἐλέφαντος,\",\n",
        "    \"οἵ ῥ᾽ ἐλεφαίρονται, ἔπε᾽ ἀκράαντα φέροντες:\",\n",
        "    \"οἱ δὲ διὰ ξεστῶν κεράων ἔλθωσι θύραζε,\",\n",
        "    \"οἵ ῥ᾽ ἔτυμα κραίνουσι, βροτῶν ὅτε κέν τις ἴδηται.\",\n",
        "]\n",
        "layer = layers.TextVectorization()\n",
        "layer.adapt(data)\n",
        "vectorized_text = layer(data)\n",
        "print(vectorized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7619914dfb40"
      },
      "source": [
        "Además, las capas adaptables siempre exponen una opción para establecer directamente el estado mediante argumentos del constructor o asignación de pesos. Si los valores de estado deseados se conocen en el momento de la construcción de la capa, o se calculan fuera de la llamada `adapt()`, pueden establecerse sin depender del cálculo interno de la capa. Por ejemplo, si ya existen archivos de vocabulario externos para las capas `TextVectorization`, `StringLookup`, o `IntegerLookup`, pueden cargarse directamente en las tablas de búsqueda pasando una ruta al archivo de vocabulario en los argumentos del constructor de la capa.\n",
        "\n",
        "A continuación se muestra un ejemplo en el que creamos una capa `StringLookup` con vocabulario precalculado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9df56efc7f3b"
      },
      "outputs": [],
      "source": [
        "vocab = [\"a\", \"b\", \"c\", \"d\"]\n",
        "data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n",
        "layer = layers.StringLookup(vocabulary=vocab)\n",
        "vectorized_data = layer(data)\n",
        "print(vectorized_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49cbfe135b00"
      },
      "source": [
        "## Preprocesamiento de datos antes del modelo o dentro del modelo\n",
        "\n",
        "Hay dos formas de utilizar las capas de preprocesamiento:\n",
        "\n",
        "**Opción 1:** Haga que sean parte del modelo, de la siguiente manera:\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = preprocessing_layer(inputs)\n",
        "outputs = rest_of_the_model(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "```\n",
        "\n",
        "Con esta opción, el preprocesamiento se realizará en el dispositivo, de forma sincronizada con el resto de la ejecución del modelo, lo que significa que se beneficiará de la aceleración de la GPU. Si está entrenando en la GPU, esta es la mejor opción para la capa `Normalization`, y para todas las capas de preprocesamiento de imágenes y aumento de datos.\n",
        "\n",
        "**Opción 2:** aplíquelo a su `tf.data.Dataset`, para obtener un conjunto de datos que produzca lotes de datos preprocesados, como este:\n",
        "\n",
        "```python\n",
        "dataset = dataset.map(lambda x, y: (preprocessing_layer(x), y))\n",
        "```\n",
        "\n",
        "Con esta opción, el preprocesamiento se realizará en la CPU, de forma asíncrona, y se almacenará en el búfer antes de pasar al modelo. Además, si llama `dataset.prefetch(tf.data.AUTOTUNE)` a su conjunto de datos, el preprocesamiento se realizará eficientemente en paralelo con el entrenamiento:\n",
        "\n",
        "```python\n",
        "dataset = dataset.map(lambda x, y: (preprocessing_layer(x), y))\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "model.fit(dataset, ...)\n",
        "```\n",
        "\n",
        "Esta es la mejor opción para `TextVectorization`, y todas las capas de preprocesamiento de datos estructurados. También puede ser una buena opción si entrena con CPU y utiliza capas de preprocesamiento de imágenes.\n",
        "\n",
        "**Cuando se ejecuta en TPU, siempre debe colocar las capas de preprocesamiento en el pipeline `tf.data`** (con la excepción de `Normalization` y `Rescaling`, que se ejecutan bien en TPU y se utilizan comúnmente ya que la primera capa es un modelo de imagen)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32f6d2a104b7"
      },
      "source": [
        "## Ventajas de preprocesar el modelo durante la inferencia\n",
        "\n",
        "Incluso si opta por la opción 2, es posible que más adelante desee exportar un modelo de extremo a extremo basado solo en la inferencia que incluya las capas de preprocesamiento. La principal ventaja de hacer esto es que **hace que el modelo sea portátil** y **ayuda a reducir la [distorsión del entrenamiento/servicio](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)**.\n",
        "\n",
        "Cuando todo el preprocesamiento de datos forma parte del modelo, otras personas pueden cargar y utilizar su modelo sin tener que saber cómo se espera que se codifique y normalice cada característica. Su modelo de inferencia podrá procesar imágenes en bruto o datos estructurados en bruto, y no requerirá que los usuarios del modelo conozcan los detalles de, por ejemplo, el esquema de tokenización utilizado para el texto, el esquema de indexación utilizado para las características categóricas, si los valores de los pixeles de la imagen se normalizan a `[-1, +1]` o a `[0, 1]`, etc. Esto es especialmente poderoso si exporta su modelo a otro tiempo de ejecución, como TensorFlow.js: no tendrá que volver a implementar su canal de preprocesamiento en JavaScript.\n",
        "\n",
        "Si inicialmente coloca sus capas de preprocesamiento en su canalización `tf.data`, puede exportar un modelo de inferencia que empaquete el preprocesamiento. Simplemente instancie un nuevo modelo que encadene sus capas de preprocesamiento y su modelo de entrenamiento:\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = preprocessing_layer(inputs)\n",
        "outputs = training_model(x)\n",
        "inference_model = keras.Model(inputs, outputs)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b41b381d48d4"
      },
      "source": [
        "## Recursos rápidos\n",
        "\n",
        "### Aumentar los datos de la imagen\n",
        "\n",
        "Tenga en cuenta que las capas de aumento de datos de la imagen solo están activas durante el entrenamiento (de forma similar a la capa `Dropout`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3793692e983"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load some data\n",
        "(x_train, y_train), _ = keras.datasets.cifar10.load_data()\n",
        "input_shape = x_train.shape[1:]\n",
        "classes = 10\n",
        "\n",
        "# Create a tf.data pipeline of augmented images (and their labels)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.batch(16).map(lambda x, y: (data_augmentation(x), y))\n",
        "\n",
        "\n",
        "# Create a model and train it on the augmented image data\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = layers.Rescaling(1.0 / 255)(inputs)  # Rescale inputs\n",
        "outputs = keras.applications.ResNet50(  # Add the rest of the model\n",
        "    weights=None, input_shape=input_shape, classes=classes\n",
        ")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "model.fit(train_dataset, steps_per_epoch=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51d369f0310f"
      },
      "source": [
        "Puede ver una configuración similar en acción en el ejemplo [clasificación de imágenes desde cero](https://keras.io/examples/vision/image_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a79a1c48b2b7"
      },
      "source": [
        "### Normalización de las características numéricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cc2607a45c8"
      },
      "outputs": [],
      "source": [
        "# Load some data\n",
        "(x_train, y_train), _ = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.reshape((len(x_train), -1))\n",
        "input_shape = x_train.shape[1:]\n",
        "classes = 10\n",
        "\n",
        "# Create a Normalization layer and set its internal state using the training data\n",
        "normalizer = layers.Normalization()\n",
        "normalizer.adapt(x_train)\n",
        "\n",
        "# Create a model that include the normalization layer\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = normalizer(inputs)\n",
        "outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Train the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62685d477010"
      },
      "source": [
        "### Codificación de funciones categóricas de las cadenas mediante la codificación en un solo paso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae0d2b0405f1"
      },
      "outputs": [],
      "source": [
        "# Define some toy data\n",
        "data = tf.constant([[\"a\"], [\"b\"], [\"c\"], [\"b\"], [\"c\"], [\"a\"]])\n",
        "\n",
        "# Use StringLookup to build an index of the feature values and encode output.\n",
        "lookup = layers.StringLookup(output_mode=\"one_hot\")\n",
        "lookup.adapt(data)\n",
        "\n",
        "# Convert new test data (which includes unknown feature values)\n",
        "test_data = tf.constant([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [\"\"]])\n",
        "encoded_data = lookup(test_data)\n",
        "print(encoded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "686aeda532f5"
      },
      "source": [
        "Observe que, aquí, el índice 0 se reserva para valores que están fuera del vocabulario (valores que no se vieron durante `adapt()`).\n",
        "\n",
        "Puede ver el `StringLookup` en acción en el ejemplo [Clasificación de datos estructurados desde cero](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc8af3e290df"
      },
      "source": [
        "### Codificación de funciones categóricas de números enteros mediante la codificación en un solo paso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75f3d6af4522"
      },
      "outputs": [],
      "source": [
        "# Define some toy data\n",
        "data = tf.constant([[10], [20], [20], [10], [30], [0]])\n",
        "\n",
        "# Use IntegerLookup to build an index of the feature values and encode output.\n",
        "lookup = layers.IntegerLookup(output_mode=\"one_hot\")\n",
        "lookup.adapt(data)\n",
        "\n",
        "# Convert new test data (which includes unknown feature values)\n",
        "test_data = tf.constant([[10], [10], [20], [50], [60], [0]])\n",
        "encoded_data = lookup(test_data)\n",
        "print(encoded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5a6be487be"
      },
      "source": [
        "Tenga en cuenta que el índice 0 está reservado para aquellos valores que faltan (que debería especificar como el valor 0), y el índice 1 está reservado para valores fuera del vocabulario (valores que no se vieron durante `adapt()`). Puede configurar esto utilizando los argumentos del constructor `mask_token` y `oov_token` de `IntegerLookup`.\n",
        "\n",
        "Puede ver `IntegerLookup` en acción en el ejemplo [clasificación de datos estructurados desde cero](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fbfaa6ab3e2"
      },
      "source": [
        "### Cómo aplicar el truco hashing a una característica categórica entera\n",
        "\n",
        "Si tiene una característica categórica que puede tomar muchos valores diferentes (del orden de 10e3 o más), en la que cada valor solo aparece unas pocas veces en los datos, resulta poco práctico e ineficaz indexar y codificar con un solo paso los valores de la característica. En cambio, puede ser una buena idea aplicar el \"truco del hash\": codificar los valores en un vector de tamaño fijo. De este modo, el tamaño del espacio de características se mantiene administrable y se elimina la necesidad de una indexación explícita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f6c1f84c43c"
      },
      "outputs": [],
      "source": [
        "# Sample data: 10,000 random integers with values between 0 and 100,000\n",
        "data = np.random.randint(0, 100000, size=(10000, 1))\n",
        "\n",
        "# Use the Hashing layer to hash the values to the range [0, 64]\n",
        "hasher = layers.Hashing(num_bins=64, salt=1337)\n",
        "\n",
        "# Use the CategoryEncoding layer to multi-hot encode the hashed values\n",
        "encoder = layers.CategoryEncoding(num_tokens=64, output_mode=\"multi_hot\")\n",
        "encoded_data = encoder(hasher(data))\n",
        "print(encoded_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df69b434d327"
      },
      "source": [
        "### Codificación de texto como secuencia de índices de tokens\n",
        "\n",
        "Así es como debe preprocesar el texto que va a pasar a una capa `Embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "361b561bc88b"
      },
      "outputs": [],
      "source": [
        "# Define some text data to adapt the layer\n",
        "adapt_data = tf.constant(\n",
        "    [\n",
        "        \"The Brain is wider than the Sky\",\n",
        "        \"For put them side by side\",\n",
        "        \"The one the other will contain\",\n",
        "        \"With ease and You beside\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a TextVectorization layer\n",
        "text_vectorizer = layers.TextVectorization(output_mode=\"int\")\n",
        "# Index the vocabulary via `adapt()`\n",
        "text_vectorizer.adapt(adapt_data)\n",
        "\n",
        "# Try out the layer\n",
        "print(\n",
        "    \"Encoded text:\\n\", text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
        ")\n",
        "\n",
        "# Create a simple model\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(input_dim=text_vectorizer.vocabulary_size(), output_dim=16)(inputs)\n",
        "x = layers.GRU(8)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Create a labeled dataset (which includes unknown tokens)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0])\n",
        ")\n",
        "\n",
        "# Preprocess the string inputs, turning them into int sequences\n",
        "train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y))\n",
        "# Train the model on the int sequences\n",
        "print(\"\\nTraining model...\")\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
        "model.fit(train_dataset)\n",
        "\n",
        "# For inference, you can export a model that accepts strings as input\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "outputs = model(x)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Call the end-to-end model on test data (which includes unknown tokens)\n",
        "print(\"\\nCalling end-to-end model on test string...\")\n",
        "test_data = tf.constant([\"The one the other will absorb\"])\n",
        "test_output = end_to_end_model(test_data)\n",
        "print(\"Model output:\", test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e725dbcae3e4"
      },
      "source": [
        "Puede ver la capa `TextVectorization` en acción, combinada con un modo `Embedding`, en el ejemplo [clasificación de texto desde cero](https://keras.io/examples/nlp/text_classification_from_scratch/).\n",
        "\n",
        "Tenga en cuenta que al entrenar un modelo de este tipo, para obtener el mejor rendimiento, siempre debe utilizar la capa `TextVectorization` como parte del canal de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28c2f2ff61fb"
      },
      "source": [
        "### Codificación de textos como matrices densas de ngramas con codificación multi-hot\n",
        "\n",
        "Esta es la forma en que debe preprocesar el texto que va a pasar a una capa `Dense`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bae1c223cd8"
      },
      "outputs": [],
      "source": [
        "# Define some text data to adapt the layer\n",
        "adapt_data = tf.constant(\n",
        "    [\n",
        "        \"The Brain is wider than the Sky\",\n",
        "        \"For put them side by side\",\n",
        "        \"The one the other will contain\",\n",
        "        \"With ease and You beside\",\n",
        "    ]\n",
        ")\n",
        "# Instantiate TextVectorization with \"multi_hot\" output_mode\n",
        "# and ngrams=2 (index all bigrams)\n",
        "text_vectorizer = layers.TextVectorization(output_mode=\"multi_hot\", ngrams=2)\n",
        "# Index the bigrams via `adapt()`\n",
        "text_vectorizer.adapt(adapt_data)\n",
        "\n",
        "# Try out the layer\n",
        "print(\n",
        "    \"Encoded text:\\n\", text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
        ")\n",
        "\n",
        "# Create a simple model\n",
        "inputs = keras.Input(shape=(text_vectorizer.vocabulary_size(),))\n",
        "outputs = layers.Dense(1)(inputs)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Create a labeled dataset (which includes unknown tokens)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0])\n",
        ")\n",
        "\n",
        "# Preprocess the string inputs, turning them into int sequences\n",
        "train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y))\n",
        "# Train the model on the int sequences\n",
        "print(\"\\nTraining model...\")\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
        "model.fit(train_dataset)\n",
        "\n",
        "# For inference, you can export a model that accepts strings as input\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "outputs = model(x)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Call the end-to-end model on test data (which includes unknown tokens)\n",
        "print(\"\\nCalling end-to-end model on test string...\")\n",
        "test_data = tf.constant([\"The one the other will absorb\"])\n",
        "test_output = end_to_end_model(test_data)\n",
        "print(\"Model output:\", test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "336a4d3426ed"
      },
      "source": [
        "### Codificación de texto como matriz densa de ngramas con ponderación TF-IDF\n",
        "\n",
        "Se trata de una forma alternativa de preprocesar el texto antes de pasarlo a una capa `Dense`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b6c0fec928e"
      },
      "outputs": [],
      "source": [
        "# Define some text data to adapt the layer\n",
        "adapt_data = tf.constant(\n",
        "    [\n",
        "        \"The Brain is wider than the Sky\",\n",
        "        \"For put them side by side\",\n",
        "        \"The one the other will contain\",\n",
        "        \"With ease and You beside\",\n",
        "    ]\n",
        ")\n",
        "# Instantiate TextVectorization with \"tf-idf\" output_mode\n",
        "# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\n",
        "text_vectorizer = layers.TextVectorization(output_mode=\"tf-idf\", ngrams=2)\n",
        "# Index the bigrams and learn the TF-IDF weights via `adapt()`\n",
        "\n",
        "with tf.device(\"CPU\"):\n",
        "    # A bug that prevents this from running on GPU for now.\n",
        "    text_vectorizer.adapt(adapt_data)\n",
        "\n",
        "# Try out the layer\n",
        "print(\n",
        "    \"Encoded text:\\n\", text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
        ")\n",
        "\n",
        "# Create a simple model\n",
        "inputs = keras.Input(shape=(text_vectorizer.vocabulary_size(),))\n",
        "outputs = layers.Dense(1)(inputs)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Create a labeled dataset (which includes unknown tokens)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0])\n",
        ")\n",
        "\n",
        "# Preprocess the string inputs, turning them into int sequences\n",
        "train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y))\n",
        "# Train the model on the int sequences\n",
        "print(\"\\nTraining model...\")\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
        "model.fit(train_dataset)\n",
        "\n",
        "# For inference, you can export a model that accepts strings as input\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "outputs = model(x)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Call the end-to-end model on test data (which includes unknown tokens)\n",
        "print(\"\\nCalling end-to-end model on test string...\")\n",
        "test_data = tf.constant([\"The one the other will absorb\"])\n",
        "test_output = end_to_end_model(test_data)\n",
        "print(\"Model output:\", test_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "143ce01c5558"
      },
      "source": [
        "## Trucos importantes\n",
        "\n",
        "### Trabajar con capas de búsqueda y vocabularios muy extensos\n",
        "\n",
        "Puede que se encuentre trabajando con un vocabulario muy grande en una capa `TextVectorization`, una capa `StringLookup` o una capa `IntegerLookup`. Normalmente, un vocabulario de más de 500 MB se consideraría \"muy grande\".\n",
        "\n",
        "En tal caso, para obtener el mejor rendimiento, debe evitar el uso de `adapt()`. En cambio, precalcule su vocabulario con antelación (puede utilizar Apache Beam o TF Transform para ello) y guárdelo en un archivo. A continuación, cargue el vocabulario en la capa en el momento de la construcción pasando la ruta del archivo como argumento `vocabulary`.\n",
        "\n",
        "### Cómo utilizar capas de búsqueda en un pod TPU o con `ParameterServerStrategy`.\n",
        "\n",
        "Hay un problema pendiente que hace que el rendimiento se reduzca cuando se utiliza una capa `TextVectorization`, `StringLookup`, o `IntegerLookup` mientras se entrena en un pod TPU o en múltiples máquinas mediante `ParameterServerStrategy`. Esto está programado para ser corregido en TensorFlow 2.7."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocessing_layers.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
