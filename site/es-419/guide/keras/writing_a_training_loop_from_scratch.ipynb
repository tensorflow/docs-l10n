{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daf323e33b84"
      },
      "source": [
        "# Cómo escribir un bucle de entrenamiento desde cero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2440f6e0c5ef"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver en TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/writing_a_training_loop_from_scratch.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Ejecutar en Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/writing_a_training_loop_from_scratch.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver código fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/keras/writing_a_training_loop_from_scratch.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4ac441b1fc"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae2407ad926f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f5a253901f8"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Keras proporciona bucles de entrenamiento y evaluación predeterminados, `fit()` y `evaluate()`. Su uso se explica en la guía [Entrenamiento y evaluación con los métodos incorporados](https://www.tensorflow.org/guide/keras/train_and_evaluate/).\n",
        "\n",
        "Si desea personalizar el algoritmo de aprendizaje de su modelo sin dejar de aprovechar la comodidad de `fit()` (por ejemplo, para entrenar un GAN utilizando `fit()`), puede subclasificar la clase `Model` e implementar su propio método `train_step()`, que se llama repetidamente durante `fit()`. Esto se explica en la guía <a href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/\" data-md-type=\"link\">Personalización de lo que sucede en `fit()`</a>.\n",
        "\n",
        "Ahora, si desea un control de muy bajo nivel sobre el entrenamiento y la evaluación, debe escribir sus propios bucles de entrenamiento y evaluación desde cero. De esto trata esta guía."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4f47351a3ec"
      },
      "source": [
        "## Uso de `GradientTape`: un primer ejemplo de extremo a extremo\n",
        "\n",
        "Llamar a un modelo dentro de un ámbito `GradientTape` permite recuperar los gradientes de los pesos entrenables de la capa con respecto a un valor de pérdida. Al utilizar una instancia del optimizador, puede emplear estos gradientes para actualizar estas variables (que puede recuperar mediante `model.trainable_weights`).\n",
        "\n",
        "Consideremos un modelo MNIST sencillo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaa775ce7dab"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8b02a5759cf"
      },
      "source": [
        "Vamos a entrenarlo utilizando un gradiente mini-lote con un bucle de entrenamiento personalizado.\n",
        "\n",
        "En primer lugar, necesitaremos un optimizador, una función de pérdida y un conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2c6257b8d02"
      },
      "outputs": [],
      "source": [
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the training dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_test, (-1, 784))\n",
        "\n",
        "# Reserve 10,000 samples for validation.\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "\n",
        "# Prepare the training dataset.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c30285b1a2e"
      },
      "source": [
        "Este es nuestro bucle de entrenamiento:\n",
        "\n",
        "- Abrimos un bucle `for` que itera sobre las épocas\n",
        "- Para cada época, abrimos un bucle `for` que itera sobre el conjunto de datos, en lotes\n",
        "- Para cada lote, abriremos un entorno `GradientTape()`.\n",
        "- Dentro de este entorno, llamamos al modelo (paso previo) y calculamos la pérdida\n",
        "- Fuera del entorno, recuperamos los gradientes de los pesos del modelo con respecto a la pérdida\n",
        "- Por último, utilizaremos el optimizador para actualizar los pesos del modelo en función de los gradientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bf4c10ceb50"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d600076b7be0"
      },
      "source": [
        "## Administración de métricas de bajo nivel\n",
        "\n",
        "Incorporemos el monitoreo de métricas a este bucle básico.\n",
        "\n",
        "Puede reutilizar fácilmente las métricas incorporadas (o las métricas personalizadas que haya escrito) en dichos bucles de entrenamiento escritos desde cero. Este es el flujo:\n",
        "\n",
        "- Cree la instancia de la métrica al principio del bucle\n",
        "- Llame a `metric.update_state()` después de cada lote\n",
        "- Llame a `metric.result()` cuando necesite mostrar el valor actual de la métrica\n",
        "- Llame a `metric.reset_states()` cuando necesite borrar el estado de la métrica (normalmente cuando termina una época).\n",
        "\n",
        "Utilicemos este conocimiento para calcular `SparseCategoricalAccuracy` en los datos de validación al final de cada época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2602509b16c7"
      },
      "outputs": [],
      "source": [
        "# Get model\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9111a5cc87dc"
      },
      "source": [
        "Este es nuestro bucle de entrenamiento y evaluación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "654e2311dbff"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update training metric.\n",
        "        train_acc_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_logits = model(x_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c9a16c21790"
      },
      "source": [
        "## Cómo acelerar el paso del entrenamiento con `tf.function`\n",
        "\n",
        "El tiempo de ejecución predeterminado en TensorFlow 2 es la [ejecución eager](https://www.tensorflow.org/guide/eager). Como tal, nuestro bucle de entrenamiento anterior se ejecuta rápidamente.\n",
        "\n",
        "Esto es genial para la depuración, pero la compilación de los gráficos tiene una clara ventaja en el rendimiento. Al describir el cálculo como un grafo estático, el marco de trabajo puede implementar optimizaciones de rendimiento globales. Esto es imposible cuando el marco de trabajo se ve obligado a ejecutar una operación tras otra, sin saber lo que viene después.\n",
        "\n",
        "Puede compilar en un grafo estático con cualquier función que tome tensores como entrada. Solo tiene que agregarle un decorador `@tf.function`, como este:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdacc2d48ade"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab61b0bf3126"
      },
      "source": [
        "Hagamos lo mismo con el paso de la evaluación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da4828fd8ef7"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d552377968f1"
      },
      "source": [
        "Ahora, volvamos a ejecutar nuestro bucle de entrenamiento con este paso de entrenamiento compilado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69d73c94e44"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8977d77a8095"
      },
      "source": [
        "Es mucho más rápido, ¿verdad?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5b5a54d339a"
      },
      "source": [
        "## La administración de bajo nivel de las pérdidas registradas por el modelo\n",
        "\n",
        "Las capas y los modelos realizan un seguimiento recursivo de las pérdidas creadas durante el paso previo por las capas que llaman a `self.add_loss(value)`. La lista correspondiente de valores de pérdidas escalares está disponible mediante la propiedad `model.losses` cuando finaliza el paso previo.\n",
        "\n",
        "Si desea utilizar estos componentes de pérdida, debe sumarlos y agregarlos a la pérdida principal en su paso en el entrenamiento.\n",
        "\n",
        "Consideremos esta capa, que crea una pérdida de regularización de la actividad:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ec7c4b16596"
      },
      "outputs": [],
      "source": [
        "class ActivityRegularizationLayer(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b12260b8bf2"
      },
      "source": [
        "Construyamos un modelo muy sencillo que lo utilice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57afe49e6b93"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "# Insert activity regularization as a layer\n",
        "x = ActivityRegularizationLayer()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aadb58115c13"
      },
      "source": [
        "Este es el aspecto que debería tener ahora nuestro paso de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf674776a0d2"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "        # Add any extra losses created during the forward pass.\n",
        "        loss_value += sum(model.losses)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0af04732fe78"
      },
      "source": [
        "## Resumen\n",
        "\n",
        "Ahora ya conoce todo lo que hay que saber sobre el uso de los bucles de entrenamiento incorporados y sobre cómo escribir los suyos desde cero.\n",
        "\n",
        "Para concluir, le presentamos un ejemplo sencillo de extremo a extremo que une todo lo que ha aprendido en esta guía: un DCGAN entrenado con dígitos MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fb325331a1e"
      },
      "source": [
        "## Ejemplo de principio a fin: un bucle de entrenamiento GAN desde cero\n",
        "\n",
        "Es posible que conozca las Redes generativas adversariales (GAN). Las GAN pueden generar nuevas imágenes que parecen casi reales, ya que aprenden la distribución latente de un conjunto de datos de imágenes de entrenamiento (el \"espacio latente\" de las imágenes).\n",
        "\n",
        "Una GAN se compone de dos partes: un modelo \"generador\" que asigna puntos en el espacio latente a puntos en el espacio de la imagen, y un modelo \"discriminador\", un clasificador que puede diferenciar entre imágenes reales (del conjunto de datos de entrenamiento) e imágenes falsas (la salida de la red generadora).\n",
        "\n",
        "Un bucle de entrenamiento GAN tiene este aspecto:\n",
        "\n",
        "1. Entrene el discriminador.\n",
        "\n",
        "- Muestree un lote de puntos aleatorios en el espacio latente.\n",
        "- Convierta los puntos en imágenes falsas mediante el modelo \"generador\".\n",
        "- Obtenga un lote de imágenes reales y combínelas con las imágenes generadas.\n",
        "- Entrene el modelo \"discriminador\" para clasificar las imágenes generadas en comparación con las reales.\n",
        "\n",
        "1. Entrene al generador.\n",
        "\n",
        "- Muestree puntos aleatorios en el espacio latente.\n",
        "- Convierta los puntos en imágenes falsas mediante la red \"generadora\".\n",
        "- Obtenga un lote de imágenes reales y combínelas con las imágenes generadas.\n",
        "- Entrene el modelo \"generador\" para \"engañar\" al discriminador y clasificar las imágenes falsas como reales.\n",
        "\n",
        "Para obtener una visión mucho más detallada de cómo funcionan las GAN, consulte [Deep Learning con Python](https://www.manning.com/books/deep-learning-with-python).\n",
        "\n",
        "Implementemos este bucle de entrenamiento. En primer lugar, cree el discriminador destinado a clasificar los dígitos falsos en comparación con los reales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fabf9cef3400"
      },
      "outputs": [],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73396eb6daf9"
      },
      "source": [
        "A continuación, crearemos una red generadora que convierta los vectores latentes en salidas con el formato `(28, 28, 1)` (que representan los dígitos MNIST):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "821d203bfb3e"
      },
      "outputs": [],
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
        "        layers.Dense(7 * 7 * 128),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((7, 7, 128)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d6d54a78a0"
      },
      "source": [
        "A continuación, encontrará la parte más importante: el bucle de entrenamiento. Como puede ver, es bastante sencillo. La función de paso de entrenamiento solo ocupa 17 líneas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a11c875142e"
      },
      "outputs": [],
      "source": [
        "# Instantiate one optimizer for the discriminator and another for the generator.\n",
        "d_optimizer = keras.optimizers.Adam(learning_rate=0.0003)\n",
        "g_optimizer = keras.optimizers.Adam(learning_rate=0.0004)\n",
        "\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_images):\n",
        "    # Sample random points in the latent space\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "    # Decode them to fake images\n",
        "    generated_images = generator(random_latent_vectors)\n",
        "    # Combine them with real images\n",
        "    combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "    # Assemble labels discriminating real from fake images\n",
        "    labels = tf.concat(\n",
        "        [tf.ones((batch_size, 1)), tf.zeros((real_images.shape[0], 1))], axis=0\n",
        "    )\n",
        "    # Add random noise to the labels - important trick!\n",
        "    labels += 0.05 * tf.random.uniform(labels.shape)\n",
        "\n",
        "    # Train the discriminator\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = discriminator(combined_images)\n",
        "        d_loss = loss_fn(labels, predictions)\n",
        "    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
        "    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
        "\n",
        "    # Sample random points in the latent space\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "    # Assemble labels that say \"all real images\"\n",
        "    misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the generator (note that we should *not* update the weights\n",
        "    # of the discriminator)!\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = discriminator(generator(random_latent_vectors))\n",
        "        g_loss = loss_fn(misleading_labels, predictions)\n",
        "    grads = tape.gradient(g_loss, generator.trainable_weights)\n",
        "    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
        "    return d_loss, g_loss, generated_images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa6bd6292488"
      },
      "source": [
        "Vamos a entrenar nuestro GAN, llamando varias veces a `train_step` en lotes de imágenes.\n",
        "\n",
        "Ya que nuestro discriminador y generador son convnets, deseará ejecutar este código en una GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6a4e3d42262"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Prepare the dataset. We use both the training & test MNIST digits.\n",
        "batch_size = 64\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "epochs = 1  # In practice you need at least 20 epochs to generate nice digits.\n",
        "save_dir = \"./\"\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart epoch\", epoch)\n",
        "\n",
        "    for step, real_images in enumerate(dataset):\n",
        "        # Train the discriminator & generator on one batch of real images.\n",
        "        d_loss, g_loss, generated_images = train_step(real_images)\n",
        "\n",
        "        # Logging.\n",
        "        if step % 200 == 0:\n",
        "            # Print metrics\n",
        "            print(\"discriminator loss at step %d: %.2f\" % (step, d_loss))\n",
        "            print(\"adversarial loss at step %d: %.2f\" % (step, g_loss))\n",
        "\n",
        "            # Save one generated image\n",
        "            img = tf.keras.preprocessing.image.array_to_img(\n",
        "                generated_images[0] * 255.0, scale=False\n",
        "            )\n",
        "            img.save(os.path.join(save_dir, \"generated_img\" + str(step) + \".png\"))\n",
        "\n",
        "        # To limit execution time we stop after 10 steps.\n",
        "        # Remove the lines below to actually train the model!\n",
        "        if step > 10:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a92959ac630b"
      },
      "source": [
        "¡Listo! Obtendrá dígitos MNIST falsos de aspecto agradable después de solo ~30s de entrenamiento en la GPU Colab."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "writing_a_training_loop_from_scratch.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
