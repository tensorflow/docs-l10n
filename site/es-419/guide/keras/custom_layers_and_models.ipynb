{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2d97c7e31aa"
      },
      "source": [
        "# Cómo crear nuevas capas y modelos mediante subclases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e352274064f"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/custom_layers_and_models\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/custom_layers_and_models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/custom_layers_and_models.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver el código fuente en GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/keras/custom_layers_and_models.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4ac441b1fc"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e7dce39dd1d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b363673d96c"
      },
      "source": [
        "## La clase `Layer`: la combinación de estado (ponderaciones) y algún cálculo\n",
        "\n",
        "Una de las abstracciones centrales en Keras es la clase `Layer`. Una capa encapsula tanto un estado (las \"ponderaciones\" de la capa) como una transformación de entradas a salidas (una \"llamada\", el paso siguiente de la capa).\n",
        "\n",
        "A continuación se muestra una capa densamente conectada. Tiene un estado: las variables `w` y `b`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59b8317dbd3c"
      },
      "outputs": [],
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(\n",
        "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dac8fb03a642"
      },
      "source": [
        "Para utilizar una capa, es necesario llamarla a una entrada o entradas de tensor, como si se tratara de una función de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdcd15d5e68a"
      },
      "outputs": [],
      "source": [
        "x = tf.ones((2, 2))\n",
        "linear_layer = Linear(4, 2)\n",
        "y = linear_layer(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "382960020a56"
      },
      "source": [
        "Tenga en cuenta que los pesos `w` y `b` son rastreados automáticamente por la capa al establecerse como atributos de la capa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3d875af9465"
      },
      "outputs": [],
      "source": [
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9d72aa7538"
      },
      "source": [
        "Tenga en cuenta que también tiene acceso a un atajo más rápido para agregar ponderación a una capa: el método `add_weight()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "168548eba841"
      },
      "outputs": [],
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "x = tf.ones((2, 2))\n",
        "linear_layer = Linear(4, 2)\n",
        "y = linear_layer(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "070ea9b4db6c"
      },
      "source": [
        "## Las capas pueden tener ponderaciones que no se pueden entrenar\n",
        "\n",
        "Además de las ponderaciones que se pueden entrenar, también se pueden agregar ponderaciones que no se pueden entrenar en una capa. Se supone que estas ponderaciones no se tendrán en cuenta durante la retropropagación, cuando la capa esté en entrenamiento.\n",
        "\n",
        "A continuación se explica cómo agregar y utilizar una ponderación que no se puede entrenar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4cb404145f"
      },
      "outputs": [],
      "source": [
        "class ComputeSum(keras.layers.Layer):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ComputeSum, self).__init__()\n",
        "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
        "        return self.total\n",
        "\n",
        "\n",
        "x = tf.ones((2, 2))\n",
        "my_sum = ComputeSum(2)\n",
        "y = my_sum(x)\n",
        "print(y.numpy())\n",
        "y = my_sum(x)\n",
        "print(y.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40f5b74d3d87"
      },
      "source": [
        "Es parte de `layer.weights`, pero se clasifica como una ponderación que no se puede entrenar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d4db4ef4fa4"
      },
      "outputs": [],
      "source": [
        "print(\"weights:\", len(my_sum.weights))\n",
        "print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))\n",
        "\n",
        "# No se incluye en las ponderaciones que se pueden entrenar:\n",
        "print(\"trainable_weights:\", my_sum.trainable_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe6942aff7c6"
      },
      "source": [
        "## Práctica recomendada: aplazar la creación de las ponderaciones hasta que se conozca la forma de las entradas.\n",
        "\n",
        "Nuestra capa `Linear` anterior tomó un argumento `input_dim ` que se utilizó para calcular la forma de las ponderaciones `w` y `b` en `__init__()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "275b68d5ea9f"
      },
      "outputs": [],
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ebcacebb348"
      },
      "source": [
        "En muchos casos, es posible que no conozca de antemano el tamaño de sus entradas, y le gustaría crear ponderaciones libremente cuando se conozca ese valor, un poco después de establecer la instancia de la capa.\n",
        "\n",
        "En la API de Keras, recomendamos crear ponderaciones de capa en el método `build(self, inputs_shape)` de su capa. De la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "118c899f427e"
      },
      "outputs": [],
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78061e0583c6"
      },
      "source": [
        "El método `__call__()` de su capa ejecutará automáticamente la creación la primera vez que se llame. Ahora tiene una capa que es lenta y por lo tanto más fácil de usar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0697afb97bc1"
      },
      "outputs": [],
      "source": [
        "# En la creación de instancias, no sabemos en qué entradas va a recibir esta llamada\n",
        "linear_layer = Linear(32)\n",
        "\n",
        "# Las ponderaciones de la capa se crean dinámicamente la primera vez que llama a la capa\n",
        "y = linear_layer(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51b81f42b466"
      },
      "source": [
        "Implementar `build()` por separado como se muestra arriba separa bien la creación de ponderaciones solo una vez de su uso en cada llamada. Sin embargo, para algunas capas personalizadas avanzadas, puede resultar poco práctico separar la creación del estado y el cálculo. Los implementadores de capas pueden diferir en la creación de las ponderaciones a la primera `__call__()`, pero deben tener cuidado de que las llamadas posteriores utilicen las mismas ponderaciones. Además, dado que `__call__()` es probable que se ejecute por primera vez dentro de una `tf.function`, cualquier creación de una variable que tenga lugar en `__call__()` debería estar envuelta en un`tf.init_scope`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b7a45f57610"
      },
      "source": [
        "## Las capas se pueden componer recursivamente\n",
        "\n",
        "Si asigna una instancia de una capa como atributo de otra capa, la capa externa comenzará a seguir las ponderaciones creadas por la capa interna.\n",
        "\n",
        "Le recomendamos crear dichas subcapas en el método `__init__()` y dejar que sea el primer `__call__()` el que active la creación de sus ponderaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aaaf82ab8ce"
      },
      "outputs": [],
      "source": [
        "class MLPBlock(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(MLPBlock, self).__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.linear_2 = Linear(32)\n",
        "        self.linear_3 = Linear(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "\n",
        "mlp = MLPBlock()\n",
        "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
        "print(\"weights:\", len(mlp.weights))\n",
        "print(\"trainable weights:\", len(mlp.trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf11b296bd2"
      },
      "source": [
        "## El método `add_loss()`\n",
        "\n",
        "Cuando escriba el método `call()` de una capa, puede crear tensores de pérdida que querrá utilizar más tarde, cuando escriba su bucle de entrenamiento. Esto se puede hacer llamando a `self.add_loss(value)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba2782dc0879"
      },
      "outputs": [],
      "source": [
        "# A layer that creates an activity regularization loss\n",
        "class ActivityRegularizationLayer(keras.layers.Layer):\n",
        "    def __init__(self, rate=1e-2):\n",
        "        super(ActivityRegularizationLayer, self).__init__()\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a883b230a9e9"
      },
      "source": [
        "Estas pérdidas (incluyendo las creadas por cualquier capa interna) pueden recuperarse mediante `layer.losses`. Esta propiedad se restablece al inicio de cada `__call__()` a la capa de nivel superior, de modo que `layer.losses` siempre contiene los valores de pérdidas creados durante el último pase siguiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b56d223a30cd"
      },
      "outputs": [],
      "source": [
        "class OuterLayer(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(OuterLayer, self).__init__()\n",
        "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.activity_reg(inputs)\n",
        "\n",
        "\n",
        "layer = OuterLayer()\n",
        "assert len(layer.losses) == 0  # No losses yet since the layer has never been called\n",
        "\n",
        "_ = layer(tf.zeros(1, 1))\n",
        "assert len(layer.losses) == 1  # We created one loss value\n",
        "\n",
        "# `layer.losses` se reinicia al comenzar cada __call__\n",
        "_ = layer(tf.zeros(1, 1))\n",
        "assert len(layer.losses) == 1  # This is the loss created during the call above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0809dec680ff"
      },
      "source": [
        "Además, la propiedad `loss` también contiene pérdidas obtenidas por regularización para las ponderaciones de cualquier capa interna:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41016153e983"
      },
      "outputs": [],
      "source": [
        "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
        "        self.dense = keras.layers.Dense(\n",
        "            32, kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "\n",
        "\n",
        "layer = OuterLayerWithKernelRegularizer()\n",
        "_ = layer(tf.zeros((1, 1)))\n",
        "\n",
        "# This is `1e-3 * sum(layer.dense.kernel ** 2)`,\n",
        "# creado por el `kernel_regularizer` anterior.\n",
        "print(layer.losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "589465e06e4f"
      },
      "source": [
        "Estas pérdidas deben tenerse en cuenta al escribir bucles de entrenamiento, como éste:\n",
        "\n",
        "```python\n",
        "# Instancia de un optimizador.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Itera sobre los lotes de un conjunto de datos.\n",
        "for x_batch_train, y_batch_train in train_dataset:\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = layer(x_batch_train)  # Logits for this minibatch\n",
        "    # El valor de la pérdida para este minilote\n",
        "    loss_value = loss_fn(y_batch_train, logits)\n",
        "    # Agrega las pérdidas adicionales creadas durante este pase siguiente:\n",
        "    loss_value += sum(model.losses)\n",
        "\n",
        "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fb41ca8c3b0"
      },
      "source": [
        "Para obtener una guía detallada sobre la escritura de bucles de entrenamiento, consulte la [guía para escribir un bucle de entrenamiento desde cero](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/).\n",
        "\n",
        "Estas pérdidas también funcionan perfectamente con `fit()` (se suman automáticamente y se agregan a la pérdida principal, si la hay):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "769bc6612ebf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = keras.Input(shape=(3,))\n",
        "outputs = ActivityRegularizationLayer()(inputs)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Si pasa una pérdida en `compilar`, la regularización\n",
        "# se le agregan las pérdidas\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
        "\n",
        "# También es posible no pasar ninguna pérdida en `compile`,\n",
        "# ya que el modelo ya tiene una pérdida que minimizar, mediante la función `add_loss`\n",
        "# ¡llame durante el pase siguiente!\n",
        "model.compile(optimizer=\"adam\")\n",
        "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "149c71e442bb"
      },
      "source": [
        "## El método `add_metric()`\n",
        "\n",
        "De forma similar a `add_loss()`, las capas también tienen un método `add_metric()` para seguir el promedio móvil de una cantidad durante el entrenamiento.\n",
        "\n",
        "Consideremos la siguiente capa: una capa de \"punto final logístico\". Toma como entradas predicciones y objetivos, calcula una pérdida que rastrea mediante `add_loss()`, y calcula un escalar de precisión, que se rastrea mediante `add_metric()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfb2df515096"
      },
      "outputs": [],
      "source": [
        "class LogisticEndpoint(keras.layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super(LogisticEndpoint, self).__init__(name=name)\n",
        "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    def call(self, targets, logits, sample_weights=None):\n",
        "        # Calcule el valor de la pérdida en el periodo de entrenamiento y súmelo\n",
        "        # a la capa utilizando `self.add_loss()`.\n",
        "        loss = self.loss_fn(targets, logits, sample_weights)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # Registre la precisión como métrica y agreguela\n",
        "        # a la capa mediante `self.add_metric()`.\n",
        "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
        "        self.add_metric(acc, name=\"accuracy\")\n",
        "\n",
        "        # Devuelve el tensor de predicción en tiempo de inferencia (a `.predict()`).\n",
        "        return tf.nn.softmax(logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e68f88373800"
      },
      "source": [
        "Las métricas rastreadas de este modo son accesibles mediante `layer.metrics`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1834d74450b6"
      },
      "outputs": [],
      "source": [
        "layer = LogisticEndpoint()\n",
        "\n",
        "targets = tf.ones((2, 2))\n",
        "logits = tf.ones((2, 2))\n",
        "y = layer(targets, logits)\n",
        "\n",
        "print(\"layer.metrics:\", layer.metrics)\n",
        "print(\"current accuracy value:\", float(layer.metrics[0].result()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546cfbd4ea05"
      },
      "source": [
        "Al igual que en `add_loss()`, estas métricas se rastrean mediante `fit()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5e74cb4da34"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
        "targets = keras.Input(shape=(10,), name=\"targets\")\n",
        "logits = keras.layers.Dense(10)(inputs)\n",
        "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
        "\n",
        "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
        "model.compile(optimizer=\"adam\")\n",
        "\n",
        "data = {\n",
        "    \"inputs\": np.random.random((3, 3)),\n",
        "    \"targets\": np.random.random((3, 10)),\n",
        "}\n",
        "model.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4012fa8683e5"
      },
      "source": [
        "## Puede activar opcionalmente la serialización en sus capas\n",
        "\n",
        "Si es necesario que sus capas personalizadas se puedan serializar como parte de un {[Functional model](https://www.tensorflow.org/guide/keras/functional/), puede implementar opcionalmente un método `get_config()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a720cbd5f54"
      },
      "outputs": [],
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"units\": self.units}\n",
        "\n",
        "\n",
        "# Now you can recreate the layer from its config:\n",
        "layer = Linear(64)\n",
        "config = layer.get_config()\n",
        "print(config)\n",
        "new_layer = Linear.from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b43aad6c145"
      },
      "source": [
        "Tenga en cuenta que el método `__init__()` de la clase base `Layer` toma algunos argumentos de la palabra clave, en particular un `name` y un `dtype`. Es una práctica recomendada pasar estos argumentos a la clase padre en `__init__()` e incluirlos en la configuración de la capa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cbad8a6e6cd"
      },
      "outputs": [],
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, **kwargs):\n",
        "        super(Linear, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Linear, self).get_config()\n",
        "        config.update({\"units\": self.units})\n",
        "        return config\n",
        "\n",
        "\n",
        "layer = Linear(64)\n",
        "config = layer.get_config()\n",
        "print(config)\n",
        "new_layer = Linear.from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2421f80b5b86"
      },
      "source": [
        "Si necesita mayor flexibilidad para deserializar la capa desde su configuración, también puede sobreescribir el método de la clase `from_config()`. Esta es la implementación base de `from_config()`:\n",
        "\n",
        "```python\n",
        "def from_config(cls, config):\n",
        "  return cls(**config)\n",
        "```\n",
        "\n",
        "Para obtener más información sobre la serialización y el guardado, consulte la [guía completa para guardar y serializar modelos](https://www.tensorflow.org/guide/keras/save_and_serialize/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d7e2304a047"
      },
      "source": [
        "## Argumento `training` privilegiado en el método `call()`\n",
        "\n",
        "Algunas capas, en particular la capa `BatchNormalization` y la capa `Dropout`, tienen comportamientos diferentes durante el entrenamiento y la inferencia. Para estas capas, es una práctica habitual como introducir un argumento `training` (booleano) en el método `call()`.\n",
        "\n",
        "Al exponer este argumento en `call()`, permite que los bucles de entrenamiento y evaluación incorporados (por ejemplo `fit()`) utilicen correctamente la capa en el entrenamiento y la inferencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a169812c2c00"
      },
      "outputs": [],
      "source": [
        "class CustomDropout(keras.layers.Layer):\n",
        "    def __init__(self, rate, **kwargs):\n",
        "        super(CustomDropout, self).__init__(**kwargs)\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            return tf.nn.dropout(inputs, rate=self.rate)\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e1482c9f010"
      },
      "source": [
        "## Argumento `mask` privilegiado en el método `call()`\n",
        "\n",
        "El otro argumento privilegiado soportado por `call()` es el argumento `mask`.\n",
        "\n",
        "Lo encontrará en todas las capas RNN de Keras. Una máscara es un tensor booleano (un valor booleano por paso de tiempo en la entrada) utilizado para omitir ciertos pasos de tiempo de entrada al procesar datos de series temporales.\n",
        "\n",
        "Keras pasará automáticamente el argumento `mask` correcto a `__call__()` para las capas que lo admitan, cuando una capa anterior genere una máscara. Las capas que generan máscaras son la capa `Embedding` configurada con `mask_zero=True`, y la capa `Masking`.\n",
        "\n",
        "Para obtener más información sobre el enmascaramiento y cómo escribir capas con enmascaramiento, consulte la guía [\"cómo comprender el relleno y el enmascaramiento\"](https://www.tensorflow.org/guide/keras/masking_and_padding/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "344110f9e134"
      },
      "source": [
        "## La clase `Model`\n",
        "\n",
        "En general, utilizará la clase `Layer` para definir los bloques del cálculo interno y utilizará la clase `Model` para definir el modelo externo, el objeto que entrenará.\n",
        "\n",
        "Por ejemplo, en un modelo ResNet50, tendría varios bloques ResNet que subclasificarían `Layer`, y un único `Model` que abarcaría toda la red ResNet50.\n",
        "\n",
        "La clase `Model` tiene la misma API que `Layer`, con las siguientes diferencias:\n",
        "\n",
        "- Expone bucles de entrenamiento, evaluación y predicción incorporados (`model.fit()`, `model.evaluate()`, `model.predict()`).\n",
        "- Expone la lista de sus capas internas, mediante la propiedad `model.layers`.\n",
        "- Expone las API de guardado y serialización (`save()`, `save_weights()`...)\n",
        "\n",
        "En efecto, la clase `Layer` corresponde a lo que en la literatura se denomina \"capa\" (como en \"capa de convolución\" o \"capa recurrente\") o \"bloque\" (como en \"bloque ResNet\" o \"bloque Inception\").\n",
        "\n",
        "Por otra parte, la clase `Model` corresponde a lo que en la literatura se denomina \"modelo\" (como en \"modelo de aprendizaje profundo\") o \"red\" (como en \"red neuronal profunda\").\n",
        "\n",
        "Así que si se pregunta \"¿debería usar la clase `Layer` o la clase `Model`?\", pregúntese: ¿necesitaré llamar a `fit()` en ella? ¿Tendré que llamar a `save()`? Si es así, utilice `Model`. Si no es así (ya sea porque su clase es sólo un bloque en un sistema más grande, o porque está escribiendo el código de entrenamiento y guardado usted mismo), utilice `Layer`.\n",
        "\n",
        "Por ejemplo, podríamos tomar nuestro ejemplo de mini-resnet anterior, y utilizarlo para construir un `Model` que podríamos entrenar con `fit()`, y que podríamos guardar con `save_weights()`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09caa642b72e"
      },
      "source": [
        "```python\n",
        "class ResNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.block_1 = ResNetBlock()\n",
        "        self.block_2 = ResNetBlock()\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.classifier = Dense(num_classes)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.block_1(inputs)\n",
        "        x = self.block_2(x)\n",
        "        x = self.global_pool(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "resnet = ResNet()\n",
        "dataset = ...\n",
        "resnet.fit(dataset, epochs=10)\n",
        "resnet.save(filepath)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2e32d225a1b"
      },
      "source": [
        "## Todo en uno: un ejemplo de principio a fin\n",
        "\n",
        "Esto es lo que ha aprendido hasta ahora:\n",
        "\n",
        "- A `Layer` encapsulate a state (created in `__init__()` or `build()`) and some computation (defined in `call()`).\n",
        "- Las capas pueden anidarse recursivamente para crear nuevos bloques de cálculo más grandes.\n",
        "- Las capas pueden crear y rastrear pérdidas (normalmente pérdidas de regularización), así como métricas, mediante `add_loss()` y `add_metric()`.\n",
        "- El contenedor externo, lo que desea entrenar, es un `Model`. Un `Model` es igual que un `Layer`, pero con utilidades adicionales de entrenamiento y serialización.\n",
        "\n",
        "Pongamos todas estas cosas juntas en un ejemplo de principio a fin: vamos a implementar un Autoencoder Variacional (VAE). Lo entrenaremos con dígitos MNIST.\n",
        "\n",
        "Nuestra VAE será una subclase de `Model`, construida como una composición anidada de capas que subclasifican a `Layer`. Incluirá una pérdida de regularización (divergencia KL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56aaae7af872"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
        "        self.dense_mean = layers.Dense(latent_dim)\n",
        "        self.dense_log_var = layers.Dense(latent_dim)\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense_proj(inputs)\n",
        "        z_mean = self.dense_mean(x)\n",
        "        z_log_var = self.dense_log_var(x)\n",
        "        z = self.sampling((z_mean, z_log_var))\n",
        "        return z_mean, z_log_var, z\n",
        "\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
        "\n",
        "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
        "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense_proj(inputs)\n",
        "        return self.dense_output(x)\n",
        "\n",
        "\n",
        "class VariationalAutoEncoder(keras.Model):\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_dim,\n",
        "        intermediate_dim=64,\n",
        "        latent_dim=32,\n",
        "        name=\"autoencoder\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
        "        self.original_dim = original_dim\n",
        "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
        "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        # Add KL divergence regularization loss.\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
        "        )\n",
        "        self.add_loss(kl_loss)\n",
        "        return reconstructed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f8ae035a7c9"
      },
      "source": [
        "Escriba un bucle de entrenamiento simple en MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40f11d1ef3bc"
      },
      "outputs": [],
      "source": [
        "original_dim = 784\n",
        "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "# Iterate over epochs.\n",
        "for epoch in range(epochs):\n",
        "    print(\"Start of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, x_batch_train in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstructed = vae(x_batch_train)\n",
        "            # Compute reconstruction loss\n",
        "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
        "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
        "\n",
        "        grads = tape.gradient(loss, vae.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "\n",
        "        loss_metric(loss)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d65fae5d3d"
      },
      "source": [
        "Tenga en cuenta que, dado que la VAE es una subclase de `Model`, incorpora bucles de entrenamiento. Así que también podría haberlo entrenado así:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5af13f70d528"
      },
      "outputs": [],
      "source": [
        "vae = VariationalAutoEncoder(784, 64, 32)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d34b7ba21662"
      },
      "source": [
        "## Más allá del desarrollo orientado a objetos: la API funcional\n",
        "\n",
        "¿Le resultó este ejemplo demasiado orientado a objetos? También puede construir modelos utilizando la [API Funcional](https://www.tensorflow.org/guide/keras/functional/). Es importante destacar que la elección de un estilo u otro no le impide aprovechar los componentes escritos en el otro estilo: siempre puede mezclar y hacer las combinaciones que desee.\n",
        "\n",
        "Por ejemplo, el siguiente ejemplo de API funcional reutiliza la misma capa `Sampling` que definimos en el ejemplo anterior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be77fc8f9b26"
      },
      "outputs": [],
      "source": [
        "original_dim = 784\n",
        "intermediate_dim = 64\n",
        "latent_dim = 32\n",
        "\n",
        "# Define encoder model.\n",
        "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
        "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()((z_mean, z_log_var))\n",
        "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
        "\n",
        "# Define decoder model.\n",
        "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
        "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
        "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
        "\n",
        "# Define VAE model.\n",
        "outputs = decoder(z)\n",
        "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
        "\n",
        "# Add KL divergence regularization loss.\n",
        "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "vae.add_loss(kl_loss)\n",
        "\n",
        "# Train.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2f135ea7cf5"
      },
      "source": [
        "Para obtener más información, asegúrese de leer la [Guía funcional de la API](https://www.tensorflow.org/guide/keras/functional/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "custom_layers_and_models.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
