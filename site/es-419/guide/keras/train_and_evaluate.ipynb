{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb291b62b1aa"
      },
      "source": [
        "# Entrenamiento y evaluación con los métodos integrados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1820d9bdfb9"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver en TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/train_and_evaluate.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Ejecutar en Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver código fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4ac441b1fc"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0472bf67b2bf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfdc6f08988e"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Esta guía cubre los modelos de entrenamiento, evaluación y predicción (inferencia) cuando se utilizan las API incorporadas para el entrenamiento y la validación (como `Model.fit()`, `Model.evaluate()` y `Model.predict()`).\n",
        "\n",
        "Si está interesado en aprovechar `fit()` mientras especifica su propia función de pasos de entrenamiento, consulte la guía <a href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/\" data-md-type=\"link\">Personalización de lo que ocurre en `fit()`</a>.\n",
        "\n",
        "Si está interesado en escribir sus propios bucles de entrenamiento y evaluación desde cero, consulte la guía [\"escribir un bucle de entrenamiento desde cero\"](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/).\n",
        "\n",
        "En general, tanto si utiliza bucles incorporados como si escribe los suyos propios, el entrenamiento y la evaluación de modelos funcionan estrictamente de la misma manera en todos los tipos de modelos Keras: modelos secuenciales, modelos construidos con la API funcional y modelos escritos desde cero mediante subclases de modelos.\n",
        "\n",
        "Esta guía no cubre el entrenamiento distribuido, que está cubierto en nuestra [guía de multi-GPU &amp; entrenamiento distribuido](https://keras.io/guides/distributed_training/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e270faa413e"
      },
      "source": [
        "## Visión general de la API: un primer ejemplo de extremo a extremo\n",
        "\n",
        "Cuando pase datos a los bucles de entrenamiento incorporados de un modelo, debería utilizar matrices **NumPy** (si sus datos son pequeños y caben en la memoria) u objetos **`tf.data Dataset`**. En los próximos párrafos, utilizaremos el conjunto de datos MNIST como matrices NumPy, con el fin de demostrar cómo utilizar optimizadores, pérdidas y métricas.\n",
        "\n",
        "Consideremos el siguiente modelo (aquí, construimos con la API Funcional, pero podría ser un modelo Secuencial o un modelo subclaseado también):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "170a6a18b2a3"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6d5724a90ab"
      },
      "source": [
        "Este es el flujo de trabajo típico de principio a fin:\n",
        "\n",
        "- Entrenamiento\n",
        "- Validación en un conjunto de retención generado a partir de los datos de entrenamiento originales\n",
        "- Evaluación de los datos de las pruebas\n",
        "\n",
        "Para este ejemplo utilizaremos datos MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b55b3903edb"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data (these are NumPy arrays)\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
        "\n",
        "y_train = y_train.astype(\"float32\")\n",
        "y_test = y_test.astype(\"float32\")\n",
        "\n",
        "# Reserve 10,000 samples for validation\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77a84eb1985b"
      },
      "source": [
        "Especificamos la configuración de entrenamiento (optimizador, pérdida, métrica):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26a7f1819796"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
        "    # Loss function to minimize\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    # List of metrics to monitor\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58dc05fa2736"
      },
      "source": [
        "Llamamos a `fit()`, que entrenará el modelo cortando los datos en \"lotes\" de tamaño `batch_size`, e iterando repetidamente sobre todo el conjunto de datos para obtener un número determinado de `epochs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b92f67b105e"
      },
      "outputs": [],
      "source": [
        "print(\"Fit model on training data\")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=2,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    validation_data=(x_val, y_val),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "896edfc3d7c4"
      },
      "source": [
        "El objeto `history` devuelto contiene un registro de los valores de pérdida y los valores métricos durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a20b8f5b9fcc"
      },
      "outputs": [],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6105b646df66"
      },
      "source": [
        "Evaluamos el modelo en los datos de prueba mediante `evaluate()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69f524a93f9d"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "# Generate predictions (probabilities -- the output of the last layer)\n",
        "# on new data using `predict`\n",
        "print(\"Generate predictions for 3 samples\")\n",
        "predictions = model.predict(x_test[:3])\n",
        "print(\"predictions shape:\", predictions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f19d074eb88c"
      },
      "source": [
        "Ahora, revisemos cada pieza de este flujo de trabajo en detalle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3669f026d14"
      },
      "source": [
        "## El método `compile()`: especifica una pérdida, métricas y un optimizador\n",
        "\n",
        "Para entrenar un modelo con `fit()`, necesita especificar una función de pérdida, un optimizador y, de forma opcional, algunas métricas que monitorear.\n",
        "\n",
        "Se pasan al modelo como argumentos del método `compile()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb7a8deb494c"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b934b428dc43"
      },
      "source": [
        "El argumento `metrics` debe ser una lista -- su modelo puede tener cualquier número de métricas.\n",
        "\n",
        "Si su modelo tiene múltiples salidas, puede especificar diferentes pérdidas y métricas para cada salida, y puede modular la contribución de cada salida a la pérdida total del modelo. Encontrará más detalles al respecto en la sección **Paso de datos a modelos con múltiples entradas y salidas**.\n",
        "\n",
        "Tenga en cuenta que si está satisfecho con la configuración predeterminada, en muchos casos el optimizador, la pérdida y las métricas se pueden especificar mediante identificadores de cadena como un acceso directo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6444839ff300"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5493ab963254"
      },
      "source": [
        "Para su posterior re-uso, pongamos nuestra definición del modelo y el paso de compilación en funciones; las llamaremos varias veces por medio de diferentes ejemplos en esta guía."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31c3e3c70f06"
      },
      "outputs": [],
      "source": [
        "def get_uncompiled_model():\n",
        "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_compiled_model():\n",
        "    model = get_uncompiled_model()\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"sparse_categorical_accuracy\"],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "535137cf19b2"
      },
      "source": [
        "### Hay disponibles muchos optimizadores, pérdidas y métricas integrados.\n",
        "\n",
        "En general, no tendrá que crear sus propias pérdidas, métricas u optimizadores desde cero, porque es probable que lo que necesite ya forme parte de la API de Keras:\n",
        "\n",
        "Optimizadores:\n",
        "\n",
        "- `SGD()` (con o sin momentum)\n",
        "- `RMSprop()`\n",
        "- `Adam()`\n",
        "- etc.\n",
        "\n",
        "Pérdidas:\n",
        "\n",
        "- `MeanSquaredError()`\n",
        "- `KLDivergence()`\n",
        "- `CosineSimilarity()`\n",
        "- etc.\n",
        "\n",
        "Métricas:\n",
        "\n",
        "- `AUC()`\n",
        "- `Precision()`\n",
        "- `Recall()`\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdc4c3d72a21"
      },
      "source": [
        "### Pérdidas personalizadas\n",
        "\n",
        "Si necesita crear una pérdida personalizada, Keras proporciona dos formas de hacerlo.\n",
        "\n",
        "El primer método consiste en crear una función que acepte las entradas `y_true` y `y_pred`. El siguiente ejemplo muestra una función de pérdida que calcula el error cuadrático promedio entre los datos reales y las predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc4edd47bb5a"
      },
      "outputs": [],
      "source": [
        "def custom_mean_squared_error(y_true, y_pred):\n",
        "    return tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "\n",
        "model = get_uncompiled_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\n",
        "\n",
        "# We need to one-hot encode the labels to use MSE\n",
        "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
        "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25b9fa7941ca"
      },
      "source": [
        "Si necesita una función de pérdida que tome parámetros además de `y_true` y `y_pred`, puede subclasificar la clase `tf.keras.losses.Loss` e implementar los dos siguientes métodos:\n",
        "\n",
        "- `__init__(self)`: aceptar parámetros para pasar durante la llamada de su función de pérdida\n",
        "- `call(self, y_true, y_pred)`: utilizar los objetivos (y_true) y las predicciones del modelo (y_pred) para calcular la pérdida del modelo\n",
        "\n",
        "Digamos que desea utilizar el error cuadrático promedio, pero con un término adicional que desincentivará los valores de predicción alejados de 0.5 (suponemos que los objetivos categóricos se codifican con un solo disparo y toman valores entre 0 y 1). Esto crea un incentivo para que el modelo no se confíe demasiado, lo que puede ayudar a reducir el sobreajuste (¡no sabremos si funciona hasta que lo probemos!).\n",
        "\n",
        "Así es como debe hacerlo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b09463a8c568"
      },
      "outputs": [],
      "source": [
        "class CustomMSE(keras.losses.Loss):\n",
        "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
        "        super().__init__(name=name)\n",
        "        self.regularization_factor = regularization_factor\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
        "        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
        "        return mse + reg * self.regularization_factor\n",
        "\n",
        "\n",
        "model = get_uncompiled_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n",
        "\n",
        "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
        "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2d7d358b4eb"
      },
      "source": [
        "### Métricas personalizadas\n",
        "\n",
        "Si necesita una métrica que no forma parte de la API, puede crear fácilmente métricas personalizadas subclasificando la clase `tf.keras.metrics.Metric`. Necesitará implementar 4 métodos:\n",
        "\n",
        "- `__init__(self)`, en el que creará variables de estado para su métrica.\n",
        "- `update_state(self, y_true, y_pred, sample_weight=None)`, que utiliza los objetivos y_true y las predicciones del modelo y_pred para actualizar las variables de estado.\n",
        "- `result(self)`, que utiliza las variables de estado para calcular los resultados finales.\n",
        "- `reset_state(self)`, que reinicializa el estado de la métrica.\n",
        "\n",
        "La actualización del estado y el cálculo de resultados se mantienen separados (en `update_state()` y `result()`, respectivamente) porque en algunos casos, el cálculo de resultados puede ser muy costoso y solo se realizaría periódicamente.\n",
        "\n",
        "A continuación se muestra un ejemplo sencillo de cómo implementar una métrica `CategoricalTruePositives` que cuenta cuántas muestras se clasificaron correctamente como si pertenecieran a una clase determinada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ad9c57c0683"
      },
      "outputs": [],
      "source": [
        "class CategoricalTruePositives(keras.metrics.Metric):\n",
        "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
        "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
        "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
        "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
        "        values = tf.cast(values, \"float32\")\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
        "            values = tf.multiply(values, sample_weight)\n",
        "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
        "\n",
        "    def result(self):\n",
        "        return self.true_positives\n",
        "\n",
        "    def reset_state(self):\n",
        "        # The state of the metric will be reset at the start of each epoch.\n",
        "        self.true_positives.assign(0.0)\n",
        "\n",
        "\n",
        "model = get_uncompiled_model()\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[CategoricalTruePositives()],\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1547a2d92f6a"
      },
      "source": [
        "### Manejo de pérdidas y métricas que no se ajustan a la firma estándar\n",
        "\n",
        "La inmensa mayoría de las pérdidas y métricas pueden calcularse a partir de `y_true` y `y_pred`, donde `y_pred` es una salida del modelo, pero no todas. Por ejemplo, una pérdida de regularización puede requerir solo la activación de una capa (no hay objetivos en este caso), y esta activación puede no ser una salida del modelo.\n",
        "\n",
        "En tales casos, puede llamar a `self.add_loss(loss_value)` desde dentro del método de llamada de una capa personalizada. Las pérdidas agregadas de esta manera se suman a la pérdida \"principal\" durante el entrenamiento (la que se pasa a `compile()`). Aquí hay un ejemplo simple que agrega regularización de la actividad (tenga en cuenta que la regularización de la actividad está incorporada en todas las capas de Keras - esta capa es solo para proporcionar un ejemplo concreto):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b494d47437a0"
      },
      "outputs": [],
      "source": [
        "class ActivityRegularizationLayer(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
        "        return inputs  # Pass-through layer.\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "\n",
        "# Insert activity regularization as a layer\n",
        "x = ActivityRegularizationLayer()(x)\n",
        "\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "# The displayed loss will be much higher than before\n",
        "# due to the regularization component.\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaebb5829011"
      },
      "source": [
        "Puede hacer lo mismo para registrar valores métricos, utilizando `add_metric()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa58091be092"
      },
      "outputs": [],
      "source": [
        "class MetricLoggingLayer(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        # The `aggregation` argument defines\n",
        "        # how to aggregate the per-batch values\n",
        "        # over each epoch:\n",
        "        # in this case we simply average them.\n",
        "        self.add_metric(\n",
        "            keras.backend.std(inputs), name=\"std_of_activation\", aggregation=\"mean\"\n",
        "        )\n",
        "        return inputs  # Pass-through layer.\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "\n",
        "# Insert std logging as a layer.\n",
        "x = MetricLoggingLayer()(x)\n",
        "\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3c18154d057"
      },
      "source": [
        "En la [API funcional](https://www.tensorflow.org/guide/keras/functional/), también puede llamar a `model.add_loss(loss_tensor)`, o a `model.add_metric(metric_tensor, name, aggregation)`.\n",
        "\n",
        "Este es un ejemplo sencillo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e19afe78b3a"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x1)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
        "\n",
        "model.add_metric(keras.backend.std(x1), name=\"std_of_activation\", aggregation=\"mean\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b06d48035369"
      },
      "source": [
        "Tenga en cuenta que cuando se pasan las pérdidas mediante `add_loss()`, se hace posible llamar a `compile()` sin una función de pérdida, ya que el modelo ya tiene una pérdida que minimizar.\n",
        "\n",
        "Considere la siguiente capa `LogisticEndpoint`: toma como entradas objetivos y logits, y rastrea una pérdida de entropía cruzada mediante `add_loss()`. También realiza un seguimiento de la precisión de la clasificación mediante `add_metric()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d56d2c504258"
      },
      "outputs": [],
      "source": [
        "class LogisticEndpoint(keras.layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super(LogisticEndpoint, self).__init__(name=name)\n",
        "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    def call(self, targets, logits, sample_weights=None):\n",
        "        # Compute the training-time loss value and add it\n",
        "        # to the layer using `self.add_loss()`.\n",
        "        loss = self.loss_fn(targets, logits, sample_weights)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # Log accuracy as a metric and add it\n",
        "        # to the layer using `self.add_metric()`.\n",
        "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
        "        self.add_metric(acc, name=\"accuracy\")\n",
        "\n",
        "        # Return the inference-time prediction tensor (for `.predict()`).\n",
        "        return tf.nn.softmax(logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0698f3c98cbe"
      },
      "source": [
        "Puede utilizarlo en un modelo con dos entradas (datos de entrada y objetivos), compilado sin un argumento `loss`, así:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f6842f2bbe6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
        "targets = keras.Input(shape=(10,), name=\"targets\")\n",
        "logits = keras.layers.Dense(10)(inputs)\n",
        "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
        "\n",
        "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
        "model.compile(optimizer=\"adam\")  # No loss argument!\n",
        "\n",
        "data = {\n",
        "    \"inputs\": np.random.random((3, 3)),\n",
        "    \"targets\": np.random.random((3, 10)),\n",
        "}\n",
        "model.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328b021aa6b8"
      },
      "source": [
        "Para obtener más información sobre el entrenamiento de modelos multientrada, consulte la sección **Paso de datos a modelos multientrada y multisalida**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9e6ea0045c9"
      },
      "source": [
        "### Apartar automáticamente un conjunto de retención de validación\n",
        "\n",
        "En el primer ejemplo de extremo a extremo que vio, usamos el argumento `validation_data` para pasar una tupla de matrices NumPy `(x_val, y_val)` al modelo para evaluar una pérdida de validación y métricas de validación al final de cada época.\n",
        "\n",
        "Aquí hay otra opción: el argumento `validation_split` permite reservar automáticamente parte de los datos de entrenamiento para realizar la validación. El valor del argumento representa la fracción de los datos que se reservará para la validación, por lo que debe establecerse en un número superior a 0 e inferior a 1. Por ejemplo, `validation_split=0.2` significa \"utilizar el 20% de los datos para la validación\", y `validation_split=0.6` significa \"utilizar el 60% de los datos para la validación\".\n",
        "\n",
        "La forma en que se calcula la validación es tomar las últimas muestras x% de las matrices recibidas por la llamada `fit()`, antes de mezclar.\n",
        "\n",
        "Tenga en cuenta que solo puede utilizar `validation_split` cuando entrene con datos NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "232fd59c751b"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "770d19613c53"
      },
      "source": [
        "## Entrenamiento y evaluación a partir de conjuntos de datos tf.data\n",
        "\n",
        "En los últimos párrafos, habrá visto cómo manejar pérdidas, métricas y optimizadores, y habrá visto cómo usar los argumentos `validation_data` y `validation_split` en `fit()`, cuando sus datos se pasan como matrices NumPy.\n",
        "\n",
        "Veamos ahora el caso en el que los datos se presentan en forma de objeto `tf.data.Dataset`.\n",
        "\n",
        "La API `tf.data` es un conjunto de utilidades de TensorFlow 2.0 para cargar y preprocesar datos de forma rápida y escalable.\n",
        "\n",
        "Para obtener una guía completa sobre la creación de `Datasets`, consulte la [documentación tf.data](https://www.tensorflow.org/guide/data).\n",
        "\n",
        "Puede pasar una instancia `Dataset` directamente a los métodos `fit()`, `evaluate()` y `predict()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bf4ded224f8"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# First, let's create a training Dataset instance.\n",
        "# For the sake of our example, we'll use the same MNIST data as before.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "# Shuffle and slice the dataset.\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Now we get a test dataset.\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(64)\n",
        "\n",
        "# Since the dataset already takes care of batching,\n",
        "# we don't pass a `batch_size` argument.\n",
        "model.fit(train_dataset, epochs=3)\n",
        "\n",
        "# You can also evaluate or predict on a dataset.\n",
        "print(\"Evaluate\")\n",
        "result = model.evaluate(test_dataset)\n",
        "dict(zip(model.metrics_names, result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "421d16914ce3"
      },
      "source": [
        "Tenga en cuenta que el conjunto de datos se restablece al final de cada época, por lo que puede volver a utilizarse en la siguiente.\n",
        "\n",
        "Si desea ejecutar el entrenamiento solo en un número específico de lotes de este conjunto de datos, puede pasar el argumento `steps_per_epoch`, que especifica cuántos pasos de entrenamiento debe ejecutar el modelo utilizando este conjunto de datos antes de pasar a la siguiente época.\n",
        "\n",
        "Si se hace esto, el conjunto de datos no se reinicia al final de cada época, sino que se siguen dibujando los siguientes lotes. El conjunto de datos eventualmente se quedará sin datos (a menos que sea un conjunto de datos de bucle infinito)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "273c5dff16b4"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Only use the 100 batches per epoch (that's 64 * 100 samples)\n",
        "model.fit(train_dataset, epochs=3, steps_per_epoch=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2dcd180da7b"
      },
      "source": [
        "### Utilizar un conjunto de datos de validación\n",
        "\n",
        "Puede pasar una instancia de `Dataset` como argumento de `validation_data` en `fit()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf4f3d78e69a"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Prepare the validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(64)\n",
        "\n",
        "model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7f0ebf5f1d"
      },
      "source": [
        "Al final de cada época, el modelo iterará sobre el conjunto de datos de validación y calculará la pérdida de validación y las métricas de validación.\n",
        "\n",
        "Si desea ejecutar la validación solo en un número específico de lotes de este conjunto de datos, puede pasar el argumento `validation_steps`, que especifica cuántos pasos de validación debe ejecutar el modelo con el conjunto de datos de validación antes de interrumpir la validación y pasar a la siguiente época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f47342fed069"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Prepare the validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(64)\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=1,\n",
        "    # Only run validation using the first 10 batches of the dataset\n",
        "    # using the `validation_steps` argument\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67b4418e9f26"
      },
      "source": [
        "Tenga en cuenta que el conjunto de datos de validación se restablecerá después de cada uso (de modo que siempre evaluará las mismas muestras de una época a otra).\n",
        "\n",
        "El argumento `validation_split` (generar un conjunto de retención a partir de los datos de entrenamiento) no es compatible cuando se entrena a partir de objetos `Dataset`, ya que esta función requiere la capacidad de indexar las muestras de los conjuntos de datos, lo que no es posible en general con la API `Dataset`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8160beb766a0"
      },
      "source": [
        "## Otros formatos de entrada compatibles\n",
        "\n",
        "Además de las matrices NumPy, los tensores eager y los `Datasets` de TensorFlow, es posible entrenar un modelo Keras usando dataframes Pandas, o desde generadores Python que producen lotes de datos y etiquetas.\n",
        "\n",
        "En particular, la clase `keras.utils.Sequence` ofrece una interfaz sencilla para construir generadores de datos Python compatibles con el multiprocesamiento y que se pueden mezclar.\n",
        "\n",
        "En general, le recomendamos que utilice:\n",
        "\n",
        "- Datos de entrada NumPy si sus datos son pequeños y caben en la memoria\n",
        "- `Dataset` objetos si tiene grandes conjuntos de datos y necesita realizar un entrenamiento distribuido.\n",
        "- `Sequence` si tiene grandes conjuntos de datos y necesita hacer mucho procesamiento personalizado en Python que no se puede hacer en TensorFlow (por ejemplo, si depende de bibliotecas externas para cargar o preprocesar datos).\n",
        "\n",
        "## Utilizando un objeto `keras.utils.Sequence` como entrada\n",
        "\n",
        "`keras.utils.Sequence` es una utilidad que podrá subclasificar para obtener un generador de Python con dos propiedades importantes:\n",
        "\n",
        "- Funciona bien con el multiprocesamiento.\n",
        "- Se puede mezclar (por ejemplo, al pasar `shuffle=True` en `fit()`).\n",
        "\n",
        "Un `Sequence` debe implementar dos métodos:\n",
        "\n",
        "- `__getitem__`\n",
        "- `__len__`\n",
        "\n",
        "El método `__getitem__` debe devolver un lote completo. Si desea modificar su conjunto de datos entre épocas, puede implementar `on_epoch_end`.\n",
        "\n",
        "Este es un ejemplo rápido:\n",
        "\n",
        "```python\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "\n",
        "# Here, `filenames` is list of path to the images\n",
        "# and `labels` are the associated labels.\n",
        "\n",
        "class CIFAR10Sequence(Sequence):\n",
        "    def __init__(self, filenames, labels, batch_size):\n",
        "        self.filenames, self.labels = filenames, labels\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return np.array([\n",
        "            resize(imread(filename), (200, 200))\n",
        "               for filename in batch_x]), np.array(batch_y)\n",
        "\n",
        "sequence = CIFAR10Sequence(filenames, labels, batch_size)\n",
        "model.fit(sequence, epochs=10)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a28343b1967"
      },
      "source": [
        "## Cómo utilizar la ponderación de muestras y la ponderación de clases\n",
        "\n",
        "Con la configuración predeterminada, el peso de una muestra se decide por su frecuencia en el conjunto de datos. Existen dos métodos para ponderar los datos, independientes de la frecuencia de las muestras:\n",
        "\n",
        "- Pesos de la clase\n",
        "- Pesos de la muestra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f234a9a75b6d"
      },
      "source": [
        "### Pesos de la clase\n",
        "\n",
        "Se establece pasando un diccionario al argumento `class_weight` de `Model.fit()`. Este diccionario asigna índices de clase al peso que debe utilizarse para las muestras que pertenecen a esta clase.\n",
        "\n",
        "Esto se puede utilizar para equilibrar las clases sin remuestreo, o para entrenar un modelo que dé más importancia a una clase en particular.\n",
        "\n",
        "Por ejemplo, si la clase \"0\" tiene la mitad de representación que la clase \"1\" en sus datos, podría utilizar `Model.fit(..., class_weight={0: 1., 1: 0.5})`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9929d26d91b8"
      },
      "source": [
        "Este es un ejemplo de NumPy en el que utilizamos pesos de la clase o pesos de la muestra para dar más importancia a la clasificación correcta de la clase #5 (que es el dígito \"5\" en el conjunto de datos MNIST)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1844f2329a6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class_weight = {\n",
        "    0: 1.0,\n",
        "    1: 1.0,\n",
        "    2: 1.0,\n",
        "    3: 1.0,\n",
        "    4: 1.0,\n",
        "    # Set weight \"2\" for class \"5\",\n",
        "    # making this class 2x more important\n",
        "    5: 2.0,\n",
        "    6: 1.0,\n",
        "    7: 1.0,\n",
        "    8: 1.0,\n",
        "    9: 1.0,\n",
        "}\n",
        "\n",
        "print(\"Fit with class weight\")\n",
        "model = get_compiled_model()\n",
        "model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce27221fad08"
      },
      "source": [
        "### Pesos de la muestra\n",
        "\n",
        "Para un control más preciso, o si no está construyendo un clasificador, puede utilizar \"pesos de la muestra\".\n",
        "\n",
        "- Cuando se entrena a partir de datos NumPy: Pase el argumento `sample_weight` a `Model.fit()`.\n",
        "- Cuando se entrena a partir de `tf.data` o cualquier otro tipo de iterador: Produce tuplas `(input_batch, label_batch, sample_weight_batch)`.\n",
        "\n",
        "Un arreglo de \"pesos de la muestra\" es un arreglo de números que especifica cuánto peso debe tener cada muestra de un lote en el cálculo de la pérdida total. Se suele utilizar en problemas de clasificación desequilibrada (la idea es dar más peso a las clases poco frecuentes).\n",
        "\n",
        "Cuando los pesos utilizados son unos y ceros, el arreglo puede utilizarse como una *máscara* para la función de pérdida (se descarta por completo la contribución de ciertas muestras en la pérdida total)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9819d647793"
      },
      "outputs": [],
      "source": [
        "sample_weight = np.ones(shape=(len(y_train),))\n",
        "sample_weight[y_train == 5] = 2.0\n",
        "\n",
        "print(\"Fit with sample weight\")\n",
        "model = get_compiled_model()\n",
        "model.fit(x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eae5837c5f56"
      },
      "source": [
        "A continuación se muestra un ejemplo de `Dataset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c870f3f0c66c"
      },
      "outputs": [],
      "source": [
        "sample_weight = np.ones(shape=(len(y_train),))\n",
        "sample_weight[y_train == 5] = 2.0\n",
        "\n",
        "# Create a Dataset that includes sample weights\n",
        "# (3rd element in the return tuple).\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, sample_weight))\n",
        "\n",
        "# Shuffle and slice the dataset.\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "model = get_compiled_model()\n",
        "model.fit(train_dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5f94cd76df5"
      },
      "source": [
        "## Pasar datos a modelos multientrada y multisalida\n",
        "\n",
        "En los ejemplos anteriores, considerábamos un modelo con una única entrada (un tensor de forma `(764,)`) y una única salida (un tensor de predicción de forma `(10,)`). Pero, ¿qué ocurre con los modelos que tienen varias entradas o salidas?\n",
        "\n",
        "Consideremos el siguiente modelo, que tiene una entrada de imagen de forma `(32, 32, 3)` (es decir, `(height, width, channels)`) y una entrada de serie temporal de forma `(None, 10)` (es decir, `(timesteps, features)`). Nuestro modelo tendrá dos salidas calculadas a partir de la combinación de estas entradas: una \"puntuación\" (de forma `(1,)`) y una distribución de probabilidad sobre cinco clases (de forma `(5,)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f958449a057"
      },
      "outputs": [],
      "source": [
        "image_input = keras.Input(shape=(32, 32, 3), name=\"img_input\")\n",
        "timeseries_input = keras.Input(shape=(None, 10), name=\"ts_input\")\n",
        "\n",
        "x1 = layers.Conv2D(3, 3)(image_input)\n",
        "x1 = layers.GlobalMaxPooling2D()(x1)\n",
        "\n",
        "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
        "x2 = layers.GlobalMaxPooling1D()(x2)\n",
        "\n",
        "x = layers.concatenate([x1, x2])\n",
        "\n",
        "score_output = layers.Dense(1, name=\"score_output\")(x)\n",
        "class_output = layers.Dense(5, name=\"class_output\")(x)\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs=[image_input, timeseries_input], outputs=[score_output, class_output]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df3ed34fe78b"
      },
      "source": [
        "Vamos a trazar este modelo, para que pueda ver claramente lo que estamos haciendo aquí (tenga en cuenta que las formas que se muestran en el gráfico son formas de lote, en vez de formas por muestra)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac8c1baca9e3"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d979e89b335"
      },
      "source": [
        "En el momento de la compilación, podemos especificar diferentes pérdidas para diferentes salidas, pasando las funciones de pérdida como una lista:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9655c0084d70"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5fc73405283"
      },
      "source": [
        "Si solo pasamos una única función de pérdida al modelo, se aplicará la misma función de pérdida a cada salida (lo que no es apropiado en este caso).\n",
        "\n",
        "Lo mismo sucede con las métricas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4c0c6c564bc"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
        "    metrics=[\n",
        "        [\n",
        "            keras.metrics.MeanAbsolutePercentageError(),\n",
        "            keras.metrics.MeanAbsoluteError(),\n",
        "        ],\n",
        "        [keras.metrics.CategoricalAccuracy()],\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dd9fb0343cc"
      },
      "source": [
        "Como dimos nombres a nuestras capas de salida, también podríamos especificar pérdidas y métricas por salida mediante un dict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42cb75110fc3"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={\n",
        "        \"score_output\": keras.losses.MeanSquaredError(),\n",
        "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
        "    },\n",
        "    metrics={\n",
        "        \"score_output\": [\n",
        "            keras.metrics.MeanAbsolutePercentageError(),\n",
        "            keras.metrics.MeanAbsoluteError(),\n",
        "        ],\n",
        "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd95ac0dd8b"
      },
      "source": [
        "Recomendamos el uso de nombres explícitos y dicts si tiene más de 2 salidas.\n",
        "\n",
        "Es posible dar diferentes pesos a diferentes pérdidas específicas de la salida (por ejemplo, uno podría desear privilegiar la pérdida de \"puntuación\" en nuestro ejemplo, dando a 2x la importancia de la pérdida de la clase), utilizando el argumento `loss_weights`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23a71e5f5227"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={\n",
        "        \"score_output\": keras.losses.MeanSquaredError(),\n",
        "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
        "    },\n",
        "    metrics={\n",
        "        \"score_output\": [\n",
        "            keras.metrics.MeanAbsolutePercentageError(),\n",
        "            keras.metrics.MeanAbsoluteError(),\n",
        "        ],\n",
        "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
        "    },\n",
        "    loss_weights={\"score_output\": 2.0, \"class_output\": 1.0},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147b5f581c32"
      },
      "source": [
        "También puede elegir no calcular una pérdida para ciertas salidas, si estas salidas están destinadas a la predicción pero no al entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d51aa372ef4"
      },
      "outputs": [],
      "source": [
        "# List loss version\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[None, keras.losses.CategoricalCrossentropy()],\n",
        ")\n",
        "\n",
        "# Or dict loss version\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={\"class_output\": keras.losses.CategoricalCrossentropy()},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c00d5f56d3f0"
      },
      "source": [
        "Pasar datos a un modelo multientrada o multisalida en `fit()` funciona de forma similar a especificar una función de pérdida en compilación: puede pasar **listas de matrices NumPy** (con mapeo 1:1 a las salidas que recibieron una función de pérdida) o **dictos que mapean nombres de salida a matrices NumPy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0539da84328b"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
        ")\n",
        "\n",
        "# Generate dummy NumPy data\n",
        "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
        "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
        "score_targets = np.random.random_sample(size=(100, 1))\n",
        "class_targets = np.random.random_sample(size=(100, 5))\n",
        "\n",
        "# Fit on lists\n",
        "model.fit([img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=1)\n",
        "\n",
        "# Alternatively, fit on dicts\n",
        "model.fit(\n",
        "    {\"img_input\": img_data, \"ts_input\": ts_data},\n",
        "    {\"score_output\": score_targets, \"class_output\": class_targets},\n",
        "    batch_size=32,\n",
        "    epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e53eda8e1399"
      },
      "source": [
        "Este es el caso de uso de `Dataset`: de forma similar a lo que hicimos para las matrices de NumPy, `Dataset` debería devolver una tupla de dicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4df41a12ed2c"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        {\"img_input\": img_data, \"ts_input\": ts_data},\n",
        "        {\"score_output\": score_targets, \"class_output\": class_targets},\n",
        "    )\n",
        ")\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "model.fit(train_dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c792cd43a4"
      },
      "source": [
        "## Cómo utilizar las retrollamadas\n",
        "\n",
        "Las retrollamadas en Keras son objetos que se llaman en diferentes puntos durante el entrenamiento (al inicio de una época, al final de un lote, al final de una época, etc.). Pueden ser utilizados para implementar ciertos comportamientos, tales como:\n",
        "\n",
        "- Realizar la validación en diferentes puntos durante el entrenamiento (más allá de la validación incorporada por época).\n",
        "- Verificación del modelo a intervalos regulares o cuando supera un determinado umbral de precisión.\n",
        "- Modificación del ritmo de aprendizaje del modelo cuando el entrenamiento parece estancarse\n",
        "- Realizar un ajuste fino de las capas superiores cuando el entrenamiento parece estancarse.\n",
        "- Envío de notificaciones por correo electrónico o mensaje instantáneo cuando finalice el entrenamiento o se supere un determinado umbral de rendimiento.\n",
        "- Etc.\n",
        "\n",
        "Las retrollamadas pueden pasarse como una lista a su llamada a `fit()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15036ddbee42"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        # Stop training when `val_loss` is no longer improving\n",
        "        monitor=\"val_loss\",\n",
        "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
        "        min_delta=1e-2,\n",
        "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "        patience=2,\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15f5af3b6da9"
      },
      "source": [
        "### Dispone de muchas retrollamadas integradas\n",
        "\n",
        "Hay muchas retrollamadas incorporadas ya disponibles en Keras, tales como:\n",
        "\n",
        "- `ModelCheckpoint`: Guarda periódicamente el modelo.\n",
        "- `Detención temprana`: Detiene el entrenamiento cuando éste ya no mejora las métricas de validación.\n",
        "- `TensorBoard`: escribe periódicamente registros del modelo que pueden visualizarse en [TensorBoard](https://www.tensorflow.org/tensorboard) (más detalles en la sección \"Visualización\").\n",
        "- `CSVLogger`: transmite datos de pérdidas y métricas a un archivo CSV.\n",
        "- etc.\n",
        "\n",
        "Consulte la [documentación sobre retrollamadas](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/) para ver la lista completa.\n",
        "\n",
        "### Cómo escribir su propia retrollamada\n",
        "\n",
        "Puede crear una retrollamada personalizada ampliando la clase base `keras.callbacks.Callback`. Una retrollamada tiene acceso a su modelo asociado mediante la propiedad de la clase `self.model`.\n",
        "\n",
        "Asegúrese de leer la [guía completa para escribir retrollamadas personalizadas](https://www.tensorflow.org/guide/keras/custom_callback/).\n",
        "\n",
        "Este es un ejemplo sencillo en el que se guarda una lista de valores de pérdida por lote durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b265d36ce608"
      },
      "outputs": [],
      "source": [
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs):\n",
        "        self.per_batch_losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        self.per_batch_losses.append(logs.get(\"loss\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ee672524987"
      },
      "source": [
        "## Modelos de punto de verificación\n",
        "\n",
        "Cuando se entrena un modelo en conjuntos de datos relativamente grandes, es crucial guardar puntos de verificación del modelo a intervalos frecuentes.\n",
        "\n",
        "La forma más sencilla de conseguirlo es con la retrollamada `ModelCheckpoint`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83614be57725"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_loss` score has improved.\n",
        "        # The saved model name will include the current epoch.\n",
        "        filepath=\"mymodel_{epoch}\",\n",
        "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "        monitor=\"val_loss\",\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "model.fit(\n",
        "    x_train, y_train, epochs=2, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f6afa36950c"
      },
      "source": [
        "La retrollamada `ModelCheckpoint` puede utilizarse para implementar la tolerancia ante errores: la capacidad de reiniciar el entrenamiento desde el último estado guardado del modelo en caso de que el entrenamiento se interrumpa aleatoriamente. Este es un ejemplo básico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27ce92b2ad58"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Prepare a directory to store all the checkpoints.\n",
        "checkpoint_dir = \"./ckpt\"\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "\n",
        "def make_or_restore_model():\n",
        "    # Either restore the latest model, or create a fresh one\n",
        "    # if there is no checkpoint available.\n",
        "    checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]\n",
        "    if checkpoints:\n",
        "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
        "        print(\"Restoring from\", latest_checkpoint)\n",
        "        return keras.models.load_model(latest_checkpoint)\n",
        "    print(\"Creating a new model\")\n",
        "    return get_compiled_model()\n",
        "\n",
        "\n",
        "model = make_or_restore_model()\n",
        "callbacks = [\n",
        "    # This callback saves a SavedModel every 100 batches.\n",
        "    # We include the training loss in the saved model name.\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_dir + \"/ckpt-loss={loss:.2f}\", save_freq=100\n",
        "    )\n",
        "]\n",
        "model.fit(x_train, y_train, epochs=1, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da3ab58d5235"
      },
      "source": [
        "También puede escribir su propia retrollamada para guardar y restaurar modelos.\n",
        "\n",
        "Para obtener una guía completa sobre serialización y guardado, consulte la [guía para guardar y serializar Modelos](https://www.tensorflow.org/guide/keras/save_and_serialize/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9342cc2ddba"
      },
      "source": [
        "## Cómo utilizar los programas de aprendizaje\n",
        "\n",
        "Un patrón común cuando se entrenan modelos de aprendizaje profundo es reducir gradualmente el aprendizaje conforme avanza el entrenamiento. Esto se conoce generalmente como \"decaimiento de la tasa de aprendizaje\".\n",
        "\n",
        "El programa de decaimiento del aprendizaje puede ser estático (fijado por adelantado, en función de la época actual o del índice del lote actual), o dinámico (que responde al comportamiento actual del modelo, en particular a la pérdida de validación).\n",
        "\n",
        "### Cómo pasar un programa a un optimizador\n",
        "\n",
        "Puede utilizar fácilmente un programa estático de decaimiento de la tasa de aprendizaje pasando un objeto del programa como argumento `learning_rate` en su optimizador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "684f0ab6d3de"
      },
      "outputs": [],
      "source": [
        "initial_learning_rate = 0.1\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03b61ddd9586"
      },
      "source": [
        "Se dispone de varias programaciones incorporadas: `ExponentialDecay`, `PiecewiseConstantDecay`, `PolynomialDecay`, y `InverseTimeDecay`.\n",
        "\n",
        "### Uso de retrollamadas para aplicar una programación dinámica de la tasa de aprendizaje\n",
        "\n",
        "Una programación dinámica de la tasa de aprendizaje (por ejemplo, la disminución de la tasa de aprendizaje cuando la pérdida de validación ya no está mejorando) no se puede lograr con estos objetos de programación, ya que el optimizador no tiene acceso a las métricas de validación.\n",
        "\n",
        "Sin embargo, las retrollamadas tienen acceso a todas las métricas, ¡incluidas las de validación! Por lo tanto, puede lograr este patrón mediante el uso de una retrollamada que modifica la tasa de aprendizaje actual en el optimizador. De hecho, esto incluso está incorporado como la retrollamada `ReduceLROnPlateau`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f8b9539cd57"
      },
      "source": [
        "## Visualización de pérdidas y métricas durante el entrenamiento\n",
        "\n",
        "La mejor manera de mantener un ojo en su modelo durante el entrenamiento es utilizar [TensorBoard](https://www.tensorflow.org/tensorboard) -- una aplicación basada en el navegador que puede ejecutar localmente y le proporciona:\n",
        "\n",
        "- Gráficos en vivo de la pérdida y métricas para el entrenamiento y la evaluación\n",
        "- (opcionalmente) Visualizaciones de los histogramas de las activaciones de sus capas\n",
        "- (opcionalmente) visualizaciones 3D de los espacios de incorporación aprendidos por sus capas `Embedding`\n",
        "\n",
        "Si instaló TensorFlow con la herramienta pip, debería poder iniciar TensorBoard desde la línea de comandos:\n",
        "\n",
        "```\n",
        "tensorboard --logdir=/full_path_to_your_logs\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2685d7ce531"
      },
      "source": [
        "### Cómo utilizar la retrollamada de TensorBoard\n",
        "\n",
        "La forma más sencilla de utilizar TensorBoard con un modelo Keras y el método `fit()` es la retrollamada `TensorBoard`.\n",
        "\n",
        "En el caso más sencillo, basta con especificar dónde desea que la retrollamada escriba los registros, y listo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f74247282ff6"
      },
      "outputs": [],
      "source": [
        "keras.callbacks.TensorBoard(\n",
        "    log_dir=\"/full_path_to_your_logs\",\n",
        "    histogram_freq=0,  # How often to log histogram visualizations\n",
        "    embeddings_freq=0,  # How often to log embedding visualizations\n",
        "    update_freq=\"epoch\",\n",
        ")  # How often to write logs (default: once per epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3614f8ba1e03"
      },
      "source": [
        "Para obtener más información, consulte la [documentación para la retrollamada `TensorBoard`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/tensorboard/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "train_and_evaluate.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
