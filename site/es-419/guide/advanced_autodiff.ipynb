{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Diferenciación automática avanzada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/advanced_autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/advanced_autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/advanced_autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver código fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/advanced_autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a859404ce7e"
      },
      "source": [
        "La guía [Introducción a los gradientes y la diferenciación automática](autodiff.ipynb) incluye todo lo necesario para calcular gradientes en TensorFlow. Esta guía se enfoca en características más profundas y menos comunes de la API `tf.GradientTape`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGRJJRi8TCkJ"
      },
      "source": [
        "## Controlar el registro de gradiente\n",
        "\n",
        "En la [guía de diferenciación automática](autodiff.ipynb) vio cómo controlar qué variables y tensores son observados por la cinta mientras se construye el cálculo del gradiente.\n",
        "\n",
        "La cinta también dispone de métodos para manipular la grabación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB_i0VnhQKt2"
      },
      "source": [
        "### Detener la grabación\n",
        "\n",
        "Si desea detener la grabación de gradientes, puede usar `tf.GradientTape.stop_recording` para suspender temporalmente la grabación.\n",
        "\n",
        "Esto puede ser útil para reducir la sobrecarga si no desea diferenciar una operación complicada a mitad de su modelo. Puede incluir calcular una métrica o un resultado intermedio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhFSYf7uQWxR"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  x_sq = x * x\n",
        "  with t.stop_recording():\n",
        "    y_sq = y * y\n",
        "  z = x_sq + y_sq\n",
        "\n",
        "grad = t.gradient(z, {'x': x, 'y': y})\n",
        "\n",
        "print('dz/dx:', grad['x'])  # 2*x => 4\n",
        "print('dz/dy:', grad['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEHbEZ1h4p8A"
      },
      "source": [
        "### Reiniciar/empezar a grabar desde cero\n",
        "\n",
        "Si desea empezar de cero por completo, use `tf.GradientTape.reset`. Si simplemente sale del bloque de cinta de gradiente y vuelve a empezar, suele ser más fácil de leer, pero puede usar el método `reset` cuando salir del bloque de cinta sea difícil o imposible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsMHsmrh4pqM"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "reset = True\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  y_sq = y * y\n",
        "  if reset:\n",
        "    # Throw out all the tape recorded so far.\n",
        "    t.reset()\n",
        "  z = x * x + y_sq\n",
        "\n",
        "grad = t.gradient(z, {'x': x, 'y': y})\n",
        "\n",
        "print('dz/dx:', grad['x'])  # 2*x => 4\n",
        "print('dz/dy:', grad['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zS7cLmS6zMf"
      },
      "source": [
        "## Detener el flujo de gradiente con precisión\n",
        "\n",
        "A diferencia de los controles globales de cinta anteriores, la función `tf.stop_gradient` es mucho más precisa. Puede usarse para detener el flujo de gradientes a lo largo de una ruta concreta, sin necesidad de acceder a la propia cinta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30qnZMe48BkB"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  y_sq = y**2\n",
        "  z = x**2 + tf.stop_gradient(y_sq)\n",
        "\n",
        "grad = t.gradient(z, {'x': x, 'y': y})\n",
        "\n",
        "print('dz/dx:', grad['x'])  # 2*x => 4\n",
        "print('dz/dy:', grad['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbb-9lnGVngH"
      },
      "source": [
        "## Gradientes personalizados\n",
        "\n",
        "En algunos casos, es posible que desee controlar exactamente cómo se calculan los gradientes en lugar de usar el valor por defecto. Estas situaciones incluyen:\n",
        "\n",
        "1. No hay un gradiente definido para la nueva op que está escribiendo.\n",
        "2. Los cálculos por defecto son numéricamente inestables.\n",
        "3. Desea almacenar en caché un cálculo costoso de la pasada hacia delante.\n",
        "4. Quiere modificar un valor (por ejemplo, usando `tf.clip_by_value` o `tf.math.round`) sin modificar el gradiente.\n",
        "\n",
        "En el primer caso, para escribir una nueva op puede usar `tf.RegisterGradient` para configurar la suya propia (consulte los documentos de la API para más detalles y tenga en cuenta que el registro de gradientes es global, así que modifíquelo con precaución).\n",
        "\n",
        "Para los tres últimos casos, puede usar `tf.custom_gradient`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHr31kc_irF_"
      },
      "source": [
        "Aquí tiene un ejemplo que aplica `tf.clip_by_norm` al gradiente intermedio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjj01w4NYtwd"
      },
      "outputs": [],
      "source": [
        "# Establish an identity operation, but clip during the gradient pass.\n",
        "@tf.custom_gradient\n",
        "def clip_gradients(y):\n",
        "  def backward(dy):\n",
        "    return tf.clip_by_norm(dy, 0.5)\n",
        "  return y, backward\n",
        "\n",
        "v = tf.Variable(2.0)\n",
        "with tf.GradientTape() as t:\n",
        "  output = clip_gradients(v * v)\n",
        "print(t.gradient(output, v))  # calls \"backward\", which clips 4 to 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4t7S0scYrD3"
      },
      "source": [
        "Consulte la documentación de la API del decorador `tf.custom_gradient` para obtener más detalles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ODp4Oi--I0"
      },
      "source": [
        "### Gradientes personalizados en SavedModel\n",
        "\n",
        "Nota: Esta función está disponible a partir de TensorFlow 2.6.\n",
        "\n",
        "Los gradientes personalizados pueden guardarse en SavedModel usando la opción `tf.saved_model.SaveOptions(experimental_custom_gradients=True)`.\n",
        "\n",
        "Para guardarse en el SavedModel, la función de gradiente debe ser trazable (para saber más, consulte la guía [Mejor rendimiento con tf.function](function.ipynb))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5JBgIBYjN1I"
      },
      "outputs": [],
      "source": [
        "class MyModule(tf.Module):\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(None)])\n",
        "  def call_custom_grad(self, x):\n",
        "    return clip_gradients(x)\n",
        "\n",
        "model = MyModule()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZTrgy2q-9pq"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(\n",
        "    model,\n",
        "    'saved_model',\n",
        "    options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n",
        "\n",
        "# The loaded gradients will be the same as the above example.\n",
        "v = tf.Variable(2.0)\n",
        "loaded = tf.saved_model.load('saved_model')\n",
        "with tf.GradientTape() as t:\n",
        "  output = loaded.call_custom_grad(v * v)\n",
        "print(t.gradient(output, v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-LfRs5FbJCk"
      },
      "source": [
        "Una nota sobre el ejemplo anterior: Si intenta reemplazar el código anterior por `tf.saved_model.SaveOptions(experimental_custom_gradients=False)`, el gradiente seguirá produciendo el mismo resultado al cargar. El motivo es que el registro de gradientes aún contiene el gradiente personalizado usado en la función `call_custom_op`. Sin embargo, si reinicia el runtime después de guardar sin gradientes personalizados, al ejecutar el modelo cargado bajo la función `tf.GradientTape` se producirá el error: `LookupError: No gradient defined for operation 'IdentityN' (op type: IdentityN)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aENEt6Veryb"
      },
      "source": [
        "## Múltiples cintas\n",
        "\n",
        "Las cintas múltiples interactúan a la perfección.\n",
        "\n",
        "Por ejemplo, aquí cada cinta vigila un conjunto diferente de tensores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ0HdMvte0VZ"
      },
      "outputs": [],
      "source": [
        "x0 = tf.constant(0.0)\n",
        "x1 = tf.constant(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\n",
        "  tape0.watch(x0)\n",
        "  tape1.watch(x1)\n",
        "\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.sigmoid(x1)\n",
        "\n",
        "  y = y0 + y1\n",
        "\n",
        "  ys = tf.reduce_sum(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ApAoMNFfNz6"
      },
      "outputs": [],
      "source": [
        "tape0.gradient(ys, x0).numpy()   # cos(x) => 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF1jrAJsfYW_"
      },
      "outputs": [],
      "source": [
        "tape1.gradient(ys, x1).numpy()   # sigmoid(x1)*(1-sigmoid(x1)) => 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK05KXrAAld3"
      },
      "source": [
        "### Gradientes de orden alto\n",
        "\n",
        "Las operaciones dentro del administrador de contexto `tf.GradientTape` se registran para la diferenciación automática. Si se calculan gradientes en ese contexto, también se registra el cálculo del gradiente. Como resultado, la misma API funciona también para gradientes de orden alto.\n",
        "\n",
        "Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPQgthZ7ugRJ"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n",
        "\n",
        "with tf.GradientTape() as t2:\n",
        "  with tf.GradientTape() as t1:\n",
        "    y = x * x * x\n",
        "\n",
        "  # Compute the gradient inside the outer `t2` context manager\n",
        "  # which means the gradient computation is differentiable as well.\n",
        "  dy_dx = t1.gradient(y, x)\n",
        "d2y_dx2 = t2.gradient(dy_dx, x)\n",
        "\n",
        "print('dy_dx:', dy_dx.numpy())  # 3 * x**2 => 3.0\n",
        "print('d2y_dx2:', d2y_dx2.numpy())  # 6 * x => 6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0HV-Ah4_76i"
      },
      "source": [
        "Aunque eso le da la segunda derivada de una función *escalar*, este patrón no se generaliza para producir una matriz hessiana, ya que `tf.GradientTape.gradient` sólo calcula el gradiente de un escalar. Para construir una [matriz hessiana](https://en.wikipedia.org/wiki/Hessian_matrix), vaya al [ejemplo hessiano](#hessian) en la [sección jacobiana](#jacobians).\n",
        "\n",
        "\"Llamadas anidadas a `tf.GradientTape.gradient`\" es un buen patrón cuando se calcula un escalar a partir de un gradiente, y luego el escalar resultante actúa como fuente para un segundo cálculo de gradiente, como en el siguiente ejemplo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7LRlcpVKHv1"
      },
      "source": [
        "#### Ejemplo: Regularización del gradiente de entrada\n",
        "\n",
        "Muchos modelos son susceptibles a los \"ejemplos adversariales\". Esta colección de técnicas modifica la entrada del modelo para confundir la salida del mismo. La implementación más sencilla (como el [Ejemplo adversarial que utiliza el ataque del Método de Gradiente Rápido por Signos](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm)) toma un solo paso a lo largo del gradiente de la salida con respecto a la entrada; el \"gradiente de entrada\".\n",
        "\n",
        "Una técnica para aumentar la robustez ante ejemplos adversariales es la [regularización del gradiente de entrada](https://arxiv.org/abs/1905.11468) (Finlay &amp; Oberman, 2019), que intenta minimizar la magnitud del gradiente de entrada. Si el gradiente de entrada es pequeño, entonces el cambio en la salida también debería serlo.\n",
        "\n",
        "Aquí tiene una implementación ingenua de la regularización por gradiente de entrada. La implementación es:\n",
        "\n",
        "1. Calcule el gradiente de la salida con respecto a la entrada usando una cinta interior.\n",
        "2. Calcule la magnitud de ese gradiente de entrada.\n",
        "3. Calcule el gradiente de esa magnitud con respecto al modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH3ZFuUfDLrR"
      },
      "outputs": [],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "\n",
        "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6yOFsjEDR9u"
      },
      "outputs": [],
      "source": [
        "with tf.GradientTape() as t2:\n",
        "  # The inner tape only takes the gradient with respect to the input,\n",
        "  # not the variables.\n",
        "  with tf.GradientTape(watch_accessed_variables=False) as t1:\n",
        "    t1.watch(x)\n",
        "    y = layer(x)\n",
        "    out = tf.reduce_sum(layer(x)**2)\n",
        "  # 1. Calculate the input gradient.\n",
        "  g1 = t1.gradient(out, x)\n",
        "  # 2. Calculate the magnitude of the input gradient.\n",
        "  g1_mag = tf.norm(g1)\n",
        "\n",
        "# 3. Calculate the gradient of the magnitude with respect to the model.\n",
        "dg1_mag = t2.gradient(g1_mag, layer.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "123QMq6PqK_d"
      },
      "outputs": [],
      "source": [
        "[var.shape for var in dg1_mag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4xiYigexMtQ"
      },
      "source": [
        "## Jacobianos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-hVHVIeExkI"
      },
      "source": [
        "Todos los ejemplos anteriores tomaban los gradientes de un objetivo escalar con respecto a algún(os) tensor(es) fuente.\n",
        "\n",
        "La [matriz Jacobiana](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) representa los gradientes de una función con valor vectorial. Cada fila contiene el gradiente de uno de los elementos del vector.\n",
        "\n",
        "El método `tf.GradientTape.jacobian` permite calcular eficazmente una matriz jacobiana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzNyIM0QBYIH"
      },
      "source": [
        "Tenga en cuenta que:\n",
        "\n",
        "- Como `gradient`: El argumento `sources` puede ser un tensor o un contenedor de tensores.\n",
        "- A diferencia de `gradient`: El tensor `target` debe ser un único tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O74K3hlxBC8a"
      },
      "source": [
        "### Fuente escalar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B08OKn1Orkuc"
      },
      "source": [
        "Como primer ejemplo, he aquí la jacobiana de un vector-objetivo con respecto a un escalar-fuente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAFeIE8EuVIq"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-10.0, 10.0, 200+1)\n",
        "delta = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = tf.nn.sigmoid(x+delta)\n",
        "\n",
        "dy_dx = tape.jacobian(y, delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgHbUk3zr-WU"
      },
      "source": [
        "Cuando se toma el jacobiano con respecto a un escalar el resultado tiene la forma del **objetivo**, y da el gradiente de cada elemento con respecto al origen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ6awnDzr_BA"
      },
      "outputs": [],
      "source": [
        "print(y.shape)\n",
        "print(dy_dx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siNZaklc0_-e"
      },
      "outputs": [],
      "source": [
        "plt.plot(x.numpy(), y, label='y')\n",
        "plt.plot(x.numpy(), dy_dx, label='dy/dx')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsOMSD_1BGkD"
      },
      "source": [
        "### Fuente de tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3iXKN7KF-st"
      },
      "source": [
        "Tanto si la entrada es escalar como tensorial, `tf.GradientTape.jacobian` calcula eficazmente el gradiente de cada elemento de la fuente con respecto a cada elemento del objetivo u objetivos.\n",
        "\n",
        "Por ejemplo, la salida de esta capa tiene una forma de `(10, 7)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39YXItgLxMBk"
      },
      "outputs": [],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = layer(x)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tshNRtfKuVP_"
      },
      "source": [
        "Y la forma del kernel de la capa es `(5, 10)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CigTWyfPvPuv"
      },
      "outputs": [],
      "source": [
        "layer.kernel.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN96JRpnAjpx"
      },
      "source": [
        "La forma del jacobiano de la salida con respecto al kernel son esas dos formas concatenadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRLzTTbvEimH"
      },
      "outputs": [],
      "source": [
        "j = tape.jacobian(y, layer.kernel)\n",
        "j.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lrv7miMvTll"
      },
      "source": [
        "Si suma sobre las dimensiones del objetivo, le queda el gradiente de la suma que habría calculado `tf.GradientTape.gradient`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJjZpYRnDjVa"
      },
      "outputs": [],
      "source": [
        "g = tape.gradient(y, layer.kernel)\n",
        "print('g.shape:', g.shape)\n",
        "\n",
        "j_sum = tf.reduce_sum(j, axis=[0, 1])\n",
        "delta = tf.reduce_max(abs(g - j_sum)).numpy()\n",
        "assert delta < 1e-3\n",
        "print('delta:', delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKajuGlk_krs"
      },
      "source": [
        "<a id=\"hessian\"> </a>\n",
        "\n",
        "#### Ejemplo: Hessiana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYcsXeo8TDLi"
      },
      "source": [
        "Aunque `tf.GradientTape` no da un método explícito para construir una [matriz hessiana](https://en.wikipedia.org/wiki/Hessian_matrix) es posible construir una usando el método `tf.GradientTape.jacobian`.\n",
        "\n",
        "Nota: La matriz hessiana contiene `N**2` parámetros. Por este y otros motivos no es práctico para la mayoría de los modelos. Este ejemplo se incluye más como demostración de cómo usar el método `tf.GradientTape.jacobian`, y no para apoyar que la optimización directa se base en la hessiana. Un producto vectorial hessiano puede [calcularse eficientemente con cintas anidadas](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/benchmarks/resnet50/hvp_test.py), y es un enfoque mucho más eficiente para la optimización de segundo orden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELGTaell_j81"
      },
      "outputs": [],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
        "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.relu)\n",
        "\n",
        "with tf.GradientTape() as t2:\n",
        "  with tf.GradientTape() as t1:\n",
        "    x = layer1(x)\n",
        "    x = layer2(x)\n",
        "    loss = tf.reduce_mean(x**2)\n",
        "\n",
        "  g = t1.gradient(loss, layer1.kernel)\n",
        "\n",
        "h = t2.jacobian(g, layer1.kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVqQuZj4XGjm"
      },
      "outputs": [],
      "source": [
        "print(f'layer.kernel.shape: {layer1.kernel.shape}')\n",
        "print(f'h.shape: {h.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M7XElgaiMeP"
      },
      "source": [
        "Para usar esta hessiana para un paso del método de [Newton](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization), primero habría que aplanar sus ejes en una matriz, y aplanar el gradiente en un vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6te7N6wVXwXX"
      },
      "outputs": [],
      "source": [
        "n_params = tf.reduce_prod(layer1.kernel.shape)\n",
        "\n",
        "g_vec = tf.reshape(g, [n_params, 1])\n",
        "h_mat = tf.reshape(h, [n_params, n_params])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9rO8b-0mgOH"
      },
      "source": [
        "La matriz hessiana debe ser simétrica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TCHc7Vrf52S"
      },
      "outputs": [],
      "source": [
        "def imshow_zero_center(image, **kwargs):\n",
        "  lim = tf.reduce_max(abs(image))\n",
        "  plt.imshow(image, vmin=-lim, vmax=lim, cmap='seismic', **kwargs)\n",
        "  plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DExOxd7Ok2H0"
      },
      "outputs": [],
      "source": [
        "imshow_zero_center(h_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13fBswmtQes4"
      },
      "source": [
        "A continuación se muestra el paso de actualización del método de Newton:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DdnbynBdSor"
      },
      "outputs": [],
      "source": [
        "eps = 1e-3\n",
        "eye_eps = tf.eye(h_mat.shape[0])*eps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zPdtyoWeUeV"
      },
      "source": [
        "Nota: [En realidad no invierta la matriz](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1LYftgmswOO"
      },
      "outputs": [],
      "source": [
        "# X(k+1) = X(k) - (∇²f(X(k)))^-1 @ ∇f(X(k))\n",
        "# h_mat = ∇²f(X(k))\n",
        "# g_vec = ∇f(X(k))\n",
        "update = tf.linalg.solve(h_mat + eye_eps, g_vec)\n",
        "\n",
        "# Reshape the update and apply it to the variable.\n",
        "_ = layer1.kernel.assign_sub(tf.reshape(update, layer1.kernel.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF6qjlHKWxF4"
      },
      "source": [
        "Aunque esto es relativamente sencillo para una sola `tf.Variable`, aplicarlo a un modelo no trivial requeriría una cuidadosa concatenación y corte para producir un hessiano completo a través de múltiples variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQWM0uN-GO5t"
      },
      "source": [
        "### Jacobiano de lote"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKtB3rY6EySJ"
      },
      "source": [
        "En algunos casos, se desea tomar la jacobiana de cada uno de un apilamiento de objetivos con respecto a un apilamiento de fuentes, donde las jacobianas de cada par objetivo-origen son independientes.\n",
        "\n",
        "Por ejemplo, aquí la entrada `x` tiene forma `(batch, ins)` y la salida `y` tiene forma `(batch, outs)`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQMndhIUHMes"
      },
      "outputs": [],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "\n",
        "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
        "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
        "\n",
        "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x)\n",
        "  y = layer1(x)\n",
        "  y = layer2(y)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff2spRHEJXBU"
      },
      "source": [
        "La jacobiana completa de `y` con respecto a `x` tiene una forma de `(batch, ins, batch, outs)`, aunque sólo quiera `(batch, ins, outs)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zSl2A5-HhMH"
      },
      "outputs": [],
      "source": [
        "j = tape.jacobian(y, x)\n",
        "j.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UibJijPLJrpQ"
      },
      "source": [
        "Si los gradientes de cada artículo de la pila son independientes, entonces cada `(batch, batch)` corte de este tensor es una matriz diagonal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFl9uj3ueVSH"
      },
      "outputs": [],
      "source": [
        "imshow_zero_center(j[:, 0, :, 0])\n",
        "_ = plt.title('A (batch, batch) slice')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4ZoRJcJNmy5"
      },
      "outputs": [],
      "source": [
        "def plot_as_patches(j):\n",
        "  # Reorder axes so the diagonals will each form a contiguous patch.\n",
        "  j = tf.transpose(j, [1, 0, 3, 2])\n",
        "  # Pad in between each patch.\n",
        "  lim = tf.reduce_max(abs(j))\n",
        "  j = tf.pad(j, [[0, 0], [1, 1], [0, 0], [1, 1]],\n",
        "             constant_values=-lim)\n",
        "  # Reshape to form a single image.\n",
        "  s = j.shape\n",
        "  j = tf.reshape(j, [s[0]*s[1], s[2]*s[3]])\n",
        "  imshow_zero_center(j, extent=[-0.5, s[2]-0.5, s[0]-0.5, -0.5])\n",
        "\n",
        "plot_as_patches(j)\n",
        "_ = plt.title('All (batch, batch) slices are diagonal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXpTBKyeK84z"
      },
      "source": [
        "Para obtener el resultado deseado, puede sumar sobre la dimensión duplicada `batch`, o bien seleccionar las diagonales utilizando `tf.einsum`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v65OAjEgLQwl"
      },
      "outputs": [],
      "source": [
        "j_sum = tf.reduce_sum(j, axis=2)\n",
        "print(j_sum.shape)\n",
        "j_select = tf.einsum('bxby->bxy', j)\n",
        "print(j_select.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT_VfR6lcwxD"
      },
      "source": [
        "Sería mucho más eficiente hacer el cálculo sin la dimensión extra en primer lugar. El método `tf.GradientTape.batch_jacobian` hace exactamente eso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJLIl9WpHqYq"
      },
      "outputs": [],
      "source": [
        "jb = tape.batch_jacobian(y, x)\n",
        "jb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5t_q5SfHw7T"
      },
      "outputs": [],
      "source": [
        "error = tf.reduce_max(abs(jb - j_sum))\n",
        "assert error < 1e-3\n",
        "print(error.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUeY2ZCiL31I"
      },
      "source": [
        "Precaución: `tf.GradientTape.batch_jacobian` sólo comprueba que la primera dimensión del origen y del objetivo coincidan. No comprueba que los gradientes sean realmente independientes. Depende de usted asegurarse de que sólo usa `batch_jacobian` cuando tiene sentido. Por ejemplo, añadir un `tf.keras.layers.BatchNormalization` destruye la independencia, ya que normaliza a través de la dimensión `batch`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnDugVc-L4fj"
      },
      "outputs": [],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "\n",
        "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
        "bn = tf.keras.layers.BatchNormalization()\n",
        "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
        "\n",
        "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x)\n",
        "  y = layer1(x)\n",
        "  y = bn(y, training=True)\n",
        "  y = layer2(y)\n",
        "\n",
        "j = tape.jacobian(y, x)\n",
        "print(f'j.shape: {j.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNyZ1WhJMVLm"
      },
      "outputs": [],
      "source": [
        "plot_as_patches(j)\n",
        "\n",
        "_ = plt.title('These slices are not diagonal')\n",
        "_ = plt.xlabel(\"Don't use `batch_jacobian`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_x7ih5sarvG"
      },
      "source": [
        "En este caso, `batch_jacobian` aún se ejecuta y devuelve *algo* con la forma esperada, pero su contenido tiene un significado poco claro:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8_mICHoasCi"
      },
      "outputs": [],
      "source": [
        "jb = tape.batch_jacobian(y, x)\n",
        "print(f'jb.shape: {jb.shape}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "advanced_autodiff.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
