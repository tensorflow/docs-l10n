{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhGuhbZ6M5tl"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AwOEIRJC6Une"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Regresión logística para clasificaciones binarias con las API Core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBIlTPscrIT9"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/core/logistic_regression_core\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver el código fuente en GitHub</a> </td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauaqJ7WhIhO"
      },
      "source": [
        "En esta guía se muestra cómo utilizar las [API de bajo nivel de TensorFlow Core](https://www.tensorflow.org/guide/core) para realizar [clasificación binaria](https://developers.google.com/machine-learning/glossary#binary_classification){:.external} con [regresión logística](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external}. Este utiliza el conjunto de datos [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external} para la clasificación de tumores.\n",
        "\n",
        "[La regresión logística](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external} es uno de los algoritmos más populares para la clasificación binaria. Dado un conjunto de ejemplos con características, el objetivo de la regresión logística es obtener valores entre 0 y 1, que pueden interpretarse como las probabilidades de que cada ejemplo pertenezca a una clase concreta. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nchsZfwEVtVs"
      },
      "source": [
        "## Preparación\n",
        "\n",
        "Este tutorial utiliza [pandas](https://pandas.pydata.org){:.external} para leer un archivo CSV en un [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html){:.external}, [seaborn](https://seaborn.pydata.org){:.external} para trazar una relación por pares en un conjunto de datos, [Scikit-learn](https://scikit-learn.org/){:.external} para calcular una matriz de confusión, y [matplotlib](https://matplotlib.org/){:.external} para crear visualizaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lZoUK6AVTos"
      },
      "outputs": [],
      "source": [
        "!pip install -q seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.metrics as sk_metrics\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Preset matplotlib figure sizes.\n",
        "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
        "\n",
        "print(tf.__version__)\n",
        "# To make the results reproducible, set the random seed value.\n",
        "tf.random.set_seed(22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9ne3FZ-On"
      },
      "source": [
        "## Cargar los datos\n",
        "\n",
        "A continuación, cargue el conjunto de datos [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external} del repositorio [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/){:.external}. Este conjunto de datos contiene varias características, como el radio, la textura y la concavidad de un tumor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "outputs": [],
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
        "\n",
        "features = ['radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness',\n",
        "            'concavity', 'concave_poinits', 'symmetry', 'fractal_dimension']\n",
        "column_names = ['id', 'diagnosis']\n",
        "\n",
        "for attr in ['mean', 'ste', 'largest']:\n",
        "  for feature in features:\n",
        "    column_names.append(feature + \"_\" + attr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3VR1aTP92nV"
      },
      "source": [
        "Lea el conjunto de datos en un [DataFrame](){:.externo} de pandas utilizando [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html){:.externo}:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvR2Bzb691lJ"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(url, names=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB9eq6Zq-IZ4"
      },
      "outputs": [],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Z1V6Dg-La_"
      },
      "source": [
        "Muestra las cinco primeras filas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWxktwbv-KPp"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-Wn2jzVC1W"
      },
      "source": [
        "Divida el conjunto de datos en conjuntos de entrenamiento y prueba utilizando [`pandas.DataFrame.sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html){:.external}, [`pandas.DataFrame.drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html){:.external} y [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html){:.external}. Asegúrese de separar las características de las etiquetas del objetivo. El conjunto de prueba se utiliza para evaluar la generalizabilidad de su modelo a los datos que no se hayan visto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2O60B-IVG9Q"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset.sample(frac=0.75, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i06vHFv_QB24"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19JaochhaQ3m"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset.drop(train_dataset.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmHRcbAfaSag"
      },
      "outputs": [],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6JxBhBc_wwO"
      },
      "outputs": [],
      "source": [
        "# The `id` column can be dropped since each row is unique\n",
        "x_train, y_train = train_dataset.iloc[:, 2:], train_dataset.iloc[:, 1]\n",
        "x_test, y_test = test_dataset.iloc[:, 2:], test_dataset.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWuJTKEDM-f"
      },
      "source": [
        "## Tratamiento previo de los datos\n",
        "\n",
        "Este conjunto de datos contiene el promedio, el error estándar y los valores más grandes para cada una de las 10 medidas del tumor recogidas por el ejemplo. La columna objetivo `\"diagnosis\"` es una variable categórica con `\"M\"` lo cual indica un tumor maligno y `\"B\"` lo cual indica un diagnóstico de tumor benigno. Esta columna debe convertirse a un formato numérico binario para el entrenamiento del modelo.\n",
        "\n",
        "La función [`pandas.Series.map`](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html){:.external} es útil para asignar valores binarios a las categorías.\n",
        "\n",
        "El conjunto de datos también debe convertirse en un tensor con la función `tf.convert_to_tensor` una vez que se haya completado el preprocesamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEJHhN65a2VV"
      },
      "outputs": [],
      "source": [
        "y_train, y_test = y_train.map({'B': 0, 'M': 1}), y_test.map({'B': 0, 'M': 1})\n",
        "x_train, y_train = tf.convert_to_tensor(x_train, dtype=tf.float32), tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "x_test, y_test = tf.convert_to_tensor(x_test, dtype=tf.float32), tf.convert_to_tensor(y_test, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ubs136WLNp"
      },
      "source": [
        "Utilice [`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html){:.external} para revisar la distribución conjunta de algunos pares de características basadas en el promedio del conjunto de entrenamiento y observe cómo se relacionan con el objetivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRKO_x8gWKv-"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(train_dataset.iloc[:, 1:6], hue = 'diagnosis', diag_kind='kde');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YOG5iKYKW_3"
      },
      "source": [
        "En este diagrama de pares se observa que determinadas características, como el radio, el perímetro y el área, están muy correlacionadas. Esto es de esperarse ya que el radio del tumor está directamente involucrado en el cálculo tanto del perímetro como del área. Además, hay que tener en cuenta que los diagnósticos malignos parecen estar más sesgados a la derecha en muchas de las características.\n",
        "\n",
        "Asegúrese también de revisar las estadísticas generales. Observe cómo cada característica cubre una gama de valores muy diferente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi2FzC3T21jR"
      },
      "outputs": [],
      "source": [
        "train_dataset.describe().transpose()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8pDCIFjMla8"
      },
      "source": [
        "Debido a los rangos inconsistentes, es aconsejable normalizar los datos de manera que cada característica tenga un promedio cero y una varianza igual a uno. Este proceso se denomina [normalización](https://developers.google.com/machine-learning/glossary#normalization){:.external}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrzKNFNjLQDl"
      },
      "outputs": [],
      "source": [
        "class Normalize(tf.Module):\n",
        "  def __init__(self, x):\n",
        "    # Initialize the mean and standard deviation for normalization\n",
        "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
        "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0))\n",
        "\n",
        "  def norm(self, x):\n",
        "    # Normalize the input\n",
        "    return (x - self.mean)/self.std\n",
        "\n",
        "  def unnorm(self, x):\n",
        "    # Unnormalize the input\n",
        "    return (x * self.std) + self.mean\n",
        "\n",
        "norm_x = Normalize(x_train)\n",
        "x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3CrycBXA2s"
      },
      "source": [
        "## Regresión logística\n",
        "\n",
        "Antes de construir un modelo de regresión logística, es crucial comprender las diferencias entre este método y la regresión lineal tradicional.\n",
        "\n",
        "### Fundamentos de la regresión logística\n",
        "\n",
        "La regresión lineal devuelve una combinación lineal de sus entradas; esta salida es ilimitada. La salida de una [regresión logística](https://developers.google.com/machine-learning/glossary#logistic_regression){:.external} está en el rango `(0, 1)`. Para cada ejemplo, representa la probabilidad de que el ejemplo pertenezca a la clase *positiva*.\n",
        "\n",
        "La regresión logística transforma los resultados continuos de la regresión lineal tradicional, `(-∞, ∞)`, en las probabilidades, `(0, 1)`. Esta transformación también es simétrica, de modo que al invertir el signo de la salida lineal se obtiene una probabilidad inversa a la original.\n",
        "\n",
        "Sea $Y$ la probabilidad de estar en la clase `1` (el tumor es maligno). El mapeo deseado puede lograrse interpretando el resultado de la regresión lineal como la [relación logarítmica de probabilidades](https://developers.google.com/machine-learning/glossary#log-odds){:.external} de estar en la clase `1` frente a la clase `0`:\n",
        "\n",
        "$$\\ln(\\frac{Y}{1-Y}) = wX + b$$\n",
        "\n",
        "Si se establece $wX + b = z$, esta ecuación puede resolverse para $Y$:\n",
        "\n",
        "$$Y = \\frac{e^{z}}{1 + e^{z}} = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "La expresión $\\frac{1}{1 + e^{-z}}$ se conoce como la [función sigmoidal](https://developers.google.com/machine-learning/glossary#sigmoid_function){:.external} $\\sigma(z)$. Por lo tanto, la ecuación para la regresión logística se puede escribir como $Y = \\sigma(wX + b)$.\n",
        "\n",
        "El conjunto de datos de este tutorial trata una matriz de características de alta dimensión. Por lo tanto, la ecuación anterior debe reescribirse en forma de matriz vectorial de la siguiente manera:\n",
        "\n",
        "$${\\mathrm{Y}} = \\sigma({\\mathrm{X}}w + b)$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $\\underset{m\\times 1}{\\mathrm{Y}}$: un vector objetivo\n",
        "- $\\underset{m\\times n}{\\mathrm{X}}$: una matriz de características\n",
        "- $\\underset{n\\times 1}w$: un vector de pesos\n",
        "- $b$: un sesgo\n",
        "- $\\sigma$: una función sigmoidal aplicada a cada elemento del vector de salida\n",
        "\n",
        "Empiece visualizando la función sigmoidal, que transforma la salida lineal, `(-∞, ∞)`, para que caiga entre `0` y `1`. La función sigmoidal está disponible en `tf.math.sigmoid`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThHaV_RmucZl"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-10, 10, 500)\n",
        "x = tf.cast(x, tf.float32)\n",
        "f = lambda x : (1/20)*x + 0.6\n",
        "plt.plot(x, tf.math.sigmoid(x))\n",
        "plt.ylim((-0.1,1.1))\n",
        "plt.title(\"Sigmoid function\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMXEhrZuKECV"
      },
      "source": [
        "### La función de pérdida logarítmica\n",
        "\n",
        "La [función de pérdida logarítmica ](https://developers.google.com/machine-learning/glossary#Log_Loss){:.external}, o pérdida de entropía cruzada binaria, es la función de pérdida ideal para resolver un problema de clasificación binaria con regresión logística. Para cada ejemplo, la pérdida logarítmica cuantifica la similitud entre una probabilidad predicha y el valor verdadero del ejemplo. Esta función se determina mediante la siguiente ecuación:\n",
        "\n",
        "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}y_i\\cdot\\log(\\hat{y}_i) + (1- y_i)\\cdot\\log(1 - \\hat{y}_i)$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $\\hat{y}$: un vector de probabilidades previstas\n",
        "- $y$: un vector de objetivos verdaderos\n",
        "\n",
        "Puede utilizar la función `tf.nn.sigmoid_cross_entropy_with_logits` para calcular la pérdida logarítmica. Esta función aplica automáticamente la activación sigmoidal a la salida de la regresión:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVBInnSqS36W"
      },
      "outputs": [],
      "source": [
        "def log_loss(y_pred, y):\n",
        "  # Compute the log loss function\n",
        "  ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
        "  return tf.reduce_mean(ce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_mutLj0KNUb"
      },
      "source": [
        "### La regla de actualización del descenso por gradiente\n",
        "\n",
        "Las API de TensorFlow Core admiten la diferenciación automática con `tf.GradientTape`. Si tiene curiosidad acerca de las matemáticas detrás de la regresión logística [las actualizaciones del gradiente](https://developers.google.com/machine-learning/glossary#gradient_descent){:.external}, a continuación encontrará una breve explicación:\n",
        "\n",
        "En la ecuación anterior para la pérdida logarítmica, recuerde que cada $\\hat{y}_i$ puede ser reescrito en términos de las entradas como $\\sigma({\\mathrm{X_i}}w + b)$.\n",
        "\n",
        "El objetivo es encontrar $w^{em0}$ y $b^{/em0}$ que minimicen la pérdida logarítmica:\n",
        "\n",
        "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}y_i\\cdot\\log(\\sigma({\\mathrm{X_i}}w + b)) + (1- y_i)\\cdot\\log(1 - \\sigma({\\mathrm{X_i}}w + b))$$\n",
        "\n",
        "Si se toma el gradiente $L$ con respecto a $w$, se obtiene lo siguiente:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m}(\\sigma({\\mathrm{X}}w + b) - y)X$$\n",
        "\n",
        "Al tomar el gradiente $L$ con respecto a $b$, se obtiene lo siguiente:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}\\sigma({\\mathrm{X_i}}w + b) - y_i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTCndUecKZho"
      },
      "source": [
        "Ahora, construya el modelo de regresión logística."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0sXM7qLlKfZ"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.built = False\n",
        "    \n",
        "  def __call__(self, x, train=True):\n",
        "    # Initialize the model parameters on the first call\n",
        "    if not self.built:\n",
        "      # Randomly generate the weights and the bias term\n",
        "      rand_w = tf.random.uniform(shape=[x.shape[-1], 1], seed=22)\n",
        "      rand_b = tf.random.uniform(shape=[], seed=22)\n",
        "      self.w = tf.Variable(rand_w)\n",
        "      self.b = tf.Variable(rand_b)\n",
        "      self.built = True\n",
        "    # Compute the model output\n",
        "    z = tf.add(tf.matmul(x, self.w), self.b)\n",
        "    z = tf.squeeze(z, axis=1)\n",
        "    if train:\n",
        "      return z\n",
        "    return tf.sigmoid(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObQu9fDnXGL"
      },
      "source": [
        "Para validar, asegúrese de que el modelo no entrenado produce valores en el rango de `(0, 1)` para un pequeño subconjunto de datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bIovC0Z4QHJ"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ2ievISyf0p"
      },
      "outputs": [],
      "source": [
        "y_pred = log_reg(x_train_norm[:5], train=False)\n",
        "y_pred.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PribnwDHUksC"
      },
      "source": [
        "A continuación, escriba una función de precisión para calcular la proporción de las clasificaciones correctas durante el entrenamiento. Con el fin de recuperar las clasificaciones de las probabilidades predichas, establecer un umbral para que todas las probabilidades superiores al umbral pertenezcan a la clase `1`. Se trata de un hiperparámetro configurable que puede establecerse en `0,5` como valor predeterminado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssnVcKg7oMe6"
      },
      "outputs": [],
      "source": [
        "def predict_class(y_pred, thresh=0.5):\n",
        "  # Return a tensor with  `1` if `y_pred` > `0.5`, and `0` otherwise\n",
        "  return tf.cast(y_pred > thresh, tf.float32)\n",
        "\n",
        "def accuracy(y_pred, y):\n",
        "  # Return the proportion of matches between `y_pred` and `y`\n",
        "  y_pred = tf.math.sigmoid(y_pred)\n",
        "  y_pred_class = predict_class(y_pred)\n",
        "  check_equal = tf.cast(y_pred_class == y,tf.float32)\n",
        "  acc_val = tf.reduce_mean(check_equal)\n",
        "  return acc_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_0KHQ25_2dF"
      },
      "source": [
        "### Entrenar al modelo\n",
        "\n",
        "El uso de minilotes para el entrenamiento proporciona eficiencia en la memoria y una convergencia más rápida. La API `tf.data.Dataset` tiene funciones útiles para el procesamiento por lotes y la mezcla. La API le permite construir canalizaciones de entrada complejas a partir de piezas sencillas y reutilizables. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJD7-4U0etqa"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0]).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test))\n",
        "test_dataset = test_dataset.shuffle(buffer_size=x_test.shape[0]).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLiWZZPBSDip"
      },
      "source": [
        "Ahora escriba un bucle de entrenamiento para el modelo de regresión logística. El bucle utiliza la función de pérdida logarítmica y sus gradientes con respecto a la entrada para actualizar repetidamente los parámetros del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNC3D1DGsGgK"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "epochs = 200\n",
        "learning_rate = 0.01\n",
        "train_losses, test_losses = [], []\n",
        "train_accs, test_accs = [], []\n",
        "\n",
        "# Set up the training loop and begin training\n",
        "for epoch in range(epochs):\n",
        "  batch_losses_train, batch_accs_train = [], []\n",
        "  batch_losses_test, batch_accs_test = [], []\n",
        "\n",
        "  # Iterate over the training data\n",
        "  for x_batch, y_batch in train_dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred_batch = log_reg(x_batch)\n",
        "      batch_loss = log_loss(y_pred_batch, y_batch)\n",
        "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
        "    # Update the parameters with respect to the gradient calculations\n",
        "    grads = tape.gradient(batch_loss, log_reg.variables)\n",
        "    for g,v in zip(grads, log_reg.variables):\n",
        "      v.assign_sub(learning_rate * g)\n",
        "    # Keep track of batch-level training performance\n",
        "    batch_losses_train.append(batch_loss)\n",
        "    batch_accs_train.append(batch_acc)\n",
        "\n",
        "  # Iterate over the testing data\n",
        "  for x_batch, y_batch in test_dataset:\n",
        "    y_pred_batch = log_reg(x_batch)\n",
        "    batch_loss = log_loss(y_pred_batch, y_batch)\n",
        "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
        "    # Keep track of batch-level testing performance\n",
        "    batch_losses_test.append(batch_loss)\n",
        "    batch_accs_test.append(batch_acc)\n",
        "\n",
        "  # Keep track of epoch-level model performance\n",
        "  train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n",
        "  test_loss, test_acc = tf.reduce_mean(batch_losses_test), tf.reduce_mean(batch_accs_test)\n",
        "  train_losses.append(train_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accs.append(test_acc)\n",
        "  if epoch % 20 == 0:\n",
        "    print(f\"Epoch: {epoch}, Training log loss: {train_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoLiAg7fYft7"
      },
      "source": [
        "### Evaluación del desempeño\n",
        "\n",
        "Observe los cambios en la pérdida y precisión de su modelo a lo largo del tiempo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv3oCQPvWhr0"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(epochs), train_losses, label = \"Training loss\")\n",
        "plt.plot(range(epochs), test_losses, label = \"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Log loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Log loss vs training iterations\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2HDVGLPODIE"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(epochs), train_accs, label = \"Training accuracy\")\n",
        "plt.plot(range(epochs), test_accs, label = \"Testing accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy vs training iterations\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jonKhUzuPyfa"
      },
      "outputs": [],
      "source": [
        "print(f\"Final training log loss: {train_losses[-1]:.3f}\")\n",
        "print(f\"Final testing log Loss: {test_losses[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3DF4qyrPyke"
      },
      "outputs": [],
      "source": [
        "print(f\"Final training accuracy: {train_accs[-1]:.3f}\")\n",
        "print(f\"Final testing accuracy: {test_accs[-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrj1TbOJasjA"
      },
      "source": [
        "El modelo demuestra una alta precisión y una baja pérdida cuando se trata de clasificar tumores en el conjunto de datos de entrenamiento y también generaliza bien a los datos de prueba que no se han visualizado. Para ir un paso más allá, puede explorar las tasas de error que ofrecen más información que la puntuación de precisión general. Las dos tasas de error más populares para los problemas de clasificación binaria son la tasa de falsos positivos (FPR) y la tasa de falsos negativos (FNR).\n",
        "\n",
        "Para este problema, el FPR es la proporción de predicciones de tumores malignos entre los tumores que son realmente benignos. A la inversa, el FNR es la proporción de predicciones de tumores benignos entre los tumores que son realmente malignos.\n",
        "\n",
        "Calcule una matriz de confusión utilizando [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix){:.external}, la cual evalúa la precisión de la clasificación, y utilice matplotlib para mostrar la matriz:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJO7YkA8ZDMU"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(y, y_classes, typ):\n",
        "  # Compute the confusion matrix and normalize it\n",
        "  plt.figure(figsize=(10,10))\n",
        "  confusion = sk_metrics.confusion_matrix(y.numpy(), y_classes.numpy())\n",
        "  confusion_normalized = confusion / confusion.sum(axis=1, keepdims=True)\n",
        "  axis_labels = range(2)\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.4f', square=True)\n",
        "  plt.title(f\"Confusion matrix: {typ}\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "y_pred_train, y_pred_test = log_reg(x_train_norm, train=False), log_reg(x_test_norm, train=False)\n",
        "train_classes, test_classes = predict_class(y_pred_train), predict_class(y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ5DFcleiDFm"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(y_train, train_classes, 'Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtfcsAp_iCNR"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(y_test, test_classes, 'Testing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlivxaDmTnGq"
      },
      "source": [
        "Observe las medidas de la tasa de error e interprete su significado en el contexto de este ejemplo. En muchos estudios de pruebas médicas, como la detección del cáncer, tener una tasa de falsos positivos alta para garantizar una tasa de falsos negativos baja es perfectamente aceptable y, de hecho, se fomenta, ya que el riesgo de no diagnosticar un tumor maligno (falso negativo) es mucho peor que clasificar erróneamente un tumor benigno como maligno (falso positivo).\n",
        "\n",
        "Para controlar el FPR y el FNR, pruebe a cambiar el hiperparámetro umbral antes de clasificar las predicciones de probabilidad. Un umbral más bajo aumenta las posibilidades generales del modelo de realizar una clasificación de tumor maligno. Esto aumenta inevitablemente el número de falsos positivos y el FPR, pero también ayuda a disminuir el número de falsos negativos y el FNR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADEN2rb4Nhj"
      },
      "source": [
        "## Guardar el modelo\n",
        "\n",
        "Comience por crear un módulo de exportación que reciba los datos sin procesar y realice las siguientes operaciones:\n",
        "\n",
        "- Normalización\n",
        "- Predicción de probabilidades\n",
        "- Predicción de clases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KPRHCzg4ZxH"
      },
      "outputs": [],
      "source": [
        "class ExportModule(tf.Module):\n",
        "  def __init__(self, model, norm_x, class_pred):\n",
        "    # Initialize pre- and post-processing functions\n",
        "    self.model = model\n",
        "    self.norm_x = norm_x\n",
        "    self.class_pred = class_pred\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], dtype=tf.float32)])\n",
        "  def __call__(self, x):\n",
        "    # Run the `ExportModule` for new data points\n",
        "    x = self.norm_x.norm(x)\n",
        "    y = self.model(x, train=False)\n",
        "    y = self.class_pred(y)\n",
        "    return y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzRclo5-yjO"
      },
      "outputs": [],
      "source": [
        "log_reg_export = ExportModule(model=log_reg,\n",
        "                              norm_x=norm_x,\n",
        "                              class_pred=predict_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtofGIBN_qFd"
      },
      "source": [
        "Si desea guardar el modelo en su estado actual, puede hacerlo con la función `tf.saved_model.save`. Para cargar un modelo guardado y realizar predicciones, utilice la función `tf.saved_model.load`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4Qum1Ts_pmF"
      },
      "outputs": [],
      "source": [
        "models = tempfile.mkdtemp()\n",
        "save_path = os.path.join(models, 'log_reg_export')\n",
        "tf.saved_model.save(log_reg_export, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KPILr1i_M_c"
      },
      "outputs": [],
      "source": [
        "log_reg_loaded = tf.saved_model.load(save_path)\n",
        "test_preds = log_reg_loaded(x_test)\n",
        "test_preds[:10].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgGQuV-yqYZH"
      },
      "source": [
        "## Conclusión\n",
        "\n",
        "En este bloc de notas se presentaron algunas técnicas para tratar un problema de regresión logística. Aquí encontrará algunos consejos más que pueden serle útiles:\n",
        "\n",
        "- Las [API de TensorFlow Core](https://www.tensorflow.org/guide/core) se pueden utilizar para crear flujos de trabajo de aprendizaje automático con altos niveles de configuración.\n",
        "- El análisis de las tasas de error es una forma excelente de obtener más información sobre el rendimiento de un modelo de clasificación, más allá de su puntuación de precisión global.\n",
        "- El sobreajuste es otro problema común para los modelos de regresión logística, aunque no fue un problema para este tutorial. Visite el tutorial [Sobreajuste y subajuste](../../tutorials/keras/overfit_and_underfit.ipynb) para obtener más ayuda al respecto.\n",
        "\n",
        "Para obtener más ejemplos sobre el uso de las API de TensorFlow Core, consulte la [guía](https://www.tensorflow.org/guide/core). Si desea obtener más información sobre la carga y preparación de datos, consulte los tutoriales sobre la [carga de datos de imagen](../../tutorials/load_data/images.ipynb) o la [carga de datos CSV](../../tutorials/load_data/csv.ipynb)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "logistic_regression_core.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
