{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxv6goXm7oGF"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "llMNufAK7nfK"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Byow2J6LaPl"
      },
      "source": [
        "# tf.data: Construir canalizaciones de entrada de TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGXS3UWBBNoc"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/data\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/guide/data.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/guide/data.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver código fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/guide/data.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qo3HgDjbDcI"
      },
      "source": [
        "La API `tf.data` le permite construir canalizaciones de entrada complejas con partes simples y reutilizables. Por ejemplo, la canalización para un modelo de imagen puede agregar datos desde archivos en un sistema de archivos distribuido, aplicar perturbaciones aleatorias a cada imagen y combinar imágenes seleccionadas de forma aleatoria en un lote para entrenamiento. La canalización para un modelo de texto puede incluir extraer símbolos de datos de texto sin procesar, convertirlos en identificadores incrustados con una tabla de búsqueda y combinar secuencias de diferentes longitudes en lotes. La API  `tf.data` posibilita que se puedan manipular grandes cantidades de datos, leer desde distintos formatos de datos y realizar transformaciones complejas.\n",
        "\n",
        "La API `tf.data` introduce una abstracción `tf.data.Dataset` que representa una secuencia de elementos, en el que cada elemento consiste en uno o más componentes. Por ejemplo, en una canalización de imagen, un elemento puede ser un ejemplo de un solo entrenamiento, con un par de componentes de tensor que representan la imagen y su etiqueta.\n",
        "\n",
        "Hay dos formas distintas de crear un conjunto de datos:\n",
        "\n",
        "- Un **origen** de datos construye un `Dataset` con datos alamacenados en la memoria o en uno o más archivos.\n",
        "\n",
        "- Una **transformación** de datos construye un conjunto de datos con uno o más objetos `tf.data.Dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJIEjEIBdf-h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y0JtWBNR9E5"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l4a0ALxdaWF"
      },
      "source": [
        "## Mécanica básica\n",
        "\n",
        "<a id=\"basic-mechanics\"></a>\n",
        "\n",
        "Para crear una canalización de entrada, debe comenzar con un *origen* de datos. Por ejemplo, para construir un `Dataset` con datos almacenados en la memoria, puede usar `tf.data.Dataset.from_tensors()` o `tf.data.Dataset.from_tensor_slices()`. Como alternativa, si sus datos de entrada están almacenados en un archivo en el formato TFRecord recomendado, puede usar `tf.data.TFRecordDataset()`.\n",
        "\n",
        "Una vez que tenga el objeto, `Dataset`, puede *transformarlo* en un nuevo `Dataset` al encadenar las llamadas de método en el objeto `tf.data.Dataset`. Por ejemplo, puede aplicar transformaciones por cada elemento como `Dataset.map` y transformaciones de varios elementos como `Dataset.batch`. Consulte la documentación de `tf.data.Dataset` para ver una lista completa de las transformaciones.\n",
        "\n",
        "El objeto `Dataset` es un elemento de iteración de Python. Esto hace que se puedan consumir sus elementos con un bucle for:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F-FDnjB6t6J"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwJsRJ-FbDcJ"
      },
      "outputs": [],
      "source": [
        "for elem in dataset:\n",
        "  print(elem.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0yy80MobDcM"
      },
      "source": [
        "O al crear explícitamente un elemento de iteración con `iter` y consumir sus elementos con `next`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03w9oxFfbDcM"
      },
      "outputs": [],
      "source": [
        "it = iter(dataset)\n",
        "\n",
        "print(next(it).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4CgCL8qbDcO"
      },
      "source": [
        "De forma alternativa, se pueden consumir los elementos del conjunto de datos con la transformación `reduce`, que reduce todos los elementos para producir un resultado único. En el siguiente ejemplo, se ilustra cómo usar la transformación `reduce` para calcular la suma de los conjuntos de datos de enteros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2bHAeNxbDcO"
      },
      "outputs": [],
      "source": [
        "print(dataset.reduce(0, lambda state, value: state + value).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Fzwt2nbDcR"
      },
      "source": [
        "<!-- TODO(jsimsa): Talk about `tf.function` support. -->\n",
        "\n",
        "<a id=\"dataset_structure\"></a>\n",
        "\n",
        "### Estructura del conjunto de datos\n",
        "\n",
        "Un conjunto de datos produce una secuencia de *elementos*, donde cada elemento es la misma estructura (anidada) de *componentes*. Los componentes individuales de la estructura pueden ser de cualquier tipo que pueda representarse con `tf.TypeSpec`, incluidos el `tf.Tensor`, `tf.sparse.SparseTensor`, `tf.RaggedTensor`, `tf.TensorArray` o `tf.data.Dataset`.\n",
        "\n",
        "Entre las construcciones de Python que pueden usarse para expresar una estructura (anidada) de elementos se incluyen  `tuple`, `dict`, `NamedTuple` y `OrderedDict`. En especial, `list` no es una construcción válida para expresar la estructura de los elementos del conjunto de datos. Esto es así porque los primeros usuarios de `tf.data` estaban convencidos de que las entradas de `list` (por ejemplo, al pasarlas a  `tf.data.Dataset.from_tensors`) se empaquetan automáticamente como tensores y las salidas de `list` (por ejemplo, valores de retorno de las funciones definidas por el usuario) se coaccionan en una `tuple`. Como consecuencia, si quiere que se trate una entrada de `list` como una estructura, deberá convertirla en `tuple` y si quiere que una salida de `list` sea un componente individual, deberá empaquetarla explícitamente con `tf.stack`.\n",
        "\n",
        "La propiedad `Dataset.element_spec` le permite inspeccionar el tipo de cada componente del elemento. La propiedad devuelve una *estructura anidada* de objetos `tf.TypeSpec` para coincidir con la estructura del elemento, que puede ser un componente único, una tupla de componentes o una tupla anidada de componentes. Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg0m1beIhXGn"
      },
      "outputs": [],
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
        "\n",
        "dataset1.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwyemaghhXaG"
      },
      "outputs": [],
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "   (tf.random.uniform([4]),\n",
        "    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
        "\n",
        "dataset2.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CL7aB0ahXn_"
      },
      "outputs": [],
      "source": [
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "\n",
        "dataset3.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5bz7R1xhX1f"
      },
      "outputs": [],
      "source": [
        "# Dataset containing a sparse tensor.\n",
        "dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))\n",
        "\n",
        "dataset4.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVOPHur_hYQv"
      },
      "outputs": [],
      "source": [
        "# Use value_type to see the type of value represented by the element spec\n",
        "dataset4.element_spec.value_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5xNsFFvhUnr"
      },
      "source": [
        "Las transformaciones de `Dataset` admiten conjuntos de datos de cualquier estructura. Cuando se usan las transformaciones de `Dataset.map` y de `Dataset.filter`, que aplican una función a cada elemento, la estructura del elemento determina los argumentos de la función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2myAr3Pxd-zF"
      },
      "outputs": [],
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
        "    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))\n",
        "\n",
        "dataset1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woPXMP14gUTg"
      },
      "outputs": [],
      "source": [
        "for z in dataset1:\n",
        "  print(z.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53PA4x6XgLar"
      },
      "outputs": [],
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "   (tf.random.uniform([4]),\n",
        "    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
        "\n",
        "dataset2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ju4sNSebDcR"
      },
      "outputs": [],
      "source": [
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "\n",
        "dataset3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgxsfAS2g6gk"
      },
      "outputs": [],
      "source": [
        "for a, (b,c) in dataset3:\n",
        "  print('shapes: {a.shape}, {b.shape}, {c.shape}'.format(a=a, b=b, c=c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1s2K0g-bDcT"
      },
      "source": [
        "## Leer datos de entrada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3JG2f0h2683"
      },
      "source": [
        "### Consumir arreglos de NumPy\n",
        "\n",
        "Consulte el tutorial de [Cargar arreglos de NumPy](../tutorials/load_data/numpy.ipynb) para ver más ejemplos.\n",
        "\n",
        "Si todos los datos de entrada entran en la memoria, la forma más simple de crear un `Dataset` con ellos es convertirlos en objetos `tf.Tensor` y usar `Dataset.from_tensor_slices`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmaE6PjjhQ47"
      },
      "outputs": [],
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6cNiuDBbDcU"
      },
      "outputs": [],
      "source": [
        "images, labels = train\n",
        "images = images/255\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkwrDHN5bDcW"
      },
      "source": [
        "Nota: El fragmento de código anterior encrustará los arreglos de  `features` y `labels` como operaciones `tf.constant()` en su gráfico de TensorFlow. Esto funciona bien para conjuntos de datos pequeños, pero gasta memoria, porque el contenido del arreglo se copiará varias veces, y puede ejecutarse dentro del límite de 2 GB para el búfer del protocolo `tf.GraphDef`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO4ua2gEmIhR"
      },
      "source": [
        "### Consumir generadores de Python\n",
        "\n",
        "Otro origen de datos común que puede ingresarse fácilmente como `tf.data.Dataset` es el generador de Python.\n",
        "\n",
        "Advertencia: Si bien puede ser un enfoque práctico, tiene portabilidad y escalabilidad limitadas. Debe ejecutarse en el mismo proceso de Python que con el que se creó el generador, y aún así está sujeto al [GIL](https://en.wikipedia.org/wiki/Global_interpreter_lock) (bloqueo global del intérprete) de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9njpME-jmDza"
      },
      "outputs": [],
      "source": [
        "def count(stop):\n",
        "  i = 0\n",
        "  while i<stop:\n",
        "    yield i\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwqLrjnTpD8Y"
      },
      "outputs": [],
      "source": [
        "for n in count(5):\n",
        "  print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_BB_PhxnVVx"
      },
      "source": [
        "El constructor `Dataset.from_generator` convierte el generador de Python en un `tf.data.Dataset` completamente funcional.\n",
        "\n",
        "El constructor toma un invocable como entrada, no un elemento de iteración. Esto le permite reiniciar el generador cuando finaliza. Toma argumentos `args` opcionales, que se pasan como los argumentos del invocable.\n",
        "\n",
        "Se requiere el argumento `output_types` porque `tf.data` construye un `tf.Graph` internamente y los bordes del gráfico requieren un `tf.dtype`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFga_OTwm0Je"
      },
      "outputs": [],
      "source": [
        "ds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes = (), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fel1SUuBnDUE"
      },
      "outputs": [],
      "source": [
        "for count_batch in ds_counter.repeat().batch(10).take(10):\n",
        "  print(count_batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxy9hDMTq1zD"
      },
      "source": [
        "El argumento de `output_shapes` no es *obligatorio* pero se recomienda su uso, ya que muchas operaciones de TensorFlow no admiten tensores con un rango desconocido. Si la longitud de un eje en particular es desconocida o variable, establezca el valor como `None` en el `output_shapes`.\n",
        "\n",
        "También es importante tener en cuenta que `output_shapes` y `output_types` siguen las mismas reglas de anidación que otros métodos de conjuntos de datos.\n",
        "\n",
        "A continuación, se muestra un generador de ejemplo que muestra los dos aspectos: devuelve tuplas de arreglos, donde el segundo arreglo es un vector con una longitud desconocida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "allFX1g8rGKe"
      },
      "outputs": [],
      "source": [
        "def gen_series():\n",
        "  i = 0\n",
        "  while True:\n",
        "    size = np.random.randint(0, 10)\n",
        "    yield i, np.random.normal(size=(size,))\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ku26Yb9rcJX"
      },
      "outputs": [],
      "source": [
        "for i, series in gen_series():\n",
        "  print(i, \":\", str(series))\n",
        "  if i > 5:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmkynGilx0qf"
      },
      "source": [
        " La primera salida es un `int32` y la segunda es un `float32`.\n",
        "\n",
        "El primer elemento es un escalar, forma `()` y el segundo es un vector de longitud desconocida, forma `(None,)` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDTfhEzhsliM"
      },
      "outputs": [],
      "source": [
        "ds_series = tf.data.Dataset.from_generator(\n",
        "    gen_series, \n",
        "    output_types=(tf.int32, tf.float32), \n",
        "    output_shapes=((), (None,)))\n",
        "\n",
        "ds_series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWxvSyQiyN0o"
      },
      "source": [
        "Ahora se puede usar como un `tf.data.Dataset` normal. Tenga en cuenta que al poner un conjunto de datos en un lote con una forma variable, se debe usar `Dataset.padded_batch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7jEpj3As1lO"
      },
      "outputs": [],
      "source": [
        "ds_series_batch = ds_series.shuffle(20).padded_batch(10)\n",
        "\n",
        "ids, sequence_batch = next(iter(ds_series_batch))\n",
        "print(ids.numpy())\n",
        "print()\n",
        "print(sequence_batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hcqOccJ1CxG"
      },
      "source": [
        "Para ver un ejemplo más realista, intente encapsular `preprocessing.image.ImageDataGenerator` como un `tf.data.Dataset`.\n",
        "\n",
        "Primero, descargue los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-_JCFRQ1CXM"
      },
      "outputs": [],
      "source": [
        "flowers = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIjPhvQ87jUT"
      },
      "source": [
        "Cree el `image.ImageDataGenerator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPCZeBQE5DfH"
      },
      "outputs": [],
      "source": [
        "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my4PxqfH26p6"
      },
      "outputs": [],
      "source": [
        "images, labels = next(img_gen.flow_from_directory(flowers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd96nH1w3eKH"
      },
      "outputs": [],
      "source": [
        "print(images.dtype, images.shape)\n",
        "print(labels.dtype, labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvRwvt5E2rTH"
      },
      "outputs": [],
      "source": [
        "ds = tf.data.Dataset.from_generator(\n",
        "    lambda: img_gen.flow_from_directory(flowers), \n",
        "    output_types=(tf.float32, tf.float32), \n",
        "    output_shapes=([32,256,256,3], [32,5])\n",
        ")\n",
        "\n",
        "ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcaULBCXj_2_"
      },
      "outputs": [],
      "source": [
        "for images, labels in ds.take(1):\n",
        "  print('images.shape: ', images.shape)\n",
        "  print('labels.shape: ', labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma4XoYzih2f4"
      },
      "source": [
        "### Consumir datos TFRecord\n",
        "\n",
        "Consulte el tutorial de [Cargar datos TFRecord](../tutorials/load_data/tfrecord.ipynb) para ver un ejemplo de principio a fin.\n",
        "\n",
        "La API `tf.data` admite una variedad de formatos de archivos para poder procesar conjuntos de datos grandes que no entren en la memoria. Por ejemplo, el formato de archivo TFRecord es un formato binario simple orientado a registros que muchas de las aplicaciones de TensorFlow usan para entrenar datos. La clase `tf.data.TFRecordDataset` permite transmitir el contenido de uno o más archivos TFRecord como parte de una canalización de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiatWUloRJc4"
      },
      "source": [
        "Aquí tiene un ejemplo que usa el archivo de prueba del nombre de las calles francesas (FSNS, por sus siglas en inglés)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZo_4fzdbDcW"
      },
      "outputs": [],
      "source": [
        "# Creates a dataset that reads all of the examples from two files.\n",
        "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seD5bOH3RhBP"
      },
      "source": [
        "El argumento `filenames` del inicializador `TFRecordDataset` puede ser una cadena de texto, un lista de cadenas de texto o un `tf.Tensor` de cadenas de texto. Por eso, si tiene dos conjuntos de archivos para entrenamiento y validación, puede crear un método factory que genera los conjuntos de datos y toma los nombres de archivos como un argumento de entrada:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2WV5d7DRUA-"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62NC3vz9U8ww"
      },
      "source": [
        "Muchos de los proyectos de TensorFlow usan registros `tf.train.Example` en serie en sus archivos TFRecord. Deben decodificarse antes de inspeccionarlos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tk29nlMl5P3"
      },
      "outputs": [],
      "source": [
        "raw_example = next(iter(dataset))\n",
        "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "parsed.features.feature['image/text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJAUib10bDcb"
      },
      "source": [
        "### Consumir datos de texto\n",
        "\n",
        "Consulte el tutorial de [Cargar texto](../tutorials/load_data/text.ipynb) para ver un ejemplo de principio a fin.\n",
        "\n",
        "Muchos conjuntos de datos están distribuidos en uno o más archivos de texto. El `tf.data.TextLineDataset` proporciona una forma fácil de extraer líneas de uno o más archivos de texto. Si se ingresan uno o más nombres de archivo, un `TextLineDataset` generará un elemento con valor de cadena de texto por cada línea de los archivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQMoFu2TbDcc"
      },
      "outputs": [],
      "source": [
        "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "file_paths = [\n",
        "    tf.keras.utils.get_file(file_name, directory_url + file_name)\n",
        "    for file_name in file_names\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il4cOjiVwj95"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.TextLineDataset(file_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MevIbDiwy4MC"
      },
      "source": [
        "Estas son las primeras líneas del primer archivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpEHKyvHxu8A"
      },
      "outputs": [],
      "source": [
        "for line in dataset.take(5):\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJyVw8ro7fey"
      },
      "source": [
        "Para alternar líneas entre archivos use `Dataset.interleave`. Esto hará que sea más fácil pasar de un archivo a otro de forma aleatoria. Estas son la primera, la segunda y la tercera línea de cada traducción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UCveWOt7fDE"
      },
      "outputs": [],
      "source": [
        "files_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
        "lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)\n",
        "\n",
        "for i, line in enumerate(lines_ds.take(9)):\n",
        "  if i % 3 == 0:\n",
        "    print()\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F_pOIDubDce"
      },
      "source": [
        "De forma predeterminada, un `TextLineDataset` da *cada* línea de cada archivo, que tal vez no sea conveniente, por ejemplo, si el archivo comienza con una línea de encabezado o tiene comentarios. Se pueden eliminar estas líneas con las transformaciones `Dataset.skip()` o `Dataset.filter`. Aquí, se omite la primera línea y luego se filtra para encontrar solo las sobrevivientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6b20Gua2jPO"
      },
      "outputs": [],
      "source": [
        "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic_lines = tf.data.TextLineDataset(titanic_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M1pauNT68B2"
      },
      "outputs": [],
      "source": [
        "for line in titanic_lines.take(10):\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEIP95cibDcf"
      },
      "outputs": [],
      "source": [
        "def survived(line):\n",
        "  return tf.not_equal(tf.strings.substr(line, 0, 1), \"0\")\n",
        "\n",
        "survivors = titanic_lines.skip(1).filter(survived)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odQ4618h1XqD"
      },
      "outputs": [],
      "source": [
        "for line in survivors.take(10):\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5z5B11UjDTd"
      },
      "source": [
        "### Consumir datos CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChDHNi3qbDch"
      },
      "source": [
        "Consulte los tutoriales de [Cargar archivos CSV](../tutorials/load_data/csv.ipynb) y [Cargar DataFrames de Panda](../tutorials/load_data/pandas_dataframe.ipynb) para ver más ejemplos.\n",
        "\n",
        "El formato de archivo CSV es un formato conocido para almacenar datos tabulares en texto sin formato.\n",
        "\n",
        "Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj28j5u49Bjm"
      },
      "outputs": [],
      "source": [
        "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghvtmW40LM0B"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(titanic_file)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9uBqt5oGsR-"
      },
      "source": [
        "Si sus datos entran en la memoria, el mismo método de `Dataset.from_tensor_slices` funciona con diccionarios, lo que permite que se puedan importar los datos de forma fácil:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmAMCiPJA0qO"
      },
      "outputs": [],
      "source": [
        "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
        "\n",
        "for feature_batch in titanic_slices.take(1):\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"  {!r:20s}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47yippqaHFk6"
      },
      "source": [
        "Un enfoque más escalable es cargar datos desde el disco según sea necesario.\n",
        "\n",
        "El módulo `tf.data` proporciona métodos para extraer registros de uno o más archivos CSV que cumplan con el estándar [RFC 4180](https://tools.ietf.org/html/rfc4180).\n",
        "\n",
        "La función `tf.data.experimental.make_csv_dataset` es la interfaz de alto nivel para leer conjuntos de archivos CSV. Admite inferencias de tipo de columna y muchas otras funcionalidades, como poner datos en lotes y en orden aleatorio, para simplificar el uso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHUDrM_s_brq"
      },
      "outputs": [],
      "source": [
        "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
        "    titanic_file, batch_size=4,\n",
        "    label_name=\"survived\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsZfhz79_Wlg"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in titanic_batches.take(1):\n",
        "  print(\"'survived': {}\".format(label_batch))\n",
        "  print(\"features:\")\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"  {!r:20s}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_5N7CdNGYAa"
      },
      "source": [
        "Puede usar el argumento `select_columns` si solo necesita un subconjunto de columnas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9KNHyDwF2Sc"
      },
      "outputs": [],
      "source": [
        "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
        "    titanic_file, batch_size=4,\n",
        "    label_name=\"survived\", select_columns=['class', 'fare', 'survived'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C2uosFnGIT8"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in titanic_batches.take(1):\n",
        "  print(\"'survived': {}\".format(label_batch))\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"  {!r:20s}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSVgJJ1HJD6M"
      },
      "source": [
        "También hay una clase `experimental.CsvDataset` de nivel más bajo que proporciona un control más específico. No admite las inferencias de tipo de columna. Se debe especificar el tipo de cada columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP1Y_NXA8bYl"
      },
      "outputs": [],
      "source": [
        "titanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string]\n",
        "dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True)\n",
        "\n",
        "for line in dataset.take(10):\n",
        "  print([item.numpy() for item in line])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZSuLVsTbDcj"
      },
      "source": [
        "Si algunas columnas están vacías, la inferencia de bajo nivel le permite proporcionar valores predeterminados en lugar de tipos de columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qry-g90FMo2I"
      },
      "outputs": [],
      "source": [
        "%%writefile missing.csv\n",
        "1,2,3,4\n",
        ",2,3,4\n",
        "1,,3,4\n",
        "1,2,,4\n",
        "1,2,3,\n",
        ",,,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5_hbiE9bDck"
      },
      "outputs": [],
      "source": [
        "# Creates a dataset that reads all of the records from two CSV files, each with\n",
        "# four float columns which may have missing values.\n",
        "\n",
        "record_defaults = [999,999,999,999]\n",
        "dataset = tf.data.experimental.CsvDataset(\"missing.csv\", record_defaults)\n",
        "dataset = dataset.map(lambda *items: tf.stack(items))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__jc7iD9M9FC"
      },
      "outputs": [],
      "source": [
        "for line in dataset:\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_4g0cIvbDcl"
      },
      "source": [
        "De forma predeterminada, un `CsvDataset` genera *cada* columna de *cada* línea del archivo, que quizás no sea conveniente si, por ejemplo, el archivo comienza con una línea de encabezado que debería ser ignorada o si no se necesitan algunas columnas en la entrada. Se pueden eliminar estas líneas y campos con los argumentos `header` y `select_cols` respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2IF_K0obDcm"
      },
      "outputs": [],
      "source": [
        "# Creates a dataset that reads all of the records from two CSV files with\n",
        "# headers, extracting float data from columns 2 and 4.\n",
        "record_defaults = [999, 999] # Only provide defaults for the selected columns\n",
        "dataset = tf.data.experimental.CsvDataset(\"missing.csv\", record_defaults, select_cols=[1, 3])\n",
        "dataset = dataset.map(lambda *items: tf.stack(items))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5aLprDeRNb0"
      },
      "outputs": [],
      "source": [
        "for line in dataset:\n",
        "  print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJfhb03koVN"
      },
      "source": [
        "### Consumir conjuntos de archivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAO7SZDSk57_"
      },
      "source": [
        "Hay muchos conjuntos de datos distribuidos como conjuntos de archivos, donde cada archivo es un ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dZwN3CS-jV2"
      },
      "outputs": [],
      "source": [
        "flowers_root = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)\n",
        "flowers_root = pathlib.Path(flowers_root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4099UU8n-jHP"
      },
      "source": [
        "Nota: estas imágenes están bajo la licencia de CC-BY, consulte LICENSE.txt para obtener más detalles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCyTYpmDs_jE"
      },
      "source": [
        "El directorio de raíz contiene un directorio para cada clase:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2iCXsHu6jJH"
      },
      "outputs": [],
      "source": [
        "for item in flowers_root.glob(\"*\"):\n",
        "  print(item.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylj9fgkamgWZ"
      },
      "source": [
        "Los archivos en cada directorio de clase son ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAkQp5uxoINu"
      },
      "outputs": [],
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
        "\n",
        "for f in list_ds.take(5):\n",
        "  print(f.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91CPfUUJ_8SZ"
      },
      "source": [
        "Lea los datos con la función `tf.io.read_file` y extraiga la etiqueta desde la ruta de acceso, esto devolverá pares de `(image, label)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xhBRgvNqRRe"
      },
      "outputs": [],
      "source": [
        "def process_path(file_path):\n",
        "  label = tf.strings.split(file_path, os.sep)[-2]\n",
        "  return tf.io.read_file(file_path), label\n",
        "\n",
        "labeled_ds = list_ds.map(process_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxrl0lGdnpRz"
      },
      "outputs": [],
      "source": [
        "for image_raw, label_text in labeled_ds.take(1):\n",
        "  print(repr(image_raw.numpy()[:100]))\n",
        "  print()\n",
        "  print(label_text.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEh46Ee0oSH5"
      },
      "source": [
        "<!--\n",
        "TODO(mrry): Add this section.\n",
        "\n",
        "### Handling text data with unusual sizes\n",
        "-->\n",
        "\n",
        "## Procesar elementos del conjunto de datos por lotes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR-2xY-8oSH4"
      },
      "source": [
        "### Procesamiento por lotes\n",
        "\n",
        "La forma más simple de procesamiento por lotes apila `n` elementos consecutivos de un conjunto de datos en un solo elemento. La transformación `Dataset.batch()` hace exactamente esto con las mismas restricciones que el operador `tf.stack()`, aplicado a cada componente de los elementos: esto significa que para cada componente *i*, todos los elementos deben tener un tensor de exactamente la misma forma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB7KeceLoSH0"
      },
      "outputs": [],
      "source": [
        "inc_dataset = tf.data.Dataset.range(100)\n",
        "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
        "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
        "batched_dataset = dataset.batch(4)\n",
        "\n",
        "for batch in batched_dataset.take(4):\n",
        "  print([arr.numpy() for arr in batch])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlV1tpFdoSH0"
      },
      "source": [
        "Mientras que `tf.data` intenta propagar la información de la forma, la configuración predeterminada de `Dataset.batch` da como resultado un tamaño desconocido de lote porque tal vez el último lote no está lleno. Observe los `None`s en la forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN7hn7OBoSHx"
      },
      "outputs": [],
      "source": [
        "batched_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It1fPA3NoSHw"
      },
      "source": [
        "Use el argumento `drop_remainder` para ignorar el último lote y obtenga una propagación completa de la forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BycWC7WCoSHt"
      },
      "outputs": [],
      "source": [
        "batched_dataset = dataset.batch(7, drop_remainder=True)\n",
        "batched_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj9nRxFZoSHs"
      },
      "source": [
        "### Procesar tensores con espaciado por lotes\n",
        "\n",
        "La receta anterior funciona con tensores del mismo tamaño. Sin embargo, muchos modelos (incluidos los modelos de secuencia) funcionan con datos de entrada que pueden tener varios tamaños (por ejemplo, secuencias de diferentes longitudes). Para tratar este caso, la transformación `Dataset.padded_batch` le permite procesar en lotes tensores de diferentes formas por lote al especificar una o más dimensiones en las que se pueda agregar espaciado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kycwO0JooSHn"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.range(100)\n",
        "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
        "dataset = dataset.padded_batch(4, padded_shapes=(None,))\n",
        "\n",
        "for batch in dataset.take(2):\n",
        "  print(batch.numpy())\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl3yhth1oSHm"
      },
      "source": [
        "La transformación `Dataset.padded_batch` permite establecer diferentes espaciados para cada dimensión de cada componente y puede tener longitudes variables (está representado con `None` en el ejemplo anterior) o longitudes constantes. También se puede reemplazar el valor del espaciado, cuyo valor predeterminado es 0.\n",
        "\n",
        "<!--\n",
        "TODO(mrry): Add this section.\n",
        "\n",
        "### Dense ragged -> tf.SparseTensor\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8zbAxMwoSHl"
      },
      "source": [
        "## Flujos de entrenamiento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnlhzF_AoSHk"
      },
      "source": [
        "### Procesar varias épocas\n",
        "\n",
        "La API `tf.data` ofrece dos formas principales de procesar varias épocas de los mismos datos.\n",
        "\n",
        "La forma más simple de hacer una iteración en un conjunto de datos en varias épocas es con la transformación `Dataset.repeat()`. Primero, cree un conjunto de datos de datos del titanic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tODHZzRoSHg"
      },
      "outputs": [],
      "source": [
        "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic_lines = tf.data.TextLineDataset(titanic_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMO6mlXxoSHc"
      },
      "outputs": [],
      "source": [
        "def plot_batch_sizes(ds):\n",
        "  batch_sizes = [batch.shape[0] for batch in ds]\n",
        "  plt.bar(range(len(batch_sizes)), batch_sizes)\n",
        "  plt.xlabel('Batch number')\n",
        "  plt.ylabel('Batch size')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfVzmqL7oSHa"
      },
      "source": [
        "Si se aplica la transformación `Dataset.repeat()` sin argumentos, se repetirá la entrada indefinidamente.\n",
        "\n",
        "La transformación `Dataset.repeat` concatena sus argumentos sin marcar el final de una época ni el comienzo de la siguiente época. Por eso, si se aplica un `Dataset.batch` después de un `Dataset.repeat` generará lotes que amplían los límites de la época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ0G1cztoSHX"
      },
      "outputs": [],
      "source": [
        "titanic_batches = titanic_lines.repeat(3).batch(128)\n",
        "plot_batch_sizes(titanic_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moH-4gBEoSHW"
      },
      "source": [
        "Si necesita una separación clara entre las épocas, escriba `Dataset.batch` antes de la repetición:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmbmdK1qoSHS"
      },
      "outputs": [],
      "source": [
        "titanic_batches = titanic_lines.batch(128).repeat(3)\n",
        "\n",
        "plot_batch_sizes(titanic_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlEM5f9loSHR"
      },
      "source": [
        "Si quiere realizar un cálculo personalizado (por ejemplo, para recopilar estadísticas) al final de cada época, lo más simple es reiniciar la iteración del conjunto de datos en cada época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyekyeY7oSHO"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "dataset = titanic_lines.batch(128)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch in dataset:\n",
        "    print(batch.shape)\n",
        "  print(\"End of epoch: \", epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bci79WCoSHN"
      },
      "source": [
        "### Poner los datos de entrada en orden aleatorio\n",
        "\n",
        "La transformación `Dataset.shuffle()` mantiene un búfer de tamaño fijo y elige el siguiente elemento desde el búfer de forma uniforme y al azar.\n",
        "\n",
        "Nota: Aunque los bufer_sizes grandes pueden ordenarse de manera aleatoriade forma más diligente, pueden usar mucha memoria y un tiempo considerable para completarse. Considere usar `Dataset.interleave` en los archivos si tiene este problema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YvXr-qeoSHL"
      },
      "source": [
        "Agregue un índice al conjunto de datos para poder ver el efecto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io4iJH1toSHI"
      },
      "outputs": [],
      "source": [
        "lines = tf.data.TextLineDataset(titanic_file)\n",
        "counter = tf.data.experimental.Counter()\n",
        "\n",
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "dataset = dataset.shuffle(buffer_size=100)\n",
        "dataset = dataset.batch(20)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6tNYRcsoSHH"
      },
      "source": [
        "Dado que el `buffer_size` es 100 y el tamaño del lote es 20, el primer lote no tiene ningún elemento con un índice mayor a 120."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayM3FFFAoSHC"
      },
      "outputs": [],
      "source": [
        "n,line_batch = next(iter(dataset))\n",
        "print(n.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLrfIjTHoSHB"
      },
      "source": [
        "Al igual que con `Dataset.batch` el orden en relación con `Dataset.repeat` importa.\n",
        "\n",
        "`Dataset.shuffle` no señala el final de una época hasta que el búfer del orden aleatorio esté vacío. Entonces, si se agrega un orden aleatorio antes de una repetición se mostrará cada elemento de una época antes de ir a la siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX3pe7zZoSG6"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "shuffled = dataset.shuffle(buffer_size=100).batch(10).repeat(2)\n",
        "\n",
        "print(\"Here are the item ID's near the epoch boundary:\\n\")\n",
        "for n, line_batch in shuffled.skip(60).take(5):\n",
        "  print(n.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9hlE-lGoSGz"
      },
      "outputs": [],
      "source": [
        "shuffle_repeat = [n.numpy().mean() for n, line_batch in shuffled]\n",
        "plt.plot(shuffle_repeat, label=\"shuffle().repeat()\")\n",
        "plt.ylabel(\"Mean item ID\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UucIgCxWoSGx"
      },
      "source": [
        "Pero si hay una repetición antes de un orden aleatorio se mezclan los límites de la época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhxb5YGZoSGm"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "shuffled = dataset.repeat(2).shuffle(buffer_size=100).batch(10)\n",
        "\n",
        "print(\"Here are the item ID's near the epoch boundary:\\n\")\n",
        "for n, line_batch in shuffled.skip(55).take(15):\n",
        "  print(n.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAM4cbpZoSGL"
      },
      "outputs": [],
      "source": [
        "repeat_shuffle = [n.numpy().mean() for n, line_batch in shuffled]\n",
        "\n",
        "plt.plot(shuffle_repeat, label=\"shuffle().repeat()\")\n",
        "plt.plot(repeat_shuffle, label=\"repeat().shuffle()\")\n",
        "plt.ylabel(\"Mean item ID\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ianlfbrxbDco"
      },
      "source": [
        "## Procesar datos\n",
        "\n",
        "La transformación `Dataset.map(f)` genera un conjunto de datos nuevo al aplicar la función ingresada `f` en cada elemento del conjunto de datos de entrada. Se basa en la función [`map()`](https://en.wikipedia.org/wiki/Map_(higher-order_function)) que comúnmente se aplica a listas (y otras estructuras) en los lenguajes de programación funcionales. La función `f` toma los objetos de `tf.Tensor` que representan un elemento único en la entrada y devuelve los objetos `tf.Tensor` que representan un elemento único en el conjunto de datos nuevo. Al implementarse está función, se usan operaciones estándar de TensorFlow para transformar un elemento en otro.\n",
        "\n",
        "En esta sección, se cubren ejemplos comúnes de cómo usar `Dataset.map()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXw1IZVdbDcq"
      },
      "source": [
        "### Decodificar datos de imagen y cambiar el tamaño\n",
        "\n",
        "<!-- TODO(markdaoust): link to image augmentation when it exists -->\n",
        "\n",
        "Cuando se entrena una red neuronal con datos de imágenes del mundo real, suele ser necesario convertir las imágenes de diferentes tamaños a un tamaño común, para que se puedan procesar por lotes en un tamaño fijo.\n",
        "\n",
        "Reconstruya el conjunto de datos de nombres de archivos de flores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMGlj8V-u-NH"
      },
      "outputs": [],
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyhZLB8N5jBm"
      },
      "source": [
        "Escriba una función que manipule los elementos del conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZObC0debDcr"
      },
      "outputs": [],
      "source": [
        "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
        "# to a fixed shape.\n",
        "def parse_image(filename):\n",
        "  parts = tf.strings.split(filename, os.sep)\n",
        "  label = parts[-2]\n",
        "\n",
        "  image = tf.io.read_file(filename)\n",
        "  image = tf.io.decode_jpeg(image)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  image = tf.image.resize(image, [128, 128])\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0dVJlCA5qHA"
      },
      "source": [
        "Pruebe que funcione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8xuN_HBzGup"
      },
      "outputs": [],
      "source": [
        "file_path = next(iter(list_ds))\n",
        "image, label = parse_image(file_path)\n",
        "\n",
        "def show(image, label):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(label.numpy().decode('utf-8'))\n",
        "  plt.axis('off')\n",
        "\n",
        "show(image, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3P8N-S55vDu"
      },
      "source": [
        "Asígnela a un conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzO8LI_H5Sk_"
      },
      "outputs": [],
      "source": [
        "images_ds = list_ds.map(parse_image)\n",
        "\n",
        "for image, label in images_ds.take(2):\n",
        "  show(image, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ff7IqB9bDcs"
      },
      "source": [
        "### Aplicar una lógica arbitraria de Python\n",
        "\n",
        "Por el rendimiento, use operaciones de TensorFlow para preprocesar sus datos cuando sea posible. Sin embargo, a veces sirve llamar a las bibliotecas externas de Python cuando parsea sus datos de entrada. Puede usar la operación `tf.py_function` en una transformación `Dataset.map`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u7CeA67DU8"
      },
      "source": [
        "Por ejemplo, si quiere aplicar una rotación aleatoria, el módulo `tf.image` solo tiene `tf.image.rot90`, que no es muy útil para aumentar imágenes.\n",
        "\n",
        "Nota: `tensorflow_addons` tiene un `rotate` compatible de TensorFlow en `tensorflow_addons.image.rotate`.\n",
        "\n",
        "Para demostrar `tf.py_function`, pruebe usar la función `scipy.ndimage.rotate` en su lugar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBUmbERt7Czz"
      },
      "outputs": [],
      "source": [
        "import scipy.ndimage as ndimage\n",
        "\n",
        "def random_rotate_image(image):\n",
        "  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wEyL7bS9S6t"
      },
      "outputs": [],
      "source": [
        "image, label = next(iter(images_ds))\n",
        "image = random_rotate_image(image)\n",
        "show(image, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxVx7z-ABNyq"
      },
      "source": [
        "Para usar la función con `Dataset.map` aplica la misma advertencia que con `Dataset.from_generator`, debe describir las formas y los tipos de retorno cuando aplique la función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn2nIu92BMp0"
      },
      "outputs": [],
      "source": [
        "def tf_random_rotate_image(image, label):\n",
        "  im_shape = image.shape\n",
        "  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])\n",
        "  image.set_shape(im_shape)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWPqKbTnbDct"
      },
      "outputs": [],
      "source": [
        "rot_ds = images_ds.map(tf_random_rotate_image)\n",
        "\n",
        "for image, label in rot_ds.take(2):\n",
        "  show(image, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykx59-cMBwOT"
      },
      "source": [
        "### Parsear mensajes de búfer del protocolo `tf.Example`\n",
        "\n",
        "Muchas canalizaciones de entrada extraen mensajes de búfer del protocolo `tf.train.Example` desde un formato TFRecord. Cada registro de `tf.train.Example` contiene una o más \"funciones\", y la canalización de entrada suele convertir estas funciones en tensores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wnE134b32KY"
      },
      "outputs": [],
      "source": [
        "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\n",
        "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGypdgYOlXZz"
      },
      "source": [
        "Puede trabajar con protocolos de `tf.train.Example` fuera de un `tf.data.Dataset` para entender los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4znsVNqnF73C"
      },
      "outputs": [],
      "source": [
        "raw_example = next(iter(dataset))\n",
        "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "feature = parsed.features.feature\n",
        "raw_img = feature['image/encoded'].bytes_list.value[0]\n",
        "img = tf.image.decode_png(raw_img)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "_ = plt.title(feature[\"image/text\"].bytes_list.value[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwzqp8IGC_vQ"
      },
      "outputs": [],
      "source": [
        "raw_example = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2X1dQNfC8Lu"
      },
      "outputs": [],
      "source": [
        "def tf_parse(eg):\n",
        "  example = tf.io.parse_example(\n",
        "      eg[tf.newaxis], {\n",
        "          'image/encoded': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
        "          'image/text': tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "      })\n",
        "  return example['image/encoded'][0], example['image/text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGJhKDp_61A_"
      },
      "outputs": [],
      "source": [
        "img, txt = tf_parse(raw_example)\n",
        "print(txt.numpy())\n",
        "print(repr(img.numpy()[:20]), \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vFIUFzD5qIC"
      },
      "outputs": [],
      "source": [
        "decoded = dataset.map(tf_parse)\n",
        "decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRYNYkEej7Ix"
      },
      "outputs": [],
      "source": [
        "image_batch, text_batch = next(iter(decoded.batch(10)))\n",
        "image_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1n0UBeczit"
      },
      "source": [
        "<a id=\"time_series_windowing\"></a>\n",
        "\n",
        "### Segmentar series temporales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0JMgvXEz9y1"
      },
      "source": [
        "Para ver un ejemplo de principio a fin de una serie temporal consulte: [Predecir series temporales](../../tutorials/structured_data/time_series.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzBABBkAkkVJ"
      },
      "source": [
        "Los datos de las series temporales suelen organizarse con el eje de tiempo intacto.\n",
        "\n",
        "Use un `Dataset.range` simple para demostrar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTQgo49skjuY"
      },
      "outputs": [],
      "source": [
        "range_ds = tf.data.Dataset.range(100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6GLGhxgpazJ"
      },
      "source": [
        "Por lo general, los modelos que se basan en este tipo de datos necesitarán un segmento de tiempo contiguo.\n",
        "\n",
        "El enfoque más simple sería procesar los datos por lotes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETqB7QvTCNty"
      },
      "source": [
        "#### Con `batch`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSs9XqwQpvIN"
      },
      "outputs": [],
      "source": [
        "batches = range_ds.batch(10, drop_remainder=True)\n",
        "\n",
        "for batch in batches.take(5):\n",
        "  print(batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgb2qikEtk5W"
      },
      "source": [
        "O para realizar predicciones densas un paso adelantado, es posible que quiera cambiar las características y etiquetas por un paso relacionado entre sí:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47XfwPhetkIN"
      },
      "outputs": [],
      "source": [
        "def dense_1_step(batch):\n",
        "  # Shift features and labels one step relative to each other.\n",
        "  return batch[:-1], batch[1:]\n",
        "\n",
        "predict_dense_1_step = batches.map(dense_1_step)\n",
        "\n",
        "for features, label in predict_dense_1_step.take(3):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjsXuINKqsS_"
      },
      "source": [
        "Para predecir una ventana completa en lugar de un desplazamiento fijo, puede dividir los lotes en dos partes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMmkQB1Gqo6x"
      },
      "outputs": [],
      "source": [
        "batches = range_ds.batch(15, drop_remainder=True)\n",
        "\n",
        "def label_next_5_steps(batch):\n",
        "  return (batch[:-5],   # Inputs: All except the last 5 steps\n",
        "          batch[-5:])   # Labels: The last 5 steps\n",
        "\n",
        "predict_5_steps = batches.map(label_next_5_steps)\n",
        "\n",
        "for features, label in predict_5_steps.take(3):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a611Qr3jlhl"
      },
      "source": [
        "Para permitir que las características de un lote y las etiquetas de otro se superpongan un poco, use `Dataset.zip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11dF3wyFjk2J"
      },
      "outputs": [],
      "source": [
        "feature_length = 10\n",
        "label_length = 3\n",
        "\n",
        "features = range_ds.batch(feature_length, drop_remainder=True)\n",
        "labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:label_length])\n",
        "\n",
        "predicted_steps = tf.data.Dataset.zip((features, labels))\n",
        "\n",
        "for features, label in predicted_steps.take(5):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adew3o2mCURC"
      },
      "source": [
        "#### Con `window`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF6pEdlduq8E"
      },
      "source": [
        "Si bien usar `Dataset.batch` funciona, hay situaciones en las que tal vez necesite un control más específico. El método `Dataset.window` le da control total, pero necesita más atención: este método devuelve un `Dataset` de los `Datasets`. Para obtener más detalles, consulte la sección [Estructura del conjunto de datos](#dataset_structure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEI2W_EBw2OX"
      },
      "outputs": [],
      "source": [
        "window_size = 5\n",
        "\n",
        "windows = range_ds.window(window_size, shift=1)\n",
        "for sub_ds in windows.take(5):\n",
        "  print(sub_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r82hWdk4x-46"
      },
      "source": [
        "El método `Dataset.flat_map` puede tomar un conjunto de datos de los conjuntos de datos y acoplarlo en un solo conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB8AI03mnF8u"
      },
      "outputs": [],
      "source": [
        " for x in windows.flat_map(lambda x: x).take(30):\n",
        "   print(x.numpy(), end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgLIwq9Anc34"
      },
      "source": [
        "En la mayoría de los casos, primero querrá `Dataset.batch` el conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j_y84rmyVQa"
      },
      "outputs": [],
      "source": [
        "def sub_to_batch(sub):\n",
        "  return sub.batch(window_size, drop_remainder=True)\n",
        "\n",
        "for example in windows.flat_map(sub_to_batch).take(5):\n",
        "  print(example.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVugrmND3Grp"
      },
      "source": [
        "Ahora puede ver que el argumento `shift` controla cuánto se mueve cada ventana.\n",
        "\n",
        "Si lo une todo, puede escribir esta función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdFRv_0D4FqW"
      },
      "outputs": [],
      "source": [
        "def make_window_dataset(ds, window_size=5, shift=1, stride=1):\n",
        "  windows = ds.window(window_size, shift=shift, stride=stride)\n",
        "\n",
        "  def sub_to_batch(sub):\n",
        "    return sub.batch(window_size, drop_remainder=True)\n",
        "\n",
        "  windows = windows.flat_map(sub_to_batch)\n",
        "  return windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iVxcVfEdf5b"
      },
      "outputs": [],
      "source": [
        "ds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3)\n",
        "\n",
        "for example in ds.take(10):\n",
        "  print(example.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMGMTPQ4w8pr"
      },
      "source": [
        "Y así, es más fácil extraer las etiquetas, como antes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0fPfZkZw6j_"
      },
      "outputs": [],
      "source": [
        "dense_labels_ds = ds.map(dense_1_step)\n",
        "\n",
        "for inputs,labels in dense_labels_ds.take(3):\n",
        "  print(inputs.numpy(), \"=>\", labels.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyi_-ft0kvy4"
      },
      "source": [
        "### Nuevo muestreo\n",
        "\n",
        "Si tiene un conjunto de datos de diferentes clases, deberá realizar un nuevo muestreo del conjunto de datos. `tf.data` proporciona dos métodos para hacerlo. El conjunto de datos de fraude de tarjeta de crédito es un buen ejemplo de este tipo de problema.\n",
        "\n",
        "Nota: visite [Clasificación de datos desequilibrados](../tutorials/structured_data/imbalanced_data.ipynb) para ver un tutorial completo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2e8dxVUlFHO"
      },
      "outputs": [],
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip',\n",
        "    fname='creditcard.zip',\n",
        "    extract=True)\n",
        "\n",
        "csv_path = zip_path.replace('.zip', '.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhkkM4Wx75S_"
      },
      "outputs": [],
      "source": [
        "creditcard_ds = tf.data.experimental.make_csv_dataset(\n",
        "    csv_path, batch_size=1024, label_name=\"Class\",\n",
        "    # Set the column types: 30 floats and an int.\n",
        "    column_defaults=[float()]*30+[int()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8O47EmHlxYX"
      },
      "source": [
        "Ahora, verifique la distribución de clases, está marcadamente asimétrica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8-Ss69XlzXD"
      },
      "outputs": [],
      "source": [
        "def count(counts, batch):\n",
        "  features, labels = batch\n",
        "  class_1 = labels == 1\n",
        "  class_1 = tf.cast(class_1, tf.int32)\n",
        "\n",
        "  class_0 = labels == 0\n",
        "  class_0 = tf.cast(class_0, tf.int32)\n",
        "\n",
        "  counts['class_0'] += tf.reduce_sum(class_0)\n",
        "  counts['class_1'] += tf.reduce_sum(class_1)\n",
        "\n",
        "  return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1a3t_B4l_f6"
      },
      "outputs": [],
      "source": [
        "counts = creditcard_ds.take(10).reduce(\n",
        "    initial_state={'class_0': 0, 'class_1': 0},\n",
        "    reduce_func = count)\n",
        "\n",
        "counts = np.array([counts['class_0'].numpy(),\n",
        "                   counts['class_1'].numpy()]).astype(np.float32)\n",
        "\n",
        "fractions = counts/counts.sum()\n",
        "print(fractions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1b8lFhSnDdv"
      },
      "source": [
        "Un enfoque común para entrenar con conjuntos de datos desequilibrados es equilibrarlos. `tf.data` incluye algunos métodos que permiten este flujo de trabajo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8jQWsgMnjQG"
      },
      "source": [
        "#### Muestreo del conjuntos de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov14SRrQyQE3"
      },
      "source": [
        "Un enfoque para realizar un nuevo muestreo de un conjunto de datos es usar `sample_from_datasets`. Es más aplicable cuando se tiene un `tf.data.Dataset` diferente para cada clase.\n",
        "\n",
        "A continuación, solo use el filtro para generarlos con los datos de fraude de tarjeta de crédito:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YKfCPa-nioA"
      },
      "outputs": [],
      "source": [
        "negative_ds = (\n",
        "  creditcard_ds\n",
        "    .unbatch()\n",
        "    .filter(lambda features, label: label==0)\n",
        "    .repeat())\n",
        "positive_ds = (\n",
        "  creditcard_ds\n",
        "    .unbatch()\n",
        "    .filter(lambda features, label: label==1)\n",
        "    .repeat())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FNd3sQjzl9-"
      },
      "outputs": [],
      "source": [
        "for features, label in positive_ds.batch(10).take(1):\n",
        "  print(label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxLAr-7p0ATX"
      },
      "source": [
        "Para usar `tf.data.Dataset.sample_from_datasets` pase los conjuntos de datos y el peso para cada uno:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjdPVIFCngOb"
      },
      "outputs": [],
      "source": [
        "balanced_ds = tf.data.Dataset.sample_from_datasets(\n",
        "    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K4ObOms082B"
      },
      "source": [
        "Ahora el conjunto de datos produce ejemplos de cada clase con una probabilidad de 50/50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myvkw21Rz-fH"
      },
      "outputs": [],
      "source": [
        "for features, labels in balanced_ds.take(10):\n",
        "  print(labels.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUTE3eb9nckY"
      },
      "source": [
        "#### Rechazo del nuevo muestreo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ9ezkK6irMD"
      },
      "source": [
        "Un problema con el enfoque `Dataset.sample_from_datasets` anterior es que necesita un `tf.data.Dataset` diferente por clase. Podría usar `Dataset.filter` para crear esos dos conjuntos de datos, pero esto hará que se carguen los datos dos veces.\n",
        "\n",
        "Se puede aplicar el método `tf.data.Dataset.rejection_resample` en el conjunto de datos para volver a equilibrarlo, y solo se carga una vez. Se excluirán o repetirán elementos para lograr el equilibrio.\n",
        "\n",
        "El método `rejection_resample` toma un argumento `class_func`. Se aplica `class_func` en cada elemento del conjunto de datos y se usa para determinar a qué clase pertenece un ejemplo con el propósito de equilibrio.\n",
        "\n",
        "El objetivo es equilibrar la distribución de las etiquetas, y los elementos de `creditcard_ds` ya son pares de `(features, label)`. Por eso la `class_func` solo debe devolver esas etiquetas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC_Cuzw8lhI5"
      },
      "outputs": [],
      "source": [
        "def class_func(features, label):\n",
        "  return label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxJrOZVToGuE"
      },
      "source": [
        "El método de volver a muestrear trata ejemplos individuales, por eso, en este caso debe `unbatch` el conjunto de datos antes de aplicar el método.\n",
        "\n",
        "El método necesita una distribución de destino y, de forma opcional, un cálculo estimado de la distribución inicial como entradas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY6VIhr3oGHG"
      },
      "outputs": [],
      "source": [
        "resample_ds = (\n",
        "    creditcard_ds\n",
        "    .unbatch()\n",
        "    .rejection_resample(class_func, target_dist=[0.5,0.5],\n",
        "                        initial_dist=fractions)\n",
        "    .batch(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-HnC1s8idqV"
      },
      "source": [
        "El método `rejection_resample` devuelve los pares de `(class, example)` donde la `class` es una salida de la `class_func`. En este caso, el  `example` ya era un par de `(feature, label)`, por eso use `map` para excluir la copia adicional de las etiquetas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpfCGU6BiaZq"
      },
      "outputs": [],
      "source": [
        "balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3d2jyEhx9kD"
      },
      "source": [
        "Ahora el conjunto de datos genera ejemplos de cada clase con una probabilidad de 50/50:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGLYChBQwkDV"
      },
      "outputs": [],
      "source": [
        "for features, labels in balanced_ds.take(10):\n",
        "  print(labels.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYFKQx3bUBeU"
      },
      "source": [
        "## Punto de verificación del elemento de iteración"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOGg1UFhUE4z"
      },
      "source": [
        "TensorFlow admite [guardar puntos de verificación](./checkpoint.ipynb) para que cuando el proceso de entrenamiento se reinicie, se pueda restaurar el último punto de verificación para recuperar la mayoría del progreso. Además de guardar los puntos de verificación de las variables del modelo, también puede guardar los puntos de verificación del progreso del elemento de iteración del conjunto de datos. Eso podría servirle si tiene un conjunto de datos grande y no quiere empezar desde el principio del conjunto de datos cada vez que lo reinicia. De todas maneras, tenga en cuenta que los puntos de verificación de un elemento de iteración pueden ser grandes, ya que las transformaciones como `Dataset.shuffle` y `Dataset.prefetch` requieren almacenar elementos en el búfer dentro del elemento de iteración.\n",
        "\n",
        "Para incluir su elemento de iteración en el punto de verificación, pase el elemento de iteración al constructor `tf.train.Checkpoint`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fsm9wvKUsNC"
      },
      "outputs": [],
      "source": [
        "range_ds = tf.data.Dataset.range(20)\n",
        "\n",
        "iterator = iter(range_ds)\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)\n",
        "manager = tf.train.CheckpointManager(ckpt, '/tmp/my_ckpt', max_to_keep=3)\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])\n",
        "\n",
        "save_path = manager.save()\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])\n",
        "\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxWglTwX9Fex"
      },
      "source": [
        "Nota: no es posible guardar un punto de verificación de un elemento de iteración que depende de un estado externo, como una `tf.py_function`. Si lo intenta, provocará una excepción y reclamará el estado externo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLRdedPpbDdD"
      },
      "source": [
        "## Usar `tf.data` con `tf.keras`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTQe8daMcgFz"
      },
      "source": [
        "La API `tf.keras` simplifica muchos aspectos de la creación y ejecución de los modelos de aprendizaje automático. Sus API `Model.fit`, `Model.evaluate` y `Model.predict` admiten los conjuntos de datos como entradas. A continuación, encontrará una configuración rápida de conjuntos de datos y modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bfjqm0hOfES"
      },
      "outputs": [],
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "images, labels = train\n",
        "images = images/255.0\n",
        "labels = labels.astype(np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDhF3rGnbDdD"
      },
      "outputs": [],
      "source": [
        "fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdogg8CfHs-G"
      },
      "source": [
        "Solo se necesita pasar un conjunto de datos de pares de `(feature, label)` para `Model.fit` y la `Model.evaluate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cu4kPzOHnlt"
      },
      "outputs": [],
      "source": [
        "model.fit(fmnist_train_ds, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzpAQfJMJF41"
      },
      "source": [
        "Si pasa un conjunto de datos infinito, por ejemplo, al llamar a `Dataset.repeat`, solo deberá pasar el argumento `steps_per_epoch` también:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp1BpzlyJinb"
      },
      "outputs": [],
      "source": [
        "model.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTLsw_nqJpTw"
      },
      "source": [
        "Para evaluarlo, también puede pasar la cantidad de pasos de la evaluación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnlRHlaL-XUI"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(fmnist_train_ds)\n",
        "print(\"Loss :\", loss)\n",
        "print(\"Accuracy :\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8UBU3CJKEA4"
      },
      "source": [
        "Para conjuntos de datos grandes, establezca la cantidad de pasos para evaluar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVgamf9HKDon"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)\n",
        "print(\"Loss :\", loss)\n",
        "print(\"Accuracy :\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZYhJ_YSIl6w"
      },
      "source": [
        "No se requieren las etiquetas cuando se llama a `Model.predict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "343lXJ-pIqWD"
      },
      "outputs": [],
      "source": [
        "predict_ds = tf.data.Dataset.from_tensor_slices(images).batch(32)\n",
        "result = model.predict(predict_ds, steps = 10)\n",
        "print(result.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfzZORwLI202"
      },
      "source": [
        "Pero se ignorarán las etiquetas si pasa un conjunto de datos que tenga etiquetas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgQJTPrT-2WF"
      },
      "outputs": [],
      "source": [
        "result = model.predict(fmnist_train_ds, steps = 10)\n",
        "print(result.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
