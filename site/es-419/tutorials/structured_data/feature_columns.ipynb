{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNdWfPXCjTjY"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I1dUQ0GejU8N"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c05P9g5WjizZ"
      },
      "source": [
        "# Clasificar datos estructurados con columnas de características"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zofH_gCzgplN"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/structured_data/feature_columns\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/structured_data/feature_columns.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/structured_data/feature_columns.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tutorials/structured_data/feature_columns.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1y4OHpGgss7"
      },
      "source": [
        "> Advertencia: El módulo `tf.feature_columns` descrito en este tutorial no se recomienda para código nuevo. [Las capas de preprocesamiento Keras](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers) cubren esta funcionalidad, para instrucciones de migración consulte la guía [Migrando columnas de características](../../guide/migrate/migrating_feature_columns.ipynb). El módulo `tf.feature_columns` fue diseñado para usarse con los `Estimators` de TF1. Entra dentro de nuestras [garantías de compatibilidad](https://tensorflow.org/guide/versions), pero no recibirá más correcciones que las vulnerabilidades de seguridad.\n",
        "\n",
        "Este tutorial muestra cómo clasificar datos estructurados (por ejemplo, datos tabulares en un CSV). Usaremos [Keras](https://www.tensorflow.org/guide/keras) para definir el modelo, y `tf.feature_column` como puente para mapear desde columnas en un CSV a características usadas para entrenar el modelo. Este tutorial contiene el código completo para:\n",
        "\n",
        "- Cargar un archivo CSV usando [Pandas](https://pandas.pydata.org/).\n",
        "- Construir una canalización de entrada para procesar por lotes y mezclar las filas usando [tf.data](https://www.tensorflow.org/guide/datasets).\n",
        "- Mapear de columnas en el CSV a características usadas para entrenar el modelo usando columnas de características.\n",
        "- Construir, entrenar y evaluar un modelo usando Keras.\n",
        "\n",
        "## El conjunto de datos\n",
        "\n",
        "Usaremos una versión simplificada del conjunto de datos [de PetFinder](https://www.kaggle.com/c/petfinder-adoption-prediction). Hay varios miles de filas en el CSV. Cada fila describe una mascota y cada columna describe un atributo. Usaremos esta información para predecir la velocidad a la que se adoptará la mascota.\n",
        "\n",
        "A continuación se describe este conjunto de datos. Observe que hay columnas numéricas y categóricas. Hay una columna de texto libre que no usaremos en este tutorial.\n",
        "\n",
        "Columna | Descripción | Tipo de característica | Tipo de datos\n",
        "--- | --- | --- | ---\n",
        "Type | Tipo de animal (Dog, Cat) | Categórica | cadena\n",
        "Age | Edad de la mascota | Numérico | entero\n",
        "Breed1 | Raza principal de la mascota | Categórica | cadena\n",
        "Color1 | Color 1 de la mascota | Categórica | cadena\n",
        "Color2 | Color 2 de la mascota | Categórica | cadena\n",
        "MaturitySize | Tamaño de adulto | Categórica | cadena\n",
        "FurLength | Largo de pelo | Categórica | cadena\n",
        "Vaccinated | La mascota ha sido vacunada | Categórica | cadena\n",
        "Sterilized | La mascota ha sido esterilizada | Categórica | cadena\n",
        "Health | Condición de salud | Categórica | cadena\n",
        "Fee | Tarifa de adopción | Numérica | entero\n",
        "Description | Reseña del perfil de esta mascota | Texto | cadena\n",
        "PhotoAmt | Total de fotos subidas para esta mascota | Numérica | entero\n",
        "AdoptionSpeed | Rapidez de adopción | Clasificación | entero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxyBFc_kKazA"
      },
      "source": [
        "## Importar TensorFlow y otras librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuOWVJBz8a6G"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dEreb4QKizj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCEhSZcULZ9n"
      },
      "source": [
        "## Usar Pandas para crear un dataframe\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/) es una librería de Python con muchas utilidades útiles para cargar y trabajar con datos estructurados. Usaremos Pandas para descargar el conjunto de datos desde una URL, y cargarlo en un dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REZ57BXCLdfG"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
        "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
        "\n",
        "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
        "                        extract=True, cache_dir='.')\n",
        "dataframe = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8QIi0_jT5LM"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awGiBeBWbQC8"
      },
      "source": [
        "## Crear variable objetivo\n",
        "\n",
        "La tarea en el conjunto de datos original es predecir la velocidad a la que se adoptará una mascota (por ejemplo, en la primera semana, el primer mes, los tres primeros meses, etc.). Simplifiquemos esto para nuestro tutorial. Aquí, transformaremos esto en un problema de clasificación binaria, y simplemente predeciremos si la mascota fue adoptada, o no.\n",
        "\n",
        "Tras modificar la columna de la etiqueta, 0 indicará que la mascota no fue adoptada y 1 que sí lo fue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcbTpEXWbMDz"
      },
      "outputs": [],
      "source": [
        "# In the original dataset \"4\" indicates the pet was not adopted.\n",
        "dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
        "\n",
        "# Drop un-used columns.\n",
        "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0zhLtQqMPem"
      },
      "source": [
        "## Dividir el dataframe en entrenamiento, validación y prueba\n",
        "\n",
        "El conjunto de datos que descargamos era un único archivo CSV. Lo dividiremos en conjuntos de entrenamiento, validación y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEOpw7LhMYsI"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ef46LXMfvu"
      },
      "source": [
        "## Crear una canalización de entrada usando tf.data\n",
        "\n",
        "A continuación, encapsularemos los marcos de datos con [tf.data](https://www.tensorflow.org/guide/datasets). Esto nos permitirá usar columnas de características como puente para mapear desde las columnas del dataframe de Pandas a las características usadas para entrenar el modelo. Si estuviéramos trabajando con un archivo CSV muy grande (tan grande que no cupiera en memoria), usaríamos tf.data para leerlo directamente del disco. Eso no se trata en este tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkcaMYP-MsRe"
      },
      "outputs": [],
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXbbXkJvMy34"
      },
      "outputs": [],
      "source": [
        "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRLGSMDzM-dl"
      },
      "source": [
        "## Comprender la canalización de entrada\n",
        "\n",
        "Ahora que hemos creado la canalización de entrada, vamos a llamarla para ver el formato de los datos que devuelve. Hemos usado un tamaño de lote pequeño para mantener la salida legible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSBo3dUVNFc9"
      },
      "outputs": [],
      "source": [
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  print('Every feature:', list(feature_batch.keys()))\n",
        "  print('A batch of ages:', feature_batch['Age'])\n",
        "  print('A batch of targets:', label_batch )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5N6Se-NQsC"
      },
      "source": [
        "Podemos ver que el conjunto de datos devuelve un diccionario de nombres de columnas (del dataframe) que mapean con valores de columnas de filas del dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttIvgLRaNoOQ"
      },
      "source": [
        "## Demostrar varios tipos de columnas de características\n",
        "\n",
        "TensorFlow proporciona muchos tipos de columnas de características. En esta sección, crearemos varios tipos de columnas de características y demostraremos cómo transforman una columna del dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxwiHFHuNhmf"
      },
      "outputs": [],
      "source": [
        "# We will use this batch to demonstrate several types of feature columns\n",
        "example_batch = next(iter(train_ds))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wfLB8Q3N3UH"
      },
      "outputs": [],
      "source": [
        "# A utility method to create a feature column\n",
        "# and to transform a batch of data\n",
        "def demo(feature_column):\n",
        "  feature_layer = layers.DenseFeatures(feature_column)\n",
        "  print(feature_layer(example_batch).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7OEKe82N-Qb"
      },
      "source": [
        "### Columnas numéricas\n",
        "\n",
        "La salida de una columna de características se convierte en la entrada del modelo (usando la función demo definida anteriormente, podremos ver exactamente cómo se transforma cada columna del dataframe). Una [columna numérica](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column) es el tipo más simple de columna. Se usa para representar características de valor real. Al usar esta columna, su modelo recibirá el valor de la columna del dataframe sin cambios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZTZ0HnHOCxC"
      },
      "outputs": [],
      "source": [
        "photo_count = feature_column.numeric_column('PhotoAmt')\n",
        "demo(photo_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a6ddSyzOKpq"
      },
      "source": [
        "En el conjunto de datos PetFinder, la mayoría de las columnas del dataframe son categóricas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcSxUoYgOlA1"
      },
      "source": [
        "### Columnas por cubos\n",
        "\n",
        "A menudo, no querrá introducir un número directamente en el modelo, sino dividir su valor en diferentes categorías basadas en rangos numéricos. Consideremos los datos brutos que representan la edad de una persona. En lugar de representar la edad como una columna numérica, podríamos dividir la edad en varios cubos usando una columna [por cubos](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column). Observe que los valores de un punto que aparecen a continuación describen a qué rango de edad corresponde cada fila."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ4Wt3SAOpTQ"
      },
      "outputs": [],
      "source": [
        "age = feature_column.numeric_column('Age')\n",
        "age_buckets = feature_column.bucketized_column(age, boundaries=[1, 3, 5])\n",
        "demo(age_buckets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tArzewPb-b"
      },
      "source": [
        "### Columnas categóricas\n",
        "\n",
        "En este conjunto de datos, el tipo se representa como una cadena (por ejemplo, 'Dog', o 'Cat'). No podemos introducir cadenas directamente en un modelo. En su lugar, primero debemos mapearlas a valores numéricos. Las columnas del vocabulario categórico proporcionan una forma de representar las cadenas como un vector de un solo punto (muy parecido a lo que ha visto anteriormente con los cubos de edad). El vocabulario puede pasarse como una lista usando [categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list), o cargarse desde un archivo usando [categorical_column_with_vocabulary_file](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ6QnSHkPtOC"
      },
      "outputs": [],
      "source": [
        "animal_type = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Type', ['Cat', 'Dog'])\n",
        "\n",
        "animal_type_one_hot = feature_column.indicator_column(animal_type)\n",
        "demo(animal_type_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEFPjUr6QmwS"
      },
      "source": [
        "### Incorporación de columnas\n",
        "\n",
        "Supongamos que en lugar de tener sólo unas pocas cadenas posibles, tenemos miles (o más) de valores por categoría. Por diversos motivos, a medida que el número de categorías progresa, resulta inviable entrenar una red neuronal usando codificaciones de un solo paso. Podemos usar una columna de incorporación para superar esta limitación. En lugar de representar los datos como un vector unívoco de muchas dimensiones, una [columna de incorporación](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column) representa esos datos como un vector denso de dimensiones inferiores en el que cada celda puede contener cualquier número, no sólo 0 o 1. El tamaño de la incorporación (8, en el ejemplo siguiente) es un parámetro que debe ajustarse.\n",
        "\n",
        "Punto clave: usar una columna de incorporación es lo mejor cuando una columna categórica tiene muchos valores posibles. Aquí estamos usando una con fines de demostración, para que tenga un ejemplo completo que pueda modificar para un conjunto de datos diferente en el futuro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSlohmr2Q_UU"
      },
      "outputs": [],
      "source": [
        "# Notice the input to the embedding column is the categorical column\n",
        "# we previously created\n",
        "breed1 = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Breed1', dataframe.Breed1.unique())\n",
        "breed1_embedding = feature_column.embedding_column(breed1, dimension=8)\n",
        "demo(breed1_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urFCAvTVRMpB"
      },
      "source": [
        "### Columnas de características hasheadas\n",
        "\n",
        "Otra forma de representar una columna categórica con un gran número de valores es usar una [categorical_column_with_hash_bucket](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket). Esta columna categórica calcula un valor hash de la entrada y, a continuación, selecciona uno de los cubos `hash_bucket_size` para codificar una cadena. Al usar esta columna, no necesita proporcionar el vocabulario, y puede seleccionar que el número de hash_buckets sea significativamente menor que el número de categorías reales para ahorrar espacio.\n",
        "\n",
        "Punto clave: Un inconveniente importante de esta técnica es que puede haber colisiones en las que se mapeen cadenas diferentes en el mismo cubo. En la práctica, esto puede funcionar bien para algunos conjuntos de datos a pesar de todo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHU_Aj2nRRDC"
      },
      "outputs": [],
      "source": [
        "breed1_hashed = feature_column.categorical_column_with_hash_bucket(\n",
        "      'Breed1', hash_bucket_size=10)\n",
        "demo(feature_column.indicator_column(breed1_hashed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB94M27DRXtZ"
      },
      "source": [
        "### Columnas de características cruzadas\n",
        "\n",
        "La combinación de características en una única característica, más conocida como [cruces de características](https://developers.google.com/machine-learning/glossary/#feature_cross), permite a un modelo aprender ponderaciones separadas para cada combinación de características. Aquí, crearemos una nueva característica que es el cruce de Edad y Tipo. Tenga en cuenta que `crossed_column` no construye la tabla completa de todas las combinaciones posibles (que podría ser muy grande). En su lugar, está respaldada por una `hashed_column`, de modo que puede seleccionar el tamaño de la tabla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaPVERd9Rep6"
      },
      "outputs": [],
      "source": [
        "crossed_feature = feature_column.crossed_column([age_buckets, animal_type], hash_bucket_size=10)\n",
        "demo(feature_column.indicator_column(crossed_feature))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypkI9zx6Rj1q"
      },
      "source": [
        "## Seleccionar qué columnas usar\n",
        "\n",
        "Hemos visto cómo usar varios tipos de columnas de características. Ahora las usaremos para entrenar un modelo. La meta de este tutorial es mostrarle el código completo (por ejemplo, la mecánica) necesario para trabajar con columnas de características. A continuación, hemos seleccionado arbitrariamente algunas columnas para entrenar nuestro modelo.\n",
        "\n",
        "Punto clave: Si su objetivo es construir un modelo preciso, pruebe con un conjunto de datos propio más amplio y piense detenidamente qué características son las más significativas que debe incluir y cómo deben representarse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PlLY7fORuzA"
      },
      "outputs": [],
      "source": [
        "feature_columns = []\n",
        "\n",
        "# numeric cols\n",
        "for header in ['PhotoAmt', 'Fee', 'Age']:\n",
        "  feature_columns.append(feature_column.numeric_column(header))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdF4rXkcDmBl"
      },
      "outputs": [],
      "source": [
        "# bucketized cols\n",
        "age = feature_column.numeric_column('Age')\n",
        "age_buckets = feature_column.bucketized_column(age, boundaries=[1, 2, 3, 4, 5])\n",
        "feature_columns.append(age_buckets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsteO7FGDmNc"
      },
      "outputs": [],
      "source": [
        "# indicator_columns\n",
        "indicator_column_names = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
        "                          'FurLength', 'Vaccinated', 'Sterilized', 'Health']\n",
        "for col_name in indicator_column_names:\n",
        "  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
        "      col_name, dataframe[col_name].unique())\n",
        "  indicator_column = feature_column.indicator_column(categorical_column)\n",
        "  feature_columns.append(indicator_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MhdqQ5uDmYU"
      },
      "outputs": [],
      "source": [
        "# embedding columns\n",
        "breed1 = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Breed1', dataframe.Breed1.unique())\n",
        "breed1_embedding = feature_column.embedding_column(breed1, dimension=8)\n",
        "feature_columns.append(breed1_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkzRNfCLDsQf"
      },
      "outputs": [],
      "source": [
        "# crossed columns\n",
        "age_type_feature = feature_column.crossed_column([age_buckets, animal_type], hash_bucket_size=100)\n",
        "feature_columns.append(feature_column.indicator_column(age_type_feature))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-nDp8krS_ts"
      },
      "source": [
        "### Crear una capa de características\n",
        "\n",
        "Ahora que hemos definido nuestras columnas de características, usaremos una capa [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) para introducirlas en nuestro modelo Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o-El1R2TGQP"
      },
      "outputs": [],
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cf6vKfgTH0U"
      },
      "source": [
        "Anteriormente, usamos un tamaño de lote pequeño para demostrar cómo funcionaban las columnas de características. Creamos una nueva canalización de entrada con un tamaño de lote mayor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcemszoGSse_"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBx4Xu0eTXWq"
      },
      "source": [
        "## Crear, compilar y entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YJPPb3xTPeZ"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dropout(.1),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds,\n",
        "          validation_data=val_ds,\n",
        "          epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnFmMOW0Tcaa"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdfbq20V6zu"
      },
      "source": [
        "Punto clave: Normalmente obtendrá mejores resultados con el aprendizaje profundo con conjuntos de datos mucho más grandes y complejos. Cuando trabaje con un conjunto de datos pequeño como éste, le recomendamos usar un árbol de decisión o un bosque aleatorio como línea de referencia sólida. La meta de este tutorial no es entrenar un modelo preciso, sino demostrar la mecánica de trabajo con datos estructurados, para que disponga de código que pueda usar como punto de partida cuando trabaje con sus propios conjuntos de datos en el futuro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SotnhVWuHQCw"
      },
      "source": [
        "## Siguientes pasos\n",
        "\n",
        "La mejor manera de aprender más sobre la clasificación de datos estructurados es intentarlo usted mismo. Le sugerimos que encuentre otro conjunto de datos con el que trabajar y entrene un modelo para clasificarlo utilizando un código similar al anterior. Para mejorar la precisión, piense detenidamente qué características incluir en su modelo y cómo deben representarse."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "feature_columns.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
