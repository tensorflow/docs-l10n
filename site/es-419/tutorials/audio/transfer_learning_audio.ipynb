{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4espuLKJiA"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DjZQV2njKJ3U"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTL0TERThT6z"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/transfer_learning_audio\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a> </td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver en GitHub</a> </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar cuaderno</a> </td>\n",
        "  <td>     <a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">Ver modelo en TF Hub</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2madPFAGHb3"
      },
      "source": [
        "# Aprendizaje de transferencia con YAMNet para la clasificación de sonidos ambientales\n",
        "\n",
        "[YAMNet](https://tfhub.dev/google/yamnet/1) es una red neuronal profunda preentrenada que puede predecir eventos de audio de [521 clases](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv), como risas, ladridos o una sirena.\n",
        "\n",
        "En este tutorial aprenderá a:\n",
        "\n",
        "- Cargar y usar el modelo YAMNet para la inferencia.\n",
        "- Construir un nuevo modelo usando las incrustaciones de YAMNet para clasificar los sonidos de gatos y perros.\n",
        "- Evaluar y exportar su modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mdp2TpBh96Y"
      },
      "source": [
        "## Importar TensorFlow y otras librerías\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCcKYqu_hvKe"
      },
      "source": [
        "Empiece instalando [TensorFlow I/O](https://www.tensorflow.org/io), que le facilitará la carga de archivos de audio del disco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urBpRWDHTHHU"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"tensorflow==2.11.*\"\n",
        "# tensorflow_io 0.28 is compatible with TensorFlow 2.11\n",
        "!pip install -q \"tensorflow_io==0.28.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l3nqdWVF-kC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_io as tfio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ZhybCnt_bM"
      },
      "source": [
        "## Acerca de YAMNet\n",
        "\n",
        "[YAMNet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet) es una red neuronal preentrenada que emplea la arquitectura de convolución separable en profundidad [MobileNetV1](https://arxiv.org/abs/1704.04861). Puede usar una forma de onda de audio como entrada y hacer predicciones independientes para cada uno de los 521 eventos de audio del corpus de [AudioSet](http://g.co/audioset).\n",
        "\n",
        "Internamente, el modelo extrae \"cuadros\" de la señal de audio y procesa lotes de estos cuadros. Esta versión del modelo usa cuadros de 0.96 segundos de duración y extrae un cuadro cada 0.48 segundos.\n",
        "\n",
        "El modelo acepta un Tensor 1-D float32 o un arreglo NumPy que contenga una forma de onda de longitud arbitraria, representada como muestreo monocanal (mono) de 16 kHz en el rango `[-1.0, +1.0]`. Este tutorial incluye código para ayudarle a convertir archivos WAV al formato compatible.\n",
        "\n",
        "El modelo devuelve 3 salidas, incluidas las puntuaciones de las clases, las incrustaciones (que usará para el aprendizaje por transferencia) y el [espectrograma del logaritmo mel](https://www.tensorflow.org/tutorials/audio/simple_audio#spectrogram). Puede encontrar más detalles [aquí](https://tfhub.dev/google/yamnet/1).\n",
        "\n",
        "Un uso particular de YAMNet es como extractor de características de alto nivel: la salida incorporada de 1,024 dimensiones. Tomará las características de entrada del modelo base (YAMNet) y las alimentará a su modelo menos profundo, formado por una capa oculta `tf.keras.layers.Dense`. Luego entrenará la red con una pequeña cantidad de datos para la clasificación de audio *sin necesidad de muchos datos etiquetados ni de un entrenamiento de principio a fin. (Esto es similar al [aprendizaje por transferencia para la clasificación de imágenes con TensorFlow Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub) para más información).*\n",
        "\n",
        "Primero, probará el modelo y verá los resultados de la clasificación del audio. Luego construirá la canalización de preprocesamiento de datos.\n",
        "\n",
        "### Cargar YAMNet desde TensorFlow Hub\n",
        "\n",
        "Va a usar una YAMNet preentrenada de [Tensorflow Hub](https://tfhub.dev/) para extraer las incrustaciones de los archivos de sonido.\n",
        "\n",
        "Cargar un modelo desde TensorFlow Hub es sencillo: seleccione el modelo, copie su URL y use la función `load`.\n",
        "\n",
        "Nota: para leer la documentación del modelo, use la URL del modelo en su navegador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06CWkBV5v3gr"
      },
      "outputs": [],
      "source": [
        "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "yamnet_model = hub.load(yamnet_model_handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmrPJ0GHw9rr"
      },
      "source": [
        "Con el modelo cargado, puede seguir el [tutorial de uso básico de YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) y descargar un archivo WAV de muestra para ejecutar la inferencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5i6xktEq00P"
      },
      "outputs": [],
      "source": [
        "testing_wav_file_name = tf.keras.utils.get_file('miaow_16k.wav',\n",
        "                                                'https://storage.googleapis.com/audioset/miaow_16k.wav',\n",
        "                                                cache_dir='./',\n",
        "                                                cache_subdir='test_data')\n",
        "\n",
        "print(testing_wav_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBm9y9iV2U_-"
      },
      "source": [
        "Va a necesitar una función para cargar archivos de audio, que se usará también más adelante al trabajar con los datos de entrenamiento. (Más información sobre la lectura de archivos de audio y sus etiquetas en [Reconocimiento simple de audio](https://www.tensorflow.org/tutorials/audio/simple_audio#reading_audio_files_and_their_labels)).\n",
        "\n",
        "Nota: El `wav_data` devuelto por `load_wav_16k_mono` ya está normalizado a valores en el rango `[-1.0, 1.0]` (consulte [la documentación de YAMNet sobre el TF Hub](https://tfhub.dev/google/yamnet/1) para saber más)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwc9Wrdg2EtY"
      },
      "outputs": [],
      "source": [
        "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
        "\n",
        "@tf.function\n",
        "def load_wav_16k_mono(filename):\n",
        "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(\n",
        "          file_contents,\n",
        "          desired_channels=1)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRqpjkwB0Jjw"
      },
      "outputs": [],
      "source": [
        "testing_wav_data = load_wav_16k_mono(testing_wav_file_name)\n",
        "\n",
        "_ = plt.plot(testing_wav_data)\n",
        "\n",
        "# Play the audio file.\n",
        "display.Audio(testing_wav_data, rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z6rqlEz20YB"
      },
      "source": [
        "### Cargue el mapeo de clases\n",
        "\n",
        "Es importante cargar los nombres de clase que YAMNet es capaz de reconocer. El archivo de mapeo se encuentra en `yamnet_model.class_map_path()` en formato CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gyj23e_3Mgr"
      },
      "outputs": [],
      "source": [
        "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\n",
        "class_names =list(pd.read_csv(class_map_path)['display_name'])\n",
        "\n",
        "for name in class_names[:20]:\n",
        "  print(name)\n",
        "print('...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xbycDnT40u0"
      },
      "source": [
        "### Ejecutar inferencia\n",
        "\n",
        "YAMNet da puntuaciones por clase a nivel de cuadro (es decir, 521 puntuaciones por cada cuadro). Para determinar las predicciones a nivel de clip, las puntuaciones pueden agregarse por clase a través de los cuadros (por ejemplo, usando la agregación media o máxima). Esto se hace a continuación con `scores_np.mean(axis=0)`. Por último, para encontrar la clase mejor puntuada a nivel de clip, se toma el máximo de las 521 puntuaciones agregadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT0otp-A4Y3u"
      },
      "outputs": [],
      "source": [
        "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n",
        "class_scores = tf.reduce_mean(scores, axis=0)\n",
        "top_class = tf.math.argmax(class_scores)\n",
        "inferred_class = class_names[top_class]\n",
        "\n",
        "print(f'The main sound is: {inferred_class}')\n",
        "print(f'The embeddings shape: {embeddings.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBaLNg5H5IWa"
      },
      "source": [
        "Nota: El modelo ha deducido correctamente el sonido de un animal. Su meta en este tutorial es aumentar la precisión del modelo para clases específicas. Además, observe que el modelo generó 13 incorporaciones, 1 por cuadro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmthELBg1A2-"
      },
      "source": [
        "## Conjunto de datos ESC-50\n",
        "\n",
        "El conjunto de datos [ESC-50](https://github.com/karolpiczak/ESC-50#repository-content) ([Piczak, 2015](https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf)) es una recolección etiquetada de 2,000 grabaciones de audio ambiental de cinco segundos de duración. El conjunto de datos consta de 50 clases, con 40 ejemplos por clase.\n",
        "\n",
        "Descárguelo y extráigalo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWobqK8JmZOU"
      },
      "outputs": [],
      "source": [
        "_ = tf.keras.utils.get_file('esc-50.zip',\n",
        "                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcruxiuX1cO5"
      },
      "source": [
        "### Explorar los datos\n",
        "\n",
        "Los metadatos de cada archivo se especifican en el archivo csv en `./datasets/ESC-50-master/meta/esc50.csv`\n",
        "\n",
        "y todos los archivos de audio están en `./datasets/ESC-50-master/audio/`\n",
        "\n",
        "Creará un `DataFrame` de pandas con el mapeo y lo usará para tener una visión más clara de los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwmLygPrMAbH"
      },
      "outputs": [],
      "source": [
        "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
        "base_data_path = './datasets/ESC-50-master/audio/'\n",
        "\n",
        "pd_data = pd.read_csv(esc50_csv)\n",
        "pd_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d4rHBEQ2QAU"
      },
      "source": [
        "### Filtrar los datos\n",
        "\n",
        "Ahora que los datos están almacenados en el `DataFrame`, aplique algunas transformaciones:\n",
        "\n",
        "- Filtre las filas y use sólo las clases seleccionadas: `dog` y `cat`. Si quiere usar otras clases, aquí puede elegir.\n",
        "- Modifique el nombre del archivo para que contenga la ruta completa. Esto facilitará la carga más adelante.\n",
        "- Cambie los objetivos para que estén dentro de un rango específico. En este ejemplo, `dog` permanecerá en `0`, pero `cat` pasará a ser `1` en lugar de su valor original de `5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFnEoQjgs14I"
      },
      "outputs": [],
      "source": [
        "my_classes = ['dog', 'cat']\n",
        "map_class_to_id = {'dog':0, 'cat':1}\n",
        "\n",
        "filtered_pd = pd_data[pd_data.category.isin(my_classes)]\n",
        "\n",
        "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
        "filtered_pd = filtered_pd.assign(target=class_id)\n",
        "\n",
        "full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))\n",
        "filtered_pd = filtered_pd.assign(filename=full_path)\n",
        "\n",
        "filtered_pd.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkDcBS-aJdCz"
      },
      "source": [
        "### Cargar los archivos de audio y recuperar las incorporaciones\n",
        "\n",
        "Aquí aplicará el `load_wav_16k_mono` y preparará los datos WAV para el modelo.\n",
        "\n",
        "Al extraer las incorporaciones de los datos WAV, obtiene un arreglo de forma `(N, 1024)` donde `N` es el número de cuadros que encontró YAMNet (uno por cada 0.48 segundos de audio)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKDT5RomaDKO"
      },
      "source": [
        "Su modelo usará cada cuadro como una entrada. Por eso, tiene que crear una nueva columna que tenga un cuadro por fila. También tiene que ampliar las etiquetas y la columna `fold` para reflejar adecuadamente estas nuevas filas.\n",
        "\n",
        "La columna expandida `fold` conserva los valores originales. No se puede mezclar cuadros porque, al dividirse, podría acabar teniendo partes del mismo audio en distintas divisiones, con lo que la validación y la prueba serían menos eficaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5Rq3_PyKLtU"
      },
      "outputs": [],
      "source": [
        "filenames = filtered_pd['filename']\n",
        "targets = filtered_pd['target']\n",
        "folds = filtered_pd['fold']\n",
        "\n",
        "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
        "main_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsEfovDVAHGY"
      },
      "outputs": [],
      "source": [
        "def load_wav_for_map(filename, label, fold):\n",
        "  return load_wav_16k_mono(filename), label, fold\n",
        "\n",
        "main_ds = main_ds.map(load_wav_for_map)\n",
        "main_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0tG8DBNAHcE"
      },
      "outputs": [],
      "source": [
        "# applies the embedding extraction model to a wav data\n",
        "def extract_embedding(wav_data, label, fold):\n",
        "  ''' run YAMNet to extract embedding from the wav data '''\n",
        "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
        "  num_embeddings = tf.shape(embeddings)[0]\n",
        "  return (embeddings,\n",
        "            tf.repeat(label, num_embeddings),\n",
        "            tf.repeat(fold, num_embeddings))\n",
        "\n",
        "# extract embedding\n",
        "main_ds = main_ds.map(extract_embedding).unbatch()\n",
        "main_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdfPIeD0Qedk"
      },
      "source": [
        "### Dividir los datos\n",
        "\n",
        "Va a usar la columna `fold` para dividir el conjunto de datos en conjuntos de entrenamiento, validación y prueba.\n",
        "\n",
        "ESC-50 está organizado en cinco `fold` de validación cruzada con tamaño uniforme, para que los clips de la misma fuente original estén siempre en el mismo `fold`. Más información en el artículo [ESC: Conjunto de datos para la clasificación de sonidos ambientales](https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf).\n",
        "\n",
        "El último paso es eliminar la columna `fold` del conjunto de datos, ya que no se usa durante el entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZYvlFiVsffC"
      },
      "outputs": [],
      "source": [
        "cached_ds = main_ds.cache()\n",
        "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)\n",
        "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)\n",
        "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)\n",
        "\n",
        "# remove the folds column now that it's not needed anymore\n",
        "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
        "\n",
        "train_ds = train_ds.map(remove_fold_column)\n",
        "val_ds = val_ds.map(remove_fold_column)\n",
        "test_ds = test_ds.map(remove_fold_column)\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5PaMwvtcAIe"
      },
      "source": [
        "## Crear su modelo\n",
        "\n",
        "¡Ya hizo la mayor parte del trabajo! Ahora, defina un modelo [Sequential](https://www.tensorflow.org/guide/keras/sequential_model) muy sencillo con una capa oculta y dos salidas para reconocer gatos y perros a partir de sonidos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYCE0Fr1GpN3"
      },
      "outputs": [],
      "source": [
        "my_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
        "                          name='input_embedding'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(my_classes))\n",
        "], name='my_model')\n",
        "\n",
        "my_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1qgH35HY0SE"
      },
      "outputs": [],
      "source": [
        "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                            patience=3,\n",
        "                                            restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3sj84eOZ3pk"
      },
      "outputs": [],
      "source": [
        "history = my_model.fit(train_ds,\n",
        "                       epochs=20,\n",
        "                       validation_data=val_ds,\n",
        "                       callbacks=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbraYKYpdoE"
      },
      "source": [
        "Vamos a ejecutar el método `evaluate` en los datos de prueba para asegurarnos de que no hay ningún sobreajuste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4Nh5nec3Sky"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = my_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cid-qIrIpqHS"
      },
      "source": [
        "¡Excelente!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCKZonrJcXab"
      },
      "source": [
        "## Cómo probar su modelo\n",
        "\n",
        "A continuación, pruebe su modelo sobre la incorporación de la prueba anterior usando sólo YAMNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79AFpA3_ctCF"
      },
      "outputs": [],
      "source": [
        "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n",
        "result = my_model(embeddings).numpy()\n",
        "\n",
        "inferred_class = my_classes[result.mean(axis=0).argmax()]\n",
        "print(f'The main sound is: {inferred_class}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2yleeev645r"
      },
      "source": [
        "## Guardar un modelo que pueda tomar directamente un archivo WAV como entrada\n",
        "\n",
        "Su modelo funciona cuando le da las incorporaciones como entrada.\n",
        "\n",
        "En un escenario real, necesitará usar datos de audio como entrada directa.\n",
        "\n",
        "Para ello, combinará YAMNet con su modelo en un único modelo que podrá exportar para otras aplicaciones.\n",
        "\n",
        "Por comodidad a la hora de usar los resultados del modelo, la capa final será una operación `reduce_media`. Si está usando este modelo para el servicio (aprenderá más adelante sobre eso en el tutorial), necesitará el nombre de la capa final. Si no define uno, TensorFlow definirá automáticamente uno incremental, dificultando las pruebas, ya que cambia cada vez que entrena el modelo. Si está usando una operación TensorFlow sin procesar, no puede darle un nombre. Para resolver este problema, creará una capa personalizada que aplique `reduce_media` y la llamará `'classifier'`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUVCI2Suunpw"
      },
      "outputs": [],
      "source": [
        "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, axis=0, **kwargs):\n",
        "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.math.reduce_mean(input, axis=self.axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE_Npm0nzlwc"
      },
      "outputs": [],
      "source": [
        "saved_model_path = './dogs_and_cats_yamnet'\n",
        "\n",
        "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
        "embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
        "                                            trainable=False, name='yamnet')\n",
        "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
        "serving_outputs = my_model(embeddings_output)\n",
        "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
        "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
        "serving_model.save(saved_model_path, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-0bY5FMme1C"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(serving_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btHQDN9mqxM_"
      },
      "source": [
        "Cargue su modelo guardado para verificar que funciona como se espera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkYVpJS72WWB"
      },
      "outputs": [],
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BkmvvNzq49l"
      },
      "source": [
        "Y para la prueba final: dados unos datos de sonido, ¿su modelo devuelve el resultado correcto?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeXtD5HO28y-"
      },
      "outputs": [],
      "source": [
        "reloaded_results = reloaded_model(testing_wav_data)\n",
        "cat_or_dog = my_classes[tf.math.argmax(reloaded_results)]\n",
        "print(f'The main sound is: {cat_or_dog}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRrOcBYTUgwn"
      },
      "source": [
        "Si quiere probar su nuevo modelo en un escenario de servicio, puede usar la firma 'serving_default'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycC8zzDSUG2s"
      },
      "outputs": [],
      "source": [
        "serving_results = reloaded_model.signatures['serving_default'](testing_wav_data)\n",
        "cat_or_dog = my_classes[tf.math.argmax(serving_results['classifier'])]\n",
        "print(f'The main sound is: {cat_or_dog}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da7blblCHs8c"
      },
      "source": [
        "## (Opcional) Unas pruebas más\n",
        "\n",
        "El modelo está listo.\n",
        "\n",
        "Comparémoslo con YAMNet en el conjunto de datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDf5MASIIN1z"
      },
      "outputs": [],
      "source": [
        "test_pd = filtered_pd.loc[filtered_pd['fold'] == 5]\n",
        "row = test_pd.sample(1)\n",
        "filename = row['filename'].item()\n",
        "print(filename)\n",
        "waveform = load_wav_16k_mono(filename)\n",
        "print(f'Waveform values: {waveform}')\n",
        "_ = plt.plot(waveform)\n",
        "\n",
        "display.Audio(waveform, rate=16000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYUzFxYJIcE1"
      },
      "outputs": [],
      "source": [
        "# Run the model, check the output.\n",
        "scores, embeddings, spectrogram = yamnet_model(waveform)\n",
        "class_scores = tf.reduce_mean(scores, axis=0)\n",
        "top_class = tf.math.argmax(class_scores)\n",
        "inferred_class = class_names[top_class]\n",
        "top_score = class_scores[top_class]\n",
        "print(f'[YAMNet] The main sound is: {inferred_class} ({top_score})')\n",
        "\n",
        "reloaded_results = reloaded_model(waveform)\n",
        "your_top_class = tf.math.argmax(reloaded_results)\n",
        "your_inferred_class = my_classes[your_top_class]\n",
        "class_probabilities = tf.nn.softmax(reloaded_results, axis=-1)\n",
        "your_top_score = class_probabilities[your_top_class]\n",
        "print(f'[Your model] The main sound is: {your_inferred_class} ({your_top_score})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8Tsym8Rq-0V"
      },
      "source": [
        "## Siguientes pasos\n",
        "\n",
        "Ha creado un modelo que puede clasificar sonidos de perros o gatos. Con la misma idea y un conjunto de datos distinto puede intentar, por ejemplo, construir un [identificador acústico de pájaros](https://www.kaggle.com/c/birdclef-2021/) basado en su canto.\n",
        "\n",
        "¡Comparta su proyecto con el equipo de TensorFlow en las redes sociales!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transfer_learning_audio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
