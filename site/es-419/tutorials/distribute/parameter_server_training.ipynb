{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA5Mubike7OJ"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fY0a3LRYfHUl"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNz7xXMSsAQa"
      },
      "source": [
        "# Entrenamiento del servidor de parámetros con ParameterServerStrategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyqRIqxsJuc"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/distribute/parameter_server_training\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/distribute/parameter_server_training.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/distribute/parameter_server_training.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tutorials/distribute/parameter_server_training.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v4D6QfcfTrm"
      },
      "source": [
        "## Descripción general\n",
        "\n",
        "[El entrenamiento de servidores de parámetros](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) es un método común de datos paralelos para ampliar el entrenamiento de modelos en varias máquinas.\n",
        "\n",
        "Un clúster de entrenamiento de servidores de parámetros consiste en *workers* y *servidores de parámetros*. Las variables se crean en los servidores de parámetros y se leen y actualizan mediante los workers en cada paso. De forma predeterminada, los workers leen y actualizan estas variables de forma independiente, sin sincronizarse entre sí. Por eso, a veces, el entrenamiento tipo servidor de parámetros se denomina *entrenamiento asíncrono*.\n",
        "\n",
        "En TensorFlow 2, el entrenamiento del servidor de parámetros se realiza mediante la clase `tf.distribute.ParameterServerStrategy`, que distribuye los pasos del entrenamiento a un clúster que escala hasta miles de workers (acompañados de servidores de parámetros)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1LGfTdgOF-J"
      },
      "source": [
        "### Métodos de entrenamiento admitidos\n",
        "\n",
        "Hay dos métodos principales de entrenamiento compatibles:\n",
        "\n",
        "- La API `Model.fit` de Keras: si prefiere una abstracción y manejo de alto nivel del entrenamiento. Generalmente se recomienda si está entrenando un `tf.keras.Model`.\n",
        "- Un bucle de entrenamiento personalizado: si prefiere definir los detalles de su bucle de entrenamiento (puede consultar las guías sobre [Entrenamiento personalizado](../customization/custom_training_walkthrough.ipynb), [Escribir un bucle de entrenamiento desde cero](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch) y [Bucle de entrenamiento personalizado con Keras y MultiWorkerMirroredStrategy](multi_worker_with_ctl.ipynb) para obtener más detalles)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjbULGvV7NRz"
      },
      "source": [
        "### Un cluster con trabajos y tareas\n",
        "\n",
        "Independientemente de la API elegida (`Model.fit` o un bucle de entrenamiento personalizado), el entrenamiento distribuido en TensorFlow 2 implica un `'cluster'` con varios `'jobs'`, y cada uno de los jobs puede tener una o más `'tasks'`.\n",
        "\n",
        "Cuando se utiliza el entrenamiento del servidor de parámetros, se recomienda tener:\n",
        "\n",
        "- Un puesto de *coordinador* (que tiene el nombre de `chief`)\n",
        "- Múltiples puestos de *trabajador* (nombre del puesto `worker`)\n",
        "- Múltiples *parámetros del servidor* (nombre del puesto `ps`)\n",
        "\n",
        "El *coordinador* crea recursos, despacha tareas de entrenamiento, escribe puntos de control y se ocupa de los fallos de las tareas. Los *trabajadores* y *servidores de parámetros* ejecutan instancias `tf.distribute.Server` que reciben solicitudes del coordinador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLV1FbpLtqtB"
      },
      "source": [
        "### Entrenamiento del servidor de parámetros con la API `Model.fit`\n",
        "\n",
        "El entrenamiento del servidor de parámetros con la API `Model.fit` requiere que el coordinador utilice un objeto `tf.distribute.ParameterServerStrategy`. De forma similar al uso de `Model.fit` sin estrategia, o con otras estrategias, el flujo de trabajo implica crear y compilar el modelo, preparar las retrollamadas y llamar a `Model.fit`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ5AosxFyfzk"
      },
      "source": [
        "### Entrenamiento del servidor de parámetros con un bucle de entrenamiento personalizado\n",
        "\n",
        "Con bucles de entrenamiento personalizados, la clase `tf.distribute.coordinator.ClusterCoordinator` es el componente clave utilizado para el coordinador.\n",
        "\n",
        "- La clase `ClusterCoordinator` debe funcionar junto con un objeto `tf.distribute.ParameterServerStrategy`.\n",
        "- Este objeto `tf.distribute.Strategy` es necesario para proporcionar la información del conglomerado y se utiliza para definir un paso del entrenamiento, como se demuestra en el [Entrenamiento personalizado con tf.distribute.Strategy](custom_training.ipynb).\n",
        "- A continuación, el objeto `ClusterCoordinator` envía la ejecución de estos pasos de entrenamiento a los workers remotos.\n",
        "\n",
        "La API más importante que proporciona el objeto `ClusterCoordinator` es `schedule`:\n",
        "\n",
        "- La API `schedule` pone en espera una `tf.function` y devuelve un `RemoteValue` de tipo futuro inmediatamente.\n",
        "- Las funciones en la fila se enviarán a los workers remotos en hilos de fondo y sus `RemoteValue` se rellenarán de forma asíncrona.\n",
        "- Dado que `schedule` no requiere la asignación de un worker, la `tf.function` anterior puede ejecutarse en cualquier worker disponible.\n",
        "- Si el worker en el que se ejecuta deja de estar disponible antes de su finalización, la función se volverá a intentar en otro worker disponible.\n",
        "- Debido a este hecho y a que la ejecución de funciones no es atómica, una única llamada a una función puede ejecutarse más de una vez.\n",
        "\n",
        "Además de despachar funciones remotas, el `ClusterCoordinator` también ayuda a crear conjuntos de datos en todos los workers y a reconstruir estos conjuntos de datos cuando un worker se recupera de un error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyDnWjmOje5-"
      },
      "source": [
        "## Preparación del tutorial\n",
        "\n",
        "El tutorial se bifurcará en `Model.fit` y rutas de bucle de entrenamiento personalizado, y podrá elegir la que se ajuste a sus necesidades. Las secciones distintas de \"Entrenamiento con X\" se aplican a ambas rutas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-V3LUcIs4a-"
      },
      "outputs": [],
      "source": [
        "!pip install portpicker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlI_NAVFae3J"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import multiprocessing\n",
        "import os\n",
        "import random\n",
        "import portpicker\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvwgM2rzgzIC"
      },
      "source": [
        "## Preparación del clúster\n",
        "\n",
        "Como se mencionó anteriormente, un clúster de entrenamiento de servidor de parámetros requiere una tarea coordinadora que ejecute su programa de entrenamiento, uno o varios workers y tareas de servidor de parámetros que ejecuten servidores TensorFlow-`tf.distribute.Server`-y posiblemente una tarea de evaluación adicional que ejecute la evaluación sidecar (consulte la sección [evaluación sidecar](#sidecar_evaluation) a continuación). Los requisitos necesarios para su configuración son:\n",
        "\n",
        "- La tarea coordinadora necesita conocer las direcciones y puertos de todos los demás servidores de TensorFlow, excepto la del evaluador.\n",
        "- Los workers y los servidores de parámetros necesitan saber en qué puerto deben escuchar. Para mayor simplicidad, normalmente puede pasar la información completa del clúster al crear servidores TensorFlow en estas tareas.\n",
        "- La tarea evaluadora no tiene por qué conocer la preparación del cluster de entrenamiento. Si la conoce, no debe intentar conectarse al clúster de entrenamiento.\n",
        "- Los workers y los servidores de parámetros deben tener como tipos de tarea `\"worker\"` y `\"ps\"`, respectivamente. El coordinador debe utilizar `\"chief\"` como tipo de tarea por razones de legado.\n",
        "\n",
        "En este tutorial, creará un clúster en proceso para que todo el entrenamiento del servidor de parámetros pueda ejecutarse en Colab. Aprenderá a configurar [clusters reales](#real_clusters) en una sección posterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UNs7Lm2g19n"
      },
      "source": [
        "### Clúster en proceso\n",
        "\n",
        "Comenzará mediante la creación de varios servidores de TensorFlow por adelantado y se conectará a ellos más tarde. Tenga en cuenta que esto es sólo con el propósito de demostración de este tutorial, y en el entrenamiento real los servidores se iniciarán en máquinas `\"worker\"` y `\"ps\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbrP5pXuaoVH"
      },
      "outputs": [],
      "source": [
        "def create_in_process_cluster(num_workers, num_ps):\n",
        "  \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\n",
        "  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\n",
        "  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\n",
        "\n",
        "  cluster_dict = {}\n",
        "  cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\n",
        "  if num_ps > 0:\n",
        "    cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\n",
        "\n",
        "  cluster_spec = tf.train.ClusterSpec(cluster_dict)\n",
        "\n",
        "  # Workers need some inter_ops threads to work properly.\n",
        "  worker_config = tf.compat.v1.ConfigProto()\n",
        "  if multiprocessing.cpu_count() < num_workers + 1:\n",
        "    worker_config.inter_op_parallelism_threads = num_workers + 1\n",
        "\n",
        "  for i in range(num_workers):\n",
        "    tf.distribute.Server(\n",
        "        cluster_spec,\n",
        "        job_name=\"worker\",\n",
        "        task_index=i,\n",
        "        config=worker_config,\n",
        "        protocol=\"grpc\")\n",
        "\n",
        "  for i in range(num_ps):\n",
        "    tf.distribute.Server(\n",
        "        cluster_spec,\n",
        "        job_name=\"ps\",\n",
        "        task_index=i,\n",
        "        protocol=\"grpc\")\n",
        "\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\n",
        "      cluster_spec, rpc_layer=\"grpc\")\n",
        "  return cluster_resolver\n",
        "\n",
        "# Set the environment variable to allow reporting worker and ps failure to the\n",
        "# coordinator. This is a workaround and won't be necessary in the future.\n",
        "os.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\n",
        "\n",
        "NUM_WORKERS = 3\n",
        "NUM_PS = 2\n",
        "cluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX_91OByt0J2"
      },
      "source": [
        "La configuración de grupos en proceso se utiliza con frecuencia en las pruebas unitarias, como se muestra [aquí](https://github.com/tensorflow/tensorflow/blob/eb4c40fc91da260199fa2aed6fe67d36ad49fafd/tensorflow/python/distribute/coordinator/cluster_coordinator_test.py#L447).\n",
        "\n",
        "Otra opción para realizar pruebas locales es lanzar procesos en la máquina local - consulte [Entrenamiento multi-worker con Keras](multi_worker_with_keras.ipynb) para ver un ejemplo de este enfoque."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyby6M2Jqg6J"
      },
      "source": [
        "## Instanciar un ParameterServerStrategy\n",
        "\n",
        "Antes de sumergirse en el código de entrenamiento, vamos a instanciar un objeto `tf.distribute.ParameterServerStrategy`. Tenga en cuenta que esto es necesario independientemente de si está procediendo con `Model.fit` o con un bucle de entrenamiento personalizado. El argumento `variable_partitioner` se explicará en la [sección Fragmentación de variables](#variable_sharding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YyEPgisrC35"
      },
      "outputs": [],
      "source": [
        "variable_partitioner = (\n",
        "    tf.distribute.experimental.partitioners.MinSizePartitioner(\n",
        "        min_shard_bytes=(256 << 10),\n",
        "        max_shards=NUM_PS))\n",
        "\n",
        "strategy = tf.distribute.ParameterServerStrategy(\n",
        "    cluster_resolver,\n",
        "    variable_partitioner=variable_partitioner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlAQxuMDJ3k9"
      },
      "source": [
        "Para utilizar las GPU para realizar el entrenamiento, asigne GPU visibles a cada worker. `ParameterServerStrategy` utilizará todas las GPU disponibles en cada trabajador, con la restricción de que todos los trabajadores deben tener el mismo número de GPU disponibles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMmBLsf6sEXh"
      },
      "source": [
        "### Fragmentación de variables\n",
        "\n",
        "La fragmentación de variables se refiere a la división de una variable en múltiples variables más pequeñas, que se denominan *frgamentos*. La fragmentación de variables puede ser útil para distribuir la carga de la red cuando se accede a estos fragmentos. También es útil para distribuir el cálculo y el almacenamiento de una variable normal entre varios servidores de parámetros, por ejemplo, cuando se utilizan incrustaciones muy grandes que quizá no quepan en la memoria de una sola máquina.\n",
        "\n",
        "Para habilitar la fragmentación de variables, puede pasar un `variable_partitioner` al construir un objeto `ParameterServerStrategy`. El `variable_partitioner` se invocará cada vez que se cree una variable y se espera que devuelva el número de fragmentos a lo largo de cada dimensión de la variable. Se proporcionan algunos `variable_partitioner`como `tf.distribute.experimental.partitioners.MinSizePartitioner`. Se recomienda utilizar particionadores basados en el tamaño como `tf.distribute.experimental.partitioners.MinSizePartitioner` para evitar particionar variables pequeñas, lo que podría tener un impacto negativo en la velocidad de entrenamiento del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1--SxlxtsOb7"
      },
      "source": [
        "Cuando se pasa un `partitioner_variable`, y se crea una variable directamente bajo `Strategy.scope`, la variable se convertirá en un tipo contenedor con una propiedad `variables`, la cual proporciona acceso a la lista de los fragmentos. En la mayoría de los casos, este contenedor se convertirá automáticamente en un tensor que concatena todos los fragmentos. Como resultado, puede utilizarse como una variable normal. Por otro lado, algunos métodos de TensorFlow como `tf.nn.embedding_lookup` proporcionan una implementación eficiente para este tipo de contenedores y en estos métodos se evitará la concatenación automática.\n",
        "\n",
        "Consulte la documentación de la API de `tf.distribute.ParameterServerStrategy` para obtener más detalles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlOq-O-26O1d"
      },
      "source": [
        "## Entrenamiento con `Model.fit`\n",
        "\n",
        "<a id=\"training_with_modelfit\"></a>\n",
        "\n",
        "Keras proporciona una API de entrenamiento fácil de usar a través de `Model.fit` que maneja el bucle de entrenamiento de manera encubierta, con la flexibilidad de un `train_step` anulable, y retrollamadas que proporcionan funcionalidades como guardar puntos de control o guardar resúmenes para TensorBoard. Con `Model.fit`, el mismo código de entrenamiento puede utilizarse con otras estrategias con un simple intercambio del objeto de estrategia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMZ9Cu5J6ZGi"
      },
      "source": [
        "### Datos introducidos\n",
        "\n",
        "Keras `Model.fit` con `tf.distribute.ParameterServerStrategy` puede tomar datos de entrada en forma de un `tf.data.Dataset`, `tf. distribute.DistributedDataset`, o un `tf.keras.utils.experimental.DatasetCreator`, siendo `Dataset` la opción recomendada por su facilidad de uso. Sin embargo, si tiene problemas de memoria al utilizar `Dataset`, puede que necesite utilizar `DatasetCreator` con un argumento `dataset_fn` invocable (consulte la documentación de la API `tf.keras.utils.experimental.DatasetCreator` para obtener más detalles).\n",
        "\n",
        "Si transforma su conjunto de datos en un `tf.data.Dataset`, deberá utilizar `Dataset.shuffle` y `Dataset.repeat`, como se demuestra en el siguiente ejemplo de código.\n",
        "\n",
        "- Keras `Model.fit` con el parámetro server training asume que cada worker recibe el mismo conjunto de datos, excepto cuando se mezclan de forma diferente. Por lo tanto, al llamar a `Dataset.shuffle`, se aseguran iteraciones más uniformes sobre los datos.\n",
        "- Dado que los workers no se sincronizan, pueden terminar de procesar sus conjuntos de datos en momentos diferentes. Por lo tanto, la forma más sencilla de definir épocas con parámetros de entrenamiento del servidor es utilizar `Dataset.repeat` -que repite un conjunto de datos indefinidamente cuando se llama sin argumento- y especificar el argumento `steps_per_epoch` en la llamada `Model.fit`.\n",
        "\n",
        "Consulte la sección \"Flujos de trabajo de entrenamiento\" de la guía [tf.data](../../guide/data.ipynb) para obtener más detalles sobre `shuffle` y `repeat`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shAo1CCS7wU1"
      },
      "outputs": [],
      "source": [
        "global_batch_size = 64\n",
        "\n",
        "x = tf.random.uniform((10, 10))\n",
        "y = tf.random.uniform((10,))\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()\n",
        "dataset = dataset.batch(global_batch_size)\n",
        "dataset = dataset.prefetch(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_jhF70K7zON"
      },
      "source": [
        "Si en cambio crea su conjunto de datos con `tf.keras.utils.experimental.DatasetCreator`, el código en `dataset_fn` se invocará en el dispositivo de entrada, que generalmente es el CPU, en cada una de las máquinas de trabajo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w60PuWrWwBD4"
      },
      "source": [
        "### Construcción y compilación de modelos\n",
        "\n",
        "Ahora, creará un `tf.keras.Model`-un modelo trivial `tf.keras.models.Sequential` con fines de demostración-seguido de una llamada `Model.compile` para incorporar componentes, como un optimizador, métricas y otros parámetros como `steps_per_execution`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhTHUYaD74vT"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
        "\n",
        "  model.compile(tf.keras.optimizers.legacy.SGD(), loss=\"mse\", steps_per_execution=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWb_Ekm377YX"
      },
      "source": [
        "### Retrollamadas y entrenamiento\n",
        "\n",
        "<a id=\"callbacks-and-training\"> </a>\n",
        "\n",
        "Antes de llamar a Keras `Model.fit` para el entrenamiento real, prepare cualquiera de las [retrollamadas](https://www.tensorflow.org/guide/keras/train_and_evaluate) necesarias para realizar tareas comunes, como:\n",
        "\n",
        "- `tf.keras.callbacks.ModelCheckpoint`: guarda el modelo con cierta frecuencia, por ejemplo, después de cada época.\n",
        "- `tf.keras.callbacks.BackupAndRestore`: proporciona tolerancia ante errores haciendo una copia de seguridad del modelo y del número de época actual, si el clúster experimenta falta de disponibilidad (como abortar o adelantarse). Posteriormente, puede restaurar el estado de entrenamiento al reiniciar desde un error en el trabajo, y continuar el entrenamiento desde el principio de la época que se interrumpió.\n",
        "- `tf.keras.callbacks.TensorBoard`: escribe periódicamente los registros del modelo en archivos de resumen que pueden visualizarse en la herramienta TensorBoard.\n",
        "\n",
        "Nota: Debido a consideraciones de rendimiento, las llamadas de retorno personalizadas no pueden tener llamadas de nivel de lote anuladas cuando se utilizan con `ParameterServerStrategy`. Modifique sus retrollamadas personalizadas para que sean llamadas a nivel de época y ajuste `steps_per_epoch` a un valor adecuado. Además, `steps_per_epoch` es un argumento obligatorio para `Model.fit` cuando se utiliza con `ParameterServerStrategy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ddUvUZk7_wm"
      },
      "outputs": [],
      "source": [
        "working_dir = \"/tmp/my_working_dir\"\n",
        "log_dir = os.path.join(working_dir, \"log\")\n",
        "ckpt_filepath = os.path.join(working_dir, \"ckpt\")\n",
        "backup_dir = os.path.join(working_dir, \"backup\")\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),\n",
        "    tf.keras.callbacks.BackupAndRestore(backup_dir=backup_dir),\n",
        "]\n",
        "\n",
        "model.fit(dataset, epochs=5, steps_per_epoch=20, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWgP1h2z8B3j"
      },
      "source": [
        "### Uso directo con `ClusterCoordinator` (opcional)\n",
        "\n",
        "Incluso si elige la ruta de entrenamiento `Model.fit`, puede instanciar opcionalmente un objeto `tf.distribute.coordinator.ClusterCoordinator` para programar otras funciones que desee que se ejecuten en los workers. Consulte la sección [Entrenamiento con un bucle de entrenamiento personalizado](#training_with_custom_training_loop) para obtener más detalles y ejemplos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxypEyIthR0z"
      },
      "source": [
        "## Entrenamiento con un bucle de entrenamiento personalizado\n",
        "\n",
        "<a id=\"training_with_custom_training_loop\"> </a>\n",
        "\n",
        "El uso de bucles de entrenamiento personalizados con `tf.distribute.Strategy` proporciona una gran flexibilidad para definir bucles de entrenamiento. Con el `ParameterServerStrategy` definido anteriormente (como `strategy`), utilizará un `tf.distribute.coordinator.ClusterCoordinator` para enviar la ejecución de los pasos de entrenamiento a los workers remotos.\n",
        "\n",
        "A continuación, creará un modelo, definirá un conjunto de datos y definirá una función de paso, como ya lo hizo en el bucle de entrenamiento con otros `tf.distribute.Strategy`. Encontrará más detalles en el tutorial [Entrenamiento personalizado con tf.distribute.Strategy](custom_training.ipynb).\n",
        "\n",
        "Para garantizar una extracción previa eficiente de los conjuntos de datos, utilice las API recomendadas para la creación distribuida de conjuntos de datos que se mencionan en la sección [Despacho de pasos de entrenamiento a trabajadores remotos](#dispatch_training_steps_to_remote_workers) que aparecerá más adelante. Además, asegúrese de llamar a `Strategy.run` dentro de `worker_fn` para aprovechar al máximo las GPU asignadas a los trabajadores. El resto de los pasos son los mismos para el entrenamiento con o sin una GPU.\n",
        "\n",
        "Vamos a crear estos componentes en los siguientes pasos:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QNkCtV8VivM"
      },
      "source": [
        "### Prepare los datos\n",
        "\n",
        "En primer lugar, escriba una función que cree un conjunto de datos.\n",
        "\n",
        "Si desea preprocesar los datos con [capas de preprocesamiento de Keras](https://www.tensorflow.org/guide/keras/preprocessing_layers) o [capas de transformación de Tensorflow](https://www.tensorflow.org/tfx/tutorials/transform/simple), cree estas capas **fuera del `dataset_fn`** y **bajo `Strategy.scope`**, como lo haría para cualquier otra capa de Keras. Esto se debe a que el `dataset_fn` se envolverá en una `tf.function` y después se ejecutará en cada worker para generar la canalización de datos.\n",
        "\n",
        "Si no sigue el procedimiento anterior, la creación de las capas podría crear estados de Tensorflow que se elevarían de la `tf.function` al coordinador. En este caso, acceder a ellos desde los workers incurriría en llamadas RPC repetitivas entre el coordinador y los workers, y causaría una desaceleración significativa.\n",
        "\n",
        "Si coloca las capas bajo `Strategy.scope` las creará en todos los workers. A continuación, aplicará la transformación dentro del `dataset_fn` mediante `tf.data.Dataset.map`. Consulte *Preprocesamiento de datos* en el tutorial [Entrada distribuida](input.ipynb) para obtener más información sobre el preprocesamiento de datos con entrada distribuida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GUwATssauus"
      },
      "outputs": [],
      "source": [
        "feature_vocab = [\n",
        "    \"avenger\", \"ironman\", \"batman\", \"hulk\", \"spiderman\", \"kingkong\", \"wonder_woman\"\n",
        "]\n",
        "label_vocab = [\"yes\", \"no\"]\n",
        "\n",
        "with strategy.scope():\n",
        "  feature_lookup_layer = tf.keras.layers.StringLookup(\n",
        "      vocabulary=feature_vocab,\n",
        "      mask_token=None)\n",
        "  label_lookup_layer = tf.keras.layers.StringLookup(\n",
        "      vocabulary=label_vocab,\n",
        "      num_oov_indices=0,\n",
        "      mask_token=None)\n",
        "\n",
        "  raw_feature_input = tf.keras.layers.Input(\n",
        "      shape=(3,),\n",
        "      dtype=tf.string,\n",
        "      name=\"feature\")\n",
        "  feature_id_input = feature_lookup_layer(raw_feature_input)\n",
        "  feature_preprocess_stage = tf.keras.Model(\n",
        "      {\"features\": raw_feature_input},\n",
        "      feature_id_input)\n",
        "\n",
        "  raw_label_input = tf.keras.layers.Input(\n",
        "      shape=(1,),\n",
        "      dtype=tf.string,\n",
        "      name=\"label\")\n",
        "  label_id_input = label_lookup_layer(raw_label_input)\n",
        "\n",
        "  label_preprocess_stage = tf.keras.Model(\n",
        "      {\"label\": raw_label_input},\n",
        "      label_id_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgp8MX_7OR_A"
      },
      "source": [
        "Generar ejemplos simulados en un conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chIY4fFANaFH"
      },
      "outputs": [],
      "source": [
        "def feature_and_label_gen(num_examples=200):\n",
        "  examples = {\"features\": [], \"label\": []}\n",
        "  for _ in range(num_examples):\n",
        "    features = random.sample(feature_vocab, 3)\n",
        "    label = [\"yes\"] if \"avenger\" in features else [\"no\"]\n",
        "    examples[\"features\"].append(features)\n",
        "    examples[\"label\"].append(label)\n",
        "  return examples\n",
        "\n",
        "examples = feature_and_label_gen()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AtZBya7OeyZ"
      },
      "source": [
        "A continuación, cree el conjunto de datos de entrenamiento envuelto en un `dataset_fn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs0QYRZoNbvw"
      },
      "outputs": [],
      "source": [
        "def dataset_fn(_):\n",
        "  raw_dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
        "\n",
        "  train_dataset = raw_dataset.map(\n",
        "      lambda x: (\n",
        "          {\"features\": feature_preprocess_stage(x[\"features\"])},\n",
        "          label_preprocess_stage(x[\"label\"])\n",
        "      )).shuffle(200).batch(32).repeat()\n",
        "  return train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT9PQexJiFtB"
      },
      "source": [
        "### Construir el modelo\n",
        "\n",
        "Después, cree el modelo y los demás objetos. Asegúrese de crear todas las variables bajo `Strategy.scope`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Quxud1uEazeo"
      },
      "outputs": [],
      "source": [
        "# These variables created under the `Strategy.scope` will be placed on parameter\n",
        "# servers in a round-robin fashion.\n",
        "with strategy.scope():\n",
        "  # Create the model. The input needs to be compatible with Keras processing layers.\n",
        "  model_input = tf.keras.layers.Input(\n",
        "      shape=(3,), dtype=tf.int64, name=\"model_input\")\n",
        "\n",
        "  emb_layer = tf.keras.layers.Embedding(\n",
        "      input_dim=len(feature_lookup_layer.get_vocabulary()), output_dim=16384)\n",
        "  emb_output = tf.reduce_mean(emb_layer(model_input), axis=1)\n",
        "  dense_output = tf.keras.layers.Dense(\n",
        "      units=1, activation=\"sigmoid\",\n",
        "      kernel_regularizer=tf.keras.regularizers.L2(1e-4),\n",
        "  )(emb_output)\n",
        "  model = tf.keras.Model({\"features\": model_input}, dense_output)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)\n",
        "  accuracy = tf.keras.metrics.Accuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyuxiqCQU50m"
      },
      "source": [
        "Confirmemos que el uso de `FixedShardsPartitioner` dividió todas las variables en dos fragmentos y que cada uno de ellos se asignó a un servidor de parámetros diferente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04r1nO4WVDO1"
      },
      "outputs": [],
      "source": [
        "assert len(emb_layer.weights) == 2\n",
        "assert emb_layer.weights[0].shape == (4, 16384)\n",
        "assert emb_layer.weights[1].shape == (4, 16384)\n",
        "\n",
        "print(emb_layer.weights[0].device)\n",
        "print(emb_layer.weights[1].device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWhfXZLRiHyM"
      },
      "source": [
        "### Definir el paso del entrenamiento\n",
        "\n",
        "En tercer lugar, cree el paso de entrenamiento envuelto en una `tf.function`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNNVo0bFa1K9"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def step_fn(iterator):\n",
        "\n",
        "  def replica_fn(batch_data, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "      pred = model(batch_data, training=True)\n",
        "      per_example_loss = tf.keras.losses.BinaryCrossentropy(\n",
        "          reduction=tf.keras.losses.Reduction.NONE)(labels, pred)\n",
        "      loss = tf.nn.compute_average_loss(per_example_loss)\n",
        "      model_losses = model.losses\n",
        "      if model_losses:\n",
        "        loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n",
        "    accuracy.update_state(labels, actual_pred)\n",
        "    return loss\n",
        "\n",
        "  batch_data, labels = next(iterator)\n",
        "  losses = strategy.run(replica_fn, args=(batch_data, labels))\n",
        "  return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvrYQUeYiLNy"
      },
      "source": [
        "En la función de pasos de entrenamiento anterior, llamar a `Strategy.run` y `Strategy.reduce` en el `step_fn` puede ser compatible con varias GPU por worker. Si los workers tienen GPUs asignadas, `Strategy.run` distribuirá los conjuntos de datos en múltiples réplicas (GPUs). Sus llamadas paralelas a `tf.nn.compute_average_loss()` calculan la media de la pérdida en las réplicas (GPU) de un trabajador, independientemente del número total de trabajadores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPJ3PV_L2zAY"
      },
      "source": [
        "### Enviar pasos de entrenamiento a workers remotos\n",
        "\n",
        "<a id=\"dispatch_training_steps_to_remote_workers\"> </a>\n",
        "\n",
        "Una vez que haya definido todos los cálculos mediante `ParameterServerStrategy`, utilizará la clase `tf.distribute.coordinator.ClusterCoordinator` para crear recursos y distribuir los pasos de entrenamiento entre los workers remotos.\n",
        "\n",
        "Primero creemos un objeto `ClusterCoordinator` y pasémosle el objeto de estrategia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpcMlH7Pa3DB"
      },
      "outputs": [],
      "source": [
        "coordinator = tf.distribute.coordinator.ClusterCoordinator(strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xRIgKxciOSe"
      },
      "source": [
        "A continuación, cree un conjunto de datos por worker y un iterador mediante la API `ClusterCoordinator.create_per_worker_dataset{/code0, que replica el conjunto de datos a todos los trabajadores. En el <code data-md-type=\"codespan\">per_worker_dataset_fn` siguiente, se recomienda envolver el `dataset_fn` en `strategy.distribute_datasets_from_function` para permitir una extracción previa eficiente a las GPU sin interrupciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9DCvTJTa4Q2"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def per_worker_dataset_fn():\n",
        "  return strategy.distribute_datasets_from_function(dataset_fn)\n",
        "\n",
        "per_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\n",
        "per_worker_iterator = iter(per_worker_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2pnOx78iRwW"
      },
      "source": [
        "El último paso consiste en distribuir el cómputo a los workers remotos utilizando `ClusterCoordinator.schedule`:\n",
        "\n",
        "- El método `schedule` pone en espera una `tf.function` y devuelve un `RemoteValue` de tipo futuro inmediatamente. Las funciones en espera se enviarán a los workers remotos en hilos de fondo y el `RemoteValue` se rellenará de forma asíncrona.\n",
        "- El método `join` (`ClusterCoordinator.join`) puede utilizarse para esperar hasta que se ejecuten todas las funciones programadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmPvactfa6Eh"
      },
      "outputs": [],
      "source": [
        "num_epochs = 4\n",
        "steps_per_epoch = 5\n",
        "for i in range(num_epochs):\n",
        "  accuracy.reset_states()\n",
        "  for _ in range(steps_per_epoch):\n",
        "    coordinator.schedule(step_fn, args=(per_worker_iterator,))\n",
        "  # Wait at epoch boundaries.\n",
        "  coordinator.join()\n",
        "  print(\"Finished epoch %d, accuracy is %f.\" % (i, accuracy.result().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBn-gn-OP3DR"
      },
      "source": [
        "A continuación le mostraremos cómo puede obtener el resultado de un `RemoteValue`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-15a2I_lQDO1"
      },
      "outputs": [],
      "source": [
        "loss = coordinator.schedule(step_fn, args=(per_worker_iterator,))\n",
        "print(\"Final loss is %f\" % loss.fetch())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htY4QKc9iXg9"
      },
      "source": [
        "Alternativamente, puede lanzar todos los pasos y hacer algo mientras espera a que se completen:\n",
        "\n",
        "```python\n",
        "for _ in range(total_steps):\n",
        "  coordinator.schedule(step_fn, args=(per_worker_iterator,))\n",
        "while not coordinator.done():\n",
        "  time.sleep(10)\n",
        "  # Do something like logging metrics or writing checkpoints.\n",
        "```\n",
        "\n",
        "Para ver el flujo de trabajo completo de entrenamiento y servicio de este ejemplo en particular, consulte esta [prueba](https://github.com/keras-team/keras/blob/master/keras/integration_test/parameter_server_keras_preprocessing_test.py).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzNsj2GR3BGs"
      },
      "source": [
        "### Obtener más información sobre la creación de conjuntos de datos\n",
        "\n",
        "El conjunto de datos del código anterior se crea utilizando la API `ClusterCoordinator.create_per_worker_dataset`. Crea un conjunto de datos por worker y devuelve un objeto contenedor. Puede llamar al método `iter` sobre él para crear un iterador por worker. El iterador por worker contiene un iterador por worker y la porción correspondiente de un worker se sustituirá en el argumento de entrada de la función pasada al método `ClusterCoordinator.schedule` antes de que la función se ejecute en un worker concreto.\n",
        "\n",
        "El método `ClusterCoordinator.schedule` asume que los workers son equivalentes y, por lo tanto, asume que los conjuntos de datos en diferentes workers son los mismos (excepto que pueden mezclarse de forma diferente). Debido a esto, también se recomienda repetir los conjuntos de datos y programar un número finito de pasos en vez de confiar en recibir un `OutOfRangeError` de un conjunto de datos.\n",
        "\n",
        "Otra nota importante es que los conjuntos de datos `tf.data` no admiten la serialización y deserialización implícitas más allá de los límites de las tareas. Así que es importante crear el conjunto de datos completo dentro de la función pasada a `ClusterCoordinator.create_per_worker_dataset`. La API `create_per_worker_dataset` también puede tomar directamente un `tf.data.Dataset` o `tf.distribute.DistributedDataset` como entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcfdI_M83lAM"
      },
      "source": [
        "## Evaluación\n",
        "\n",
        "Los dos enfoques principales para realizar la evaluación con `tf.distribute.ParameterServerStrategy` de entrenamiento son la evaluación en línea y la evaluación sidecar. Cada uno tiene sus pros y sus contras, como se describe a continuación. Se recomienda el método de evaluación en línea si no tiene ninguna preferencia. Para los usuarios que utilicen `Model.fit`, `Model.evaluate` utilice la evaluación en línea (distribuida) oculta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiG8EhcY3gA1"
      },
      "source": [
        "### Evaluación inline\n",
        "\n",
        "En este método, el coordinador alterna entre el entrenamiento y la evaluación, por lo que se denomina *evaluación inline*.\n",
        "\n",
        "La evaluación inline tiene varias ventajas. Por ejemplo:\n",
        "\n",
        "- Puede admitir grandes modelos de evaluación y conjuntos de datos de evaluación que una sola tarea no puede contener.\n",
        "- Los resultados de la evaluación pueden utilizarse para tomar decisiones para el entrenamiento de la siguiente época, por ejemplo, si se debe detener el entrenamiento antes de tiempo.\n",
        "\n",
        "Hay dos formas de aplicar la evaluación inline: la evaluación directa y la evaluación distribuida.\n",
        "\n",
        "- **Evaluación directa**: Para modelos y conjuntos de datos de evaluación pequeños, el coordinador puede ejecutar la evaluación directamente en el modelo distribuido con el conjunto de datos de evaluación en el coordinador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WakiAakoaHVn"
      },
      "outputs": [],
      "source": [
        "eval_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    feature_and_label_gen(num_examples=16)).map(\n",
        "          lambda x: (\n",
        "              {\"features\": feature_preprocess_stage(x[\"features\"])},\n",
        "              label_preprocess_stage(x[\"label\"])\n",
        "          )).batch(8)\n",
        "\n",
        "eval_accuracy = tf.keras.metrics.Accuracy()\n",
        "\n",
        "for batch_data, labels in eval_dataset:\n",
        "  pred = model(batch_data, training=False)\n",
        "  actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n",
        "  eval_accuracy.update_state(labels, actual_pred)\n",
        "\n",
        "print(\"Evaluation accuracy: %f\" % eval_accuracy.result())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKGHbdI7aGoJ"
      },
      "source": [
        "- **Evaluación distribuida**: Para modelos o conjuntos de datos de gran tamaño que no es factible ejecutar directamente en el coordinador, la tarea del coordinador puede distribuir las tareas de evaluación entre los workers por medio de los métodos `ClusterCoordinator.schedule`/`ClusterCoordinator.join`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcHNHJpDgEvK"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  # Define the eval metric on parameter servers.\n",
        "  eval_accuracy = tf.keras.metrics.Accuracy()\n",
        "\n",
        "@tf.function\n",
        "def eval_step(iterator):\n",
        "  def replica_fn(batch_data, labels):\n",
        "    pred = model(batch_data, training=False)\n",
        "    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n",
        "    eval_accuracy.update_state(labels, actual_pred)\n",
        "  batch_data, labels = next(iterator)\n",
        "  strategy.run(replica_fn, args=(batch_data, labels))\n",
        "\n",
        "def eval_dataset_fn():\n",
        "  return tf.data.Dataset.from_tensor_slices(\n",
        "      feature_and_label_gen(num_examples=16)).map(\n",
        "          lambda x: (\n",
        "              {\"features\": feature_preprocess_stage(x[\"features\"])},\n",
        "              label_preprocess_stage(x[\"label\"])\n",
        "          )).shuffle(16).repeat().batch(8)\n",
        "\n",
        "per_worker_eval_dataset = coordinator.create_per_worker_dataset(eval_dataset_fn)\n",
        "per_worker_eval_iterator = iter(per_worker_eval_dataset)\n",
        "\n",
        "eval_steps_per_epoch = 2\n",
        "for _ in range(eval_steps_per_epoch):\n",
        "  coordinator.schedule(eval_step, args=(per_worker_eval_iterator,))\n",
        "coordinator.join()\n",
        "print(\"Evaluation accuracy: %f\" % eval_accuracy.result())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKrQktZX5z7a"
      },
      "source": [
        "#### Permitir la evaluación exactamente una vez\n",
        "\n",
        "<a id=\"exactly_once_evaluation\"></a>\n",
        "\n",
        "Los métodos `schedule` y `join` de `tf.distribute.coordinator.ClusterCoordinator` no admiten garantías de visita ni la semántica exactly-once predeterminada. En otras palabras, en el ejemplo anterior no hay ninguna garantía de que todos los ejemplos de evaluación de un conjunto de datos se evalúen exactamente una vez; es posible que algunos no se visiten y que otros se evalúen varias veces.\n",
        "\n",
        "Se puede preferir la evaluación exactamente una vez para reducir la varianza de la evaluación entre épocas y mejorar la selección de modelos realizada mediante una detención temprana, el ajuste de hiperparámetros u otros métodos. Hay diferentes formas de habilitar la evaluación exactamente una vez:\n",
        "\n",
        "- Con un flujo de trabajo `Model.fit/.evaluate`, puede activarse si se agrega un argumento a `Model.compile`. Consulte la documentación sobre el argumento `pss_evaluation_shards`.\n",
        "- La API de servicio `tf.data` puede utilizarse para proporcionar una visita exactamente única para la evaluación cuando se utiliza `ParameterServerStrategy` (consulte la sección *Dynamic Sharding* en `tf.data.experimental.service` documentación de la API).\n",
        "- [Sidecar evaluation](#sidecar_evaluation) proporciona de forma predeterminada la evaluación exactamente una vez, ya que la evaluación tiene lugar en una única máquina. Sin embargo, esto puede ser mucho más lento que realizar la evaluación distribuida entre muchos workers.\n",
        "\n",
        "La primera opción, utilizando `Model.compile`, es la solución sugerida para la mayoría de los usuarios.\n",
        "\n",
        "La evaluación exacta tiene algunas limitaciones:\n",
        "\n",
        "- No se permite escribir un bucle de evaluación distribuido personalizado con una garantía de visita de sólo una vez. Presente una incidencia en GitHub si necesita soporte para ello.\n",
        "- No puede administrar automáticamente el cálculo de las métricas que utilizan la API `Layer.add_metric`. Estas deben excluirse de la evaluación, o reelaborarse en objetos `Metric`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H40X-9Gs3i7_"
      },
      "source": [
        "### Evaluación del Sidecar\n",
        "\n",
        "<a id=\"sidecar_evaluation\"></a>\n",
        "\n",
        "Otro método para definir y ejecutar un bucle de evaluación en el entrenamiento `tf.distribute.ParameterServerStrategy` se denomina *evaluación del sidecar*, en el que se crea una tarea evaluadora específica que lee repetidamente los puntos de control y ejecuta la evaluación en el último punto de verificación (consulte [esta guía](../../guide/checkpoint.ipynb) para obtener más detalles sobre los puntos de verificación). Las tareas del coordinador y del worker no dedican tiempo a la evaluación, por lo que para un número fijo de iteraciones el tiempo total de entrenamiento debería ser menor que utilizando otros métodos de evaluación. Sin embargo, requiere una tarea evaluadora adicional y puntos de control periódicos para activar la evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HonyjnXK9-ys"
      },
      "source": [
        "Para escribir un bucle de evaluación para realizar una evaluación Sidecar, tiene dos opciones:\n",
        "\n",
        "1. Utilizar la API `tf.keras.utils.SidecarEvaluator`.\n",
        "2. Crear un bucle de evaluación personalizado.\n",
        "\n",
        "Consulte la `tf.keras.utils.SidecarEvaluator` para obtener más detalles sobre la opción 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_c0EiwB88OG"
      },
      "source": [
        "La evaluación del sidecar sólo es compatible con una única tarea. Esto significa que:\n",
        "\n",
        "- Se garantiza que cada ejemplo se evalúa una vez. En caso de que el evaluador se adelante o se reinicie, simplemente reiniciará el bucle de evaluación desde el último punto de verificación, y se descartará el progreso parcial de la evaluación realizado antes del reinicio.\n",
        "\n",
        "- Sin embargo, ejecutar la evaluación en una sola tarea implica que una evaluación completa puede llevar mucho tiempo.\n",
        "\n",
        "- Si el tamaño del modelo es demasiado grande para que quepa en la memoria de un evaluador, la evaluación del sidecar única no se puede aplicar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNJoWVc797B1"
      },
      "source": [
        "Otra advertencia es que la implementación de `tf.keras.utils.SidecarEvaluator`, y el bucle de evaluación personalizado que aparece a continuación, pueden saltarse algunos puntos de verificación porque siempre recoge el último punto de verificación disponible, y durante una época de la evaluación, se pueden producir múltiples puntos de verificación desde el clúster de entrenamiento. Puede escribir un bucle de evaluación personalizado que evalúe cada punto de verificación, pero no se trata en este tutorial. Por otra parte, puede permanecer inactivo si los puntos de verificación se producen con menos frecuencia que el tiempo que se tarda en ejecutar la evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5jopxBd85Ji"
      },
      "source": [
        "Un bucle de evaluación personalizado proporciona más control sobre los detalles, como elegir qué punto de verificación para evaluar, o proporcionar cualquier lógica adicional que se ejecute junto con la evaluación. A continuación se muestra un posible bucle de evaluación del sidecar personalizado:\n",
        "\n",
        "```python\n",
        "checkpoint_dir = ...\n",
        "eval_model = ...\n",
        "eval_data = ...\n",
        "checkpoint = tf.train.Checkpoint(model=eval_model)\n",
        "\n",
        "for latest_checkpoint in tf.train.checkpoints_iterator(\n",
        "    checkpoint_dir):\n",
        "  try:\n",
        "    checkpoint.restore(latest_checkpoint).expect_partial()\n",
        "  except (tf.errors.OpError,) as e:\n",
        "    # checkpoint may be deleted by training when it is about to read it.\n",
        "    continue\n",
        "\n",
        "  # Optionally add callbacks to write summaries.\n",
        "  eval_model.evaluate(eval_data)\n",
        "\n",
        "  # Evaluation finishes when it has evaluated the last epoch.\n",
        "  if latest_checkpoint.endswith('-{}'.format(train_epochs)):\n",
        "    break\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TkNbtpPhFRQ"
      },
      "source": [
        "## Clústers en el mundo real\n",
        "\n",
        "<a id=\"real_clusters\"></a>\n",
        "\n",
        "Nota: esta sección no es necesaria para ejecutar el código del tutorial en esta página.\n",
        "\n",
        "En un entorno de producción real, ejecutará todas las tareas en procesos diferentes en máquinas diferentes. La forma más sencilla de configurar la información del clúster en cada tarea es establecer variables de entorno `\"TF_CONFIG\"` y utilizar un `tf.distribute.cluster_resolver.TFConfigClusterResolver` para analizar `\"TF_CONFIG\"`.\n",
        "\n",
        "Para obtener una descripción general de las variables de entorno `\"TF_CONFIG\"`, consulte \"Configuración de la variable de entorno `TF_CONFIG`\" en la guía [Entrenamiento distribuido](../../guide/distributed_training.ipynb).\n",
        "\n",
        "Si inicia sus tareas de entrenamiento utilizando Kubernetes u otras plantillas de configuración, es probable que estas plantillas ya hayan establecido `\"TF_CONFIG\"` por usted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7AK9SJGt3tQ"
      },
      "source": [
        "### Establezca la variable de entorno `\"TF_CONFIG\"`.\n",
        "\n",
        "Suponga que tiene 3 workers y 2 servidores de parámetros. Entonces el `\"TF_CONFIG\"` del worker 1 puede ser:\n",
        "\n",
        "```python\n",
        "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "    \"cluster\": {\n",
        "        \"worker\": [\"host1:port\", \"host2:port\", \"host3:port\"],\n",
        "        \"ps\": [\"host4:port\", \"host5:port\"],\n",
        "        \"chief\": [\"host6:port\"]\n",
        "    },\n",
        "    \"task\": {\"type\": \"worker\", \"index\": 1}\n",
        "})\n",
        "```\n",
        "\n",
        "El `\"TF_CONFIG\"` del evaluador puede ser:\n",
        "\n",
        "```python\n",
        "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "    \"cluster\": {\n",
        "        \"evaluator\": [\"host7:port\"]\n",
        "    },\n",
        "    \"task\": {\"type\": \"evaluator\", \"index\": 0}\n",
        "})\n",
        "```\n",
        "\n",
        "La parte `\"cluster\"` en la cadena `\"TF_CONFIG\"` anterior para el evaluador es opcional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZRjMS0pt1LM"
      },
      "source": [
        "### Si utiliza el mismo binario para todas las tareas\n",
        "\n",
        "Si prefiere ejecutar todas estas tareas utilizando un único binario, tendrá que dejar que su programa se ramifique en diferentes funciones desde el principio:\n",
        "\n",
        "```python\n",
        "cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
        "if cluster_resolver.task_type in (\"worker\", \"ps\"):\n",
        "  # Start a TensorFlow server and wait.\n",
        "elif cluster_resolver.task_type == \"evaluator\":\n",
        "  # Run sidecar evaluation\n",
        "else:\n",
        "  # Run the coordinator.\n",
        "```\n",
        "\n",
        "El siguiente código inicia un servidor de TensorFlow y espera, útil para los roles `\"worker\"` y `\"ps\"`:\n",
        "\n",
        "```python\n",
        "# Set the environment variable to allow reporting worker and ps failure to the\n",
        "# coordinator. This is a workaround and won't be necessary in the future.\n",
        "os.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\n",
        "\n",
        "server = tf.distribute.Server(\n",
        "    cluster_resolver.cluster_spec(),\n",
        "    job_name=cluster_resolver.task_type,\n",
        "    task_index=cluster_resolver.task_id,\n",
        "    protocol=cluster_resolver.rpc_layer or \"grpc\",\n",
        "    start=True)\n",
        "server.join()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWdYfK593eOL"
      },
      "source": [
        "## Gestionar el error en la tarea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl9eK5r13cOv"
      },
      "source": [
        "### Error del worker\n",
        "\n",
        "Tanto el bucle de entrenamiento personalizado `tf.distribute.coordinator.ClusterCoordinator` como el enfoque `Model.fit` proporcionan tolerancia ante errores incorporada para el fallo de los workers. Tras la recuperación de los workers, el `ClusterCoordinator` invoca la recreación del conjunto de datos en los workers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP0OHZ1-Ne-B"
      },
      "source": [
        "### Error del servidor de parámetros o del coordinador\n",
        "\n",
        "Sin embargo, cuando el coordinador vea un error del servidor de parámetros, lanzará un `UnavailableError` o `AbortedError` inmediatamente. En este caso, puede reiniciar el coordinador. El propio coordinador también puede dejar de estar disponible. Por lo tanto, se recomiendan ciertas herramientas para no perder el progreso del entrenamiento:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7m7Itoz8lsI"
      },
      "source": [
        "- Para `Model.fit`, debe utilizar una retrollamada `BackupAndRestore`, que se encarga de guardar y restaurar el progreso automáticamente. Consulte la sección anterior [Retrollamadas y entrenamiento](#callbacks-and-training) para obtener un ejemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XlLyJp53Z8A"
      },
      "source": [
        "- Para un bucle de entrenamiento personalizado, debe verificar las variables del modelo periódicamente y cargar las variables del modelo desde un punto de verificación, si lo hay, antes de iniciar el entrenamiento. El progreso del entrenamiento puede inferirse aproximadamente a partir de `optimizer.iterations` si un optimizador es sometido a un punto de verificación:\n",
        "\n",
        "```python\n",
        "checkpoint_manager = tf.train.CheckpointManager(\n",
        "    tf.train.Checkpoint(model=model, optimizer=optimizer),\n",
        "    checkpoint_dir,\n",
        "    max_to_keep=3)\n",
        "if checkpoint_manager.latest_checkpoint:\n",
        "  checkpoint = checkpoint_manager.checkpoint\n",
        "  checkpoint.restore(\n",
        "      checkpoint_manager.latest_checkpoint).assert_existing_objects_matched()\n",
        "\n",
        "global_steps = int(optimizer.iterations.numpy())\n",
        "starting_epoch = global_steps // steps_per_epoch\n",
        "\n",
        "for _ in range(starting_epoch, num_epochs):\n",
        "  for _ in range(steps_per_epoch):\n",
        "    coordinator.schedule(step_fn, args=(per_worker_iterator,))\n",
        "  coordinator.join()\n",
        "  checkpoint_manager.save()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlN1P7C53XK9"
      },
      "source": [
        "### Extracción de un `RemoteValue`\n",
        "\n",
        "Se garantiza el éxito de la extracción de un `RemoteValue` si una función se ejecuta correctamente. Esto se debe a que actualmente el valor de retorno se copia inmediatamente en el coordinador después de que se ejecute una función. Si se produce algún error en el worker durante la copia, la función se volverá a intentar en otro worker disponible. Por lo tanto, si desea optimizar el rendimiento, puede programar funciones sin valor de retorno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZcR_xNZ3UdU"
      },
      "source": [
        "## Informe de errores\n",
        "\n",
        "Una vez que el coordinador vea un error como `UnavailableError` de los servidores de parámetros u otros errores de aplicación como un `InvalidArgument` de `tf.debugging.check_numerics`, cancelará todas las funciones pendientes y en espera antes de lanzar el error. La obtención de sus correspondientes `RemoteValue`suscitará un `CancelledError`.\n",
        "\n",
        "Después de que se produzca un error, el coordinador no producirá el mismo error ni ningún error de las funciones anuladas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfhbXH-j3NVw"
      },
      "source": [
        "## Mejora del rendimiento\n",
        "\n",
        "Hay varias razones posibles por las que puede experimentar problemas de rendimiento cuando se entrena con `tf.distribute.ParameterServerStrategy` y `tf.distribute.coordinator.ClusterCoordinator`.\n",
        "\n",
        "Una razón común es que los servidores de parámetros tienen una carga desequilibrada y algunos servidores de parámetros muy cargados han alcanzado su capacidad. También puede haber múltiples causas de origen. Algunos métodos sencillos para mitigar este problema son:\n",
        "\n",
        "1. Fragmente sus variables de modelo de gran tamaño mediante la especificación de un `variable_partitioner` al construir un `ParameterServerStrategy`.\n",
        "2. Evite crear una variable hotspot que sea requerida por todos los servidores de parámetros en un solo paso, por ambos:\n",
        "\n",
        "1. Utilizar una tasa de aprendizaje constante o subclase `tf.keras.optimizers.schedules.LearningRateSchedule` en los optimizadores. Esto se debe a que el comportamiento predeterminado es que la tasa de aprendizaje se convertirá en una variable colocada en un servidor de parámetros concreto, y solicitada por todos los demás servidores de parámetros en cada paso); y\n",
        "\n",
        "2. Utilizando un `tf.keras.optimizers.legacy.Optimizer` (los `tf.keras.optimizers.Optimizer` estándar podrían seguir dando lugar a variables hotspot).\n",
        "\n",
        "1. Mezcle sus grandes vocabularios antes de pasarlos a las capas de preprocesamiento de Keras.\n",
        "\n",
        "Otra posible razón de los problemas de rendimiento es el coordinador. La implementación de `schedule`/`join` está basada en Python y, por tanto, puede tener sobrecarga de hilos. Además, la latencia entre el coordinador y los trabajadores puede ser grande. Si este es el caso:\n",
        "\n",
        "- Para `Model.fit`, puede establecer el argumento `steps_per_execution` proporcionado en `Model.compile` con un valor superior a 1.\n",
        "\n",
        "- Para un bucle de entrenamiento personalizado, puede empaquetar varios pasos en una sola `tf.function`:\n",
        "\n",
        "```python\n",
        "steps_per_invocation = 10\n",
        "\n",
        "@tf.function\n",
        "def step_fn(iterator):\n",
        "  for _ in range(steps_per_invocation):\n",
        "    features, labels = next(iterator)\n",
        "    def replica_fn(features, labels):\n",
        "      ...\n",
        "\n",
        "    strategy.run(replica_fn, args=(features, labels))\n",
        "```\n",
        "\n",
        "Conforme se vaya optimizando la biblioteca, es de esperar que la mayoría de los usuarios no tengan que empaquetar manualmente los pasos en el futuro.\n",
        "\n",
        "Además, un pequeño truco para mejorar el rendimiento es programar funciones sin valor de retorno como se explica en la sección anterior [gestionar el error en la tarea](#handling_task_failure)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chu5F7M_JmVk"
      },
      "source": [
        "## Limitaciones conocidas\n",
        "\n",
        "<a id=\"known_limitations\"> </a>\n",
        "\n",
        "La mayoría de las limitaciones conocidas ya se han tratado en las secciones anteriores. Esta sección ofrece un resumen.\n",
        "\n",
        "### `ParameterServerStrategy` general\n",
        "\n",
        "- `os.environment[\"grpc_fail_fast\"]=\"use_caller\"` es necesario en todas las tareas, incluido el coordinador, para que la tolerancia ante errores funcione correctamente.\n",
        "- No se admite el entrenamiento síncrono del servidor de parámetros.\n",
        "- Suele ser necesario empaquetar varios pasos en una sola función para lograr un rendimiento óptimo.\n",
        "- No es compatible cargar un saved_model mediante `tf.saved_model.load` que contenga variables fragmentadas. Tenga en cuenta que se espera que la carga de un saved_model de este tipo mediante TensorFlow Serving funcione (consulte el tutorial [serving](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple) para obtener más detalles).\n",
        "- No es posible recuperarse de un error del servidor de parámetros sin reiniciar la tarea del coordinador.\n",
        "- La creación de `tf.lookup.StaticHashTable`, comúnmente empleada por algunas capas de preprocesamiento de Keras, como `tf.keras.layers.IntegerLookup`, `tf. keras.layers.StringLookup`, y `tf.keras.layers.TextVectorization`, deben colocarse en `Strategy.scope`. De lo contrario, los recursos se colocarán en el coordinador, y los RPC de búsqueda desde los trabajadores al coordinador tendrán implicaciones en el rendimiento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MKBF0RPSvzB"
      },
      "source": [
        "### `Modelo.fit` específicos\n",
        "\n",
        "- El argumento `steps_per_epoch` es necesario en `Model.fit`. Puede seleccionar un valor que proporcione intervalos adecuados en una época.\n",
        "- `ParameterServerStrategy` no tiene soporte para las retrollamadas personalizadas que tienen llamadas a nivel de lote por razones de rendimiento. Debe convertir esas llamadas en llamadas a nivel de época con `steps_per_epoch` adecuadamente escogidos, de forma que se llamen cada `steps_per_epoch` número de pasos. Las retrollamadas incorporadas no se ven afectadas: sus llamadas a nivel de lote se modificaron para que sean eficaces. Se está planificando el soporte de llamadas a nivel de lote para `ParameterServerStrategy`.\n",
        "- Por la misma razón, a diferencia de otras estrategias, las barras de progreso y las métricas sólo se registran en los límites de las épocas.\n",
        "- `run_eagerly` no es compatible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvY-mg35Sx5L"
      },
      "source": [
        "### Específicos del bucle de entrenamiento personalizado\n",
        "\n",
        "- `ClusterCoordinator.schedule` no permite garantías de visita para un conjunto de datos en general, aunque una garantía de visita para la evaluación es posible mediante `Model.fit/.evaluate`. Consulte [Habilitar la evaluación exactamente una vez](#exactly_once_evaluation).\n",
        "- Cuando `ClusterCoordinator.create_per_worker_dataset` se utiliza con una llamada como entrada, el conjunto de datos completo debe crearse dentro de la función que se le transfiere.\n",
        "- `tf.data.Options` se ignora en un conjunto de datos creado por `ClusterCoordinator.create_per_worker_dataset`."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "parameter_server_training.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
