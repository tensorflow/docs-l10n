{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jQ1tEQCxwRx"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V_sgB_5dx1f1"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62G8M_viUJp"
      },
      "source": [
        "# CartPole con método Actor-Crítico\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mJ2i6jvZ3sK"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     Ver en TensorFlow.org</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/reinforcement_learning/actor_critic.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Ejecutar en Google Colab</a> </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/reinforcement_learning/actor_critic.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a> </td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tutorials/reinforcement_learning/actor_critic.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar notebook</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgN7h_wiUJq"
      },
      "source": [
        "En este tutorial se demuestra cómo implementar un método [Actor-Critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) (actor-crítico) con TensorFlow para entrenar a un agente en el entorno [Open AI Gym](https://www.gymlibrary.dev/) [`CartPole-v0`](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) (carro con mástil). Se supone que el lector está, en cierto modo, familiarizado con los [métodos de gradientes de política](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) del [aprendizaje por refuerzo (profundo)](https://en.wikipedia.org/wiki/Deep_reinforcement_learning).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kA10ZKRR0hi"
      },
      "source": [
        "**Los métodos actor-crítico**\n",
        "\n",
        "Los métodos actor-crítico son [métodos de aprendizaje por diferencia temporal (TD)](https://en.wikipedia.org/wiki/Temporal_difference_learning) que representan la función política independiente de la función valor.\n",
        "\n",
        "Una función de política (o política) devuelve una distribución de probabilidad sobre acciones que el agente puede tomar basándose en un estado dado. La función de valor determina el retorno esperado para un agente que se inicia en un estado dado y actúa según una política particular de allí en adelante.\n",
        "\n",
        "En el método actor-crítico, nos referimos a la política como el *actor* que propone un conjunto de acciones posibles en un estado dado y la función de valor estimado es el *crítico*, que es quien evalúa las acciones realizadas por el *actor* basándose en la política dada.\n",
        "\n",
        "En este tutorial, tanto el *actor* como el *crítico* estarán representados por una red neuronal con dos salidas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBfiafKSRs2k"
      },
      "source": [
        "**`CartPole-v0`**\n",
        "\n",
        "En el entorno [`CartPole-v0` ](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)hay un mástil en posición vertical sobre un carro que se mueve a lo largo de un riel sin fricciones. El mástil empieza derecho y el objetivo del agente es evitar que se caiga aplicando una fuerza de `-1` o `+1` al carro. Por cada vez que el mástil permanece en posición vertical, se otorga una recompensa de `+1`. Un episodio termina cuando: 1) el mástil se inclina más de 15 grados (desde la posición vertical); o 2) el carro se mueve más de 2.4 unidades del centro.\n",
        "\n",
        "<center>\n",
        "  <pre data-md-type=\"custom_pre\">&lt;figure&gt;\n",
        "    &lt;image src=\"https://tensorflow.org/tutorials/reinforcement_learning/images/cartpole-v0.gif\"&gt;\n",
        "    &lt;figcaption&gt;\n",
        "      Trained actor-critic model in Cartpole-v0 environment\n",
        "    &lt;/figcaption&gt;\n",
        "  &lt;/figure&gt;</pre>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSNVK0AeRoJd"
      },
      "source": [
        "El problema se considera \"resuelto\" cuando el promedio de recompensas totales para un episodio llega a 195 sobre 100 pruebas consecutivas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glLwIctHiUJq"
      },
      "source": [
        "## Preparar\n",
        "\n",
        "Importe los paquetes necesarios y configure los ajustes globales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13l6BbxKhCKp"
      },
      "outputs": [],
      "source": [
        "!pip install gym[classic_control]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBeQhPi2S4m5"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y python-opengl > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT4N3qYviUJr"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOUCe2D0iUJu"
      },
      "source": [
        "## El modelo\n",
        "\n",
        "El *actor* y el *crítico* se modelarán usando una red neuronal que genere las probabilidades de acción y el respectivo valor crítico. En este tutorial se usa el modelo de subclases para definir el modelo.\n",
        "\n",
        "Durante el pase hacia adelante, el modelo tomará el estado de la entrada y saldrán las dos probabilidades de acción y el valor crítico $V$, que modela la [función de valor](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions) dependiente del estado. El objetivo es entrenar un modelo que elija acciones basándose en una política $\\pi$ que maximice el [retorno](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return) esperado.\n",
        "\n",
        "Para `CartPole-v0`, hay cuatro valores que representan al estado: la posición del carro, la velocidad del carro, el ángulo del mástil y la velocidad respectiva. El agente puede realizar dos acciones para empujar el carro hacia la izquierda (`0`) y la derecha (`1`), respectivamente.\n",
        "\n",
        "Para más información, consulte la [página de documentación sobre Cart Pole de Gym](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) y los [*Neuronlike adaptive elements that can solve difficult learning control problems*](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) (Elementos adaptativos de tipo neuronal que pueden resolver problemas difíciles de control de aprendizaje) por Barto, Sutton y Anderson (1983).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXKbbMC-kmuv"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions: int, \n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    self.actor = layers.Dense(num_actions)\n",
        "    self.critic = layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.common(inputs)\n",
        "    return self.actor(x), self.critic(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWyxJgjLn68c"
      },
      "outputs": [],
      "source": [
        "num_actions = env.action_space.n  # 2\n",
        "num_hidden_units = 128\n",
        "\n",
        "model = ActorCritic(num_actions, num_hidden_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk92njFziUJw"
      },
      "source": [
        "## Entrenamiento del agente\n",
        "\n",
        "Para entrenar el agente deberá seguir los pasos que se encuentran a continuación:\n",
        "\n",
        "1. Ejecutar el agente en el entorno para recopilar datos de entrenamiento por episodio.\n",
        "2. Calcular el retorno esperado a cada paso de tiempo.\n",
        "3. Calcular la pérdida para el modelo actor-crítico combinado.\n",
        "4. Calcular gradientes y actualizar los parámetros de red.\n",
        "5. Repetir los pasos 1 a 4 hasta alcanzar lo establecido según el criterio de éxito o la cantidad máxima de episodios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2nde2XDs8Gh"
      },
      "source": [
        "### 1. Recopilación de datos de entrenamiento\n",
        "\n",
        "Tal como en el aprendizaje supervisado, para entrenar el modelo actor-crítico, deberá entrenar los datos. Pero, para recopilar tales datos, el modelo debería \"ejecutarse\" en el entorno.\n",
        "\n",
        "Los datos de entrenamiento se recopilan para cada episodio. Después, a cada paso de tiempo se ejecutará el pase hacia adelante del modelo en el estado del entorno a fin de generar probabilidades de acción y el valor crítico, según la política actual, parametrizado por los pesos del modelo.\n",
        "\n",
        "La siguiente acción se muestreará a partir de probabilidades de acciones generadas por el modelo. Después se aplicarían al entorno, causando el siguiente estado y la recompensa.\n",
        "\n",
        "Este proceso se implementa en la función `run_episode` que usa operaciones de TensorFlow para su posterior compilación en un gráfico de TensorFlow, para lograr un entrenamiento más rápido. Tenga en cuenta que los `tf.TensorArray` se usaron para admitir la iteración Tensor en arreglos de longitud variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5URrbGlDSAGx"
      },
      "outputs": [],
      "source": [
        "# Wrap Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, truncated, info = env.step(action)\n",
        "  return (state.astype(np.float32), \n",
        "          np.array(reward, np.int32), \n",
        "          np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action], \n",
        "                           [tf.float32, tf.int32, tf.int32])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4qVRV063Cl9"
      },
      "outputs": [],
      "source": [
        "def run_episode(\n",
        "    initial_state: tf.Tensor,  \n",
        "    model: tf.keras.Model, \n",
        "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  for t in tf.range(max_steps):\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "  \n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "  \n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "    # Store critic values\n",
        "    values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "  \n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "  \n",
        "    # Store reward\n",
        "    rewards = rewards.write(t, reward)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  action_probs = action_probs.stack()\n",
        "  values = values.stack()\n",
        "  rewards = rewards.stack()\n",
        "  \n",
        "  return action_probs, values, rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBnIHdz22dIx"
      },
      "source": [
        "### 2. Cálculo de los retornos esperados\n",
        "\n",
        "La secuencia de recompensas para cada paso de tiempo recopilado $t$, ${r_{t}}^{T}{em0}{t=1}$ durante un episodio se convierte en una secuencia de retornos esperados ${G{/em0}{t}}^{T}_{t=1}$ en la que la suma de recompensas se toma del paso de tiempo actual $t$ a $T$ y cada recompensa se multiplica por un factor $\\gamma$ de descuento que decae exponencialmente:\n",
        "\n",
        "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
        "\n",
        "Dado que $\\gamma\\in(0,1)$, se les asigna menos peso a las recompensas más alejadas del paso de tiempo actual.\n",
        "\n",
        "Intuitivamente, el retorno esperado simplemente implica que las recompensas de ahora son mejores que las de más adelante. En un sentido matemático, se trata de garantizar que la suma de las recompensas converja.\n",
        "\n",
        "Para estabilizar el entrenamiento, la secuencia resultante de retornos también se estandariza (es decir, para tener promedio cero y desvío estándar unitario).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpEwFyl315dl"
      },
      "outputs": [],
      "source": [
        "def get_expected_return(\n",
        "    rewards: tf.Tensor, \n",
        "    gamma: float, \n",
        "    standardize: bool = True) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = reward + gamma * discounted_sum\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  if standardize:\n",
        "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
        "               (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "  return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhr50_Czxazw"
      },
      "source": [
        "### 3. Pérdida en el modelo actor-crítico\n",
        "\n",
        "Dado que está usando un modelo híbrido actor-crítico, la función de pérdida elegida es una combinación de las pérdidas de actor y de crítico para el entrenamiento, tal como se muestra a continuación:\n",
        "\n",
        "$$L = L_{actor} + L_{critic}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOQIJuG1xdTH"
      },
      "source": [
        "#### La pérdida de actor\n",
        "\n",
        "La pérdida de actor se basa en [gradientes de políticas con el crítico como línea de base dependiente del estado](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) y se calcula con estimaciones de una sola muestra (por episodio).\n",
        "\n",
        "$$L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $T$: la cantidad de pasos de tiempo por episodio que puede variar por episodio\n",
        "- $s_{t}$: el estado al paso de tiempo $t$\n",
        "- $a_{t}$: la acción elegida al paso de tiempo a un $t$ estado dado $s$\n",
        "- $\\pi_{\\theta}$: es la política (actor) parametrizada por $\\theta$\n",
        "- $V^{\\pi}_{\\theta}$: es la función de valor (crítico) también parametrizada por $\\theta$\n",
        "- $G = G_{t}$: el retorno esperado para un estado dado, se empareja con la acción en el paso de tiempo $t$\n",
        "\n",
        "Se agrega un término negativo a la suma, ya que la idea es maximizar las probabilidades de acciones que produzcan mayores recompensas al minimizar la pérdida combinada.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y304O4OAxiAv"
      },
      "source": [
        "##### La ventaja\n",
        "\n",
        "El término $G - V$ en nuestra formulación $L_{actor}$ se denomina [ventaja](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), lo que indica en qué medida a una acción se le da mejor un estado dado sobre una acción aleatoria seleccionada según la política $\\pi$ para ese estado.\n",
        "\n",
        "Si bien es posible excluir una línea de base, hacerlo podría causar una variación grande durante el entrenamiento. Y lo bueno de elegir el crítico $V$ como base es que está entrenado para estar lo más cerca posible de $G$, lo que lleva a una variación inferior.\n",
        "\n",
        "Además, sin el crítico, el algoritmo intentaría las probabilidades para las acciones realizadas en un estado en particular basándose en el retorno esperado, que puede no hacer una gran diferencia si las probabilidades relativas entre las acciones siguen siendo las mismas.\n",
        "\n",
        "Por ejemplo, supongamos que dos acciones para un estado dado produjeran el mismo retorno esperado. Sin el crítico, el algoritmo intentaría elevar las probabilidades de esas acciones basándose en el objetivo $J$. Con el crítico, puede resultar que no haya ventaja ($G - V = 0$) y, por lo tanto, no se obtenga ningún beneficio al aumentar las probabilidades de las acciones. Además, el algoritmo establecería los gradientes en cero.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hrPLrgGxlvb"
      },
      "source": [
        "#### La pérdida de crítico\n",
        "\n",
        "El entrenamiento de $V$ para que esté lo más cerca posible de $G$ se puede configurar como un problema de regresión con la siguiente función de pérdida:\n",
        "\n",
        "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
        "\n",
        "donde $L_{\\delta}$ es la [pérdida Huber](https://en.wikipedia.org/wiki/Huber_loss), que es menos sensible a los valores atípicos (outliers) en los datos que la pérdida del error al cuadrado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EXwbEez6n9m"
      },
      "outputs": [],
      "source": [
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,  \n",
        "    values: tf.Tensor,  \n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSYkQOmRfV75"
      },
      "source": [
        "### 4. Definición del paso de entrenamiento para actualizar los parámetros\n",
        "\n",
        "Todos los pasos anteriores se combinan para formar un paso de entrenamiento que se ejecuta en cada episodio. Todos los pasos que llevan a la función de pérdida se ejecutan con el contexto `tf.GradientTape` para permitir la diferenciación automática.\n",
        "\n",
        "En este tutorial se usa el optimizador Adam para aplicar los gradientes a los parámetros del modelo.\n",
        "\n",
        "En este paso también se calcula la suma de recompensas no descontadas, `episode_reward`. Este valor se usará más adelante para evaluar si se cumple con el criterio de éxito.\n",
        "\n",
        "El contexto `tf.function` se aplica a la función `train_step` para que se pueda compilar en un gráfico TensorFlow invocable, que puede aumentar 10 veces la velocidad en el entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoccrkF3IFCg"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor, \n",
        "    model: tf.keras.Model, \n",
        "    optimizer: tf.keras.optimizers.Optimizer, \n",
        "    gamma: float, \n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode) \n",
        "\n",
        "    # Calculate the expected returns\n",
        "    returns = get_expected_return(rewards, gamma)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
        "\n",
        "    # Calculate the loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFvZiDoAflGK"
      },
      "source": [
        "### 5. Ejecución del bucle de entrenamiento\n",
        "\n",
        "El entrenamiento se lleva a cabo ejecutando el paso de entrenamiento hasta alcanzar lo establecido según el criterio de éxito o hasta alcanzar la cantidad máxima de episodios.\n",
        "\n",
        "En una cola se guarda un registro en ejecución de recompensas del episodio. Una vez que se llega a las 100 pruebas, la recompensa más antigua se elimina del extremo de la izquierda (extremo de la hilera) de la cola y la nueva se agrega a la cabeza (derecha). También se mantiene una suma de las recompensas en ejecución, para eficiencia en los cálculos.\n",
        "\n",
        "Dependiendo del tiempo de ejecución, el entrenamiento puede terminar en menos de un minuto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbmBxnzLiUJx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "min_episodes_criterion = 100\n",
        "max_episodes = 10000\n",
        "max_steps_per_episode = 500\n",
        "\n",
        "# `CartPole-v1` is considered solved if average reward is >= 475 over 500 \n",
        "# consecutive trials\n",
        "reward_threshold = 475\n",
        "running_reward = 0\n",
        "\n",
        "# The discount factor for future rewards\n",
        "gamma = 0.99\n",
        "\n",
        "# Keep the last episodes reward\n",
        "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "t = tqdm.trange(max_episodes)\n",
        "for i in t:\n",
        "    initial_state, info = env.reset()\n",
        "    initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "    \n",
        "    episodes_reward.append(episode_reward)\n",
        "    running_reward = statistics.mean(episodes_reward)\n",
        "  \n",
        "\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "  \n",
        "    # Show the average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "  \n",
        "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru8BEwS1EmAv"
      },
      "source": [
        "## Visualización\n",
        "\n",
        "Después del entrenamiento, será bueno visualizar cómo se comporta el modelo en el entorno. Lo que puede hacer es ejecutar las celdas que se encuentran a continuación para generar una animación GIF de la ejecución de un episodio del modelo. Tenga en cuenta que debe instalar paquetes adicionales para que Gym renderice las imágenes del entorno correctamente en Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbIMMkfmRHyC"
      },
      "outputs": [],
      "source": [
        "# Render an episode and save as a GIF file\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "\n",
        "render_env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "\n",
        "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
        "  state, info = env.reset()\n",
        "  state = tf.constant(state, dtype=tf.float32)\n",
        "  screen = env.render()\n",
        "  images = [Image.fromarray(screen)]\n",
        " \n",
        "  for i in range(1, max_steps + 1):\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    action_probs, _ = model(state)\n",
        "    action = np.argmax(np.squeeze(action_probs))\n",
        "\n",
        "    state, reward, done, truncated, info = env.step(action)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render()\n",
        "      images.append(Image.fromarray(screen))\n",
        "  \n",
        "    if done:\n",
        "      break\n",
        "  \n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(render_env, model, max_steps_per_episode)\n",
        "image_file = 'cartpole-v1.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLd720SejKmf"
      },
      "outputs": [],
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnq9Hzo1Po6X"
      },
      "source": [
        "## Próximos pasos\n",
        "\n",
        "En este tutorial demostramos cómo implementar el método actor-crítico en TensorFlow.\n",
        "\n",
        "Un paso siguiente podría ser el de entrenar un modelo en un entorno diferente en Gym.\n",
        "\n",
        "Para más información sobre el método actor-crítico y el problema Cartpole-v0, puede consultar los siguientes recursos:\n",
        "\n",
        "- [El método actor-crítico](https://hal.inria.fr/hal-00840470/document)\n",
        "- [Exposición sobre actor-crítico (CAL)](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=7&t=0s)\n",
        "- [Problema del control de aprendizaje de Cart Pole [Barto, et al. 1983]](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n",
        "\n",
        "Para más ejemplos de aprendizaje por refuerzo en TensorFlow, puede consultar los siguientes recursos:\n",
        "\n",
        "- [Ejemplos (keras.io) de código de aprendizaje por refuerzo](https://keras.io/examples/rl/)\n",
        "- [Biblioteca de aprendizaje por refuerzo Agents en TF](https://www.tensorflow.org/agents)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_jQ1tEQCxwRx"
      ],
      "name": "actor_critic.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
