{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DweYe9FcbMK_"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AVV2e0XKbJeX"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZfSvVcDo6GQ"
      },
      "source": [
        "# Carga de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giK0nMbZFnoR"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwlfPb11GH8J"
      },
      "source": [
        "En este tutorial se demuestran dos formas de cargar y preprocesar texto.\n",
        "\n",
        "- Primero, deberá usar las utilidades y las capas de procesamiento de Keras. Esto incluye `tf.keras.utils.text_dataset_from_directory` para convertir los datos en un `tf.data.Dataset` y `tf.keras.layers.TextVectorization` para la estandarización, la \"tokenización\" y la vectorización de datos. Si no ha trabajado antes con TensorFlow, debería empezar con esto.\n",
        "- Después, usará utilidades de bajo nivel como `tf.data.TextLineDataset` para cargar los archivos de texto y las API [TensorFlow Text](https://www.tensorflow.org/text), como `text.UnicodeScriptTokenizer` y `text.case_fold_utf8`, para procesar previamente los datos y lograr un control de granulado fino."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa6IKWvADqH7"
      },
      "outputs": [],
      "source": [
        "!pip install \"tensorflow-text==2.11.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import pathlib\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az-d_K5_HQ5k"
      },
      "source": [
        "## Ejemplo 1: predicción de la etiqueta para una pregunta de Stack Overflow\n",
        "\n",
        "Como primer ejemplo, deberá descargar un conjunto de datos de preguntas sobre programación de Stack Overflow. Cada pregunta (*\"How do I sort a dictionary by value?\"*) (¿Cómo ordeno un diccionario por valores?) está marcada exactamente con una etiqueta (`Python`, `CSharp`, `JavaScript` o `Java`). La tarea consiste en desarrollar un modelo que prediga la etiqueta para una pregunta. Este es un ejemplo de clasificación en clases múltiples, un tipo importante y ampliamente aplicable de problemas relacionados con el aprendizaje automático."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjC3yLa5IjP7"
      },
      "source": [
        "### Descarga y exploración de conjuntos de datos\n",
        "\n",
        "Comience por descargar el conjunto de datos de Stack Overflow con `tf.keras.utils.get_file` y explore la estructura del directorio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ELgzA6SHTuV"
      },
      "outputs": [],
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "\n",
        "dataset_dir = utils.get_file(\n",
        "    origin=data_url,\n",
        "    untar=True,\n",
        "    cache_dir='stack_overflow',\n",
        "    cache_subdir='')\n",
        "\n",
        "dataset_dir = pathlib.Path(dataset_dir).parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIrPl5fUH2gb"
      },
      "outputs": [],
      "source": [
        "list(dataset_dir.iterdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEoV7YByJoWQ"
      },
      "outputs": [],
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "list(train_dir.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mxAN17MhEh0"
      },
      "source": [
        "Los directorios `train/csharp`, `train/java`, `train/python` y `train/javascript` contienen muchos archivos de texto, cada uno es una pregunta de Stack Overflow.\n",
        "\n",
        "Imprima un archivo de ejemplo e inspeccione los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go1vTSGdJu08"
      },
      "outputs": [],
      "source": [
        "sample_file = train_dir/'python/1755.txt'\n",
        "\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deWBTkpJiO7D"
      },
      "source": [
        "### Carga del conjunto de datos\n",
        "\n",
        "A continuación, deberá cargar los datos en memoria y prepararlos en un formato adecuado para el entrenamiento. Para hacerlo, deberá usar la utilidad `tf.keras.utils.text_dataset_from_directory`, a fin de crear un `tf.data.Dataset` marcado. Si no ha usado antes `tf.data`, deberá saber que es una recopilación potente de herramientas para construir canalizaciones de entrada. (Para más información, consulte la guía sobre [tf.data: Construir canalizaciones de entrada de TensorFlow](../../guide/data.ipynb)).\n",
        "\n",
        "La API `tf.keras.utils.text_dataset_from_directory` espera una estructura de directorio como la siguiente:\n",
        "\n",
        "```\n",
        "train/\n",
        "...csharp/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...java/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...javascript/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...python/\n",
        "......1.txt\n",
        "......2.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyl6JTAjlbQV"
      },
      "source": [
        "A la hora de hacer un experimento de aprendizaje automático, lo mejor es dividir el conjunto de datos en tres partes: [entrenamiento](https://developers.google.com/machine-learning/glossary#training_set), [validación](https://developers.google.com/machine-learning/glossary#validation_set) y [prueba](https://developers.google.com/machine-learning/glossary#test-set).\n",
        "\n",
        "El conjunto de datos Stack Overflow ya se ha dividido en conjuntos de prueba y entrenamiento, pero aún falta un conjunto de validación.\n",
        "\n",
        "Cree un conjunto de validación con una proporción de 80:20 de los datos de entrenamiento `tf.keras.utils.text_dataset_from_directory` con `validation_split` configurado como `0.2` (es decir, 20 %):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqyliMw8N-az"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMI_gPLfloD7"
      },
      "source": [
        "Tal como lo sugieren los resultados de celdas anteriores, hay 8000 ejemplos en la carpeta de entrenamiento, de los cuales se usará el 80% (o 6400) para entrenamiento. En un momento aprenderá que puede entrenar un modelo pasando un `tf.data.Dataset` directamente a `Model.fit`.\n",
        "\n",
        "Primero, itere sobre el conjunto de datos e imprima algunos ejemplos para familiarizarse con los datos.\n",
        "\n",
        "Nota: Para aumentar la dificultad del problema de clasificación, el autor del conjunto de datos reemplazó las apariciones de las palabras *Python*, *CSharp*, *JavaScript* o *Java* en la pregunta de programación con la palabra *blank*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JMTyZ6Glt_C"
      },
      "outputs": [],
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(10):\n",
        "    print(\"Question: \", text_batch.numpy()[i])\n",
        "    print(\"Label:\", label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCZGl4Q5l2sS"
      },
      "source": [
        "Las etiquetas son `0`, `1`, `2` o `3`. Para comprobar cuáles corresponden a qué etiqueta de cadena, se puede inspeccionar la propiedad `class_names` en el conjunto de datos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIpCS7YjmGkj"
      },
      "outputs": [],
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "  print(\"Label\", i, \"corresponds to\", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUsdn-37qol9"
      },
      "source": [
        "A continuación, creará un conjunto de datos de validación y prueba con `tf.keras.utils.text_dataset_from_directory`. Usará las 1600 reseñas restantes del conjunto de entrenamiento para ejecutar la validación.\n",
        "\n",
        "Nota: Cuando use los argumentos `validation_split` y `subset` de `tf.keras.utils.text_dataset_from_directory`, asegúrese de especificar una semilla aleatoria o de pasar `shuffle=False`, para que las fracciones de validación y entrenamiento no se superpongan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7m6sCWJQuYt"
      },
      "outputs": [],
      "source": [
        "# Create a validation set.\n",
        "raw_val_ds = utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXMZc7fMQwKE"
      },
      "outputs": [],
      "source": [
        "test_dir = dataset_dir/'test'\n",
        "\n",
        "# Create a test set.\n",
        "raw_test_ds = utils.text_dataset_from_directory(\n",
        "    test_dir,\n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdt-ATrGRGDL"
      },
      "source": [
        "### Preparación del conjunto de datos para entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6fRti45Rlj8"
      },
      "source": [
        "A continuación, usará la capa `tf.keras.layers.TextVectorization` para estandarizar, tokenizar y vectorizar los datos.\n",
        "\n",
        "- La *estandarización* se refiere al procesamiento previo del texto. Normalmente, quitar la puntuación o los elementos HTML para simplificar el conjunto de datos.\n",
        "- La <em>tokenización</em> se refiere a la separación de cadenas en tokens (por ejemplo, al separar una oración en palabras individuales haciendo la división en los espacios en blanco).\n",
        "- La *vectorización* se refiere a convertir tokens en números para que se puedan usar para alimentar una red neuronal.\n",
        "\n",
        "Todas estas tareas se pueden cumplir con esta capa. (Para más información sobre cada una de estas opciones, consulte los documentos sobre la API `tf.keras.layers.TextVectorization`).\n",
        "\n",
        "Tenga en cuenta que:\n",
        "\n",
        "- La estandarización predeterminada transforma el texto en minúscula y quita la puntuación (`standardize='lower_and_strip_punctuation'`).\n",
        "- El <em>tokenizador</em> predeterminado hace las divisiones en los espacios en blanco (`split='whitespace'`).\n",
        "- El modo de vectorización predeterminado es `'int'` (`output_mode='int'`). Devuelve índices enteros (uno por token). Este modo se puede usar para crear modelos que tengan en cuenta el orden de las palabras. También puede usar otros modelos, como `'binary'` para crear modelos [bolsa de palabras](https://developers.google.com/machine-learning/glossary#bag-of-words).\n",
        "\n",
        "Cree dos modelos para aprender más sobre la estandarización, la tokenización y la vectorización con `TextVectorization`:\n",
        "\n",
        "- Primero, use el modo de vectorización `'binary'` para crear un modelo de bolsa de palabras.\n",
        "- Después, use el modo `'int'` con una ConvNet de 1 D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voaC43rZR0jc"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "binary_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDPFxuf2Hfz"
      },
      "source": [
        "Para el modo `'int'`, además del tamaño de vocabulario máximo, necesitará establecer una longitud de secuencia máxima (`MAX_SEQUENCE_LENGTH`) explícita, que hará que la capa rellene o trunque las secuencias según los valores `output_sequence_length` exactos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWsY01Zl2aRe"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "int_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts6h9b5atD-Y"
      },
      "source": [
        "A continuación, llamará `TextVectorization.adapt` para que ajuste el estado de la capa de preprocesamiento al conjunto de datos. Esto hará que el modelo convierta un índice de cadenas a enteros.\n",
        "\n",
        "Nota: Es importante que solamente use los datos de entrenamiento para llamar a `TextVectorization.adapt`, ya que si usa el conjunto de prueba, se podría filtrar información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTXsdDEqSf9e"
      },
      "outputs": [],
      "source": [
        "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
        "train_text = raw_train_ds.map(lambda text, labels: text)\n",
        "binary_vectorize_layer.adapt(train_text)\n",
        "int_vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKVO6Jg7Sls0"
      },
      "source": [
        "Imprima el resultado de usar estas capas para el procesamiento previo de los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RngfPyArSsvM"
      },
      "outputs": [],
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return binary_vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1W54wf0LhQ0"
      },
      "outputs": [],
      "source": [
        "def int_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return int_vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi_sElMiSmXe"
      },
      "outputs": [],
      "source": [
        "# Retrieve a batch (of 32 reviews and labels) from the dataset.\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_question, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Question\", first_question)\n",
        "print(\"Label\", first_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGukZoYv2v3v"
      },
      "outputs": [],
      "source": [
        "print(\"'binary' vectorized question:\",\n",
        "      binary_vectorize_text(first_question, first_label)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu07FsIw2yH5"
      },
      "outputs": [],
      "source": [
        "print(\"'int' vectorized question:\",\n",
        "      int_vectorize_text(first_question, first_label)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgjeF9PdS7tN"
      },
      "source": [
        "Tal como se muestra arriba, el modo `'binary'` de `TextVectorization` devuelve un arreglo que denota qué tokens existen al menos una vez en la entrada; mientras que el modo `'int'` reemplaza a cada token por un entero y así conserva su orden.\n",
        "\n",
        "Se puede llamar a `TextVectorization.get_vocabulary` en la capa para buscar el token (<em>cadena</em>) al que corresponde cada entero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpBnTZilS8wt"
      },
      "outputs": [],
      "source": [
        "print(\"1289 ---> \", int_vectorize_layer.get_vocabulary()[1289])\n",
        "print(\"313 ---> \", int_vectorize_layer.get_vocabulary()[313])\n",
        "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kHgPE_YwHvp"
      },
      "source": [
        "Ya esta casi todo listo para entrenar el modelo.\n",
        "\n",
        "Como último paso, deberá aplicara las capas de `TextVectorization` que creó antes para los conjuntos de entrenamiento, validación y prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46LeHmnD55wJ"
      },
      "outputs": [],
      "source": [
        "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
        "\n",
        "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
        "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
        "int_test_ds = raw_test_ds.map(int_vectorize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHuAF8hYfP5Z"
      },
      "source": [
        "### Configuración del conjunto de datos para rendimiento\n",
        "\n",
        "Hay dos métodos importantes que debería usar cuando cargue los datos para garantizar que la entrada o la salida no se bloqueen.\n",
        "\n",
        "- `.cache()` conserva los datos en la memoria después de que descarga del disco. Esto evitará que el conjunto de datos se transforme en un cuello de botella mientras entrena su modelo. Si su conjunto de datos es demasiado grande para caber en la memoria, también puede usar este método para crear un potente caché en disco, que se lea de forma más eficiente que muchos archivos pequeños.\n",
        "- `Dataset.prefetch` se superpone con el procesamiento de los datos y la ejecución del modelo durante el entrenamiento.\n",
        "\n",
        "Para más información sobre ambos métodos y sobre cómo almacenar los datos en la memoria caché del disco, consulte la sección *Preextracción* de la guía [Mejor rendimiento con la API tf.data API](../../guide/data_performance.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PabA9DFIfSz7"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def configure_dataset(dataset):\n",
        "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8GcJLvb3JH0"
      },
      "outputs": [],
      "source": [
        "binary_train_ds = configure_dataset(binary_train_ds)\n",
        "binary_val_ds = configure_dataset(binary_val_ds)\n",
        "binary_test_ds = configure_dataset(binary_test_ds)\n",
        "\n",
        "int_train_ds = configure_dataset(int_train_ds)\n",
        "int_val_ds = configure_dataset(int_val_ds)\n",
        "int_test_ds = configure_dataset(int_test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYGb7z_bfpGm"
      },
      "source": [
        "### Entrenamiento del modelo\n",
        "\n",
        "Llegó la hora de crear su red neuronal.\n",
        "\n",
        "Para los datos `'binary'` vectorizados, defina un modelo lineal de bolsa de palabras simple. Luego, prepárelo y entrénelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q8iAU-VMzaN"
      },
      "outputs": [],
      "source": [
        "binary_model = tf.keras.Sequential([layers.Dense(4)])\n",
        "\n",
        "binary_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = binary_model.fit(\n",
        "    binary_train_ds, validation_data=binary_val_ds, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwidD-SwNIkz"
      },
      "source": [
        "Luego, use la capa vectorizada `'int'` para crear una ConvNet de 1 D:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ztw2XH_LbVz"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, num_labels):\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
        "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
        "      layers.GlobalMaxPooling1D(),\n",
        "      layers.Dense(num_labels)\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9rG1cFRL31Z"
      },
      "outputs": [],
      "source": [
        "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n",
        "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n",
        "int_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3J9Eeuv97zE"
      },
      "source": [
        "Compare los dos modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8ViDXw99v_u"
      },
      "outputs": [],
      "source": [
        "print(\"Linear model on binary vectorized data:\")\n",
        "print(binary_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9BOeoCwborD"
      },
      "outputs": [],
      "source": [
        "print(\"ConvNet model on int vectorized data:\")\n",
        "print(int_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYYW9tUdCtTy"
      },
      "source": [
        "Evalúe ambos modelos con los datos de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dTc4nZqf7fK"
      },
      "outputs": [],
      "source": [
        "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
        "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
        "\n",
        "print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))\n",
        "print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9dhj8Hey9DS"
      },
      "source": [
        "Nota: En este ejemplo el conjunto de datos representa un problema de clasificación bastante simple. Los conjuntos de datos y problemas más complejos presentan diferencias sutiles pero importantes en las estrategias de procesamiento de datos y las arquitecturas del modelo. Pruebe diferentes hiperparámetros y épocas para comparar los distintos métodos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GaXTsIgP-3"
      },
      "source": [
        "### Exportación del modelo\n",
        "\n",
        "En el código que vimos arriba, se aplicó `tf.keras.layers.TextVectorization` al conjunto de datos antes de alimentar el modelo con el texto. Si desea que su modelo sea capaz de procesar cadenas sin procesar (por ejemplo, para simplificar la implementación), puede incluir la capa `TextVectorization` dentro del modelo.\n",
        "\n",
        "Para hacerlo, puede crear un modelo nuevo con los pesos que ha entrenado recién:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bRe3KX8gRCX"
      },
      "outputs": [],
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [binary_vectorize_layer, binary_model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2eqTVBP4DUN"
      },
      "source": [
        "Ahora, su modelo puede tomar las cadenas sin procesar como entradas y predecir un puntaje para cada etiqueta con `Model.predict`. Defina una función para buscar la etiqueta con el puntaje máximo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU53uRXz45iO"
      },
      "outputs": [],
      "source": [
        "def get_string_labels(predicted_scores_batch):\n",
        "  predicted_int_labels = tf.math.argmax(predicted_scores_batch, axis=1)\n",
        "  predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n",
        "  return predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqnWc7Nn5eou"
      },
      "source": [
        "### Ejecución de la inferencia en datos nuevos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOR2MupW1_zS"
      },
      "outputs": [],
      "source": [
        "inputs = [\n",
        "    \"how do I extract keys from a dict into a list?\",  # 'python'\n",
        "    \"debug public static void main(string[] args) {...}\",  # 'java'\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QDVfii_4slI"
      },
      "source": [
        "Incluir la lógica de preprocesamiento de textos en su modelo le permitirá exportar un modelo para producción que simplifique la implementación y reduzca la probabilidad de que se produzca un [sesgo entre entrenamiento y prueba](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew).\n",
        "\n",
        "Hay una diferencia de rendimiento que debemos tener en cuenta a la hora de elegir dónde aplicar la capa `tf.keras.layers.TextVectorization`. Si la usa fuera del modelo puede hacer un procesamiento asincrónico en CPU y almacenar en búfer los datos cuando se entrena en GPU. Por lo tanto, si entrena su modelo en GPU, probablemente debería elegir esta opción para obtener el mejor rendimiento mientras desarrolla su modelo y luego, cambiar para incluir la capa `TextVectorization` dentro de su modelo, cuando esté listo para prepararse para la implementación.\n",
        "\n",
        "Para más información acerca de cómo guardar modelos, consulte el tutorial sobre cómo [guardar y cargar modelos](../keras/save_and_load.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4cvuFzavTRy"
      },
      "source": [
        "## Ejemplo 2: predicción de los autores de las traducciones de La Ilíada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOlJ22508RIe"
      },
      "source": [
        "A continuación, se brinda un ejemplo de cómo se usa `tf.data.TextLineDataset` para cargar ejemplos a partir de archivos de texto y [TensorFlow Text](https://www.tensorflow.org/text) para procesar los datos. Usará tres traducciones diferentes del mismo trabajo, la Ilíada de Homero, al idioma inglés y entrenará un modelo para identificar al autor con una sola línea de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pCgKbOSk7kU"
      },
      "source": [
        "### Descarga y exploración de conjuntos de datos\n",
        "\n",
        "Los autores de los textos de las tres traducciones son:\n",
        "\n",
        "- [William Cowper](https://en.wikipedia.org/wiki/William_Cowper): [texto](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n",
        "- [Edward, Earl of Derby](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby): [texto](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n",
        "- [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29): [texto](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n",
        "\n",
        "Los archivos de texto usados en este tutorial ya sufrieron algunas modificaciones realizadas con algunas tareas típicas de procesamiento como las de quitar el encabezado y el pie de página, los números de línea y los títulos de los capítulos.\n",
        "\n",
        "Descargue estos archivos ligeramente modificados en un dispositivo local:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YlKQthEYlFw"
      },
      "outputs": [],
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "  text_dir = utils.get_file(name, origin=DIRECTORY_URL + name)\n",
        "\n",
        "parent_dir = pathlib.Path(text_dir).parent\n",
        "list(parent_dir.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8PHK5J_cXE5"
      },
      "source": [
        "### Carga del conjunto de datos\n",
        "\n",
        "Anteriormente, con `tf.keras.utils.text_dataset_from_directory` todo el contenido de un archivo se trataba como un único ejemplo. Ahora, usará `tf.data.TextLineDataset`, que está diseñado para crear un `tf.data.Dataset` a partir de un archivo de texto en el que cada ejemplo es una línea de texto del archivo original. `TextLineDataset` es útil para datos de texto que se basa principalmente en líneas (por ejemplo, registros de poesía o errores).\n",
        "\n",
        "Itere en estos archivos, cargando cada uno en su propio conjunto de datos. Los ejemplos se deben etiquetar cada uno por separado. Entonces, use `Dataset.map` para aplicar una función para etiquetar a cada uno. Esto hará que se itere sobre cada ejemplo en el conjunto de datos, devolviendo pares (`example, label`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIIWIdPXgk7I"
      },
      "outputs": [],
      "source": [
        "def labeler(example, index):\n",
        "  return example, tf.cast(index, tf.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ajx7AmZnEg3"
      },
      "outputs": [],
      "source": [
        "labeled_data_sets = []\n",
        "\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  lines_dataset = tf.data.TextLineDataset(str(parent_dir/file_name))\n",
        "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
        "  labeled_data_sets.append(labeled_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPOsVK1e9NGM"
      },
      "source": [
        "A continuación, combinará estos conjuntos de datos etiquetados en un único conjunto de datos con `Dataset.concatenate` y lo aleatorizará con `Dataset.shuffle`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jAeYkTIi9-2"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SIZE = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd544E-Sh63L"
      },
      "outputs": [],
      "source": [
        "all_labeled_data = labeled_data_sets[0]\n",
        "for labeled_dataset in labeled_data_sets[1:]:\n",
        "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
        "\n",
        "all_labeled_data = all_labeled_data.shuffle(\n",
        "    BUFFER_SIZE, reshuffle_each_iteration=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4JEHrJXeG5k"
      },
      "source": [
        "Imprima algunos ejemplos como antes. El conjunto de datos todavía no se ha agrupado, por lo tanto cada entrada de `all_labeled_data` corresponde a un dato puntual:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gywKlN0xh6u5"
      },
      "outputs": [],
      "source": [
        "for text, label in all_labeled_data.take(10):\n",
        "  print(\"Sentence: \", text.numpy())\n",
        "  print(\"Label:\", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rrpU2_sfDh0"
      },
      "source": [
        "### Preparación del conjunto de datos para entrenamiento\n",
        "\n",
        "En vez de usar `tf.keras.layers.TextVectorization` para el proceso previo del conjunto de datos de texto; en este caso, usará las API TensorFlow Text para estandarizar y <em>tokenizar</em> los datos. Además, elaborará un vocabulario y usará `tf.lookup.StaticVocabularyTable` para mapear los <em>tokens</em> con los enteros a fin de incorporarlos al modelo. (Acceda a más información sobre [TensorFlow Text](https://www.tensorflow.org/text)).\n",
        "\n",
        "Defina una función para convertir el texto, pasarlo a minúsculas y <em>tokenizarlo</em>:\n",
        "\n",
        "- TensorFlow Text brinda varios <em>tokenizadores</em>. En este ejemplo, se usará el `text.UnicodeScriptTokenizer` para <em>tokenizar</em> el conjunto de datos.\n",
        "- Para aplicar la <em>tokenización</em> al conjunto de datos, deberá usar `Dataset.map`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4DpQW-Y12rm"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz8xEj0ugu51"
      },
      "outputs": [],
      "source": [
        "def tokenize(text, unused_label):\n",
        "  lower_case = tf_text.case_fold_utf8(text)\n",
        "  return tokenizer.tokenize(lower_case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzUrAzOq31QL"
      },
      "outputs": [],
      "source": [
        "tokenized_ds = all_labeled_data.map(tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx4Q2i8XLV7o"
      },
      "source": [
        "Se puede iterar sobre el conjunto de datos e imprimir algunos ejemplos tokenizados:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2mkWri7LiGq"
      },
      "outputs": [],
      "source": [
        "for text_batch in tokenized_ds.take(5):\n",
        "  print(\"Tokens: \", text_batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPd4PsskJ_Xt"
      },
      "source": [
        "A continuación, creará un vocabulario. Para ello, deberá ordenar los tokens por frecuencia y conservará los principales tokens `VOCAB_SIZE`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkHtbGnDh6mg"
      },
      "outputs": [],
      "source": [
        "tokenized_ds = configure_dataset(tokenized_ds)\n",
        "\n",
        "vocab_dict = collections.defaultdict(lambda: 0)\n",
        "for toks in tokenized_ds.as_numpy_iterator():\n",
        "  for tok in toks:\n",
        "    vocab_dict[tok] += 1\n",
        "\n",
        "vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "vocab = [token for token, count in vocab]\n",
        "vocab = vocab[:VOCAB_SIZE]\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size: \", vocab_size)\n",
        "print(\"First five vocab entries:\", vocab[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyKSsaNAKi17"
      },
      "source": [
        "Para convertir los tokens en enteros, use el conjunto `vocab` y cree con él una `tf.lookup.StaticVocabularyTable`. Lo que hará será vincular (<em>mapear</em>) los tokens con los enteros en el rango [`2`, `vocab_size + 2`]. Del mismo modo, con la capa `TextVectorization`, se reserva `0` para denotar el amortiguado y se reserva `1` para denotar un token fuera del vocabulario (OOV)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCBo2yFHD7y6"
      },
      "outputs": [],
      "source": [
        "keys = vocab\n",
        "values = range(2, len(vocab) + 2)  # Reserve `0` for padding, `1` for OOV tokens.\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(\n",
        "    keys, values, key_dtype=tf.string, value_dtype=tf.int64)\n",
        "\n",
        "num_oov_buckets = 1\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5F-EiBpOADE"
      },
      "source": [
        "Finalmente, defina una función para estandarizar, tokenizar y vectorizar el conjunto de datos con el tokenizador y la tabla de búsqueda:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcIQ7LOTh6eT"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, label):\n",
        "  standardized = tf_text.case_fold_utf8(text)\n",
        "  tokenized = tokenizer.tokenize(standardized)\n",
        "  vectorized = vocab_table.lookup(tokenized)\n",
        "  return vectorized, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6S5Qyabi-vo"
      },
      "source": [
        "Puede intentarlo con un ejemplo solo para imprimir el resultado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgxPZaxUuTbk"
      },
      "outputs": [],
      "source": [
        "example_text, example_label = next(iter(all_labeled_data))\n",
        "print(\"Sentence: \", example_text.numpy())\n",
        "vectorized_text, example_label = preprocess_text(example_text, example_label)\n",
        "print(\"Vectorized sentence: \", vectorized_text.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9qHM0v8k_Mg"
      },
      "source": [
        "Ahora, ejecute la función de preproceso del conjunto de datos con `Dataset.map`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmQVsAgJ-RM0"
      },
      "outputs": [],
      "source": [
        "all_encoded_data = all_labeled_data.map(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZToSXSm0qr"
      },
      "source": [
        "### División de los conjuntos de datos en conjuntos de entrenamiento y prueba\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itxIJwkrUXgv"
      },
      "source": [
        "La capa `TextVectorization` de Keras también se agrupa en lotes y amortigua los datos vectorizados. El amortiguado (<em>padding</em>) es necesario porque los ejemplos que se encuentran dentro de cada lote deben tener el mismo tamaño y la misma forma, pero los ejemplos de estos conjuntos de datos no son todos del mismo tamaño, cada línea de texto tiene una cantidad diferente de palabras.\n",
        "\n",
        "`tf.data.Dataset` es compatible con la división y el agrupamiento amortiguado de los conjuntos de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-rmbijQh6bf"
      },
      "outputs": [],
      "source": [
        "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
        "validation_data = all_encoded_data.take(VALIDATION_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTP0IwHBCn0Q"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "validation_data = validation_data.padded_batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-wmFq8uW1zS"
      },
      "source": [
        "Ahora, `validation_data` y `train_data` no son recopilaciones de pares (`example, label`), sino de lotes. Cada lote es un par (*muchos ejemplos*, *muchas etiquetas*) representado como arreglo.\n",
        "\n",
        "A modo ilustrativo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMslWfuwoqpB"
      },
      "outputs": [],
      "source": [
        "sample_text, sample_labels = next(iter(validation_data))\n",
        "print(\"Text batch shape: \", sample_text.shape)\n",
        "print(\"Label batch shape: \", sample_labels.shape)\n",
        "print(\"First text example: \", sample_text[0])\n",
        "print(\"First label example: \", sample_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI4I6_Sa0vWu"
      },
      "source": [
        "Como usa `0` para el amortiguado y `1` para los tokens fuera de vocabulario (OOV), el tamaño del vocabulario ha aumentado a 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u21LlkO8QGRX"
      },
      "outputs": [],
      "source": [
        "vocab_size += 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h44Ox11OYLP-"
      },
      "source": [
        "Configure los conjuntos de datos para mejorar el rendimiento como antes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpT0b_7mYRXV"
      },
      "outputs": [],
      "source": [
        "train_data = configure_dataset(train_data)\n",
        "validation_data = configure_dataset(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8SUhGFNsmRi"
      },
      "source": [
        "### Entrenamiento del modelo\n",
        "\n",
        "Igual que antes, puede entrenar el modelo de este conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJgI1pow2YR9"
      },
      "outputs": [],
      "source": [
        "model = create_model(vocab_size=vocab_size, num_labels=3)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data, validation_data=validation_data, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTPCYf_Jh6TH"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(validation_data)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_knIsO-r4pHb"
      },
      "source": [
        "### Exportación del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEuMLJA_Xiwo"
      },
      "source": [
        "Para que el modelo sea capaz de tomar cadenas sin procesar como entradas, deberá crear una capa `TextVectorization` de Keras que realice los mismos pasos que su función de preprocesamiento personalizada. Como ya ha entrenado un vocabulario, puede usar `TextVectorization.set_vocabulary`, en vez de `TextVectorization.adapt` que entrena a un vocabulario nuevo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ODkRXbk6aHb"
      },
      "outputs": [],
      "source": [
        "preprocess_layer = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    standardize=tf_text.case_fold_utf8,\n",
        "    split=tokenizer.tokenize,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "preprocess_layer.set_vocabulary(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-Cvd27y4qwt"
      },
      "outputs": [],
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [preprocess_layer, model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyg0B4zsc-UD"
      },
      "outputs": [],
      "source": [
        "# Create a test dataset of raw strings.\n",
        "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
        "test_ds = configure_dataset(test_ds)\n",
        "\n",
        "loss, accuracy = export_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Mm0Y9QYQwE"
      },
      "source": [
        "La pérdida y la exactitud del modelo en el conjunto de validación codificado y en el modelo exportado en el conjunto de validación sin procesar son iguales, tal como se espera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stk2BP8GE-qo"
      },
      "source": [
        "### Ejecución de la inferencia en datos nuevos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w1fQGJPD2Yh"
      },
      "outputs": [],
      "source": [
        "inputs = [\n",
        "    \"Join'd to th' Ionians with their flowing robes,\",  # Label: 1\n",
        "    \"the allies, and his armour flashed about him so that he seemed to all\",  # Label: 2\n",
        "    \"And with loud clangor of his arms he fell.\",  # Label: 0\n",
        "]\n",
        "\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = tf.math.argmax(predicted_scores, axis=1)\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eA8TVdnA-3L"
      },
      "source": [
        "## Descarga de más conjuntos de datos con TensorFlow Datasets (TFDS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QFSxfZ3Vqsn"
      },
      "source": [
        "Puede descargar muchos más conjuntos de datos con los [Datasets de TensorFlow](https://www.tensorflow.org/datasets/catalog/overview).\n",
        "\n",
        "Para este ejemplo, usará [conjuntos de datos para revisión de películas largas de IMDB](https://www.tensorflow.org/datasets/catalog/imdb_reviews) para entrenar un modelo de clasificación de sentimientos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzC65LOaVw0B"
      },
      "outputs": [],
      "source": [
        "# Training set.\n",
        "train_ds = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='train[:80%]',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKGkgPBkFh0k"
      },
      "outputs": [],
      "source": [
        "# Validation set.\n",
        "val_ds = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='train[80%:]',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQjf3YZAb5Ne"
      },
      "source": [
        "Imprima algunos ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq1w8MnfWt2C"
      },
      "outputs": [],
      "source": [
        "for review_batch, label_batch in val_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(\"Review: \", review_batch[i].numpy())\n",
        "    print(\"Label: \", label_batch[i].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-lVaukyb75k"
      },
      "source": [
        "Ahora puede preprocesar los datos y entrenar un modelo como antes.\n",
        "\n",
        "Nota: Deberá usar `tf.keras.losses.BinaryCrossentropy` en vez de `tf.keras.losses.SparseCategoricalCrossentropy` para su modelo, ya que se trata de un problema de clasificación binaria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciz2CxAsZw3Z"
      },
      "source": [
        "### Preparación del conjunto de datos para entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzT_t9ihZLH4"
      },
      "outputs": [],
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
        "train_text = train_ds.map(lambda text, labels: text)\n",
        "vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz-Xrd_ZZ4tB"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycn0Itd6g5aF"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(vectorize_text)\n",
        "val_ds = val_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc11jQTlZ5lj"
      },
      "outputs": [],
      "source": [
        "# Configure datasets for performance as before.\n",
        "train_ds = configure_dataset(train_ds)\n",
        "val_ds = configure_dataset(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQzoYkaGZ82Z"
      },
      "source": [
        "### Creación, configuración y entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9IOTLkyZ-a7"
      },
      "outputs": [],
      "source": [
        "model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=1)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLnDs5dhaBAk"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq59QpNzaDMa"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_ds, validation_data=val_ds, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCMWCEtyaEbR"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGtqLXVnaaFy"
      },
      "source": [
        "### Exportación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE9WZARZaZr1"
      },
      "outputs": [],
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [vectorize_layer, model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhF8tDH-afoC"
      },
      "outputs": [],
      "source": [
        "# 0 --> negative review\n",
        "# 1 --> positive review\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "]\n",
        "\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = [int(round(x[0])) for x in predicted_scores]\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1KSXDFPWiPN"
      },
      "source": [
        "## Conclusión\n",
        "\n",
        "En este tutorial se demostraron varias formas de cargar y preprocesar texto. Para avanzar al paso siguiente, puede explorar otros tutoriales de [TensorFlow Text](https://www.tensorflow.org/text) sobre preprocesamiento de texto, tales como:\n",
        "\n",
        "- [Preprocesamiento BERT con texto TF](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)\n",
        "- [Tokenización con TF Text](https://www.tensorflow.org/text/guide/tokenizers)\n",
        "- [Tokenizadores de subpalabras](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
        "\n",
        "También puede buscar conjuntos nuevos de datos en [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). Y, para más información sobre `tf.data`, consulte la guía sobre cómo [construir canalizaciones de entrada](../../guide/data.ipynb)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
