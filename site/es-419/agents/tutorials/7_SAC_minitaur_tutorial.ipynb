{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "**Copyright 2023 The TF-Agents Authors.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "# SAC en Minitaur con la API Actor-Learner\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/7_SAC_minitaur_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/agents/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a>\n",
        "</td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOUOQOrFs3zn"
      },
      "source": [
        "## Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "Este ejemplo nos muestra cómo entrenar un agente [Soft Actor Critic](https://arxiv.org/abs/1812.05905) en el entorno [Minitaur](https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/minitaur.py).\n",
        "\n",
        "Si ha trabajado con [DQN Colab](https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb), esto le resultará muy familiar. Algunos cambios importantes son los siguientes:\n",
        "\n",
        "- Cambio del agente, de DQN a SAC.\n",
        "- Entrenamiento en Minitaur, que es un entorno mucho más complejo que CartPole. El entorno Minitaur pretende entrenar a un robot cuadrúpedo para que se desplace hacia delante.\n",
        "- Uso de la API Actor-Learner de TF-Agents para el aprendizaje por refuerzo distribuido.\n",
        "\n",
        "La API admite tanto la recopilación de datos distribuidos mediante un búfer de repetición de experiencias y un contenedor de variables (servidor de parámetros) como el entrenamiento distribuido en múltiples dispositivos. La API fue diseñada para que sea muy simple y modular. Se usa [Reverb](https://deepmind.com/research/open-source/Reverb) tanto para el búfer de repetición como para el contenedor de variables y [la API DistributionStrategy de TF](https://www.tensorflow.org/guide/distributed_training) para el entrenamiento distribuido en GPU y TPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vUQms4DAY5A"
      },
      "source": [
        "Si todavía no ha instalado las siguientes dependencias, ejecute estos comandos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fskoLlB-AZ9j"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install matplotlib\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pybullet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNV5wyH3dyMl"
      },
      "source": [
        "Primero importaremos las diferentes herramientas que necesitamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import reverb\n",
        "import tempfile\n",
        "import PIL.Image\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.environments import suite_pybullet\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.train import actor\n",
        "from tf_agents.train import learner\n",
        "from tf_agents.train import triggers\n",
        "from tf_agents.train.utils import spec_utils\n",
        "from tf_agents.train.utils import strategy_utils\n",
        "from tf_agents.train.utils import train_utils\n",
        "\n",
        "tempdir = tempfile.gettempdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "outputs": [],
      "source": [
        "env_name = \"MinitaurBulletEnv-v0\" # @param {type:\"string\"}\n",
        "\n",
        "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
        "# 1e5 is just so this doesn't take too long (1 hr)\n",
        "num_iterations = 100000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256 # @param {type:\"integer\"}\n",
        "\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "actor_fc_layer_params = (256, 256)\n",
        "critic_joint_fc_layer_params = (256, 256)\n",
        "\n",
        "log_interval = 5000 # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
        "eval_interval = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "policy_save_interval = 5000 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Entorno\n",
        "\n",
        "Los entornos en RL representan la tarea o problema que tratamos de resolver. Los entornos estándar se pueden crear fácilmente en TF-Agents utilizando `suites`. Hay diferentes `suites` para cargar entornos desde fuentes como OpenAI Gym, Atari, DM Control, etc., dado un nombre de entorno de cadena.\n",
        "\n",
        "Ahora, carguemos el entorno Minitaur desde el paquete de Pybullet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [],
      "source": [
        "env = suite_pybullet.load(env_name)\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY179d1xlmoM"
      },
      "source": [
        "En este entorno, el objetivo es que el agente entrene una política que controlará el robot Minitaur y hará que avance lo más rápido posible. Los episodios duran 1000 pasos y la devolución será la suma de recompensas a lo largo del episodio.\n",
        "\n",
        "Miremos la información que proporciona el entorno como una `observation` que la política utilizará para generar `actions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "outputs": [],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg5ysVTnctIm"
      },
      "source": [
        "La observación es bastante compleja. Recibimos 28 valores que representan los ángulos, las velocidades y los pares de todos los motores. A cambio, el entorno espera 8 valores para las acciones entre `[-1, 1]`. Estos son los ángulos del motor deseados.\n",
        "\n",
        "Por lo general, creamos dos entornos: uno para recopilar datos durante el entrenamiento y otro para evaluación. Los entornos están escritos en Python puro y utilizan arreglos numpy, que la API Actor Learner consume directamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "outputs": [],
      "source": [
        "collect_env = suite_pybullet.load(env_name)\n",
        "eval_env = suite_pybullet.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da-z2yF66FR9"
      },
      "source": [
        "## Estrategia de distribución\n",
        "\n",
        "Usamos la API DistributionStrategy para habilitar la ejecución de la computación del paso del entrenamiento en múltiples dispositivos, como en varias GPU o TPU, a través del paralelismo de datos. El paso de entrenamiento:\n",
        "\n",
        "- Recibe un lote de datos de entrenamiento.\n",
        "- Lo divide entre los dispositivos.\n",
        "- Calcula el paso hacia adelante.\n",
        "- Agrega y calcula la MEDIA de la pérdida.\n",
        "- Calcula el paso hacia atrás y ejecuta una actualización de la variable del gradiente.\n",
        "\n",
        "Con la API TF-Agents Learner y la API DistributionStrategy es bastante fácil pasar de ejecución del paso de entrenamiento en GPU (con MirroredStrategy) a TPU (con TPUStrategy) sin cambiar ninguna de las siguientes lógicas de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGREYZCaDB1h"
      },
      "source": [
        "### Cómo habilitar la GPU\n",
        "\n",
        "Si desea probar la ejecución en una GPU, primero deberá habilitar las GPU para el bloc de notas:\n",
        "\n",
        "- Vaya a Editar → Configuración del bloc de notas\n",
        "- Seleccione GPU en el menú desplegable Acelerador de hardware"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZuvwDV66Mn1"
      },
      "source": [
        "### Cómo elegir una estrategia\n",
        "\n",
        "Use `strategy_utils` para generar una estrategia. A nivel interno, pase el siguiente parámetro:\n",
        "\n",
        "- `use_gpu = False` devuelve `tf.distribute.get_strategy()`, que usa CPU\n",
        "- `use_gpu = True` devuelve `tf.distribute.MirroredStrategy()`, que usa todas las GPU visibles para TensorFlow en una máquina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff5ZZRZI15ds"
      },
      "outputs": [],
      "source": [
        "use_gpu = True #@param {type:\"boolean\"}\n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMn5FTs5kHvt"
      },
      "source": [
        "Todas las variables y agentes deben crearse en `strategy.scope()`, como verá a continuación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agente\n",
        "\n",
        "Para crear un agente SAC, primero debemos crear las redes que se entrenarán. SAC es un agente actor-crítico, por lo que necesitaremos dos redes.\n",
        "\n",
        "El crítico nos dará estimaciones de valor para `Q(s,a)`. Es decir, recibirá como entrada una observación y una acción, y nos indicará en qué medida fue buena esa acción para el estado dado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(collect_env))\n",
        "\n",
        "with strategy.scope():\n",
        "  critic_net = critic_network.CriticNetwork(\n",
        "        (observation_spec, action_spec),\n",
        "        observation_fc_layer_params=None,\n",
        "        action_fc_layer_params=None,\n",
        "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        last_kernel_initializer='glorot_uniform')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYy4AH4V7Ph4"
      },
      "source": [
        "Usaremos este crítico para entrenar una red `actor` que nos permitirá generar acciones a partir de una observación.\n",
        "\n",
        "`ActorNetwork` predecirá los parámetros para una distribución tanh-squashed [MultivariateNormalDiag](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag). Luego, se tomarán muestras de esta distribución, condicionadas a la observación actual, cada vez que sea necesario generar acciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB5Y3Oub4u7f"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=actor_fc_layer_params,\n",
        "      continuous_projection_net=(\n",
        "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Ahora, cestas redes a mano, podemos crear una instancia del agente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = sac_agent.SacAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        actor_network=actor_net,\n",
        "        critic_network=critic_net,\n",
        "        actor_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=actor_learning_rate),\n",
        "        critic_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=critic_learning_rate),\n",
        "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=alpha_learning_rate),\n",
        "        target_update_tau=target_update_tau,\n",
        "        target_update_period=target_update_period,\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\n",
        "        gamma=gamma,\n",
        "        reward_scale_factor=reward_scale_factor,\n",
        "        train_step_counter=train_step)\n",
        "\n",
        "  tf_agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Búfer de repetición\n",
        "\n",
        "Para hacer un seguimiento de los datos recogidos del entorno, se usará [Reverb](https://deepmind.com/research/open-source/Reverb), un sistema de repetición eficiente, extensible y fácil de usar de Deepmind. Almacena los datos de experiencia recopilados por los Actores y utilizados por el Aprendiz durante el entrenamiento.\n",
        "\n",
        "Para este tutorial, esto es menos importante que `max_size` -- pero en un entorno distribuido con recopilación y entrenamiento asíncronos, probablemente quiera experimentar con `rate_limiters.SampleToInsertRatio`, usando un samples_per_insert entre 2 y 1000. Por ejemplo:\n",
        "\n",
        "```\n",
        "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRNvAnkO7JK2"
      },
      "source": [
        "El búfer de repetición se construye a partir de especificaciones que describen los tensores que se van a almacenar, y que se pueden obtener del agente si se usa `tf_agent.collect_data_spec`.\n",
        "\n",
        "Dado que el agente SAC necesita tanto la observación actual como la siguiente para calcular la pérdida, configuramos `sequence_length=2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVLUxyUo7HQR"
      },
      "outputs": [],
      "source": [
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2,\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "Ahora, generamos un conjunto de datos de TensorFlow a partir del búfer de repetición Reverb. Le pasaremos esto al alumno para extraer muestras de experiencias para el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba7bilizt_qW"
      },
      "outputs": [],
      "source": [
        "dataset = reverb_replay.as_dataset(\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
        "experience_dataset_fn = lambda: dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Políticas\n",
        "\n",
        "En TF-Agents, las políticas representan la noción estándar de políticas en RL: dado que un `time_step` produce una acción o una distribución de acciones. El método principal es `policy_step = policy.step(time_step)` donde `policy_step` es una tupla nombrada `PolicyStep(action, state, info)`. `policy_step.action` es la `action` que se aplicará al entorno, `state` representa el estado de las políticas de estado (RNN) y `info` podría contener información auxiliar como probabilidades logarítmicas de las acciones.\n",
        "\n",
        "Los agentes contienen dos políticas:\n",
        "\n",
        "- `agent.policy`: la política principal que se usa para la evaluación y la implementación.\n",
        "- `agent.collect_policy`: una segunda política que se usa para la recopilación de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq7JE8IwFe0E"
      },
      "outputs": [],
      "source": [
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_eval_policy, use_tf_function=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_A4rZveEQzW"
      },
      "outputs": [],
      "source": [
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_collect_policy, use_tf_function=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azkJZ8oaF8uc"
      },
      "source": [
        "Las políticas pueden crearse independientemente de los agentes. Por ejemplo, utilice `tf_agents.policies.random_py_policy` para crear una política que seleccionará aleatoriamente una acción para cada time_step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "outputs": [],
      "source": [
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  collect_env.time_step_spec(), collect_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1LMqw60Kuso"
      },
      "source": [
        "## Actores\n",
        "\n",
        "El actor gestiona las interacciones entre una política y un entorno.\n",
        "\n",
        "- Los componentes del Actor contienen una instancia del entorno (como `py_environment`) y una copia de las variables de la política.\n",
        "- Cada agente Actor ejecuta una secuencia de pasos de recopilación de datos en función de los valores locales de las variables de la política.\n",
        "- Las variables se actualizan explícitamente a través de la instancia del cliente contenedor de variables en el script de entrenamiento antes de llamar a `actor.run()`.\n",
        "- La experiencia observada se escribe en el búfer de repetición en cada paso de recopilación de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjE59ct9fU7W"
      },
      "source": [
        "A medida que los Actores ejecutan los pasos de recopilación de datos, pasan trayectorias de (estado, acción, recompensa) al observador, que las almacena en caché y las escribe en el sistema de repetición Reverb.\n",
        "\n",
        "Estamos almacenando trayectorias para marcos [(t0,t1) (t1,t2) (t2,t3), ...] dado que `stride_length=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbyGmdiNfNDc"
      },
      "outputs": [],
      "source": [
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yaVVC22fOcF"
      },
      "source": [
        "Creamos un Actor con la política aleatoria y recopilamos experiencias para introducirlas en el búfer de repetición."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGq3SY0kKwsa"
      },
      "outputs": [],
      "source": [
        "initial_collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "initial_collect_actor.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pkg-0vZP_Ya"
      },
      "source": [
        "Cree una instancia Actor con la política de recopilación para reunir más experiencias durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6ooXyk0FZ5j"
      },
      "outputs": [],
      "source": [
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR9CZ-jfPN2T"
      },
      "source": [
        "Cree un Actor que se usará para evaluar la política durante el entrenamiento. Pasamos `actor.eval_metrics(num_eval_episodes)` para registrar las métricas más adelante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHY2BT5lFhgL"
      },
      "outputs": [],
      "source": [
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=os.path.join(tempdir, 'eval'),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6eBGSYiOf83"
      },
      "source": [
        "## Aprendices\n",
        "\n",
        "El componente Aprendiz contiene el agente y actualiza gradualmente las variables de la política a partir de los datos de experiencia del búfer de repetición. Después de uno o más pasos de entrenamiento, el Aprendiz puede enviar un nuevo conjunto de valores de variables al contenedor de variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi37YicSFTfF"
      },
      "outputs": [],
      "source": [
        "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
        "\n",
        "# Triggers to save the agent's policy checkpoints.\n",
        "learning_triggers = [\n",
        "    triggers.PolicySavedModelTrigger(\n",
        "        saved_model_dir,\n",
        "        tf_agent,\n",
        "        train_step,\n",
        "        interval=policy_save_interval),\n",
        "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
        "]\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  tempdir,\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  triggers=learning_triggers,\n",
        "  strategy=strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Métricas y evaluación\n",
        "\n",
        "Creamos una instancia del Actor eval con `actor.eval_metrics`, que crea las métricas más utilizadas durante la evaluación de políticas:\n",
        "\n",
        "- Rendimiento medio. El rendimiento es la suma de las recompensas que se obtienen al ejecutar una política en un entorno durante un episodio, y normalmente se calcula el promedio de varios episodios.\n",
        "- Duración media de los episodios.\n",
        "\n",
        "Para generar estas métricas, ejecutamos el Actor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83iMSHUC71RG"
      },
      "outputs": [],
      "source": [
        "def get_eval_metrics():\n",
        "  eval_actor.run()\n",
        "  results = {}\n",
        "  for metric in eval_actor.metrics:\n",
        "    results[metric.name] = metric.result()\n",
        "  return results\n",
        "\n",
        "metrics = get_eval_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnOMvX_eZvOW"
      },
      "outputs": [],
      "source": [
        "def log_eval_metrics(step, metrics):\n",
        "  eval_results = (', ').join(\n",
        "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "  print('step = {0}: {1}'.format(step, eval_results))\n",
        "\n",
        "log_eval_metrics(0, metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWWURm_rXG-f"
      },
      "source": [
        "Consulte el [módulo de métricas](https://github.com/tensorflow/agents/blob/master/tf_agents/metrics/tf_metrics.py) para conocer otras implementaciones estándar de diferentes métricas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Entrenamiento del agente\n",
        "\n",
        "El bucle de entrenamiento implica tanto la recopilación de datos del entorno como la optimización de las redes del agente. A lo largo del proceso, evaluaremos de vez en cuando la política del agente para ver nuestro rendimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Training.\n",
        "  collect_actor.run()\n",
        "  loss_info = agent_learner.run(iterations=1)\n",
        "\n",
        "  # Evaluating.\n",
        "  step = agent_learner.train_step_numpy\n",
        "\n",
        "  if eval_interval and step % eval_interval == 0:\n",
        "    metrics = get_eval_metrics()\n",
        "    log_eval_metrics(step, metrics)\n",
        "    returns.append(metrics[\"AverageReturn\"])\n",
        "\n",
        "  if log_interval and step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualización\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Gráficos\n",
        "\n",
        "Podemos hacer una gráfica comparativa del rendimiento medio y los pasos globales para ver el funcionamiento de nuestro agente. En `Minitaur`, la función de recompensa se basa en la distancia que camina el minitaur en 1000 pasos y penaliza el gasto de energía."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXKzyGt72HS8"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "Resulta útil visualizar el funcionamiento de un agente mediante el renderizado del entorno en cada paso. Pero antes de hacerlo, creemos una función para insertar videos en este colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "El siguiente código visualiza la política del agente para algunos episodios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSgaQN1nXT-h"
      },
      "outputs": [],
      "source": [
        "num_episodes = 3\n",
        "video_filename = 'sac_minitaur.mp4'\n",
        "with imageio.get_writer(video_filename, fps=60) as video:\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = eval_env.reset()\n",
        "    video.append_data(eval_env.render())\n",
        "    while not time_step.is_last():\n",
        "      action_step = eval_actor.policy.action(time_step)\n",
        "      time_step = eval_env.step(action_step.action)\n",
        "      video.append_data(eval_env.render())\n",
        "\n",
        "embed_mp4(video_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "7_SAC_minitaur_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
