{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beObUOFyuRjT"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6D70EeAZe-Q"
      },
      "source": [
        "# Controladores\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/4_drivers_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/4_drivers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/4_drivers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/agents/tutorials/4_drivers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aPHF9kXFggA"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Un patrón común en el aprendizaje por refuerzo consiste en ejecutar una política en un entorno durante un número específico de pasos o episodios. Esto sucede, por ejemplo, durante la recopilación de datos, la evaluación y la generación de un video del agente.\n",
        "\n",
        "Si bien esto se escribe relativamente fácil en Python, es mucho más complejo de escribir y depurar en TensorFlow porque involucra bucles `tf.while`, `tf.cond` y `tf.control_dependencies`. Por lo tanto, abstraemos esta noción de bucle de ejecución en una clase llamada `driver` y proporcionamos implementaciones bien probadas tanto en Python como en TensorFlow.\n",
        "\n",
        "Además, los datos que encuentra el controlador en cada paso se guardan en una tupla con nombre llamada Trajectory y se transmiten a un conjunto de observadores, como búferes de repetición y métricas. Estos datos incluyen la observación del entorno, la acción recomendada por la política, la recompensa obtenida, el tipo de paso actual y siguiente, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7PM1QfMZqkS"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w-Ykwl1bn4v"
      },
      "source": [
        "Si todavía no ha instalado tf-agents o gym, ejecute los siguientes comandos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnE2CgilrngG"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whYNP894FSkA"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.drivers import dynamic_episode_driver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V7DEcB8IeiQ"
      },
      "source": [
        "## Controladores de Python\n",
        "\n",
        "La clase `PyDriver` toma un entorno python, una política python y una lista de observadores para actualizar en cada paso. El método principal es `run()`, que recorre el entorno mediante acciones de la política hasta que se cumple al menos uno de los siguientes criterios de finalización: que el número de pasos alcance `max_steps` o que el número de episodios alcance `max_episodes`.\n",
        "\n",
        "La implementación se ve aproximadamente de la siguiente manera:\n",
        "\n",
        "```python\n",
        "class PyDriver(object):\n",
        "\n",
        "  def __init__(self, env, policy, observers, max_steps=1, max_episodes=1):\n",
        "    self._env = env\n",
        "    self._policy = policy\n",
        "    self._observers = observers or []\n",
        "    self._max_steps = max_steps or np.inf\n",
        "    self._max_episodes = max_episodes or np.inf\n",
        "\n",
        "  def run(self, time_step, policy_state=()):\n",
        "    num_steps = 0\n",
        "    num_episodes = 0\n",
        "    while num_steps &lt; self._max_steps and num_episodes &lt; self._max_episodes:\n",
        "\n",
        "      # Compute an action using the policy for the given time_step\n",
        "      action_step = self._policy.action(time_step, policy_state)\n",
        "\n",
        "      # Apply the action to the environment and get the next step\n",
        "      next_time_step = self._env.step(action_step.action)\n",
        "\n",
        "      # Package information into a trajectory\n",
        "      traj = trajectory.Trajectory(\n",
        "         time_step.step_type,\n",
        "         time_step.observation,\n",
        "         action_step.action,\n",
        "         action_step.info,\n",
        "         next_time_step.step_type,\n",
        "         next_time_step.reward,\n",
        "         next_time_step.discount)\n",
        "\n",
        "      for observer in self._observers:\n",
        "        observer(traj)\n",
        "\n",
        "      # Update statistics to check termination\n",
        "      num_episodes += np.sum(traj.is_last())\n",
        "      num_steps += np.sum(~traj.is_boundary())\n",
        "\n",
        "      time_step = next_time_step\n",
        "      policy_state = action_step.state\n",
        "\n",
        "    return time_step, policy_state\n",
        "\n",
        "```\n",
        "\n",
        "Ahora, repasemos el ejemplo de ejecución de una política aleatoria en el entorno CartPole, guardando los resultados en un búfer de repetición y calculando algunas métricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj4_-77_5ExP"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "policy = random_py_policy.RandomPyPolicy(time_step_spec=env.time_step_spec(), \n",
        "                                         action_spec=env.action_spec())\n",
        "replay_buffer = []\n",
        "metric = py_metrics.AverageReturnMetric()\n",
        "observers = [replay_buffer.append, metric]\n",
        "driver = py_driver.PyDriver(\n",
        "    env, policy, observers, max_steps=20, max_episodes=1)\n",
        "\n",
        "initial_time_step = env.reset()\n",
        "final_time_step, _ = driver.run(initial_time_step)\n",
        "\n",
        "print('Replay Buffer:')\n",
        "for traj in replay_buffer:\n",
        "  print(traj)\n",
        "\n",
        "print('Average Return: ', metric.result())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Yrxg36Ik1x"
      },
      "source": [
        "## Controladores de TensorFlow\n",
        "\n",
        "También tenemos controladores en TensorFlow que funcionan de forma similar a los controladores de Python, pero utilizan entornos de TF, políticas de TF, observadores de TF, etc. En este momento contamos con 2 controladores de TensorFlow: `DynamicStepDriver`, que termina tras un número determinado de pasos (válidos) del entorno y `DynamicEpisodeDriver`, que termina tras un número determinado de episodios. Veamos un ejemplo de DynamicEpisode en acción.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC4ba3ObSceA"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "tf_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
        "                                            time_step_spec=tf_env.time_step_spec())\n",
        "\n",
        "\n",
        "num_episodes = tf_metrics.NumberOfEpisodes()\n",
        "env_steps = tf_metrics.EnvironmentSteps()\n",
        "observers = [num_episodes, env_steps]\n",
        "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "    tf_env, tf_policy, observers, num_episodes=2)\n",
        "\n",
        "# Initial driver.run will reset the environment and initialize the policy.\n",
        "final_time_step, policy_state = driver.run()\n",
        "\n",
        "print('final_time_step', final_time_step)\n",
        "print('Number of Steps: ', env_steps.result().numpy())\n",
        "print('Number of Episodes: ', num_episodes.result().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz5jhHnU0fX1"
      },
      "outputs": [],
      "source": [
        "# Continue running from previous state\n",
        "final_time_step, _ = driver.run(final_time_step, policy_state)\n",
        "\n",
        "print('final_time_step', final_time_step)\n",
        "print('Number of Steps: ', env_steps.result().numpy())\n",
        "print('Number of Episodes: ', num_episodes.result().numpy())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "4_drivers_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
