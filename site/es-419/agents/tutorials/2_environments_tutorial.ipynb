{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma19Ks2CTDbZ"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XljsiF6lYkuV"
      },
      "source": [
        "# Entornos\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/2_environments_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/2_environments_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/2_environments_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/agents/tutorials/2_environments_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h3B-YBHopJI"
      },
      "source": [
        "## Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9c6vCPGovOM"
      },
      "source": [
        "El objetivo del Aprendizaje por Refuerzo (RL) es diseñar agentes que aprendan mediante la interacción con un entorno. En la configuración estándar del RL, el agente recibe una observación en cada paso de tiempo y elige una acción. La acción se aplica al entorno y éste devuelve una recompensa y una nueva observación. El agente entrena una política para elegir acciones que maximicen la suma de recompensas, también conocida como rendimiento.\n",
        "\n",
        "En TF-Agents, los entornos se pueden implementar tanto en Python como en TensorFlow. Los entornos Python suelen ser más fáciles de implementar, entender y depurar, pero los entornos TensorFlow son más eficientes y permiten una paralelización natural. El flujo de trabajo más común consiste en implementar un entorno en Python y utilizar una de nuestras envolturas para convertirlo automáticamente en TensorFlow.\n",
        "\n",
        "Primero, veamos los entornos Python. Los entornos TensorFlow siguen una API muy similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_16bQF0anmE"
      },
      "source": [
        "## Preparación\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qax00bg2a4Jj"
      },
      "source": [
        "Si todavía no ha instalado tf-agents o gym, ejecute los siguientes comandos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKU2iY_7at8Y"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents[reverb]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZAoFNwnRbKK"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-y4p9i9UURn"
      },
      "source": [
        "## Entornos de Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPSwHONKMNv9"
      },
      "source": [
        "Los entornos Python tienen un método `step(action) -> next_time_step` que aplica una acción al entorno y devuelve la siguiente información sobre el paso posterior:\n",
        "\n",
        "1. `observation`: esta es la parte del estado del entorno que el agente puede observar para elegir sus acciones para el siguiente paso.\n",
        "2. `reward`: el agente está aprendiendo a maximizar la suma de estas recompensas a través de múltiples pasos.\n",
        "3. `step_type`: las interacciones con el entorno suelen formar parte de una secuencia o episodio. Por ejemplo, varios movimientos en un juego de ajedrez. step_type puede ser `FIRST`, `MID` o `LAST` para indicar si este paso de tiempo es el primero, uno intermedio o el último de una secuencia.\n",
        "4. `discount`: este es un flotante que representa cuánto debe ponderarse la recompensa en el siguiente paso de tiempo respecto a la recompensa en el paso de tiempo actual.\n",
        "\n",
        "Estos se agrupan en una tupla nombrada `TimeStep(step_type, reward, discount, observation)`.\n",
        "\n",
        "La interfaz que deben implementar todos los entornos Python se encuentra en `environments/py_environment.PyEnvironment`. Estos son los métodos principales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlD2Dd2vUTtg"
      },
      "outputs": [],
      "source": [
        "class PyEnvironment(object):\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Return initial_time_step.\"\"\"\n",
        "    self._current_time_step = self._reset()\n",
        "    return self._current_time_step\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Apply action and return new time_step.\"\"\"\n",
        "    if self._current_time_step is None:\n",
        "        return self.reset()\n",
        "    self._current_time_step = self._step(action)\n",
        "    return self._current_time_step\n",
        "\n",
        "  def current_time_step(self):\n",
        "    return self._current_time_step\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    \"\"\"Return time_step_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Return observation_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def action_spec(self):\n",
        "    \"\"\"Return action_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _reset(self):\n",
        "    \"\"\"Return initial_time_step.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _step(self, action):\n",
        "    \"\"\"Apply action and return new time_step.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfF8koryiGPR"
      },
      "source": [
        "Además del método `step()`, los entornos también proporcionan un método `reset()` que inicia una nueva secuencia y ofrece un `TimeStep` inicial. No es necesario llamar al método `reset` explícitamente. Se asume que los entornos se reinician automáticamente, ya sea cuando llegan al final de un episodio o cuando se llama a step() por primera vez.\n",
        "\n",
        "Tenga en cuenta que las subclases no implementan `step()` ni `reset()` de forma directa. En lugar de eso, anulan los métodos `_step()` y `_reset()`. Los pasos de tiempo que devuelven estos métodos serán almacenados en caché y expuestos a través de `current_time_step()`.\n",
        "\n",
        "Los métodos `observation_spec` y `action_spec` devuelven un nido de `(Bounded)ArraySpecs` que describe el nombre, la forma, el tipo de datos y los intervalos de las observaciones y las acciones respectivamente.\n",
        "\n",
        "En TF-Agents, constantemente hablamos de nidos, que se definen como una estructura con forma de árbol que está compuesta de listas, tuplas, tuplas nombradas o diccionarios. Estos pueden estar compuestos arbitrariamente para mantener la estructura de las observaciones y acciones. Esto es muy útil para entornos más complejos en los que hay muchas observaciones y acciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r63R-RbjcIRw"
      },
      "source": [
        "### Cómo usar entornos estándar\n",
        "\n",
        "TF Agents ha incorporado envoltorios para muchos entornos estándar como OpenAI Gym, DeepMind-control y Atari, para que sigan nuestra interfaz `py_environment.PyEnvironment`. Estos entornos envueltos se pueden cargar fácilmente con nuestros paquetes de entornos. Carguemos el entorno CartPole desde OpenAI gym y veamos la acción y time_step_spec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kBPE5T-nb2-"
      },
      "outputs": [],
      "source": [
        "environment = suite_gym.load('CartPole-v0')\n",
        "print('action_spec:', environment.action_spec())\n",
        "print('time_step_spec.observation:', environment.time_step_spec().observation)\n",
        "print('time_step_spec.step_type:', environment.time_step_spec().step_type)\n",
        "print('time_step_spec.discount:', environment.time_step_spec().discount)\n",
        "print('time_step_spec.reward:', environment.time_step_spec().reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWXOC863Apo_"
      },
      "source": [
        "De este modo vemos que el entorno espera acciones de tipo `int64` en [0, 1] y devuelve `TimeSteps` donde las observaciones son un vector `float32` de longitud 4 y el factor de descuento es un `float32` en [0.0, 1.0]. Ahora, intentemos adoptar una acción fija `(1,)` para todo un episodio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzIbOJ0-0y12"
      },
      "outputs": [],
      "source": [
        "action = np.array(1, dtype=np.int32)\n",
        "time_step = environment.reset()\n",
        "print(time_step)\n",
        "while not time_step.is_last():\n",
        "  time_step = environment.step(action)\n",
        "  print(time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAbBl4_PMtA"
      },
      "source": [
        "### Cómo crear su propio entorno de Python\n",
        "\n",
        "Para muchos clientes, un caso de uso común es aplicar uno de los agentes estándar (ver agentes/) en TF-Agents a su problema. Para hacer esto, tienen que enmarcar su problema como un entorno. Veamos cómo implementar un entorno en Python.\n",
        "\n",
        "Supongamos que queremos entrenar a un agente para que juegue al siguiente juego de cartas (inspirado en el Black Jack):\n",
        "\n",
        "1. El juego se juega con una baraja infinita de cartas numeradas del 1 al 10.\n",
        "2. En cada turno, el agente puede hacer 2 cosas: obtener una nueva carta al azar o detener la ronda en curso.\n",
        "3. El objetivo es conseguir que la suma de las cartas se acerque lo más posible a 21 al final de la ronda, sin pasarse.\n",
        "\n",
        "Un entorno que representa un juego podría verse de esta forma:\n",
        "\n",
        "1. Acciones: tenemos 2 acciones. Acción 0: pedir una nueva carta, y Acción 1: finalizar la ronda en curso.\n",
        "2. Observaciones: suma de las cartas en la ronda actual.\n",
        "3. Recompensa: el objetivo es acercarse lo más posible a 21 sin pasarse, por lo que podemos lograrlo al usar la siguiente recompensa al final de la ronda: sum_of_cards - 21 if sum_of_cards &lt;= 21, else -21\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HD0cDykPL6I"
      },
      "outputs": [],
      "source": [
        "class CardGameEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    # Make sure episodes don't go on forever.\n",
        "    if action == 1:\n",
        "      self._episode_ended = True\n",
        "    elif action == 0:\n",
        "      new_card = np.random.randint(1, 11)\n",
        "      self._state += new_card\n",
        "    else:\n",
        "      raise ValueError('`action` should be 0 or 1.')\n",
        "\n",
        "    if self._episode_ended or self._state >= 21:\n",
        "      reward = self._state - 21 if self._state <= 21 else -21\n",
        "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
        "    else:\n",
        "      return ts.transition(\n",
        "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYEwyX7QsqeX"
      },
      "source": [
        "Veamos si hemos hecho todo correctamente al definir el entorno anterior. Al crear un entorno propio hay que asegurarse de que las observaciones y los time_steps generados sigan las formas y tipos correctos tal y como se definen en las especificaciones. Estos se utilizan para generar el gráfico TensorFlow y por lo tanto pueden generar problemas difíciles de depurar si nos equivocamos.\n",
        "\n",
        "Para validar nuestro entorno usaremos una política aleatoria para generar acciones e iteraremos durante 5 episodios para asegurarnos de que las cosas funcionen según lo previsto. Se producirá un error si recibimos un time_step que no siga las especificaciones del entorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hhm-5R7spVx"
      },
      "outputs": [],
      "source": [
        "environment = CardGameEnv()\n",
        "utils.validate_py_environment(environment, episodes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_36eM7MvkNOg"
      },
      "source": [
        "Ahora que sabemos que el entorno funciona según lo previsto, ejecutémoslo con una política fija: pedir 3 cartas y terminar la ronda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FILylafAkMEx"
      },
      "outputs": [],
      "source": [
        "get_new_card_action = np.array(0, dtype=np.int32)\n",
        "end_round_action = np.array(1, dtype=np.int32)\n",
        "\n",
        "environment = CardGameEnv()\n",
        "time_step = environment.reset()\n",
        "print(time_step)\n",
        "cumulative_reward = time_step.reward\n",
        "\n",
        "for _ in range(3):\n",
        "  time_step = environment.step(get_new_card_action)\n",
        "  print(time_step)\n",
        "  cumulative_reward += time_step.reward\n",
        "\n",
        "time_step = environment.step(end_round_action)\n",
        "print(time_step)\n",
        "cumulative_reward += time_step.reward\n",
        "print('Final Reward = ', cumulative_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vBLPN3ioyGx"
      },
      "source": [
        "### Envoltorios de entorno\n",
        "\n",
        "Un envoltorio de entorno toma un entorno Python y devuelve una versión modificada del entorno. Tanto el entorno original como el modificado son instancias de `py_environment.PyEnvironment`, y se pueden encadenar varios envoltorios.\n",
        "\n",
        "Algunos de los envoltorios comunes se pueden encontrar en `environments/wrappers.py`. Por ejemplo:\n",
        "\n",
        "1. `ActionDiscretizeWrapper`: convierte un espacio de acción continuo en un espacio de acción discreto.\n",
        "2. `RunStats`: captura las estadísticas de ejecución del entorno, como el número de pasos dados, el número de episodios completados, etc.\n",
        "3. `TimeLimit`: finaliza el episodio tras un número fijo de pasos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8aIybRdnFfb"
      },
      "source": [
        "#### Ejemplo 1: Action Discretize Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIaxJRUpvfyc"
      },
      "source": [
        "InvertedPendulum es un entorno PyBullet que acepa acciones continuas en el intervalo `[-2, 2]`. Si queremos entrenar un agente de acción discreto como DQN en este entorno, debemos discretizar (cuantificar) el espacio de acción. Esto es exactamente lo que hace `ActionDiscretizeWrapper`. Compare `action_spec` antes y después de aplicar el envoltorio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJxEoZ4HoyjR"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('Pendulum-v1')\n",
        "print('Action Spec:', env.action_spec())\n",
        "\n",
        "discrete_action_env = wrappers.ActionDiscretizeWrapper(env, num_actions=5)\n",
        "print('Discretized Action Spec:', discrete_action_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njFjW8bmwTWJ"
      },
      "source": [
        "`discrete_action_env` envuelto es una instancia de `py_environment.PyEnvironment` y se puede tratar como un entorno de Python regular.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l5dwAhsP_F_"
      },
      "source": [
        "## Entornos de TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZG39AjBkTjr"
      },
      "source": [
        "La interfaz para entornos TF se define en `environments/tf_environment.TFEnvironment` y tiene un aspecto muy similar al de los entornos Python. Los entornos TF son distintos a los entornos Python en algunos sentidos:\n",
        "\n",
        "- Generan objetos tensoriales en lugar de arreglos.\n",
        "- Los entornos TF agregan una dimensión de lote a los tensores generados cuando se comparan con las especificaciones.\n",
        "\n",
        "Convertir entornos Python en entornos TF le permite a tensorflow paralelizar las operaciones. Por ejemplo, podríamos definir una `collect_experience_op` que recopile datos del entorno y los agregue a un `replay_buffer`, y una `train_op` que lea ese `replay_buffer` y entrene al agente, y ejecutarlas en paralelo con total naturalidad en TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKBDDZqKTxsL"
      },
      "outputs": [],
      "source": [
        "class TFEnvironment(object):\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    \"\"\"Describes the `TimeStep` tensors returned by `step()`.\"\"\"\n",
        "\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Defines the `TensorSpec` of observations provided by the environment.\"\"\"\n",
        "\n",
        "  def action_spec(self):\n",
        "    \"\"\"Describes the TensorSpecs of the action expected by `step(action)`.\"\"\"\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
        "    return self._reset()\n",
        "\n",
        "  def current_time_step(self):\n",
        "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
        "    return self._current_time_step()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\"\n",
        "    return self._step(action)\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _reset(self):\n",
        "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _current_time_step(self):\n",
        "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _step(self, action):\n",
        "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFkBIA92ThWf"
      },
      "source": [
        "El método `current_time_step()` devuelve el time_step actual e inicializa el entorno según sea necesario.\n",
        "\n",
        "El método `reset()` fuerza un restablecimiento del entorno y devuelve el current_step.\n",
        "\n",
        "Si la `action` no depende del `time_step` se requiere una `tf.control_dependency` en el modo `Graph`.\n",
        "\n",
        "Por el momento, veamos cómo se crean los `TFEnvironments`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6wS3AaLdVLT"
      },
      "source": [
        "### Cómo crear su propio entorno de TensorFlow\n",
        "\n",
        "Esto es más complicado que crear entornos en Python, así que no lo trataremos en este colab. [Aquí](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/tf_environment_test.py) puede ver un ejemplo. El caso de uso más común es implementar un entorno en Python y usar el envoltorio `TFPyEnvironment` para envolverlo en TensorFlow (se explica a continuación)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Ny2lb-dU5R"
      },
      "source": [
        "### Cómo envolver un entorno Python en TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv4-UcurZ8nb"
      },
      "source": [
        "Podemos envolver fácilmente un entorno Python en un entorno TensorFlow gracias a este envoltorio `TFPyEnvironment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYerqyNfnVRL"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
        "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
        "print(\"Action Specs:\", tf_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3WFrnX9CNpC"
      },
      "source": [
        "Observe que ahora son especificaciones de tipo: `(Bounded)TensorSpec`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQPvC1ARYALj"
      },
      "source": [
        "### Ejemplos de uso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov7EIrk8dKUU"
      },
      "source": [
        "#### Ejemplo simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdvFqUqbdB7u"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "# reset() creates the initial time_step after resetting the environment.\n",
        "time_step = tf_env.reset()\n",
        "num_steps = 3\n",
        "transitions = []\n",
        "reward = 0\n",
        "for i in range(num_steps):\n",
        "  action = tf.constant([i % 2])\n",
        "  # applies the action and returns the new TimeStep.\n",
        "  next_time_step = tf_env.step(action)\n",
        "  transitions.append([time_step, action, next_time_step])\n",
        "  reward += next_time_step.reward\n",
        "  time_step = next_time_step\n",
        "\n",
        "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
        "print('\\n'.join(map(str, np_transitions)))\n",
        "print('Total reward:', reward.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWs48LNsdLnc"
      },
      "source": [
        "#### Episodios completos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t561kUXMk-KM"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "time_step = tf_env.reset()\n",
        "rewards = []\n",
        "steps = []\n",
        "num_episodes = 5\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
        "    time_step = tf_env.step(action)\n",
        "    episode_steps += 1\n",
        "    episode_reward += time_step.reward.numpy()\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps)\n",
        "  time_step = tf_env.reset()\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)\n",
        "\n",
        "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
        "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2_environments_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
