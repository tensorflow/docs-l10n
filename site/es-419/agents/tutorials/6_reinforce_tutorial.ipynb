{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNtBC6Bbb1YU"
      },
      "source": [
        "# Agente REINFORCE\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/6_reinforce_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/6_reinforce_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver código fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/agents/tutorials/6_reinforce_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOUOQOrFs3zn"
      },
      "source": [
        "## Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "En este ejemplo se muestra cómo usar la biblioteca de TF-Agents para entrenar un agente [REINFORCE](https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf) en el entorno Cartpole. Esto es similar al [tutorial de DQN](1_dqn_tutorial.ipynb).\n",
        "\n",
        "![Entorno Cartpole](images/cartpole.png)\n",
        "\n",
        "Le ayudaremos a familiarizarse con todos los componentes de un proceso de Aprendizaje por Refuerzo (RL) para el entrenamiento, la evaluación y la recopilación de datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5PNmEzIb9t4"
      },
      "source": [
        "Si todavía no ha instalado las siguientes dependencias, ejecute estos comandos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet xvfbwrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.reinforce import reinforce_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "outputs": [],
      "source": [
        "env_name = \"CartPole-v0\" # @param {type:\"string\"}\n",
        "num_iterations = 250 # @param {type:\"integer\"}\n",
        "collect_episodes_per_iteration = 2 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 2000 # @param {type:\"integer\"}\n",
        "\n",
        "fc_layer_params = (100,)\n",
        "\n",
        "learning_rate = 1e-3 # @param {type:\"number\"}\n",
        "log_interval = 25 # @param {type:\"integer\"}\n",
        "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
        "eval_interval = 50 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Entorno\n",
        "\n",
        "Los entornos en RL representan la tarea o problema que tratamos de resolver. Los entornos estándar se pueden crear fácilmente en TF-Agents utilizando `suites`. Hay diferentes `suites` para cargar entornos desde fuentes como OpenAI Gym, Atari, DM Control, etc., dado un nombre de entorno de cadena.\n",
        "\n",
        "Ahora, carguemos el entorno CartPole desde el paquete de OpenAI Gym."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYEz-S9gEv2-"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "Podemos renderizar este entorno para ver cómo luce. Un poste que oscila libremente está unido a un carro. El objetivo es mover el carro a la derecha o a la izquierda para lograr que el poste siga apuntando hacia arriba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9_lskPOey18"
      },
      "source": [
        "La instrucción `time_step = environment.step(action)` ejecuta una `action` en el entorno.  La tupla `TimeStep` que devuelve contiene la siguiente observación del entorno y una recompensa para esa acción. Los métodos `time_step_spec()` y `action_spec()` en el entorno devuelven las especificaciones (tipos, formas, límites) de `time_step` y `action` respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "outputs": [],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJCgJnx3g0yY"
      },
      "source": [
        "Entonces, vemos que la observación es un arreglo de 4 flotantes: la posición y velocidad del carro, y la posición angular y velocidad del poste. Dado que solo se pueden realizar dos acciones (mover a la izquierda o mover a la derecha), la `action_spec` es un escalar donde 0 significa \"mover a la izquierda\" y 1 significa \"mover a la derecha\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2UGR5t_iZX-"
      },
      "outputs": [],
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUqXAVmecTU"
      },
      "source": [
        "Normalmente creamos dos entornos: uno para el entrenamiento y otro para la evaluación. La mayoría de los entornos están escritos en python puro, pero se pueden convertir fácilmente a TensorFlow con la ayuda del envoltorio `TFPyEnvironment`. La API del entorno original utiliza arreglos numpy, el `TFPyEnvironment` los convierte a/desde `Tensors` para que sea más fácil interactuar con las políticas y agentes de TensorFlow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "outputs": [],
      "source": [
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agente\n",
        "\n",
        "El algoritmo que usamos para resolver un problema de RL se representa por un `Agent`. Además del agente REINFORCE, TF-Agents ofrece implementaciones estándar de una variedad de `Agents` como [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf), [DDPG](https://arxiv.org/pdf/1509.02971.pdf), [TD3](https://arxiv.org/pdf/1802.09477.pdf), [PPO](https://arxiv.org/abs/1707.06347) y [SAC](https://arxiv.org/abs/1801.01290).\n",
        "\n",
        "Para crear un agente REINFORCE, lo primero que necesitamos es una `Actor Network` que pueda aprender a predecir la acción a partir de una observación del entorno.\n",
        "\n",
        "Podemos crear fácilmente una `Actor Network` con ayuda de las especificaciones de las observaciones y las acciones. Podemos especificar las capas de la red que, en este ejemplo, son el argumento `fc_layer_params` establecido en una tupla de `ints` que representa los tamaños de cada capa oculta (Consulte la sección Hiperparámetros que figura más arriba).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "También necesitamos un `optimizer` para entrenar la red que acabamos de crear y una variable `train_step_counter` para hacer un seguimiento de la cantidad de veces que se actualizó la red.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "tf_agent = reinforce_agent.ReinforceAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    actor_network=actor_net,\n",
        "    optimizer=optimizer,\n",
        "    normalize_returns=True,\n",
        "    train_step_counter=train_step_counter)\n",
        "tf_agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Políticas\n",
        "\n",
        "En TF-Agents, las políticas representan la noción estándar de políticas en RL: dado que un `time_step` produce una acción o una distribución de acciones. El método principal es `policy_step = policy.action(time_step)` donde `policy_step` es una tupla nombrada `PolicyStep(action, state, info)`. `policy_step.action` es la `action` que se aplicará al entorno, `state` representa el estado de las políticas de estado (RNN), mientras que `info` podría contener información auxiliar como probabilidades logarítmicas de las acciones.\n",
        "\n",
        "Los agentes contienen dos políticas: la política principal que se usa para la evaluación o la implementación (agent.policy) y otra política que se usa para la recopilación de datos (agent.collect_policy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "outputs": [],
      "source": [
        "eval_policy = tf_agent.policy\n",
        "collect_policy = tf_agent.collect_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Métricas y evaluación\n",
        "\n",
        "La métrica más común que se usa para evaluar una política es el rendimiento medio. El rendimiento es la suma de las recompensas obtenidas al ejecutar una política en un entorno durante algunos episodios.  Podemos calcular la métrica de rendimiento medio de la siguiente manera.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# Please also see the metrics module for standard implementations of different\n",
        "# metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Búfer de repetición\n",
        "\n",
        "Para hacer un seguimiento de los datos recogidos del entorno, se usará [Reverb](https://deepmind.com/research/open-source/Reverb), un sistema de repetición eficiente, extensible y fácil de usar de Deepmind. Este sistema almacena los datos de experiencia cuando recogemos trayectorias y se consume durante el entrenamiento.\n",
        "\n",
        "Este búfer de repetición se construye a partir de especificaciones que describen los tensores que se van a almacenar, y que se pueden obtener del agente si se usa `tf_agent.collect_data_spec`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      tf_agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "      replay_buffer_signature)\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=None,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddEpisodeObserver(\n",
        "    replay_buffer.py_client,\n",
        "    table_name,\n",
        "    replay_buffer_capacity\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "Para la mayoría de los agentes, `collect_data_spec` es una tupla nombrada `Trajectory` que contiene la observación, la acción, la recompensa, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Recopilación de datos\n",
        "\n",
        "Como REINFORCE aprende de episodios completos, definimos una función para recopilar un episodio mediante la política de recopilación de datos dada y guardar los datos (observaciones, acciones, recompensas, etc.) como trayectorias en el búfer de repetición. Aquí se utiliza 'PyDriver' para ejecutar el bucle de recopilación de experiencias. Puede obtener más información sobre el controlador TF Agents en nuestro [tutorial sobre controladores](https://www.tensorflow.org/agents/tutorials/4_drivers_tutorial)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr1KSAEGG4h9"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "def collect_episode(environment, policy, num_episodes):\n",
        "\n",
        "  driver = py_driver.PyDriver(\n",
        "    environment,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_episodes=num_episodes)\n",
        "  initial_time_step = environment.reset()\n",
        "  driver.run(initial_time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Entrenamiento del agente\n",
        "\n",
        "El bucle de entrenamiento implica tanto la recopilación de datos del entorno como la optimización de las redes del agente. A lo largo del proceso, evaluaremos de vez en cuando la política del agente para ver nuestro rendimiento.\n",
        "\n",
        "Lo que sigue tardará unos ~3 minutos en ejecutarse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "tf_agent.train = common.function(tf_agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few episodes using collect_policy and save to the replay buffer.\n",
        "  collect_episode(\n",
        "      train_py_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
        "\n",
        "  # Use data from the buffer and update the agent's network.\n",
        "  iterator = iter(replay_buffer.as_dataset(sample_batch_size=1))\n",
        "  trajectories, _ = next(iterator)\n",
        "  train_loss = tf_agent.train(experience=trajectories)  \n",
        "\n",
        "  replay_buffer.clear()\n",
        "\n",
        "  step = tf_agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualización\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Gráficos\n",
        "\n",
        "Podemos representar gráficamente una comparativa entre el rendimiento y los pasos globales para comprobar el rendimiento de nuestro agente. `Cartpole-v0`, el entorno da una recompensa de +1 por cada paso de tiempo que el poste se mantiene erguido, y como la cantidad máxima de pasos es 200, el máximo rendimiento posible también es 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxtL1mbOYCVO"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim(top=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "Resulta útil visualizar el funcionamiento de un agente mediante el renderizado del entorno en cada paso. Pero antes de hacerlo, creemos una función para insertar videos en este colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "El siguiente código visualiza la política del agente para algunos episodios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owOVWB158NlF"
      },
      "outputs": [],
      "source": [
        "num_episodes = 3\n",
        "video_filename = 'imageio.mp4'\n",
        "with imageio.get_writer(video_filename, fps=60) as video:\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = eval_env.reset()\n",
        "    video.append_data(eval_py_env.render())\n",
        "    while not time_step.is_last():\n",
        "      action_step = tf_agent.policy.action(time_step)\n",
        "      time_step = eval_env.step(action_step.action)\n",
        "      video.append_data(eval_py_env.render())\n",
        "\n",
        "embed_mp4(video_filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "6_reinforce_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
