{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beObUOFyuRjT"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eutDVTs9aJEL"
      },
      "source": [
        "# Búferes de repetición\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/5_replay_buffers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/5_replay_buffers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/agents/tutorials/5_replay_buffers_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aPHF9kXFggA"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Los algoritmos de aprendizaje por refuerzo utilizan búferes de repetición para almacenar trayectorias de experiencia al ejecutar una política en un entorno. Durante el entrenamiento, se solicita a los búferes de repetición un subconjunto de las trayectorias (ya sea un subconjunto secuencial o una muestra) para \"repetir\" la experiencia del agente.\n",
        "\n",
        "En esta colab, exploramos dos tipos de búferes de repetición: respaldados por Python y respaldados por Tensorflow, que comparten una API común. En las siguientes secciones, describimos la API, cada una de las implementaciones del búfer y cómo usarlas durante el entrenamiento de recopilación de datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uSlqYgvaG9b"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GztmUpWKZ7kq"
      },
      "source": [
        "Si todavía no lo ha hecho, instale tf-agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnE2CgilrngG"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whYNP894FSkA"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents import specs\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQWclL9FpZl"
      },
      "source": [
        "## API de búfer de repetición\n",
        "\n",
        "La clase Replay Buffer tiene la siguiente definición y métodos:\n",
        "\n",
        "```python\n",
        "class ReplayBuffer(tf.Module):\n",
        "  \"\"\"Abstract base class for TF-Agents replay buffer.\"\"\"\n",
        "\n",
        "  def __init__(self, data_spec, capacity):\n",
        "    \"\"\"Initializes the replay buffer.\n",
        "\n",
        "    Args:\n",
        "      data_spec: A spec or a list/tuple/nest of specs describing\n",
        "        a single item that can be stored in this buffer\n",
        "      capacity: number of elements that the replay buffer can hold.\n",
        "    \"\"\"\n",
        "\n",
        "  @property\n",
        "  def data_spec(self):\n",
        "    \"\"\"Returns the spec for items in the replay buffer.\"\"\"\n",
        "\n",
        "  @property\n",
        "  def capacity(self):\n",
        "    \"\"\"Returns the capacity of the replay buffer.\"\"\"\n",
        "\n",
        "  def add_batch(self, items):\n",
        "    \"\"\"Adds a batch of items to the replay buffer.\"\"\"\n",
        "\n",
        "  def get_next(self,\n",
        "               sample_batch_size=None,\n",
        "               num_steps=None,\n",
        "               time_stacked=True):\n",
        "    \"\"\"Returns an item or batch of items from the buffer.\"\"\"\n",
        "\n",
        "  def as_dataset(self,\n",
        "                 sample_batch_size=None,\n",
        "                 num_steps=None,\n",
        "                 num_parallel_calls=None):\n",
        "    \"\"\"Creates and returns a dataset that returns entries from the buffer.\"\"\"\n",
        "\n",
        "\n",
        "  def gather_all(self):\n",
        "    \"\"\"Returns all the items in buffer.\"\"\"\n",
        "    return self._gather_all()\n",
        "\n",
        "  def clear(self):\n",
        "    \"\"\"Resets the contents of replay buffer\"\"\"\n",
        "\n",
        "```\n",
        "\n",
        "Tenga en cuenta que cuando se inicializa el objeto búfer de repetición, solicita la `data_spec` de los elementos que almacenará. Esta especificación corresponde a la `TensorSpec` de los elementos de trayectoria que se agregarán al búfer. Esta especificación se adquiere normalmente a partir de la `agent.collect_data_spec` de un agente que define las formas, tipos y estructuras que espera el agente durante el entrenamiento (esto se desarrolla mejor más adelante)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Yrxg36Ik1x"
      },
      "source": [
        "## TFUniformReplayBuffer\n",
        "\n",
        "`TFUniformReplayBuffer` es el búfer de repetición más usado en TF-Agents, por lo que lo utilizaremos en este tutorial. En `TFUniformReplayBuffer` el almacenamiento del búfer de respaldo se consigue mediante variables tensorflow y, por tanto, forma parte del gráfico de cálculo.\n",
        "\n",
        "El búfer almacena lotes de elementos y tiene una capacidad máxima de elementos `max_length` por segmento de lote. Por lo tanto, la capacidad total del búfer es `batch_size` x elementos `max_length`. Todos los elementos almacenados en el búfer deben tener una especificación de datos correspondiente. Cuando el búfer de repetición se utiliza para la recopilación de datos, la especificación es la especificación de recopilación de datos del agente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYk-bn2taXlw"
      },
      "source": [
        "### Cómo crear el búfer:\n",
        "\n",
        "Para crear un `TFUniformReplayBuffer` pasamos lo siguiente:\n",
        "\n",
        "1. la especificación de los elementos de datos que almacenará el búfer\n",
        "2. el `batch size` correspondiente al tamaño de lote del búfer\n",
        "3. el número de elementos `max_length` por segmento de lote\n",
        "\n",
        "A continuación, se muestra un ejemplo de creación de un `TFUniformReplayBuffer` con especificaciones de datos de muestra, `batch_size` 32 y `max_length` 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj4_-77_5ExP"
      },
      "outputs": [],
      "source": [
        "data_spec =  (\n",
        "        tf.TensorSpec([3], tf.float32, 'action'),\n",
        "        (\n",
        "            tf.TensorSpec([5], tf.float32, 'lidar'),\n",
        "            tf.TensorSpec([3, 2], tf.float32, 'camera')\n",
        "        )\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "max_length = 1000\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec,\n",
        "    batch_size=batch_size,\n",
        "    max_length=max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB8rOw5ATDD2"
      },
      "source": [
        "### Cómo escribir en el búfer:\n",
        "\n",
        "Para agregar elementos al búfer de repetición, se usa el método `add_batch(items)` donde `items` es una lista, tupla o nido de tensores que representa el lote de elementos que se agregarán al búfer. Cada elemento de `items` debe tener una dimensión exterior igual a `batch_size` y las dimensiones restantes deben ajustarse a las especificaciones de datos del elemento (iguales a las especificaciones de datos pasadas al constructor del búfer de repetición).\n",
        "\n",
        "Este es un ejemplo de cómo agregar un lote de elementos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOvkp4vJhBOT"
      },
      "outputs": [],
      "source": [
        "action = tf.constant(1 * np.ones(\n",
        "    data_spec[0].shape.as_list(), dtype=np.float32))\n",
        "lidar = tf.constant(\n",
        "    2 * np.ones(data_spec[1][0].shape.as_list(), dtype=np.float32))\n",
        "camera = tf.constant(\n",
        "    3 * np.ones(data_spec[1][1].shape.as_list(), dtype=np.float32))\n",
        "  \n",
        "values = (action, (lidar, camera))\n",
        "values_batched = tf.nest.map_structure(lambda t: tf.stack([t] * batch_size),\n",
        "                                       values)\n",
        "  \n",
        "replay_buffer.add_batch(values_batched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smnVAxHghKly"
      },
      "source": [
        "### Cómo leer desde el búfer\n",
        "\n",
        "Hay tres formas de leer datos de `TFUniformReplayBuffer`:\n",
        "\n",
        "1. `get_next()`: devuelve una muestra del búfer. El tamaño del lote de muestra y el número de pasos de tiempo devueltos se pueden especificar mediante argumentos de este método.\n",
        "2. `as_dataset()`: devuelve el búfer de repetición como `tf.data.Dataset`. Luego, se puede crear un iterador de conjunto de datos e iterar a través de las muestras de los elementos en el búfer.\n",
        "3. `gather_all()`: devuelve todos los elementos del búfer como un tensor con forma `[batch, time, data_spec]`\n",
        "\n",
        "En el siguiente bloque de código se muestran ejemplos de cómo usar cada uno de estos métodos para leer desde el búfer de repetición:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlQ1eGhohM3M"
      },
      "outputs": [],
      "source": [
        "# add more items to the buffer before reading\n",
        "for _ in range(5):\n",
        "  replay_buffer.add_batch(values_batched)\n",
        "\n",
        "# Get one sample from the replay buffer with batch size 10 and 1 timestep:\n",
        "\n",
        "sample = replay_buffer.get_next(sample_batch_size=10, num_steps=1)\n",
        "\n",
        "# Convert the replay buffer to a tf.data.Dataset and iterate through it\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=4,\n",
        "    num_steps=2)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "print(\"Iterator trajectories:\")\n",
        "trajectories = []\n",
        "for _ in range(3):\n",
        "  t, _ = next(iterator)\n",
        "  trajectories.append(t)\n",
        "  \n",
        "print(tf.nest.map_structure(lambda t: t.shape, trajectories))\n",
        "\n",
        "# Read all elements in the replay buffer:\n",
        "trajectories = replay_buffer.gather_all()\n",
        "\n",
        "print(\"Trajectories from gather all:\")\n",
        "print(tf.nest.map_structure(lambda t: t.shape, trajectories))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcS49HrNF34W"
      },
      "source": [
        "## PyUniformReplayBuffer\n",
        "\n",
        "`PyUniformReplayBuffer` tiene la misma funcionalidad que `TFUniformReplayBuffer` pero en lugar de variables tf, sus datos se almacenan en arreglos numpy. Este búfer se puede utilizar para la recopilación de datos fuera del gráfico. Tener el almacenamiento de respaldo en numpy puede facilitar la manipulación de datos por parte de algunas aplicaciones (como la indexación para actualizar prioridades) sin usar variables de Tensorflow. Sin embargo, esta implementación no tendrá el beneficio de las optimizaciones de gráficos con Tensorflow.\n",
        "\n",
        "A continuación, se muestra un ejemplo de cómo crear una instancia de `PyUniformReplayBuffer` a partir de las especificaciones de trayectoria de la política del agente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4neLPpL25wI"
      },
      "outputs": [],
      "source": [
        "replay_buffer_capacity = 1000*32 # same capacity as the TFUniformReplayBuffer\n",
        "\n",
        "py_replay_buffer = py_uniform_replay_buffer.PyUniformReplayBuffer(\n",
        "    capacity=replay_buffer_capacity,\n",
        "    data_spec=tensor_spec.to_nest_array_spec(data_spec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V7DEcB8IeiQ"
      },
      "source": [
        "## Cómo usar los búferes de repetición durante un entrenamiento\n",
        "\n",
        "Ahora que sabemos cómo crear un búfer de repetición, escribir elementos en él y leerlos, podemos usarlo para almacenar trayectorias durante el entrenamiento de nuestros agentes.\n",
        "\n",
        "### Recopilación de datos\n",
        "\n",
        "En primer lugar, veamos cómo se usa el búfer de repetición durante la recopilación de datos.\n",
        "\n",
        "En TF-Agents, usamos un `Driver` (consulte el tutorial de controladores para obtener más información) para recopilar experiencia en un entorno. Para usar un `Driver`, especificamos un `Observer` que es una función que el `Driver` debe ejecutar cuando recibe una trayectoria.\n",
        "\n",
        "Por lo tanto, para agregar elementos de trayectoria al búfer de repetición, agregamos un observador que llama `add_batch(items)` para agregar un lote de elementos en el búfer.\n",
        "\n",
        "Veamos un ejemplo de esto con `TFUniformReplayBuffer`. En primer lugar, se crean un entorno, una red y un agente. Luego, se crea un `TFUniformReplayBuffer`. Tenga en cuenta que las especificaciones de los elementos de trayectoria en el búfer de repetición son iguales a las especificaciones de recopilación de datos del agente. Luego, se configura su método `add_batch` como observador del controlador que recopilará los datos durante el entrenamiento:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCbTDO3Z5UCS"
      },
      "outputs": [],
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    tf_env.time_step_spec().observation,\n",
        "    tf_env.action_spec(),\n",
        "    fc_layer_params=(100,))\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=tf.compat.v1.train.AdamOptimizer(0.001))\n",
        "\n",
        "replay_buffer_capacity = 1000\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    batch_size=tf_env.batch_size,\n",
        "    max_length=replay_buffer_capacity)\n",
        "\n",
        "# Add an observer that adds to the replay buffer:\n",
        "replay_observer = [replay_buffer.add_batch]\n",
        "\n",
        "collect_steps_per_iteration = 10\n",
        "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
        "  tf_env,\n",
        "  agent.collect_policy,\n",
        "  observers=replay_observer,\n",
        "  num_steps=collect_steps_per_iteration).run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huGCDbO4GAF1"
      },
      "source": [
        "### Cómo leer los datos para un paso de entrenamiento\n",
        "\n",
        "Tras agregar elementos de trayectoria al búfer de repetición, podemos leer lotes de trayectorias desde el búfer de repetición para usarlos como datos de entrada para un paso de entrenamiento.\n",
        "\n",
        "Aquí se muestra un ejemplo de cómo entrenar trayectorias desde el búfer de repetición en un bucle de entrenamiento: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg8SUyXXnSMr"
      },
      "outputs": [],
      "source": [
        "# Read the replay buffer as a Dataset,\n",
        "# read batches of 4 elements, each with 2 timesteps:\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=4,\n",
        "    num_steps=2)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "\n",
        "num_train_steps = 10\n",
        "\n",
        "for _ in range(num_train_steps):\n",
        "  trajectories, _ = next(iterator)\n",
        "  loss = agent.train(experience=trajectories)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "5_replay_buffers_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
