{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqslkUeyEJFg"
      },
      "source": [
        "# Tutorial sobre bandidos multibrazo en TF-Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MimUC9NrYFaS"
      },
      "source": [
        "### Introducción\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/bandits_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/bandits_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver código fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/agents/tutorials/bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a>\n",
        "</td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "### Preparación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "Si todavía no ha instalado las siguientes dependencias, ejecute estos comandos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7gLdUS6b2EG"
      },
      "source": [
        "### Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oCS94Z83Jo2"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "nest = tf.nest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcIob6rYqien"
      },
      "source": [
        "# Introducción\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdnTJrzaeft3"
      },
      "source": [
        "El problema de los bandidos multibrazo (MAB, por sus siglas en inglés) es un caso especial de Aprendizaje por Refuerzo: un agente recopila recompensas en un entorno a través de la ejecución de ciertas acciones tras observar parte del estado del entorno. La principal diferencia entre el RL general y los MAB es que, en los MAB, asumimos que la acción ejecutada por el agente no influye en el siguiente estado del entorno. Por lo tanto, los agentes no modelan las transiciones de estado, no acreditan recompensas a las acciones pasadas ni \"planifican con antelación\" para llegar a estados con abundantes recompensas.\n",
        "\n",
        "Como en otros dominios del RL, el objetivo de un *agente* del MAB es el de encontrar una *política* que recopile la mayor recompensa posible. Sin embargo, sería un error que siempre se intente explotar la acción que promete la mayor recompensa, porque entonces podríamos perder de vista mejores acciones por no explorar lo suficiente. Este es el problema más importante que debemos resolver en (MAB), a menudo conocido como la *paradoja exploración-explotación*.\n",
        "\n",
        "Los entornos, las políticas y los agentes de bandidos para MAB se pueden encontrar en los subdirectorios de [tf_agents/bandits](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPzsBCTperx3"
      },
      "source": [
        "# Entornos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LOXW8i320Cp"
      },
      "source": [
        "En TF-Agents, la clase de entorno cumple la función de dar información sobre el estado actual (esto se llama **observación** o **contexto**), recibir una acción como entrada, realizar una transición de estado y generar una recompensa. Esta clase también se encarga del reinicio cuando un episodio termina, para que un nuevo episodio pueda comenzar. Esto se consigue llamando a una función `reset` cuando un estado se etiqueta como \"último\" del episodio.\n",
        "\n",
        "Para obtener más información, consulte el [tutorial sobre entornos de TF-Agents](https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb).\n",
        "\n",
        "Como se mencionó anteriormente, MAB es diferente del RL general en el sentido de que las acciones no influyen sobre la siguiente observación. Otra diferencia es que en Bandits no hay \"episodios\": cada paso de tiempo se inicia con una nueva observación, independientemente de los pasos de tiempo anteriores.\n",
        "\n",
        "Para asegurarnos de que las observaciones sean independientes y para abstraer el concepto de episodios de RL, introducimos subclases de `PyEnvironment` y `TFEnvironment`: [BanditPyEnvironment](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/environments/bandit_py_environment.py) y [BanditTFEnvironment](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/environments/bandit_tf_environment.py). Estas clases exponen dos funciones miembro privadas que deben ser implementadas por el usuario:\n",
        "\n",
        "```python\n",
        "@abc.abstractmethod\n",
        "def _observe(self):\n",
        "```\n",
        "\n",
        "y\n",
        "\n",
        "```python\n",
        "@abc.abstractmethod\n",
        "def _apply_action(self, action):\n",
        "```\n",
        "\n",
        "La función `_observe` devuelve una observación. Luego, la política elige una acción en función de esta observación. `_apply_action` recibe esa acción como entrada y devuelve la recompensa correspondiente. Estas funciones miembro privadas son llamadas por las funciones `reset` y `step`, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTaG2ZapQvHX"
      },
      "outputs": [],
      "source": [
        "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, observation_spec, action_spec):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    super(BanditPyEnvironment, self).__init__()\n",
        "\n",
        "  # Helper functions.\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _empty_observation(self):\n",
        "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
        "                                 self.observation_spec())\n",
        "\n",
        "  # These two functions below should not be overridden by subclasses.\n",
        "  def _reset(self):\n",
        "    \"\"\"Returns a time step containing an observation.\"\"\"\n",
        "    return ts.restart(self._observe(), batch_size=self.batch_size)\n",
        "\n",
        "  def _step(self, action):\n",
        "    \"\"\"Returns a time step containing the reward for the action taken.\"\"\"\n",
        "    reward = self._apply_action(action)\n",
        "    return ts.termination(self._observe(), reward)\n",
        "\n",
        "  # These two functions below are to be implemented in subclasses.\n",
        "  @abc.abstractmethod\n",
        "  def _observe(self):\n",
        "    \"\"\"Returns an observation.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _apply_action(self, action):\n",
        "    \"\"\"Applies `action` to the Environment and returns the corresponding reward.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVtLk28xVo0j"
      },
      "source": [
        "La clase abstracta provisional que acabamos de describir implementa las funciones `_reset` y `_step` de `PyEnvironment` y expone las funciones abstractas `_observe` y `_apply_action` para que las implementen las subclases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQbI-6PdtSJn"
      },
      "source": [
        "## Una clase de entorno de ejemplo simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qspwAx0tS6l"
      },
      "source": [
        "La clase que se presenta a continuación ofrece un entorno muy simple en el que la observación es un número entero aleatorio comprendido entre -2 y 2, existen 3 acciones posibles (0, 1, 2) y la recompensa es el producto de la acción y la observación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV6DhsSi227-"
      },
      "outputs": [],
      "source": [
        "class SimplePyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
        "    super(SimplePyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return action * self._observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipEQgYDIf55t"
      },
      "source": [
        "Ahora podemos utilizar este entorno para obtener observaciones y recibir recompensas por nuestras acciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo_uwSz2gAKX"
      },
      "outputs": [],
      "source": [
        "environment = SimplePyEnvironment()\n",
        "observation = environment.reset().observation\n",
        "print(\"observation: %d\" % observation)\n",
        "\n",
        "action = 2 #@param\n",
        "\n",
        "print(\"action: %d\" % action)\n",
        "reward = environment.step(action).reward\n",
        "print(\"reward: %f\" % reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuVYHI8aDgCx"
      },
      "source": [
        "## Entornos de TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP46VwLTDnOR"
      },
      "source": [
        "Se puede definir un entorno bandido al subclasificar `BanditTFEnvironment` o, de manera similar a los entornos de RL, se puede definir un `BanditPyEnvironment` y envolverlo con `TFPyEnvironment`. Para simplificar, en este tutorial optaremos por esta última opción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPPpwSi3EtWz"
      },
      "outputs": [],
      "source": [
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S9fhxF9GUaT"
      },
      "source": [
        "# Políticas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbTt5jnuGlYj"
      },
      "source": [
        "En un problema de bandidos, las *políticas* funcionan igual que en un problema de RL: ofrecen una acción (o una distribución de acciones), a partir de una observación que funciona como entrada.\n",
        "\n",
        "Para obtener más información, consulte el [tutorial sobre políticas de TF-Agents](https://github.com/tensorflow/agents/blob/master/docs/tutorials/3_policies_tutorial.ipynb).\n",
        "\n",
        "Del mismo modo que ocurre con los entornos, hay dos formas de construir una política: se puede crear una `PyPolicy` y envolverla con `TFPyPolicy`, o crear directamente una `TFPy`. Aquí optamos por el método directo.\n",
        "\n",
        "Como este ejemplo es bastante sencillo, podemos definir la política óptima manualmente. La acción solo depende del signo de la observación, 0 cuando es negativa y 2 cuando es positiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpMZlplNK5ND"
      },
      "outputs": [],
      "source": [
        "class SignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "    super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return ()\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
        "    action = observation_sign + 1\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAM7hb4LVQ70"
      },
      "source": [
        "Ahora podemos solicitar una observación del entorno, llamar a la política para que elija una acción y, entonces, el entorno emitirá la recompensa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0_5vMDCVZWT"
      },
      "outputs": [],
      "source": [
        "sign_policy = SignPolicy()\n",
        "\n",
        "current_time_step = tf_environment.reset()\n",
        "print('Observation:')\n",
        "print (current_time_step.observation)\n",
        "action = sign_policy.action(current_time_step).action\n",
        "print('Action:')\n",
        "print (action)\n",
        "reward = tf_environment.step(action).reward\n",
        "print('Reward:')\n",
        "print(reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AExuQ7u0-PF6"
      },
      "source": [
        "La forma en que se implementan los entornos bandidos garantiza que cada vez que damos un paso, recibimos tanto la recompensa por la acción efectuada como la siguiente observación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiB935of-wVv"
      },
      "outputs": [],
      "source": [
        "step = tf_environment.reset()\n",
        "action = 1\n",
        "next_step = tf_environment.step(action)\n",
        "reward = next_step.reward\n",
        "next_observation = next_step.observation\n",
        "print(\"Reward: \")\n",
        "print(reward)\n",
        "print(\"Next observation:\")\n",
        "print(next_observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFnqVHfeANZP"
      },
      "source": [
        "# Agentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pDK_faXAPSA"
      },
      "source": [
        "Ahora que ya explicamos los entornos bandidos y las políticas bandidas, debemos definir también los agentes bandidos, que se encargan de cambiar la política en función de muestras de entrenamiento.\n",
        "\n",
        "La API para los agentes bandido es similar a la de los agentes de RL: el agente solo tiene que implementar los métodos `_initialize` y `_train` y definir una `policy` y una `collect_policy`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVCb-vPJOayG"
      },
      "source": [
        "## Un entorno más complicado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ksv7i7zPGSa"
      },
      "source": [
        "Antes de que escribamos nuestro agente bandido, necesitamos contar con un entorno que sea un poco más difícil de entender. Para hacerlo un poco más interesante, el siguiente entorno siempre dará `reward = observation * action` o `reward = -observation * action`. Esto se decidirá cuando se inicialice el entorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fte7-Mr8O0QR"
      },
      "outputs": [],
      "source": [
        "class TwoWayPyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
        "\n",
        "    # Flipping the sign with probability 1/2.\n",
        "    self._reward_sign = 2 * np.random.randint(2) - 1\n",
        "    print(\"reward sign:\")\n",
        "    print(self._reward_sign)\n",
        "\n",
        "    super(TwoWayPyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return self._reward_sign * action * self._observation[0]\n",
        "\n",
        "two_way_tf_environment = tf_py_environment.TFPyEnvironment(TwoWayPyEnvironment())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zb4jWpQUA75"
      },
      "source": [
        "## Una política más complicada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz2rEEA1USJu"
      },
      "source": [
        "Un entorno más complicado requiere una política más complicada. Necesitamos una política que detecte el comportamiento del entorno subyacente. Hay tres situaciones que la política debe gestionar:\n",
        "\n",
        "1. El agente aún no ha detectado qué versión del entorno se está ejecutando.\n",
        "2. El agente detectó que se está ejecutando la versión original del entorno.\n",
        "3. El agente detectó que se está ejecutando la versión invertida del entorno.\n",
        "\n",
        "Definimos una `tf_variable` nombrada `_situation` para almacenar esta información codificada como valores en `[0, 2]`, luego hacemos que la política se comporte en consecuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srm2jsGHVM8N"
      },
      "outputs": [],
      "source": [
        "class TwoWaySignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self, situation):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "    self._situation = situation\n",
        "    super(TwoWaySignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                           action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return [self._situation]\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    sign = tf.cast(tf.sign(time_step.observation[0, 0]), dtype=tf.int32)\n",
        "    def case_unknown_fn():\n",
        "      # Choose 1 so that we get information on the sign.\n",
        "      return tf.constant(1, shape=(1,))\n",
        "\n",
        "    # Choose 0 or 2, depending on the situation and the sign of the observation.\n",
        "    def case_normal_fn():\n",
        "      return tf.constant(sign + 1, shape=(1,))\n",
        "    def case_flipped_fn():\n",
        "      return tf.constant(1 - sign, shape=(1,))\n",
        "\n",
        "    cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
        "             (tf.equal(self._situation, 1), case_normal_fn),\n",
        "             (tf.equal(self._situation, 2), case_flipped_fn)]\n",
        "    action = tf.case(cases, exclusive=True)\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6PPdRQQbE3Q"
      },
      "source": [
        "## El agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO8HpL0tUP32"
      },
      "source": [
        "Ahora debemos definir el agente que detecta el signo del entorno y establece la política adecuada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f-0W0cMbS_z"
      },
      "outputs": [],
      "source": [
        "class SignAgent(tf_agent.TFAgent):\n",
        "  def __init__(self):\n",
        "    self._situation = tf.Variable(0, dtype=tf.int32)\n",
        "    policy = TwoWaySignPolicy(self._situation)\n",
        "    time_step_spec = policy.time_step_spec\n",
        "    action_spec = policy.action_spec\n",
        "    super(SignAgent, self).__init__(time_step_spec=time_step_spec,\n",
        "                                    action_spec=action_spec,\n",
        "                                    policy=policy,\n",
        "                                    collect_policy=policy,\n",
        "                                    train_sequence_length=None)\n",
        "\n",
        "  def _initialize(self):\n",
        "    return tf.compat.v1.variables_initializer(self.variables)\n",
        "\n",
        "  def _train(self, experience, weights=None):\n",
        "    observation = experience.observation\n",
        "    action = experience.action\n",
        "    reward = experience.reward\n",
        "\n",
        "    # We only need to change the value of the situation variable if it is\n",
        "    # unknown (0) right now, and we can infer the situation only if the\n",
        "    # observation is not 0.\n",
        "    needs_action = tf.logical_and(tf.equal(self._situation, 0),\n",
        "                                  tf.not_equal(reward, 0))\n",
        "\n",
        "\n",
        "    def new_situation_fn():\n",
        "      \"\"\"This returns either 1 or 2, depending on the signs.\"\"\"\n",
        "      return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(action[0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
        "\n",
        "    new_situation = tf.cond(needs_action,\n",
        "                            new_situation_fn,\n",
        "                            lambda: self._situation)\n",
        "    new_situation = tf.cast(new_situation, tf.int32)\n",
        "    tf.compat.v1.assign(self._situation, new_situation)\n",
        "    return tf_agent.LossInfo((), ())\n",
        "\n",
        "sign_agent = SignAgent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyclF0ZZpW-f"
      },
      "source": [
        "En el código anterior, el agente define la política y el agente y la política comparten la variable `situation`.\n",
        "\n",
        "Además, el parámetro `experience` de la función `_train` corresponde a una trayectoria:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NlF228LGoiR"
      },
      "source": [
        "# Trayectorias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GbBDi1iGsnN"
      },
      "source": [
        "En TF-Agents, `trajectories` son tuplas nombradas que contienen muestras de los pasos anteriores. Luego, los agentes usan estas muestras para entrenar y actualizar la política. En el RL, las trayectorias deben contener información sobre el estado actual, el siguiente estado, y sobre si el episodio actual ha terminado. Como en el mundo bandido no necesitamos estos datos, configuramos una función ayudante para crear una trayectoria:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdSG1nv-HUJq"
      },
      "outputs": [],
      "source": [
        "# We need to add another dimension here because the agent expects the\n",
        "# trajectory of shape [batch_size, time, ...], but in this tutorial we assume\n",
        "# that both batch size and time are 1. Hence all the expand_dims.\n",
        "\n",
        "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
        "  return trajectory.Trajectory(observation=tf.expand_dims(initial_step.observation, 0),\n",
        "                               action=tf.expand_dims(action_step.action, 0),\n",
        "                               policy_info=action_step.info,\n",
        "                               reward=tf.expand_dims(final_step.reward, 0),\n",
        "                               discount=tf.expand_dims(final_step.discount, 0),\n",
        "                               step_type=tf.expand_dims(initial_step.step_type, 0),\n",
        "                               next_step_type=tf.expand_dims(final_step.step_type, 0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFEJ8kbI_e6Q"
      },
      "source": [
        "# Cómo entrenar un agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gh-41og_hDB"
      },
      "source": [
        "Ya tenemos todo lo que necesitamos para entrenar nuestro agente bandido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPx43dZgoyKg"
      },
      "outputs": [],
      "source": [
        "step = two_way_tf_environment.reset()\n",
        "for _ in range(10):\n",
        "  action_step = sign_agent.collect_policy.action(step)\n",
        "  next_step = two_way_tf_environment.step(action_step.action)\n",
        "  experience = trajectory_for_bandit(step, action_step, next_step)\n",
        "  print(experience)\n",
        "  sign_agent.train(experience)\n",
        "  step = next_step\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iVSNiYdy4U4"
      },
      "source": [
        "A partir de la salida se puede ver que después del segundo paso (a menos que la observación fuera 0 en el primer paso), la política elige la acción de forma correcta y por lo tanto la recompensa obtenida es siempre no negativa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCKyKEjOlOPE"
      },
      "source": [
        "# Un ejemplo real de bandido contextual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecnQwUpmllar"
      },
      "source": [
        "En lo que resta de este tutorial, usamos los [entornos](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/environments/) y los [agentes](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/agents/) preimplementados de la biblioteca TF-Agents Bandits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEnXUwd-nZKl"
      },
      "outputs": [],
      "source": [
        "# Imports for example.\n",
        "from tf_agents.bandits.agents import lin_ucb_agent\n",
        "from tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\n",
        "from tf_agents.bandits.metrics import tf_metrics\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37oy70dUmmie"
      },
      "source": [
        "## Entorno estocástico estacionario con funciones de recompensa lineales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euPPd8x1m7iG"
      },
      "source": [
        "El entorno que se usa en este ejemplo es [StationaryStochasticPyEnvironment](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/environments/stationary_stochastic_py_environment.py). Este entorno toma como parámetro una función (generalmente ruidosa) para ofrecer observaciones (contexto), y para cada brazo toma una función (también ruidosa) que calcula la recompensa en función de la observación dada. En nuestro ejemplo, tomamos muestras del contexto de un cubo d-dimensional uniformemente, y las funciones de recompensa son funciones lineales del contexto, más un poco de ruido gaussiano."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVa0hmQrpe6w"
      },
      "outputs": [],
      "source": [
        "batch_size = 2 # @param\n",
        "arm0_param = [-3, 0, 1, -2] # @param\n",
        "arm1_param = [1, -2, 3, 0] # @param\n",
        "arm2_param = [0, 0, 1, 1] # @param\n",
        "def context_sampling_fn(batch_size):\n",
        "  \"\"\"Contexts from [-10, 10]^4.\"\"\"\n",
        "  def _context_sampling_fn():\n",
        "    return np.random.randint(-10, 10, [batch_size, 4]).astype(np.float32)\n",
        "  return _context_sampling_fn\n",
        "\n",
        "class LinearNormalReward(object):\n",
        "  \"\"\"A class that acts as linear reward function when called.\"\"\"\n",
        "  def __init__(self, theta, sigma):\n",
        "    self.theta = theta\n",
        "    self.sigma = sigma\n",
        "  def __call__(self, x):\n",
        "    mu = np.dot(x, self.theta)\n",
        "    return np.random.normal(mu, self.sigma)\n",
        "\n",
        "arm0_reward_fn = LinearNormalReward(arm0_param, 1)\n",
        "arm1_reward_fn = LinearNormalReward(arm1_param, 1)\n",
        "arm2_reward_fn = LinearNormalReward(arm2_param, 1)\n",
        "\n",
        "environment = tf_py_environment.TFPyEnvironment(\n",
        "    sspe.StationaryStochasticPyEnvironment(\n",
        "        context_sampling_fn(batch_size),\n",
        "        [arm0_reward_fn, arm1_reward_fn, arm2_reward_fn],\n",
        "        batch_size=batch_size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haID-SPgsLyY"
      },
      "source": [
        "## El agente LinUCB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "298-1Q0bsQmR"
      },
      "source": [
        "El agente que se muestra a continuación implementa el algoritmo [LinUCB](http://rob.schapire.net/papers/www10.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4XmGgIusj-K"
      },
      "outputs": [],
      "source": [
        "observation_spec = tensor_spec.TensorSpec([4], tf.float32)\n",
        "time_step_spec = ts.time_step_spec(observation_spec)\n",
        "action_spec = tensor_spec.BoundedTensorSpec(\n",
        "    dtype=tf.int32, shape=(), minimum=0, maximum=2)\n",
        "\n",
        "agent = lin_ucb_agent.LinearUCBAgent(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eua_aC7Rt78G"
      },
      "source": [
        "## Métrica de arrepentimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBJDiJvEt-xC"
      },
      "source": [
        "La métrica más importante de Bandits es *arrepentimiento*, y se calcula como la diferencia entre la recompensa obtenida por el agente y la recompensa esperada por una política de oráculo que tenga acceso a las funciones de recompensa del entorno. Por lo tanto, [RegretMetric](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/metrics/tf_metrics.py) necesita una función *baseline_reward_fn* que calcule la mejor recompensa esperada alcanzable a partir de una observación. Para nuestro ejemplo, tenemos que tomar el máximo de los equivalentes sin ruido de las funciones de recompensa que ya hemos definido para el entorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX7MiFhNu3_L"
      },
      "outputs": [],
      "source": [
        "def compute_optimal_reward(observation):\n",
        "  expected_reward_for_arms = [\n",
        "      tf.linalg.matvec(observation, tf.cast(arm0_param, dtype=tf.float32)),\n",
        "      tf.linalg.matvec(observation, tf.cast(arm1_param, dtype=tf.float32)),\n",
        "      tf.linalg.matvec(observation, tf.cast(arm2_param, dtype=tf.float32))]\n",
        "  optimal_action_reward = tf.reduce_max(expected_reward_for_arms, axis=0)\n",
        "  return optimal_action_reward\n",
        "\n",
        "regret_metric = tf_metrics.RegretMetric(compute_optimal_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRWz-Qeb13JC"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khdKjTs516Pg"
      },
      "source": [
        "En este paso, reunimos todos los componentes que presentamos anteriormente: el entorno, la política y el agente. Ejecutamos la política sobre el entorno y generamos datos de entrenamiento con la ayuda de un *controlador*, y capacitamos al agente con los datos.\n",
        "\n",
        "Tenga en cuenta que hay dos parámetros que se unen para especificar el número de pasos dados. `num_iterations` especifica cuántas veces ejecutamos el bucle entrenador, mientras que el controlador dará `steps_per_loop` pasos por iteración. La principal razón para mantener ambos parámetros en algunas de estas operaciones es que algunas operaciones se ejecutan por iteración, mientras que a otras las ejecuta en cada paso el controlador. Por ejemplo, la función `train` del agente se llama una sola vez por iteración. La diferencia radica en que, si entrenamos más a menudo, nuestra política será más \"fresca\"; por otro lado, entrenar en lotes más grandes puede resultar más eficiente en función del tiempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ggn45g62DWx"
      },
      "outputs": [],
      "source": [
        "num_iterations = 90 # @param\n",
        "steps_per_loop = 1 # @param\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.policy.trajectory_spec,\n",
        "    batch_size=batch_size,\n",
        "    max_length=steps_per_loop)\n",
        "\n",
        "observers = [replay_buffer.add_batch, regret_metric]\n",
        "\n",
        "driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    env=environment,\n",
        "    policy=agent.collect_policy,\n",
        "    num_steps=steps_per_loop * batch_size,\n",
        "    observers=observers)\n",
        "\n",
        "regret_values = []\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  driver.run()\n",
        "  loss_info = agent.train(replay_buffer.gather_all())\n",
        "  replay_buffer.clear()\n",
        "  regret_values.append(regret_metric.result())\n",
        "\n",
        "plt.plot(regret_values)\n",
        "plt.ylabel('Average Regret')\n",
        "plt.xlabel('Number of Iterations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2diHS5IzLuo"
      },
      "source": [
        "Tras ejecutar el último fragmento de código, el gráfico resultante (con suerte) mostrará que el arrepentimiento medio disminuye a medida que el agente se entrena y la política mejora su capacidad de predecir cuál es la acción correcta, a partir de la observación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qLMnOL00-2V"
      },
      "source": [
        "# Siguientes pasos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOiRWZbf1Drs"
      },
      "source": [
        "Para ver más ejemplos prácticos, consulte [bandits/agents/examples](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2) que tiene ejemplos listos para ejecutar para diferentes agentes y entornos.\n",
        "\n",
        "La biblioteca TF-Agents también es capaz de gestionar bandidos multibrazo con características por brazo. Para ello, sugerimos que se consulte el [tutorial](https://github.com/tensorflow/agents/tree/master/docs/tutorials/per_arm_bandits_tutorial.ipynb) de bandido por brazo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bandits_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
