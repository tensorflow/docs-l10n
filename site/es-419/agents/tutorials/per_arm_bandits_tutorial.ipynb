{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPjtEgqN4SjA"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6AZJOyCA4NpL"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdBl31Dqwomt"
      },
      "source": [
        "# Tutorial sobre bandidos multibrazo con características por brazo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vvG61d35bG"
      },
      "source": [
        "### Introducción\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/per_arm_bandits_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/per_arm_bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/agents/tutorials/per_arm_bandits_tutorial.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">     Ver código fuente en GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/agents/tutorials/per_arm_bandits_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddRJNIiEwu9O"
      },
      "source": [
        "Este tutorial es una guía paso a paso del uso de la biblioteca TF-Agents para problemas de bandidos contextuales donde las acciones (brazos) tienen sus propias características, como una lista de películas representadas por características (género, año de estreno, ...)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6mUk-hZa3pB"
      },
      "source": [
        "### Requisito previo\n",
        "\n",
        "Se asume que el lector tiene cierto grado de familiaridad con la biblioteca Bandit de TF-Agents, en particular, que ha trabajado con el [tutorial para Bandits en TF-Agents](https://github.com/tensorflow/agents/tree/master/docs/tutorials/bandits_tutorial.ipynb) antes de leer este tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kscmAIA5xtJW"
      },
      "source": [
        "## Bandidos multibrazo con características de brazo\n",
        "\n",
        "En el \"clásico\" contexto de bandidos multibrazo, un agente recibe un vector de contexto (también conocido como observación) en cada paso de tiempo y tiene que elegir entre un conjunto finito de acciones numeradas (brazos) para maximizar la recompensa acumulada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDkno4bQ1vPE"
      },
      "source": [
        "Supongamos que un agente recomienda a un usuario la próxima película que debe ver. Cada vez que tiene que tomar una decisión, el agente recibe como contexto cierta información sobre el usuario (historial de películas vistas, género preferido, etc.), así como una lista de películas entre las que se puede elegir.\n",
        "\n",
        "Podríamos tratar de plantear este problema usando la información del usuario como contexto y los brazos serían `movie_1, movie_2, ..., movie_K`, pero este enfoque presenta varias limitaciones:\n",
        "\n",
        "- El número de acciones debería ser todas las películas del sistema y resulta complicado agregar una nueva película.\n",
        "- El agente debe aprender un modelo para cada película.\n",
        "- No se tiene en cuenta la similitud entre películas.\n",
        "\n",
        "En lugar de enumerar las películas, podemos probar un enfoque más intuitivo: podemos representar las películas con un conjunto de características, incluido el género, la duración, el reparto, la calificación, el año, etc. Este enfoque tiene múltiples ventajas:\n",
        "\n",
        "- Generalización entre películas.\n",
        "- El agente aprende solo una función de recompensa que modela la recompensa con características de usuario y de película.\n",
        "- Es fácil eliminar o introducir nuevas películas en el sistema.\n",
        "\n",
        "En esta nueva configuración, ni siquiera es necesario que el número de acciones sea el mismo en cada paso de tiempo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMXxIHXNRP5_"
      },
      "source": [
        "## Bandidos por brazo en TF-Agents\n",
        "\n",
        "El paquete Bandit de TF-Agents se desarrolló para que también se pueda usar en el caso por brazo. Hay entornos por brazo y, además, la mayoría de las políticas y agentes pueden operar en modo por brazo.\n",
        "\n",
        "Antes de meternos de lleno en la codificación de un ejemplo, tenemos que hacer las importaciones necesarias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl5_CCIWSFvn"
      },
      "source": [
        "### Instalación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxiNIm5XSIIp"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDKNu5JTSDmf"
      },
      "source": [
        "### Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbQXsoeKR2ui"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.bandits.agents import lin_ucb_agent\n",
        "from tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as p_a_env\n",
        "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "nest = tf.nest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4cVyq3JMM7Z"
      },
      "source": [
        "### Parámetros (siéntase libre de experimentar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfna8xm0MSCn"
      },
      "outputs": [],
      "source": [
        "# The dimension of the global features.\n",
        "GLOBAL_DIM = 40  #@param {type:\"integer\"}\n",
        "# The elements of the global feature will be integers in [-GLOBAL_BOUND, GLOBAL_BOUND).\n",
        "GLOBAL_BOUND = 10  #@param {type:\"integer\"}\n",
        "# The dimension of the per-arm features.\n",
        "PER_ARM_DIM = 50  #@param {type:\"integer\"}\n",
        "# The elements of the PER-ARM feature will be integers in [-PER_ARM_BOUND, PER_ARM_BOUND).\n",
        "PER_ARM_BOUND = 6  #@param {type:\"integer\"}\n",
        "# The variance of the Gaussian distribution that generates the rewards.\n",
        "VARIANCE = 100.0  #@param {type: \"number\"}\n",
        "# The elements of the linear reward parameter will be integers in [-PARAM_BOUND, PARAM_BOUND).\n",
        "PARAM_BOUND = 10  #@param {type: \"integer\"}\n",
        "\n",
        "NUM_ACTIONS = 70  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 20  #@param {type:\"integer\"}\n",
        "\n",
        "# Parameter for linear reward function acting on the\n",
        "# concatenation of global and per-arm features.\n",
        "reward_param = list(np.random.randint(\n",
        "      -PARAM_BOUND, PARAM_BOUND, [GLOBAL_DIM + PER_ARM_DIM]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-mEz1HvRIBC"
      },
      "source": [
        "### Un entorno por brazo simple\n",
        "\n",
        "El entorno estocástico estacionario, que se explica en el otro [tutorial](https://github.com/tensorflow/agents/tree/master/docs/tutorials/bandits_tutorial.ipynb), tiene una contraparte por brazo.\n",
        "\n",
        "Para inicializar el entorno por brazo, debemos definir funciones que generen lo siguiente:\n",
        "\n",
        "- *características globales y por brazo*: estas funciones no tienen parámetros de entrada y, cuando se las llama, generan un solo vector de características (globales o por brazo).\n",
        "- *recompensas*: esta función toma como parámetro la concatenación de un vector de características globales o por brazo, y luego genera una recompensa. Básicamente, esta es la función que deberá \"adivinar\" el agente. vale la pena aclarar que en el caso por brazo la función de recompensa es idéntica para cada brazo. Esta es la diferencia fundamental con el caso de bandidos clásico, donde el agente debe calcular las funciones de recompensa de forma independiente para cada brazo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfTa5Y4ZYjhO"
      },
      "outputs": [],
      "source": [
        "def global_context_sampling_fn():\n",
        "  \"\"\"This function generates a single global observation vector.\"\"\"\n",
        "  return np.random.randint(\n",
        "      -GLOBAL_BOUND, GLOBAL_BOUND, [GLOBAL_DIM]).astype(np.float32)\n",
        "\n",
        "def per_arm_context_sampling_fn():\n",
        "  \"\"\"\"This function generates a single per-arm observation vector.\"\"\"\n",
        "  return np.random.randint(\n",
        "      -PER_ARM_BOUND, PER_ARM_BOUND, [PER_ARM_DIM]).astype(np.float32)\n",
        "\n",
        "def linear_normal_reward_fn(x):\n",
        "  \"\"\"This function generates a reward from the concatenated global and per-arm observations.\"\"\"\n",
        "  mu = np.dot(x, reward_param)\n",
        "  return np.random.normal(mu, VARIANCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2vpRPhheTo5"
      },
      "source": [
        "Ahora ya tenemos todo lo necesario para inicializar nuestro entorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-yikfQQi9l"
      },
      "outputs": [],
      "source": [
        "per_arm_py_env = p_a_env.StationaryStochasticPerArmPyEnvironment(\n",
        "    global_context_sampling_fn,\n",
        "    per_arm_context_sampling_fn,\n",
        "    NUM_ACTIONS,\n",
        "    linear_normal_reward_fn,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "per_arm_tf_env = tf_py_environment.TFPyEnvironment(per_arm_py_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIzFl8HiAIxg"
      },
      "source": [
        "A continuación, podemos comprobar lo que produce este entorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8ZVqMU5AOzs"
      },
      "outputs": [],
      "source": [
        "print('observation spec: ', per_arm_tf_env.observation_spec())\n",
        "print('\\nAn observation: ', per_arm_tf_env.reset().observation)\n",
        "\n",
        "action = tf.zeros(BATCH_SIZE, dtype=tf.int32)\n",
        "time_step = per_arm_tf_env.step(action)\n",
        "print('\\nRewards after taking an action: ', time_step.reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIlCGssRAzIr"
      },
      "source": [
        "Vemos que la especificación de observación es un diccionario con dos elementos:\n",
        "\n",
        "- Uno con la clave `'global'`: esta es la parte del contexto global, con una forma que coincide con el parámetro `GLOBAL_DIM`.\n",
        "- Uno con la clave `'per_arm'`: este es el contexto por brazo y tiene la forma `[NUM_ACTIONS, PER_ARM_DIM]`. Esta parte es el marcador de posición para las características del brazo para cada brazo en un paso de tiempo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTpWbNVeS6ci"
      },
      "source": [
        "### El agente LinUCB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q83tgZR9LLUx"
      },
      "source": [
        "El agente LinUCB implementa el algoritmo Bandit, cuyo nombre es idéntico, que calcula el parámetro de la función de recompensa lineal al mismo tiempo que mantiene un elipsoide de confianza en torno al cálculo. El agente elige el brazo con la mayor recompensa esperada, suponiendo que el parámetro se encuentra dentro del elipsoide de confianza.\n",
        "\n",
        "Para crear un agente se precisa el conocimiento de la observación y la especificación de la acción. A la hora de definir el agente, establecemos el parámetro booleano `accepts_per_arm_features` en `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqzA9Zi0Q2No"
      },
      "outputs": [],
      "source": [
        "observation_spec = per_arm_tf_env.observation_spec()\n",
        "time_step_spec = ts.time_step_spec(observation_spec)\n",
        "action_spec = tensor_spec.BoundedTensorSpec(\n",
        "    dtype=tf.int32, shape=(), minimum=0, maximum=NUM_ACTIONS - 1)\n",
        "\n",
        "agent = lin_ucb_agent.LinearUCBAgent(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec,\n",
        "                                     accepts_per_arm_features=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaQlv1lpC-oc"
      },
      "source": [
        "### El flujo de los datos de entrenamiento\n",
        "\n",
        "Esta sección ofrece un vistazo a la mecánica de cómo las características por brazo pasan de la política al entrenamiento. No dude en pasar a la siguiente sección (Definición de la métrica de arrepentimiento) y volver aquí después si está interesado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUSy2IFK5NpU"
      },
      "source": [
        "Primero, veamos la especificación de datos en el agente. El atributo `training_data_spec` del agente especifica qué elementos y estructura deben tener los datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQRZd43o5M0j"
      },
      "outputs": [],
      "source": [
        "print('training data spec: ', agent.training_data_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyEFAHUg-m9V"
      },
      "source": [
        "Si miramos más de cerca la porción `observation` de la especificación, vemos que no contiene características por brazo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTBR1vgG-2AM"
      },
      "outputs": [],
      "source": [
        "print('observation spec in training: ', agent.training_data_spec.observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDtmUgeJA_DN"
      },
      "source": [
        "¿Qué pasó con las características por brazo? Para responder a esta pregunta, primero debemos tener en cuenta que, cuando se entrena al agente LinUCB, no necesita características por brazo de **todos** los brazos, solo necesita las del brazo **elegido**. Por lo tanto, tiene sentido eliminar el tensor de forma `[BATCH_SIZE, NUM_ACTIONS, PER_ARM_DIM]`, ya que es implica gran desperdicio, especialmente si el número de acciones es grande.\n",
        "\n",
        "Pero, de todos modos, ¡las características por brazo del brazo elegido deben estar en alguna parte! Es por eso que nos aseguramos de que la política de LinUCB almacene las características del brazo elegido dentro del campo `policy_info` de los datos de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0fHvLm0Cpq9"
      },
      "outputs": [],
      "source": [
        "print('chosen arm features: ', agent.training_data_spec.policy_info.chosen_arm_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4psGJKGIDVgN"
      },
      "source": [
        "A partir de la forma podemos apreciar que el campo `chosen_arm_features` solo tiene el vector de características de un brazo, y ese será el brazo elegido. Tenga en cuenta que la `policy_info`, y con ella las `chosen_arm_features`, es parte de los datos de entrenamiento, tal y como vimos al inspeccionar la especificación de datos de entrenamiento, y por lo tanto está disponible en el momento del entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ancYxxEHc-6Q"
      },
      "source": [
        "### Definición de la métrica de arrepentimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j2RTrBfzVJQ"
      },
      "source": [
        "Antes de iniciar el bucle de entrenamiento, definimos algunas funciones de utilidad que ayudan a calcular el arrepentimiento de nuestro agente. Estas funciones nos permiten determinar la recompensa óptima esperada teniendo en cuenta el conjunto de acciones (en función de las características de sus brazos) y el parámetro lineal oculto para el agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1_kJAOS6VPo"
      },
      "outputs": [],
      "source": [
        "def _all_rewards(observation, hidden_param):\n",
        "  \"\"\"Outputs rewards for all actions, given an observation.\"\"\"\n",
        "  hidden_param = tf.cast(hidden_param, dtype=tf.float32)\n",
        "  global_obs = observation['global']\n",
        "  per_arm_obs = observation['per_arm']\n",
        "  num_actions = tf.shape(per_arm_obs)[1]\n",
        "  tiled_global = tf.tile(\n",
        "      tf.expand_dims(global_obs, axis=1), [1, num_actions, 1])\n",
        "  concatenated = tf.concat([tiled_global, per_arm_obs], axis=-1)\n",
        "  rewards = tf.linalg.matvec(concatenated, hidden_param)\n",
        "  return rewards\n",
        "\n",
        "def optimal_reward(observation):\n",
        "  \"\"\"Outputs the maximum expected reward for every element in the batch.\"\"\"\n",
        "  return tf.reduce_max(_all_rewards(observation, reward_param), axis=1)\n",
        "\n",
        "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i06WDbjrUSog"
      },
      "source": [
        "Ya estamos listos para iniciar nuestro bucle de entrenamiento con bandidos. El controlador que se muestra a continuación se ocupa de elegir las acciones mediante el uso de la política, almacenar las acciones elegidas en el búfer de repetición, calcular la métrica de arrepentimiento predefinida y ejecutar el paso de entrenamiento del agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2Iimtmkzs0-"
      },
      "outputs": [],
      "source": [
        "num_iterations = 20 # @param\n",
        "steps_per_loop = 1 # @param\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.policy.trajectory_spec,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_length=steps_per_loop)\n",
        "\n",
        "observers = [replay_buffer.add_batch, regret_metric]\n",
        "\n",
        "driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    env=per_arm_tf_env,\n",
        "    policy=agent.collect_policy,\n",
        "    num_steps=steps_per_loop * BATCH_SIZE,\n",
        "    observers=observers)\n",
        "\n",
        "regret_values = []\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  driver.run()\n",
        "  loss_info = agent.train(replay_buffer.gather_all())\n",
        "  replay_buffer.clear()\n",
        "  regret_values.append(regret_metric.result())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG5VMgSlUqYS"
      },
      "source": [
        "Ahora, veamos el resultado. Si hicimos todo bien, el agente puede calcular bien la función de recompensa lineal y, por lo tanto, la política puede elegir acciones cuya recompensa esperada sea cercana a la óptima. Esto se indica mediante la métrica de arrepentimiento definida anteriormente, que desciende y se acerca a cero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4DOYwhMSUVh"
      },
      "outputs": [],
      "source": [
        "plt.plot(regret_values)\n",
        "plt.title('Regret of LinUCB on the Linear per-arm environment')\n",
        "plt.xlabel('Number of Iterations')\n",
        "_ = plt.ylabel('Average Regret')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ZgHgOx5Ojq"
      },
      "source": [
        "### Siguientes pasos\n",
        "\n",
        "El ejemplo anterior se [implementa](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/agents/examples/v2/train_eval_per_arm_stationary_linear.py) en nuestro código base, donde también puede elegir entre otros agentes, incluido el [agente neuronal épsilon-greedy](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/agents/neural_epsilon_greedy_agent.py)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "t7ZgHgOx5Ojq"
      ],
      "name": "per_arm_bandits_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
