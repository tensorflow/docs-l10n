{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eIrvnAbGZ1wP"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "_A4IPZ-WZ9H7"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WpaDQG8VaYFO"
      },
      "source": [
        "# Masking and padding with Keras\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/masking_and_padding\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras/masking_and_padding.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/keras/masking_and_padding.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/keras/masking_and_padding.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />\n",
        "    Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGJH5EKYoSHZ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wJEBe8hTlB6W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5ShanCQ_pSPO"
      },
      "source": [
        "## Padding sequence data\n",
        "\n",
        "When processing sequence data, it is very common for individual samples to have different lengths. Consider the following example (text tokenized as words):\n",
        "\n",
        "```\n",
        "[\n",
        "  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
        "  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
        "  [\"Hello\", \"world\", \"!\"]\n",
        "]\n",
        "```\n",
        "\n",
        "After vocabulary lookup, the data might be vectorized as integers, e.g.:\n",
        "\n",
        "```\n",
        "[\n",
        "  [83, 91, 1, 645, 1253, 927],\n",
        "  [73, 8, 3215, 55, 927],\n",
        "  [71, 1331, 4231]\n",
        "]\n",
        "```\n",
        "\n",
        "The data is a 2D list where individual samples have length 6, 5, and 3 respectively. Since the input data for a deep learning model must be a single tensor (of shape e.g. `(batch_size, 6, vocab_size)` in this case), samples that are shorter than the longest item need to be padded with some placeholder value (alternatively, one might also truncate long samples before padding short samples).\n",
        "\n",
        "Keras provides an API to easily truncate and pad sequences to a common length: `tf.keras.preprocessing.sequence.pad_sequences`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xI-lHnyxfa2T"
      },
      "outputs": [],
      "source": [
        "raw_inputs = [\n",
        "  [83, 91, 1, 645, 1253, 927],\n",
        "  [73, 8, 3215, 55, 927],\n",
        "  [711, 632, 71]\n",
        "]\n",
        "\n",
        "# By default, this will pad using 0s; it is configurable via the\n",
        "# \"value\" parameter.\n",
        "# Note that you could \"pre\" padding (at the beginning) or\n",
        "# \"post\" padding (at the end).\n",
        "# We recommend using \"post\" padding when working with RNN layers\n",
        "# (in order to be able to use the \n",
        "# CuDNN implementation of the layers).\n",
        "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
        "                                                              padding='post')\n",
        "\n",
        "print(padded_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HyHf90yAqkMr"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o16pUIBLgc_Q"
      },
      "source": [
        "Now that all samples have a uniform length, the model must be informed that some part of the data is actually padding and should be ignored. That mechanism is <b>masking</b>.\n",
        "\n",
        "There are three ways to introduce input masks in Keras models:\n",
        "\n",
        "- Add a `keras.layers.Masking` layer.\n",
        "- Configure a `keras.layers.Embedding` layer with `mask_zero=True`.\n",
        "- Pass a `mask` argument manually when calling layers that support this argument (e.g. RNN layers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f6QMNceyf1GD"
      },
      "source": [
        "## Mask-generating layers: `Embedding` and `Masking`\n",
        "\n",
        "Under the hood, these layers will create a mask tensor (2D tensor with shape `(batch, sequence_length)`), and attach it to the tensor output returned by the `Masking` or `Embedding` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rYXQ589PkC0P"
      },
      "outputs": [],
      "source": [
        "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
        "masked_output = embedding(padded_inputs)\n",
        "\n",
        "print(masked_output._keras_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-0VVscXQm1D1"
      },
      "outputs": [],
      "source": [
        "masking_layer = layers.Masking()\n",
        "# Simulate the embedding lookup by expanding the 2D input to 3D,\n",
        "# with embedding dimension of 10.\n",
        "unmasked_embedding = tf.cast(\n",
        "    tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]),\n",
        "    tf.float32)\n",
        "\n",
        "masked_embedding = masking_layer(unmasked_embedding)\n",
        "print(masked_embedding._keras_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RL2vsiCBmVck"
      },
      "source": [
        "As you can see from the printed result, the mask is a 2D boolean tensor with shape `(batch_size, sequence_length)`, where each individual `False` entry indicates that the corresponding timestep should be ignored during processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s4jsu6oTrl2f"
      },
      "source": [
        "## Mask propagation in the Functional API and Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0KgNt7fvm0Jx"
      },
      "source": [
        "When using the Functional API or the Sequential API, a mask generated by an `Embedding` or `Masking` layer will be propagated through the network for any layer that is capable of using them (for example, RNN layers). Keras will automatically fetch the mask corresponding to an input and pass it to any layer that knows how to use it.\n",
        "\n",
        "Note that in the `call` method of a subclassed model or layer, masks aren't automatically propagated, so you will need to manually pass a `mask` argument to any layer that needs one. See the section below for details.\n",
        "\n",
        "For instance, in the following Sequential model, the `LSTM` layer will automatically receive a mask, which means it will ignore padded values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zfkxyf7yVyxJ"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True),\n",
        "  layers.LSTM(32),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UqZeTeEhWHLE"
      },
      "source": [
        "This is also the case for the following Functional API model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SYaVl6WSWJal"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(None,), dtype='int32')\n",
        "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
        "outputs = layers.LSTM(32)(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "crxoxoRDWg8t"
      },
      "source": [
        "## Passing mask tensors directly to layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UfvcEl20XRYA"
      },
      "source": [
        "Layers that can handle masks (such as the `LSTM` layer) have a `mask` argument in their `__call__` method.\n",
        "\n",
        "Meanwhile, layers that produce a mask (e.g. `Embedding`) expose a `compute_mask(input, previous_mask)` method which you can call.\n",
        "\n",
        "Thus, you can do something like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "coCV26fqXmya"
      },
      "outputs": [],
      "source": [
        "class MyLayer(layers.Layer):\n",
        "  \n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyLayer, self).__init__(**kwargs)\n",
        "    self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
        "    self.lstm = layers.LSTM(32)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    # Note that you could also prepare a `mask` tensor manually.\n",
        "    # It only needs to be a boolean tensor\n",
        "    # with the right shape, i.e. (batch_size, timesteps).\n",
        "    mask = self.embedding.compute_mask(inputs)\n",
        "    output = self.lstm(x, mask=mask)  # The layer will ignore the masked values\n",
        "    return output\n",
        "\n",
        "layer = MyLayer()\n",
        "x = np.random.random((32, 10)) * 100\n",
        "x = x.astype('int32')\n",
        "layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uSZP15mtWs9Z"
      },
      "source": [
        "## Supporting masking in your custom layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w2gg7O3kVjC4"
      },
      "source": [
        "Sometimes you may need to write layers that generate a mask (like `Embedding`), or layers that need to modify the current mask.\n",
        "\n",
        "For instance, any layer that produces a tensor with a different time dimension than its input, such as a `Concatenate` layer that concatenates on the time dimension, will need to modify the current mask so that downstream layers will be able to properly take masked timesteps into account.\n",
        "\n",
        "To do this, your layer should implement the `layer.compute_mask()` method, which produces a new mask given the input and the current mask. \n",
        "\n",
        "Most layers don't modify the time dimension, so don't need to worry about masking. The default behavior of `compute_mask()` is just pass the current mask through in such cases.\n",
        "\n",
        "Here is an example of a `TemporalSplit` layer that needs to modify the current mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gaS_7dXyr-Z0"
      },
      "outputs": [],
      "source": [
        "class TemporalSplit(tf.keras.layers.Layer):\n",
        "  \"\"\"Split the input tensor into 2 tensors along the time dimension.\"\"\"\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Expect the input to be 3D and mask to be 2D, split the input tensor into 2\n",
        "    # subtensors along the time axis (axis 1).\n",
        "    return tf.split(inputs, 2, axis=1)\n",
        "    \n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    # Also split the mask into 2 if it presents.\n",
        "    if mask is None:\n",
        "      return None\n",
        "    return tf.split(mask, 2, axis=1)\n",
        "\n",
        "first_half, second_half = TemporalSplit()(masked_embedding)\n",
        "print(first_half._keras_mask)\n",
        "print(second_half._keras_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5t73OUJgjLLW"
      },
      "source": [
        "Here is another example of a `CustomEmbedding` layer that is capable of generating a mask from input values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fLSpf1iojSO7"
      },
      "outputs": [],
      "source": [
        "class CustomEmbedding(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n",
        "    super(CustomEmbedding, self).__init__(**kwargs)\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.mask_zero = mask_zero\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    self.embeddings = self.add_weight(\n",
        "      shape=(self.input_dim, self.output_dim),\n",
        "      initializer='random_normal',\n",
        "      dtype='float32')\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
        "  \n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    if not self.mask_zero:\n",
        "      return None\n",
        "    return tf.not_equal(inputs, 0)\n",
        "  \n",
        "  \n",
        "layer = CustomEmbedding(10, 32, mask_zero=True)\n",
        "x = np.random.random((3, 10)) * 9\n",
        "x = x.astype('int32')\n",
        "\n",
        "y = layer(x)\n",
        "mask = layer.compute_mask(x)\n",
        "\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fUopC-DelkG2"
      },
      "source": [
        "## Writing layers that need mask information\n",
        "\n",
        "Some layers are mask *consumers*: they accept a `mask` argument in `call` and use it to determine whether to skip certain time steps.\n",
        "\n",
        "To write such a layer, you can simply add a `mask=None` argument in your `call` signature. The mask associated with the inputs will be passed to your layer whenever it is available.\n",
        "\n",
        "```python\n",
        "class MaskConsumer(tf.keras.layers.Layer):\n",
        "  \n",
        "  def call(self, inputs, mask=None):\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_50qkZjIn8b2"
      },
      "source": [
        "## Recap\n",
        "\n",
        "That is all you need to know about masking in Keras. To recap:\n",
        "\n",
        "- \"Masking\" is how layers are able to know when to skip / ignore certain timesteps in sequence inputs.\n",
        "- Some layers are mask-generators: `Embedding` can generate a mask from input values (if `mask_zero=True`), and so can the `Masking` layer.\n",
        "- Some layers are mask-consumers: they expose a `mask` argument in their `__call__` method. This is the case for RNN layers.\n",
        "- In the Functional API and Sequential API, mask information is propagated automatically.\n",
        "- When writing subclassed models or when using layers in a standalone way, pass the `mask` arguments to layers manually.\n",
        "- You can easily write layers that modify the current mask, that generate a new mask, or that consume the mask associated with the inputs.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "masking_and_padding.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
